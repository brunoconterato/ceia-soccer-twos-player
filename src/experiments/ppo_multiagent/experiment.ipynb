{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiente da competição\n",
    "# !pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "# !pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "# !pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "# !pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install torch > /dev/null 2>&1\n",
    "# !pip install lz4 > /dev/null 2>&1\n",
    "# !pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# # Dependências necessárias para gravar os vídeos\n",
    "# !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "# !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "# !pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/soccer-twos/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Analysis\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.utils.typing import PolicyID\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "MAX_STEPS = 1000\n",
    "MATCH_STEPS = 4000\n",
    "\n",
    "def get_scalar_projection(x, y):\n",
    "    return np.dot(x, y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
    "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
    "min_ball_position_x, max_ball_position_x = - \\\n",
    "    15.563264846801758, 15.682827949523926\n",
    "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
    "min_player_position_x, max_player_position_x = - \\\n",
    "    17.26804542541504, 17.16301727294922\n",
    "min_player_position_y, max_player_position_y = - \\\n",
    "    7.399587631225586, 7.406457424163818\n",
    "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
    "    -23.366606239568615, 23.749571761530724\n",
    "\n",
    "max_ball_abs_velocity = 78.25721740722656\n",
    "max_goals_one_team = -9999999\n",
    "max_goals_one_match = -9999999\n",
    "max_steps = -999999\n",
    "\n",
    "max_diff_reward = -np.inf\n",
    "\n",
    "# Infered\n",
    "max_ball_abs_avg_velocity = max(\n",
    "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
    "\n",
    "\n",
    "SPEED_IMPORTANCE = 1.0 / (14.0)\n",
    "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
    "\n",
    "AFTER_BALL_STEP_PENALTY = 1 / MAX_STEPS #0.001\n",
    "\n",
    "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
    "# min_ball_to_goal_avg_velocity e\n",
    "# max_ball_to_goal_avg_velocity:\n",
    "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
    "\n",
    "\n",
    "def is_after_the_ball(player_id: int, player_pos: np.array, ball_pos: np.array):\n",
    "    if player_id in range(2):\n",
    "        return player_pos[0] > ball_pos[0]\n",
    "    elif player_id in [2, 3]:\n",
    "        return player_pos[0] < ball_pos[0]\n",
    "\n",
    "\n",
    "def get_center_of_goal_pos(player_id):\n",
    "    global min_ball_position_x, max_ball_position_x, \\\n",
    "        min_ball_position_y, max_ball_position_y, \\\n",
    "        min_player_position_x, max_player_position_x, \\\n",
    "        min_player_position_y, max_player_position_y\n",
    "    if player_id in [0, 1]:\n",
    "        return np.array([max_ball_position_x, 0.0])\n",
    "    elif player_id in [2, 3]:\n",
    "        return np.array([min_ball_position_x, 0.0])\n",
    "\n",
    "\n",
    "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict):\n",
    "    goal_pos = get_center_of_goal_pos(player_id)\n",
    "    # print(f\"goal_pos: {goal_pos}\")\n",
    "    ball_pos = info[\"ball_info\"][\"position\"]\n",
    "    # print(f\"ball_pos: {ball_pos}\")\n",
    "    direction_to_center_of_goal = goal_pos - ball_pos\n",
    "    # print(f\"direction_to_center_of_goal: {direction_to_center_of_goal}\")\n",
    "\n",
    "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
    "\n",
    "    # global max_ball_abs_velocity\n",
    "    # if np.linalg.norm(ball_velocity) > max_ball_abs_velocity:\n",
    "    #     max_ball_abs_velocity = np.linalg.norm(ball_velocity)\n",
    "\n",
    "    # print(f\"ball_velocity: {ball_velocity}\")\n",
    "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
    "        ball_velocity, direction_to_center_of_goal)\n",
    "    # print(f\"ball_velocity_to_center_of_goal: {ball_velocity_to_center_of_goal}\")\n",
    "    return ball_velocity_to_center_of_goal\n",
    "\n",
    "# print('ball_velocity_to_center_of_goal', calculate_ball_to_goal_scalar_velocity(0, { \"ball_info\": { \"position\": np.array([3.0, 2.0]), \"velocity\": np.array([0.0, 0.0]) }}))\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    # def __init__(self, env):\n",
    "    #     gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "\n",
    "        # print(info)\n",
    "        # if rewards[0] > 0.0:\n",
    "        #     assert False\n",
    "\n",
    "        if type(action) is dict:\n",
    "            new_rewards = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k]) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        if type(action) is dict:\n",
    "            splitted_rets = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k], splitted_returns=True) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "\n",
    "        info = {\n",
    "            i: {\n",
    "                **info[i],\n",
    "                \"ep_metrics\": {\n",
    "                    # \"total_timesteps\": np.array([0.0008], dtype=np.float32)\n",
    "                    \"total_timesteps\": self.n_step + 1,\n",
    "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
    "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
    "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
    "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
    "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
    "                    \"episode_ended\": done[\"__all__\"],\n",
    "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
    "                    \"env_reward\": splitted_rets[i][0],\n",
    "                    \"ball_to_goal_speed_reward\": splitted_rets[i][1],\n",
    "                    \"agent_position_to_ball_reward\": splitted_rets[i][2],\n",
    "                }\n",
    "            } for i in info.keys()\n",
    "        }\n",
    "\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y, \\\n",
    "        #     max_goals_one_team, max_goals_one_match\n",
    "        # if done:\n",
    "        #     print(f'min_ball_position_x: {min_ball_position_x}')\n",
    "        #     print(f'max_ball_position_x: {max_ball_position_x}')\n",
    "        #     print(f'min_ball_position_y: {min_ball_position_y}')\n",
    "        #     print(f'max_ball_position_y: {max_ball_position_y}')\n",
    "        #     print(f'min_player_position_x: {min_player_position_x}')\n",
    "        #     print(f'max_player_position_x: {max_player_position_x}')\n",
    "        #     print(f'min_player_position_y: {min_player_position_y}')\n",
    "        #     print(f'max_player_position_y: {max_player_position_y}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_goals_one_team: {max_goals_one_team}')\n",
    "        #     print(f'max_goals_one_match: {max_goals_one_match}')\n",
    "        #     print(self.scoreboard)\n",
    "        #     print(f'Done... last n_step: {self.n_step}')\n",
    "        #     if self.scoreboard[\"team_0\"] > 0 or self.scoreboard[\"team_1\"] > 0:\n",
    "        #         input(\"Press Enter to continue...\")\n",
    "\n",
    "        # global max_steps\n",
    "        # if done:\n",
    "        #     if self.n_step + 1 > max_steps:\n",
    "        #         max_steps = self.n_step + 1\n",
    "        #     print('max_steps', max_steps)\n",
    "\n",
    "        # global max_diff_reward\n",
    "        # if done:\n",
    "        #     print(f'max_diff_reward: {max_diff_reward}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'max_ball_abs_velocity: {max_ball_abs_velocity}')\n",
    "\n",
    "        self.n_step += 1\n",
    "        return obs, new_rewards, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.n_step = 0\n",
    "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
    "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
    "        self.await_press = False\n",
    "        # print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        # print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        return obs\n",
    "\n",
    "    def _calculate_reward(self, reward: float, player_id: int, info: Dict, splitted_returns=False) -> float:\n",
    "        # print('calculating reward')\n",
    "        if reward != 0.0:\n",
    "            # print('Goal was made!', reward, info)\n",
    "            self._update_scoreboard(player_id, reward)\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y\n",
    "        # print(f\"info: {info}\")\n",
    "        # if info[\"ball_info\"][\"position\"][0] < min_ball_position_x:\n",
    "        #     min_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][0] > max_ball_position_x:\n",
    "        #     max_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][1] < min_ball_position_y:\n",
    "        #     min_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"ball_info\"][\"position\"][1] > max_ball_position_y:\n",
    "        #     max_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][0] < min_player_position_x:\n",
    "        #     min_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][0] > max_player_position_x:\n",
    "        #     max_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][1] < min_player_position_y:\n",
    "        #     min_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][1] > max_player_position_y:\n",
    "        #     max_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "\n",
    "        self._update_avg_ball_speed_to_goal(\n",
    "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
    "        # global max_diff_reward\n",
    "        # if (np.abs(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity) > max_diff_reward):\n",
    "        #     max_diff_reward = SPEED_IMPORTANCE * \\\n",
    "        #         self.last_ball_speed_mean_per_player[player_id] / \\\n",
    "        #         max_ball_abs_avg_velocity\n",
    "\n",
    "        ball_pos = info[\"ball_info\"][\"position\"]\n",
    "        player_pos = info[\"player_info\"][\"position\"]\n",
    "\n",
    "        env_reward = reward\n",
    "        ball_to_goal_speed_reward = np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE,\n",
    "                               SPEED_IMPORTANCE) if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE else SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
    "        agent_position_to_ball_reward = is_after_the_ball(player_id, player_pos,\n",
    "                                  ball_pos) * (-AFTER_BALL_STEP_PENALTY)\n",
    "\n",
    "        if splitted_returns:\n",
    "            return (env_reward, ball_to_goal_speed_reward, agent_position_to_ball_reward)\n",
    "        return env_reward + ball_to_goal_speed_reward + agent_position_to_ball_reward\n",
    "        if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE:\n",
    "            # print(reward + np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE))\n",
    "            return reward + \\\n",
    "                np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE) + \\\n",
    "                is_after_the_ball(player_id, player_pos,\n",
    "                                  ball_pos) * AFTER_BALL_STEP_PENALTY\n",
    "        return reward + \\\n",
    "            SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity + \\\n",
    "            is_after_the_ball(player_id, player_pos,\n",
    "                              ball_pos) * AFTER_BALL_STEP_PENALTY\n",
    "\n",
    "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
    "        assert player_id in [0, 1, 2, 3]\n",
    "        global min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity\n",
    "\n",
    "        # Getting min/max ball to goal speed forr normalization\n",
    "        # print(f'player_id: {player_id}')\n",
    "        # print(f'self.last_ball_speed_mean_per_player: {self.last_ball_speed_mean_per_player}')\n",
    "        # print(f'self.n_step: {self.n_step}')\n",
    "        # print(f'ball_speed: {ball_speed}')\n",
    "\n",
    "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
    "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
    "        # if avg < min_ball_to_goal_avg_velocity:\n",
    "        #     min_ball_to_goal_avg_velocity = avg\n",
    "        # elif avg > max_ball_to_goal_avg_velocity:\n",
    "        #     max_ball_to_goal_avg_velocity = avg\n",
    "\n",
    "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
    "\n",
    "    def _update_scoreboard(self, player_id, reward):\n",
    "        global max_goals_one_team, max_goals_one_match\n",
    "\n",
    "        if player_id == 0 and reward == -1.0:\n",
    "            self.scoreboard[\"team_1\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_1\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_1\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n",
    "        elif player_id == 2 and reward == -1.0:\n",
    "            self.scoreboard[\"team_0\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_0\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_0\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)\n",
    "\n",
    "\n",
    "def create_custom_env(env_config: dict = {}):\n",
    "    env = create_rllib_env(env_config)\n",
    "    return CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(DefaultCallbacks):\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker: \"RolloutWorker\",\n",
    "                        base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index: Optional[int] = None,\n",
    "                        **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker: \"RolloutWorker\",\n",
    "                       base_env: BaseEnv,\n",
    "                       policies: Dict[PolicyID, Policy],\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index: Optional[int] = None,\n",
    "                       **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    # \"episodes_total\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "WARNING:ray.tune.sample:DeprecationWarning: wrapping <function <lambda> at 0x7ff3b919ed30> with tune.function() is no longer needed\n"
     ]
    }
   ],
   "source": [
    "# NUM_ENVS_PER_WORKER = 1\n",
    "NUM_ENVS_PER_WORKER = 4\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "}\n",
    "\n",
    "\n",
    "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": 1,\n",
    "    # \"num_workers\": 3,\n",
    "    \"num_workers\": 0,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 8,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_gpus_per_worker\": 1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"default\": (None, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": tune.function(lambda _: \"default\"),\n",
    "        \"policies_to_train\": [\"default\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": ENVIRONMENT_CONFIG,\n",
    "    \"callbacks\": Callback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 14:03:58,230\tWARNING trial_runner.py:446 -- Attempting to resume experiment from /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards. This will ignore any new changes to the specification.\n",
      "2021-12-03 14:03:59,695\tINFO tune.py:467 -- TrialRunner resumed, ignoring new add_experiment.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  2480</td><td style=\"text-align: right;\">          197853</td><td style=\"text-align: right;\">9920000</td><td style=\"text-align: right;\">-0.494477</td><td style=\"text-align: right;\">            0.119272</td><td style=\"text-align: right;\">            -2.20994</td><td style=\"text-align: right;\">            116.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:15,209\tINFO torch_policy.py:148 -- TorchPolicy (worker=local) running on 1 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:17,597\tINFO rollout_worker.py:1199 -- Built policy map: {'default': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7fa8407c35e0>}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:17,597\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'default': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fa8407c3c40>}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:17,597\tINFO rollout_worker.py:583 -- Built filter map: {'default': <ray.rllib.utils.filter.NoFilter object at 0x7fa8407c3430>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">  2479</td><td style=\"text-align: right;\">          197770</td><td style=\"text-align: right;\">9916000</td><td style=\"text-align: right;\">-0.517873</td><td style=\"text-align: right;\">            0.119272</td><td style=\"text-align: right;\">            -1.65443</td><td style=\"text-align: right;\">            119.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,093\tINFO trainable.py:377 -- Restored on 192.168.0.108 from checkpoint: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards/PPO_Soccer_491c2_00000_0_2021-11-30_01-16-26/tmpqan_d6iirestore_from_object/checkpoint-2479\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,094\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 2479, '_timesteps_total': None, '_time_total': 197770.27837729454, '_episodes_total': 65688}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,096\tINFO rollout_worker.py:723 -- Generating sample batch of size 800\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,117\tINFO sampler.py:590 -- Raw obs from env: { 0: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   1: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   2: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   3: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)}}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,117\tINFO sampler.py:592 -- Info return from env: { 0: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   1: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   2: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   3: {0: {}, 1: {}, 2: {}, 3: {}}}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,118\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,118\tINFO sampler.py:817 -- Filtered obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,126\tINFO sampler.py:1004 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m { 'default': [ { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:20,131\tINFO sampler.py:1022 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m { 'default': ( np.ndarray((16, 3), dtype=int64, min=0.0, max=2.0, mean=0.729),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                [],\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                { 'action_dist_inputs': np.ndarray((16, 9), dtype=float32, min=-12.079, max=10.451, mean=-0.366),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'action_logp': np.ndarray((16,), dtype=float32, min=-1.788, max=-0.003, mean=-0.259),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'action_prob': np.ndarray((16,), dtype=float32, min=0.167, max=0.997, mean=0.82),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                  'vf_preds': np.ndarray((16,), dtype=float32, min=-0.895, max=0.095, mean=-0.317)})}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:21,986\tINFO simple_list_collector.py:659 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m { 0: { 'action_dist_inputs': np.ndarray((60, 9), dtype=float32, min=-25.1, max=17.903, mean=-0.294),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'action_logp': np.ndarray((60,), dtype=float32, min=-2.565, max=-0.001, mean=-0.648),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'actions': np.ndarray((60, 3), dtype=int64, min=0.0, max=2.0, mean=0.944),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'advantages': np.ndarray((60,), dtype=float32, min=-1.509, max=0.349, mean=-0.627),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'agent_index': np.ndarray((60,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'dones': np.ndarray((60,), dtype=bool, min=0.0, max=1.0, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'eps_id': np.ndarray((60,), dtype=int64, min=1194126676.0, max=1194126676.0, mean=1194126676.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'infos': np.ndarray((60,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.031, max=1.2, mean=-3.916), 'rotation_y': 98.31837, 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'new_obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.172),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.172),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'rewards': np.ndarray((60,), dtype=float32, min=-1.0, max=0.051, mean=-0.024),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'unroll_id': np.ndarray((60,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'value_targets': np.ndarray((60,), dtype=float32, min=-1.005, max=-0.674, mean=-0.866),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'vf_preds': np.ndarray((60,), dtype=float32, min=-1.135, max=0.544, mean=-0.239)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   1: { 'action_dist_inputs': np.ndarray((60, 9), dtype=float32, min=-29.949, max=20.876, mean=-0.492),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'action_logp': np.ndarray((60,), dtype=float32, min=-3.512, max=-0.0, mean=-0.62),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'actions': np.ndarray((60, 3), dtype=int64, min=0.0, max=2.0, mean=0.811),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'advantages': np.ndarray((60,), dtype=float32, min=-1.316, max=-0.075, mean=-0.684),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'agent_index': np.ndarray((60,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'dones': np.ndarray((60,), dtype=bool, min=0.0, max=1.0, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'eps_id': np.ndarray((60,), dtype=int64, min=1194126676.0, max=1194126676.0, mean=1194126676.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'infos': np.ndarray((60,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-6.74, max=-1.251, mean=-3.996), 'rotation_y': 77.002464, 'velocity': np.ndarray((2,), dtype=float32, min=-7.976, max=-1.024, mean=-4.5)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'new_obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'rewards': np.ndarray((60,), dtype=float32, min=-1.001, max=0.051, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'unroll_id': np.ndarray((60,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'value_targets': np.ndarray((60,), dtype=float32, min=-1.014, max=-0.694, mean=-0.881),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'vf_preds': np.ndarray((60,), dtype=float32, min=-0.934, max=0.325, mean=-0.197)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   2: { 'action_dist_inputs': np.ndarray((60, 9), dtype=float32, min=-34.47, max=19.712, mean=-0.37),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'action_logp': np.ndarray((60,), dtype=float32, min=-3.932, max=0.0, mean=-0.474),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'actions': np.ndarray((60, 3), dtype=int64, min=0.0, max=2.0, mean=0.811),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'advantages': np.ndarray((60,), dtype=float32, min=-0.358, max=1.547, mean=0.806),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'agent_index': np.ndarray((60,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'dones': np.ndarray((60,), dtype=bool, min=0.0, max=1.0, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'eps_id': np.ndarray((60,), dtype=int64, min=1194126676.0, max=1194126676.0, mean=1194126676.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'infos': np.ndarray((60,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=1.247, max=5.953, mean=3.6), 'rotation_y': 275.33014, 'velocity': np.ndarray((2,), dtype=float32, min=-8.017, max=0.748, mean=-3.634)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'new_obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'rewards': np.ndarray((60,), dtype=float32, min=-0.018, max=0.963, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'unroll_id': np.ndarray((60,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'value_targets': np.ndarray((60,), dtype=float32, min=0.66, max=1.053, mean=0.853),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'vf_preds': np.ndarray((60,), dtype=float32, min=-0.806, max=1.154, mean=0.047)},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   3: { 'action_dist_inputs': np.ndarray((60, 9), dtype=float32, min=-20.906, max=12.807, mean=-0.339),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'action_logp': np.ndarray((60,), dtype=float32, min=-4.701, max=-0.001, mean=-0.825),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'actions': np.ndarray((60, 3), dtype=int64, min=0.0, max=2.0, mean=1.094),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'advantages': np.ndarray((60,), dtype=float32, min=0.207, max=1.727, mean=0.799),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'agent_index': np.ndarray((60,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'dones': np.ndarray((60,), dtype=bool, min=0.0, max=1.0, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'eps_id': np.ndarray((60,), dtype=int64, min=1194126676.0, max=1194126676.0, mean=1194126676.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'infos': np.ndarray((60,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-1.338, max=6.664, mean=2.663), 'rotation_y': 270.03668, 'velocity': np.ndarray((2,), dtype=float32, min=-2.215, max=-0.001, mean=-1.108)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'new_obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.174),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'obs': np.ndarray((60, 336), dtype=float32, min=0.0, max=1.0, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'rewards': np.ndarray((60,), dtype=float32, min=-0.018, max=0.963, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'unroll_id': np.ndarray((60,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'value_targets': np.ndarray((60,), dtype=float32, min=0.663, max=1.053, mean=0.854),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m        'vf_preds': np.ndarray((60,), dtype=float32, min=-0.895, max=0.527, mean=0.055)}}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:27,542\tINFO rollout_worker.py:761 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   'policy_batches': { 'default': { 'action_dist_inputs': np.ndarray((3200, 9), dtype=float32, min=-35.756, max=23.63, mean=-0.337),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'action_logp': np.ndarray((3200,), dtype=float32, min=-12.115, max=0.0, mean=-0.617),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'actions': np.ndarray((3200, 3), dtype=int64, min=0.0, max=2.0, mean=0.945),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'advantages': np.ndarray((3200,), dtype=float32, min=-2.341, max=2.308, mean=0.027),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'agent_index': np.ndarray((3200,), dtype=int64, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'dones': np.ndarray((3200,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'eps_id': np.ndarray((3200,), dtype=int64, min=959686112.0, max=1895186677.0, mean=1469962231.795),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'infos': np.ndarray((3200,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.031, max=1.2, mean=-3.916), 'rotation_y': 98.31837, 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'new_obs': np.ndarray((3200, 336), dtype=float32, min=0.0, max=1.0, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'obs': np.ndarray((3200, 336), dtype=float32, min=0.0, max=1.0, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'rewards': np.ndarray((3200,), dtype=float32, min=-1.014, max=0.963, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'unroll_id': np.ndarray((3200,), dtype=int64, min=0.0, max=31.0, mean=14.5),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'value_targets': np.ndarray((3200,), dtype=float32, min=-1.404, max=1.278, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'vf_preds': np.ndarray((3200,), dtype=float32, min=-1.591, max=1.371, mean=-0.054)}},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m 2021-12-03 14:04:50,381\tINFO rollout_worker.py:901 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   'policy_batches': { 'default': { 'action_dist_inputs': np.ndarray((128, 9), dtype=float32, min=-27.543, max=17.553, mean=-0.293),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'action_logp': np.ndarray((128,), dtype=float32, min=-7.129, max=-0.0, mean=-0.7),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'actions': np.ndarray((128, 3), dtype=int64, min=0.0, max=2.0, mean=0.904),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'advantages': np.ndarray((128,), dtype=float32, min=-2.435, max=2.246, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.477),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'dones': np.ndarray((128,), dtype=bool, min=0.0, max=1.0, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'eps_id': np.ndarray((128,), dtype=int64, min=73452819.0, max=1895186677.0, mean=1118961158.82),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'infos': np.ndarray((128,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-0.286, max=12.3, mean=6.007), 'rotation_y': 90.52255, 'velocity': np.ndarray((2,), dtype=float32, min=-4.443, max=1.364, mean=-1.539)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=-14.752, max=3.678, mean=-5.537), 'velocity': np.ndarray((2,), dtype=float32, min=-0.897, max=-0.673, mean=-0.785)}, 'ep_metrics': {'total_timesteps': 34, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0025571824217561832, 'agent_position_to_ball_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'new_obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.176),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.176),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'rewards': np.ndarray((128,), dtype=float32, min=-1.005, max=0.882, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'unroll_id': np.ndarray((128,), dtype=int64, min=3.0, max=170.0, mean=86.602),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'value_targets': np.ndarray((128,), dtype=float32, min=-1.44, max=1.114, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m                                    'vf_preds': np.ndarray((128,), dtype=float32, min=-0.784, max=1.196, mean=-0.085)}},\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=95010)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39680000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030434782608695674\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.1220000000000001\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6074068834671137\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.154306348065058\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7037157627838031\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 146.04347826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00267001546018264\n",
      "  episode_reward_mean: -0.5504383065997986\n",
      "  episode_reward_min: -1.082138190402616\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 65711\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6364097995758057\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05592058810591698\n",
      "          policy_loss: -0.13919533978402615\n",
      "          total_loss: 0.11810115789622068\n",
      "          vf_explained_var: 0.34981396794319153\n",
      "          vf_loss: 0.24611238038539887\n",
      "    num_agent_steps_sampled: 39680000\n",
      "    num_agent_steps_trained: 39680000\n",
      "    num_steps_sampled: 9920000\n",
      "    num_steps_trained: 9920000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.17796610169491\n",
      "    gpu_util_percent0: 0.17101694915254234\n",
      "    ram_util_percent: 49.912711864406795\n",
      "    vram_util_percent0: 0.17960301392630276\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.4771303165652867\n",
      "  policy_reward_mean:\n",
      "    default: -0.13760957664994972\n",
      "  policy_reward_min:\n",
      "    default: -1.8289534957985312\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2713622627677498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.09727940097317\n",
      "    mean_inference_ms: 3.843383712844772\n",
      "    mean_raw_obs_processing_ms: 0.9806572974144995\n",
      "  time_since_restore: 88.75047898292542\n",
      "  time_this_iter_s: 88.75047898292542\n",
      "  time_total_s: 197859.02885627747\n",
      "  timers:\n",
      "    learn_throughput: 68.404\n",
      "    learn_time_ms: 58476.027\n",
      "    sample_throughput: 132.137\n",
      "    sample_time_ms: 30271.642\n",
      "  timestamp: 1638551148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9920000\n",
      "  training_iteration: 2480\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2480</td><td style=\"text-align: right;\">          197859</td><td style=\"text-align: right;\">9920000</td><td style=\"text-align: right;\">-0.550438</td><td style=\"text-align: right;\">          0.00267002</td><td style=\"text-align: right;\">            -1.08214</td><td style=\"text-align: right;\">           146.043</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39696000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03800000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6496571387857252\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09823728055330812\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7466784377563417\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-07-08\n",
      "  done: false\n",
      "  episode_len_mean: 150.65384615384616\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00267001546018264\n",
      "  episode_reward_mean: -0.6158653884362626\n",
      "  episode_reward_min: -1.774938172026876\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 65740\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2999999999999998\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6279107947349548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.047432328760623935\n",
      "          policy_loss: -0.13525847365707158\n",
      "          total_loss: 0.18428973895311357\n",
      "          vf_explained_var: 0.31524401903152466\n",
      "          vf_loss: 0.30531851029396057\n",
      "    num_agent_steps_sampled: 39696000\n",
      "    num_agent_steps_trained: 39696000\n",
      "    num_steps_sampled: 9924000\n",
      "    num_steps_trained: 9924000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.09622641509434\n",
      "    gpu_util_percent0: 0.14622641509433962\n",
      "    ram_util_percent: 45.23679245283019\n",
      "    vram_util_percent0: 0.17270607883675737\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.488558830872584\n",
      "  policy_reward_mean:\n",
      "    default: -0.15396634710906565\n",
      "  policy_reward_min:\n",
      "    default: -1.9849438027752546\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.26155873903642346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.418709798869408\n",
      "    mean_inference_ms: 3.7495105383964504\n",
      "    mean_raw_obs_processing_ms: 0.9605794478919625\n",
      "  time_since_restore: 168.41374397277832\n",
      "  time_this_iter_s: 79.6632649898529\n",
      "  time_total_s: 197938.69212126732\n",
      "  timers:\n",
      "    learn_throughput: 72.243\n",
      "    learn_time_ms: 55368.355\n",
      "    sample_throughput: 138.844\n",
      "    sample_time_ms: 28809.401\n",
      "  timestamp: 1638551228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9924000\n",
      "  training_iteration: 2481\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2481</td><td style=\"text-align: right;\">          197939</td><td style=\"text-align: right;\">9924000</td><td style=\"text-align: right;\">-0.615865</td><td style=\"text-align: right;\">          0.00267002</td><td style=\"text-align: right;\">            -1.77494</td><td style=\"text-align: right;\">           150.654</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39712000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03224444444444447\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6496571387857252\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0147198928599845\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7466784377563417\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 127.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00267001546018264\n",
      "  episode_reward_mean: -0.5330319403449457\n",
      "  episode_reward_min: -1.774938172026876\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 65778\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45000000000000023\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6265218505859375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041965880453586575\n",
      "          policy_loss: -0.13833946209400894\n",
      "          total_loss: 0.22898843438923358\n",
      "          vf_explained_var: 0.3185596764087677\n",
      "          vf_loss: 0.34844325089454653\n",
      "    num_agent_steps_sampled: 39712000\n",
      "    num_agent_steps_trained: 39712000\n",
      "    num_steps_sampled: 9928000\n",
      "    num_steps_trained: 9928000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.898095238095234\n",
      "    gpu_util_percent0: 0.10523809523809523\n",
      "    ram_util_percent: 44.86285714285713\n",
      "    vram_util_percent0: 0.17038656199564392\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.488558830872584\n",
      "  policy_reward_mean:\n",
      "    default: -0.13325798508623643\n",
      "  policy_reward_min:\n",
      "    default: -1.9849438027752546\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2536743279767905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.943254054539043\n",
      "    mean_inference_ms: 3.6576188011016098\n",
      "    mean_raw_obs_processing_ms: 0.9488219439189306\n",
      "  time_since_restore: 247.0378065109253\n",
      "  time_this_iter_s: 78.62406253814697\n",
      "  time_total_s: 198017.31618380547\n",
      "  timers:\n",
      "    learn_throughput: 73.718\n",
      "    learn_time_ms: 54260.803\n",
      "    sample_throughput: 142.619\n",
      "    sample_time_ms: 28046.767\n",
      "  timestamp: 1638551307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9928000\n",
      "  training_iteration: 2482\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2482</td><td style=\"text-align: right;\">          198017</td><td style=\"text-align: right;\">9928000</td><td style=\"text-align: right;\">-0.533032</td><td style=\"text-align: right;\">          0.00267002</td><td style=\"text-align: right;\">            -1.77494</td><td style=\"text-align: right;\">             127.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39728000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.040260000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6496571387857252\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.016698843692278052\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7517809184110251\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 139.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.008873354217834528\n",
      "  episode_reward_mean: -0.5858106915395289\n",
      "  episode_reward_min: -2.1222816106321862\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 65804\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6406689329147339\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.034280732095241545\n",
      "          policy_loss: -0.1337253600805998\n",
      "          total_loss: 0.17582502813637257\n",
      "          vf_explained_var: 0.3505934476852417\n",
      "          vf_loss: 0.2864108952283859\n",
      "    num_agent_steps_sampled: 39728000\n",
      "    num_agent_steps_trained: 39728000\n",
      "    num_steps_sampled: 9932000\n",
      "    num_steps_trained: 9932000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.108333333333334\n",
      "    gpu_util_percent0: 0.10074999999999999\n",
      "    ram_util_percent: 44.80416666666668\n",
      "    vram_util_percent0: 0.1687643961829549\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.49202510236677\n",
      "  policy_reward_mean:\n",
      "    default: -0.14645267288488223\n",
      "  policy_reward_min:\n",
      "    default: -2.0698587052625843\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.24678173601663672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.487542118868923\n",
      "    mean_inference_ms: 3.584069462405664\n",
      "    mean_raw_obs_processing_ms: 0.934209549234813\n",
      "  time_since_restore: 325.24543714523315\n",
      "  time_this_iter_s: 78.20763063430786\n",
      "  time_total_s: 198095.52381443977\n",
      "  timers:\n",
      "    learn_throughput: 74.51\n",
      "    learn_time_ms: 53684.195\n",
      "    sample_throughput: 145.021\n",
      "    sample_time_ms: 27582.302\n",
      "  timestamp: 1638551397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9932000\n",
      "  training_iteration: 2483\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2483</td><td style=\"text-align: right;\">          198096</td><td style=\"text-align: right;\">9932000</td><td style=\"text-align: right;\">-0.585811</td><td style=\"text-align: right;\">         -0.00887335</td><td style=\"text-align: right;\">            -2.12228</td><td style=\"text-align: right;\">            139.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39744000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03446000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20100000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6217532994183699\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.10068537221545683\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7517809184110251\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-11-16\n",
      "  done: false\n",
      "  episode_len_mean: 121.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.008873354217834528\n",
      "  episode_reward_mean: -0.5234663910767436\n",
      "  episode_reward_min: -2.1222816106321862\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 65839\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6253832578659058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02658745539188385\n",
      "          policy_loss: -0.12649145771563053\n",
      "          total_loss: 0.25031849256157873\n",
      "          vf_explained_var: 0.3600297272205353\n",
      "          vf_loss: 0.34989014840126037\n",
      "    num_agent_steps_sampled: 39744000\n",
      "    num_agent_steps_trained: 39744000\n",
      "    num_steps_sampled: 9936000\n",
      "    num_steps_trained: 9936000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.06346153846154\n",
      "    gpu_util_percent0: 0.10721153846153844\n",
      "    ram_util_percent: 44.84134615384615\n",
      "    vram_util_percent0: 0.16913774774090667\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.49202510236677\n",
      "  policy_reward_mean:\n",
      "    default: -0.13086659776918588\n",
      "  policy_reward_min:\n",
      "    default: -2.0698587052625843\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23891046444228778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.001516568230333\n",
      "    mean_inference_ms: 3.4967603166438574\n",
      "    mean_raw_obs_processing_ms: 0.9205086738195498\n",
      "  time_since_restore: 404.2017958164215\n",
      "  time_this_iter_s: 78.95635867118835\n",
      "  time_total_s: 198174.48017311096\n",
      "  timers:\n",
      "    learn_throughput: 74.892\n",
      "    learn_time_ms: 53410.396\n",
      "    sample_throughput: 146.083\n",
      "    sample_time_ms: 27381.761\n",
      "  timestamp: 1638551476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9936000\n",
      "  training_iteration: 2484\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2484</td><td style=\"text-align: right;\">          198174</td><td style=\"text-align: right;\">9936000</td><td style=\"text-align: right;\">-0.523466</td><td style=\"text-align: right;\">         -0.00887335</td><td style=\"text-align: right;\">            -2.12228</td><td style=\"text-align: right;\">             121.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39760000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.036540000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20100000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6360079006226563\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0929184603510721\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7517809184110251\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 123.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.08599757407417519\n",
      "  episode_reward_mean: -0.5214243677322988\n",
      "  episode_reward_min: -2.1222816106321862\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 65879\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6286046023368835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01969975720345974\n",
      "          policy_loss: -0.11575213243067264\n",
      "          total_loss: 0.31665430557727814\n",
      "          vf_explained_var: 0.33246299624443054\n",
      "          vf_loss: 0.4024874336719513\n",
      "    num_agent_steps_sampled: 39760000\n",
      "    num_agent_steps_trained: 39760000\n",
      "    num_steps_sampled: 9940000\n",
      "    num_steps_trained: 9940000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.087619047619047\n",
      "    gpu_util_percent0: 0.1000952380952381\n",
      "    ram_util_percent: 44.842857142857135\n",
      "    vram_util_percent0: 0.16915338692238993\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.60832172598331\n",
      "  policy_reward_mean:\n",
      "    default: -0.13035609193307474\n",
      "  policy_reward_min:\n",
      "    default: -2.0698587052625843\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2346779828207147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.73416428701722\n",
      "    mean_inference_ms: 3.4572351928432044\n",
      "    mean_raw_obs_processing_ms: 0.9142040572867864\n",
      "  time_since_restore: 482.91303610801697\n",
      "  time_this_iter_s: 78.71124029159546\n",
      "  time_total_s: 198253.19141340256\n",
      "  timers:\n",
      "    learn_throughput: 75.213\n",
      "    learn_time_ms: 53182.29\n",
      "    sample_throughput: 146.78\n",
      "    sample_time_ms: 27251.749\n",
      "  timestamp: 1638551555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9940000\n",
      "  training_iteration: 2485\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2485</td><td style=\"text-align: right;\">          198253</td><td style=\"text-align: right;\">9940000</td><td style=\"text-align: right;\">-0.521424</td><td style=\"text-align: right;\">          -0.0859976</td><td style=\"text-align: right;\">            -2.12228</td><td style=\"text-align: right;\">            123.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39776000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028410000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17600000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6360079006226563\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.09313664430051947\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7166576932880108\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 106.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.06611504518707001\n",
      "  episode_reward_mean: -0.4542770580523743\n",
      "  episode_reward_min: -1.4714133806991265\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 65916\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6241727318763733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019836158908903598\n",
      "          policy_loss: -0.1199615287333727\n",
      "          total_loss: 0.3202397176325321\n",
      "          vf_explained_var: 0.3600912094116211\n",
      "          vf_loss: 0.41007507872581483\n",
      "    num_agent_steps_sampled: 39776000\n",
      "    num_agent_steps_trained: 39776000\n",
      "    num_steps_sampled: 9944000\n",
      "    num_steps_trained: 9944000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.249593495934956\n",
      "    gpu_util_percent0: 0.1053658536585366\n",
      "    ram_util_percent: 45.13983739837399\n",
      "    vram_util_percent0: 0.1689499915729661\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.60832172598331\n",
      "  policy_reward_mean:\n",
      "    default: -0.11356926451309356\n",
      "  policy_reward_min:\n",
      "    default: -1.8071345974930604\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23241496618141444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.628384983820215\n",
      "    mean_inference_ms: 3.429615555473302\n",
      "    mean_raw_obs_processing_ms: 0.9170111063754379\n",
      "  time_since_restore: 563.023824930191\n",
      "  time_this_iter_s: 80.11078882217407\n",
      "  time_total_s: 198333.30220222473\n",
      "  timers:\n",
      "    learn_throughput: 75.152\n",
      "    learn_time_ms: 53225.144\n",
      "    sample_throughput: 147.312\n",
      "    sample_time_ms: 27153.23\n",
      "  timestamp: 1638551648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9944000\n",
      "  training_iteration: 2486\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2486</td><td style=\"text-align: right;\">          198333</td><td style=\"text-align: right;\">9944000</td><td style=\"text-align: right;\">-0.454277</td><td style=\"text-align: right;\">           -0.066115</td><td style=\"text-align: right;\">            -1.47141</td><td style=\"text-align: right;\">            106.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39792000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02875000000000001\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17600000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6360079006226563\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08606697361438222\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7166576932880108\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 107.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.021579712165600506\n",
      "  episode_reward_mean: -0.4706478812939555\n",
      "  episode_reward_min: -1.4714133806991265\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 65948\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6268537113666535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01965382957458496\n",
      "          policy_loss: -0.11555854560434818\n",
      "          total_loss: 0.2513668377101421\n",
      "          vf_explained_var: 0.3283531367778778\n",
      "          vf_loss: 0.33707613253593444\n",
      "    num_agent_steps_sampled: 39792000\n",
      "    num_agent_steps_trained: 39792000\n",
      "    num_steps_sampled: 9948000\n",
      "    num_steps_trained: 9948000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.924271844660186\n",
      "    gpu_util_percent0: 0.10592233009708737\n",
      "    ram_util_percent: 44.65533980582525\n",
      "    vram_util_percent0: 0.16541114380369107\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.60832172598331\n",
      "  policy_reward_mean:\n",
      "    default: -0.1176619703234889\n",
      "  policy_reward_min:\n",
      "    default: -1.8457703296005215\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2308216838288461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.555316342373597\n",
      "    mean_inference_ms: 3.4035248822732957\n",
      "    mean_raw_obs_processing_ms: 0.918344879333217\n",
      "  time_since_restore: 640.3824169635773\n",
      "  time_this_iter_s: 77.35859203338623\n",
      "  time_total_s: 198410.66079425812\n",
      "  timers:\n",
      "    learn_throughput: 75.526\n",
      "    learn_time_ms: 52961.802\n",
      "    sample_throughput: 147.98\n",
      "    sample_time_ms: 27030.727\n",
      "  timestamp: 1638551725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9948000\n",
      "  training_iteration: 2487\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2487</td><td style=\"text-align: right;\">          198411</td><td style=\"text-align: right;\">9948000</td><td style=\"text-align: right;\">-0.470648</td><td style=\"text-align: right;\">          -0.0215797</td><td style=\"text-align: right;\">            -1.47141</td><td style=\"text-align: right;\">            107.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39808000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02875000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18100000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5169629551089108\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.02130745651518472\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7166576932880108\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 111.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.021579712165600506\n",
      "  episode_reward_mean: -0.48197058348323146\n",
      "  episode_reward_min: -1.6893886974033705\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 65984\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6327921061515808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01981413409113884\n",
      "          policy_loss: -0.11704545451700687\n",
      "          total_loss: 0.25140425345301626\n",
      "          vf_explained_var: 0.3691405951976776\n",
      "          vf_loss: 0.33835699343681336\n",
      "    num_agent_steps_sampled: 39808000\n",
      "    num_agent_steps_trained: 39808000\n",
      "    num_steps_sampled: 9952000\n",
      "    num_steps_trained: 9952000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.125242718446604\n",
      "    gpu_util_percent0: 0.10621359223300972\n",
      "    ram_util_percent: 44.59708737864076\n",
      "    vram_util_percent0: 0.16538718344371062\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6030165269856709\n",
      "  policy_reward_mean:\n",
      "    default: -0.12049264587080787\n",
      "  policy_reward_min:\n",
      "    default: -1.8457703296005215\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22935157208724388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.485017108134233\n",
      "    mean_inference_ms: 3.377454727371727\n",
      "    mean_raw_obs_processing_ms: 0.9188437820882959\n",
      "  time_since_restore: 718.1857283115387\n",
      "  time_this_iter_s: 77.80331134796143\n",
      "  time_total_s: 198488.46410560608\n",
      "  timers:\n",
      "    learn_throughput: 75.79\n",
      "    learn_time_ms: 52777.462\n",
      "    sample_throughput: 148.343\n",
      "    sample_time_ms: 26964.605\n",
      "  timestamp: 1638551803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9952000\n",
      "  training_iteration: 2488\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2488</td><td style=\"text-align: right;\">          198488</td><td style=\"text-align: right;\">9952000</td><td style=\"text-align: right;\">-0.481971</td><td style=\"text-align: right;\">          -0.0215797</td><td style=\"text-align: right;\">            -1.68939</td><td style=\"text-align: right;\">            111.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39824000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029480000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18100000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5336803281987541\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.053804924169248904\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7626271762248024\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-18-12\n",
      "  done: false\n",
      "  episode_len_mean: 119.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03616411568542999\n",
      "  episode_reward_mean: -0.4954729223756669\n",
      "  episode_reward_min: -1.744348045066972\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 66015\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6300819792747497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0191470328271389\n",
      "          policy_loss: -0.11401060306280851\n",
      "          total_loss: 0.2512607513964176\n",
      "          vf_explained_var: 0.30357709527015686\n",
      "          vf_loss: 0.336191796541214\n",
      "    num_agent_steps_sampled: 39824000\n",
      "    num_agent_steps_trained: 39824000\n",
      "    num_steps_sampled: 9956000\n",
      "    num_steps_trained: 9956000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.27058823529412\n",
      "    gpu_util_percent0: 0.09092436974789918\n",
      "    ram_util_percent: 45.179831932773105\n",
      "    vram_util_percent0: 0.1647338106022271\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6030165269856709\n",
      "  policy_reward_mean:\n",
      "    default: -0.12386823059391672\n",
      "  policy_reward_min:\n",
      "    default: -1.9572864840699493\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22815544869811513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.42271707066534\n",
      "    mean_inference_ms: 3.3575497794258093\n",
      "    mean_raw_obs_processing_ms: 0.9172055077938466\n",
      "  time_since_restore: 795.5388355255127\n",
      "  time_this_iter_s: 77.353107213974\n",
      "  time_total_s: 198565.81721282005\n",
      "  timers:\n",
      "    learn_throughput: 76.022\n",
      "    learn_time_ms: 52616.654\n",
      "    sample_throughput: 148.819\n",
      "    sample_time_ms: 26878.366\n",
      "  timestamp: 1638551892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9956000\n",
      "  training_iteration: 2489\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2489</td><td style=\"text-align: right;\">          198566</td><td style=\"text-align: right;\">9956000</td><td style=\"text-align: right;\">-0.495473</td><td style=\"text-align: right;\">           0.0361641</td><td style=\"text-align: right;\">            -1.74435</td><td style=\"text-align: right;\">            119.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39840000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.026680000000000023\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17500000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5336803281987541\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0017522112579068144\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7626271762248024\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 121.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07659804758422295\n",
      "  episode_reward_mean: -0.47794464021798894\n",
      "  episode_reward_min: -1.744348045066972\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 66050\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6267726945877076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019765070147812366\n",
      "          policy_loss: -0.11777187138795853\n",
      "          total_loss: 0.28492046701908114\n",
      "          vf_explained_var: 0.29776495695114136\n",
      "          vf_loss: 0.3726741387844086\n",
      "    num_agent_steps_sampled: 39840000\n",
      "    num_agent_steps_trained: 39840000\n",
      "    num_steps_sampled: 9960000\n",
      "    num_steps_trained: 9960000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.036893203883498\n",
      "    gpu_util_percent0: 0.10485436893203885\n",
      "    ram_util_percent: 45.22038834951458\n",
      "    vram_util_percent0: 0.16312372810422443\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6030165269856709\n",
      "  policy_reward_mean:\n",
      "    default: -0.11948616005449723\n",
      "  policy_reward_min:\n",
      "    default: -2.0061635407384837\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2270877035027101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.368170656154206\n",
      "    mean_inference_ms: 3.340859859108351\n",
      "    mean_raw_obs_processing_ms: 0.9174729975320807\n",
      "  time_since_restore: 873.2191488742828\n",
      "  time_this_iter_s: 77.68031334877014\n",
      "  time_total_s: 198643.49752616882\n",
      "  timers:\n",
      "    learn_throughput: 77.069\n",
      "    learn_time_ms: 51901.732\n",
      "    sample_throughput: 151.058\n",
      "    sample_time_ms: 26479.91\n",
      "  timestamp: 1638551970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9960000\n",
      "  training_iteration: 2490\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2490</td><td style=\"text-align: right;\">          198643</td><td style=\"text-align: right;\">9960000</td><td style=\"text-align: right;\">-0.477945</td><td style=\"text-align: right;\">            0.076598</td><td style=\"text-align: right;\">            -1.74435</td><td style=\"text-align: right;\">            121.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39856000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02769000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17500000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5336803281987541\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08763521382919837\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7626271762248024\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 120.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07659804758422295\n",
      "  episode_reward_mean: -0.49550818149487846\n",
      "  episode_reward_min: -1.7610155945929604\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 66081\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6324401817321778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01927238754183054\n",
      "          policy_loss: -0.11531069727241992\n",
      "          total_loss: 0.2260003372132778\n",
      "          vf_explained_var: 0.30433768033981323\n",
      "          vf_loss: 0.31204109716415407\n",
      "    num_agent_steps_sampled: 39856000\n",
      "    num_agent_steps_trained: 39856000\n",
      "    num_steps_sampled: 9964000\n",
      "    num_steps_trained: 9964000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.118446601941756\n",
      "    gpu_util_percent0: 0.10407766990291265\n",
      "    ram_util_percent: 45.21359223300974\n",
      "    vram_util_percent0: 0.1617547928706748\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5378390464148133\n",
      "  policy_reward_mean:\n",
      "    default: -0.1238770453737196\n",
      "  policy_reward_min:\n",
      "    default: -2.0061635407384837\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22626520147432935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.326105214833568\n",
      "    mean_inference_ms: 3.3259811943335715\n",
      "    mean_raw_obs_processing_ms: 0.9183551064641889\n",
      "  time_since_restore: 950.7638034820557\n",
      "  time_this_iter_s: 77.54465460777283\n",
      "  time_total_s: 198721.0421807766\n",
      "  timers:\n",
      "    learn_throughput: 77.231\n",
      "    learn_time_ms: 51792.757\n",
      "    sample_throughput: 151.654\n",
      "    sample_time_ms: 26375.785\n",
      "  timestamp: 1638552048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9964000\n",
      "  training_iteration: 2491\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2491</td><td style=\"text-align: right;\">          198721</td><td style=\"text-align: right;\">9964000</td><td style=\"text-align: right;\">-0.495508</td><td style=\"text-align: right;\">            0.076598</td><td style=\"text-align: right;\">            -1.76102</td><td style=\"text-align: right;\">            120.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39872000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029550000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7878172120253568\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07577343556027558\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7488355732580027\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 125.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07793262511977184\n",
      "  episode_reward_mean: -0.4903769188185621\n",
      "  episode_reward_min: -1.8538853941033753\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 66113\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6218373289108277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019697883427143098\n",
      "          policy_loss: -0.115415872707963\n",
      "          total_loss: 0.23118461441993712\n",
      "          vf_explained_var: 0.3526169955730438\n",
      "          vf_loss: 0.31668432891368864\n",
      "    num_agent_steps_sampled: 39872000\n",
      "    num_agent_steps_trained: 39872000\n",
      "    num_steps_sampled: 9968000\n",
      "    num_steps_trained: 9968000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.208403361344537\n",
      "    gpu_util_percent0: 0.08991596638655464\n",
      "    ram_util_percent: 45.307563025210115\n",
      "    vram_util_percent0: 0.1617806056282336\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.607031693124867\n",
      "  policy_reward_mean:\n",
      "    default: -0.12259422970464051\n",
      "  policy_reward_min:\n",
      "    default: -2.0061635407384837\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22556169029814468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.28715896344241\n",
      "    mean_inference_ms: 3.3153697849295334\n",
      "    mean_raw_obs_processing_ms: 0.9208068090766384\n",
      "  time_since_restore: 1028.1881656646729\n",
      "  time_this_iter_s: 77.42436218261719\n",
      "  time_total_s: 198798.4665429592\n",
      "  timers:\n",
      "    learn_throughput: 77.361\n",
      "    learn_time_ms: 51705.475\n",
      "    sample_throughput: 151.848\n",
      "    sample_time_ms: 26342.213\n",
      "  timestamp: 1638552138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9968000\n",
      "  training_iteration: 2492\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2492</td><td style=\"text-align: right;\">          198798</td><td style=\"text-align: right;\">9968000</td><td style=\"text-align: right;\">-0.490377</td><td style=\"text-align: right;\">           0.0779326</td><td style=\"text-align: right;\">            -1.85389</td><td style=\"text-align: right;\">            125.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39888000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02748000000000001\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7878172120253568\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.01295665140582814\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7488355732580027\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 119.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.15260712160429923\n",
      "  episode_reward_mean: -0.4686383940198146\n",
      "  episode_reward_min: -1.8538853941033753\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 66150\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6357439956665039\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019838563956320287\n",
      "          policy_loss: -0.11861261969059705\n",
      "          total_loss: 0.29686782824993135\n",
      "          vf_explained_var: 0.3372920751571655\n",
      "          vf_loss: 0.38535063004493714\n",
      "    num_agent_steps_sampled: 39888000\n",
      "    num_agent_steps_trained: 39888000\n",
      "    num_steps_sampled: 9972000\n",
      "    num_steps_trained: 9972000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.93009708737864\n",
      "    gpu_util_percent0: 0.10485436893203885\n",
      "    ram_util_percent: 45.22524271844664\n",
      "    vram_util_percent0: 0.1617563902280068\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.607031693124867\n",
      "  policy_reward_mean:\n",
      "    default: -0.11715959850495365\n",
      "  policy_reward_min:\n",
      "    default: -1.8782871051402363\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22498701971443452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.25396422641904\n",
      "    mean_inference_ms: 3.306463625135913\n",
      "    mean_raw_obs_processing_ms: 0.9222695146210859\n",
      "  time_since_restore: 1105.623649597168\n",
      "  time_this_iter_s: 77.43548393249512\n",
      "  time_total_s: 198875.9020268917\n",
      "  timers:\n",
      "    learn_throughput: 77.516\n",
      "    learn_time_ms: 51602.509\n",
      "    sample_throughput: 151.701\n",
      "    sample_time_ms: 26367.616\n",
      "  timestamp: 1638552215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9972000\n",
      "  training_iteration: 2493\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2493</td><td style=\"text-align: right;\">          198876</td><td style=\"text-align: right;\">9972000</td><td style=\"text-align: right;\">-0.468638</td><td style=\"text-align: right;\">            0.152607</td><td style=\"text-align: right;\">            -1.85389</td><td style=\"text-align: right;\">            119.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39904000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.023930000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.1470000000000001\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7878172120253568\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08083872052910065\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6519996540276433\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 107.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.15260712160429923\n",
      "  episode_reward_mean: -0.41961780457686026\n",
      "  episode_reward_min: -1.3906448139130534\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 66190\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.624368127822876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019459630109369755\n",
      "          policy_loss: -0.11453789868205785\n",
      "          total_loss: 0.3191593708395958\n",
      "          vf_explained_var: 0.3213348686695099\n",
      "          vf_loss: 0.4041429579257965\n",
      "    num_agent_steps_sampled: 39904000\n",
      "    num_agent_steps_trained: 39904000\n",
      "    num_steps_sampled: 9976000\n",
      "    num_steps_trained: 9976000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.184761904761906\n",
      "    gpu_util_percent0: 0.11171428571428572\n",
      "    ram_util_percent: 45.183809523809515\n",
      "    vram_util_percent0: 0.16135946975038784\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.531614282391645\n",
      "  policy_reward_mean:\n",
      "    default: -0.10490445114421508\n",
      "  policy_reward_min:\n",
      "    default: -1.8782871051402363\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.224597831321556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.230631885782156\n",
      "    mean_inference_ms: 3.30147110916762\n",
      "    mean_raw_obs_processing_ms: 0.9227414313425567\n",
      "  time_since_restore: 1184.595050573349\n",
      "  time_this_iter_s: 78.97140097618103\n",
      "  time_total_s: 198954.8734278679\n",
      "  timers:\n",
      "    learn_throughput: 77.5\n",
      "    learn_time_ms: 51612.746\n",
      "    sample_throughput: 151.756\n",
      "    sample_time_ms: 26358.145\n",
      "  timestamp: 1638552294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9976000\n",
      "  training_iteration: 2494\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2494</td><td style=\"text-align: right;\">          198955</td><td style=\"text-align: right;\">9976000</td><td style=\"text-align: right;\">-0.419618</td><td style=\"text-align: right;\">            0.152607</td><td style=\"text-align: right;\">            -1.39064</td><td style=\"text-align: right;\">             107.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39920000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.027350000000000027\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19200000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6296480219959063\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.01724883806130557\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7190389553176083\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 108.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.02516309321953969\n",
      "  episode_reward_mean: -0.44553192773974587\n",
      "  episode_reward_min: -1.8942997939929795\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 66220\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6348519186973571\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01920682603120804\n",
      "          policy_loss: -0.11581581116467714\n",
      "          total_loss: 0.24739198871701956\n",
      "          vf_explained_var: 0.3029997646808624\n",
      "          vf_loss: 0.33403743624687193\n",
      "    num_agent_steps_sampled: 39920000\n",
      "    num_agent_steps_trained: 39920000\n",
      "    num_steps_sampled: 9980000\n",
      "    num_steps_trained: 9980000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.74285714285714\n",
      "    gpu_util_percent0: 0.1043809523809524\n",
      "    ram_util_percent: 45.03904761904761\n",
      "    vram_util_percent0: 0.16223538444663815\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.531614282391645\n",
      "  policy_reward_mean:\n",
      "    default: -0.11138298193493647\n",
      "  policy_reward_min:\n",
      "    default: -1.8903724017982713\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22443932677330955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.22543869368381\n",
      "    mean_inference_ms: 3.2968898353670433\n",
      "    mean_raw_obs_processing_ms: 0.922242594960166\n",
      "  time_since_restore: 1263.056523323059\n",
      "  time_this_iter_s: 78.46147274971008\n",
      "  time_total_s: 199033.3349006176\n",
      "  timers:\n",
      "    learn_throughput: 77.542\n",
      "    learn_time_ms: 51584.623\n",
      "    sample_throughput: 151.74\n",
      "    sample_time_ms: 26360.959\n",
      "  timestamp: 1638552373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9980000\n",
      "  training_iteration: 2495\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2495</td><td style=\"text-align: right;\">          199033</td><td style=\"text-align: right;\">9980000</td><td style=\"text-align: right;\">-0.445532</td><td style=\"text-align: right;\">          -0.0251631</td><td style=\"text-align: right;\">             -1.8943</td><td style=\"text-align: right;\">            108.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39936000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03225000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19200000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.620505485286237\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0545773288911821\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7382932284877082\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-27-44\n",
      "  done: false\n",
      "  episode_len_mean: 112.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.04111457240494554\n",
      "  episode_reward_mean: -0.4701085579967776\n",
      "  episode_reward_min: -2.114264759612092\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 66257\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6179470229148865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019471346966922282\n",
      "          policy_loss: -0.11589123256504535\n",
      "          total_loss: 0.26953160572052004\n",
      "          vf_explained_var: 0.35538044571876526\n",
      "          vf_loss: 0.3558507287502289\n",
      "    num_agent_steps_sampled: 39936000\n",
      "    num_agent_steps_trained: 39936000\n",
      "    num_steps_sampled: 9984000\n",
      "    num_steps_trained: 9984000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.220000000000002\n",
      "    gpu_util_percent0: 0.09016666666666666\n",
      "    ram_util_percent: 45.507499999999986\n",
      "    vram_util_percent0: 0.1615937260063617\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5473488495702783\n",
      "  policy_reward_mean:\n",
      "    default: -0.11752713949919441\n",
      "  policy_reward_min:\n",
      "    default: -1.899556252751081\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22429647844844966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.217789568738553\n",
      "    mean_inference_ms: 3.295250939242779\n",
      "    mean_raw_obs_processing_ms: 0.9219455830425932\n",
      "  time_since_restore: 1341.5114874839783\n",
      "  time_this_iter_s: 78.45496416091919\n",
      "  time_total_s: 199111.78986477852\n",
      "  timers:\n",
      "    learn_throughput: 77.786\n",
      "    learn_time_ms: 51422.989\n",
      "    sample_throughput: 151.764\n",
      "    sample_time_ms: 26356.777\n",
      "  timestamp: 1638552464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9984000\n",
      "  training_iteration: 2496\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2496</td><td style=\"text-align: right;\">          199112</td><td style=\"text-align: right;\">9984000</td><td style=\"text-align: right;\">-0.470109</td><td style=\"text-align: right;\">           0.0411146</td><td style=\"text-align: right;\">            -2.11426</td><td style=\"text-align: right;\">            112.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39952000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03307000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.620505485286237\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07944639223861891\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7382932284877082\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 117.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.04111457240494554\n",
      "  episode_reward_mean: -0.489500689887824\n",
      "  episode_reward_min: -2.114264759612092\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 66293\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6302147550582886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01907058646529913\n",
      "          policy_loss: -0.11379354517161847\n",
      "          total_loss: 0.25212252560257914\n",
      "          vf_explained_var: 0.32224559783935547\n",
      "          vf_loss: 0.3369526190757752\n",
      "    num_agent_steps_sampled: 39952000\n",
      "    num_agent_steps_trained: 39952000\n",
      "    num_steps_sampled: 9988000\n",
      "    num_steps_trained: 9988000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.39142857142857\n",
      "    gpu_util_percent0: 0.11333333333333334\n",
      "    ram_util_percent: 45.61904761904762\n",
      "    vram_util_percent0: 0.1631661417446215\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5743584501849799\n",
      "  policy_reward_mean:\n",
      "    default: -0.122375172471956\n",
      "  policy_reward_min:\n",
      "    default: -1.899556252751081\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22420695991687112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.214845233559373\n",
      "    mean_inference_ms: 3.2927876494481034\n",
      "    mean_raw_obs_processing_ms: 0.9223515455832597\n",
      "  time_since_restore: 1420.6100640296936\n",
      "  time_this_iter_s: 79.09857654571533\n",
      "  time_total_s: 199190.88844132423\n",
      "  timers:\n",
      "    learn_throughput: 77.6\n",
      "    learn_time_ms: 51546.646\n",
      "    sample_throughput: 151.476\n",
      "    sample_time_ms: 26406.87\n",
      "  timestamp: 1638552543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9988000\n",
      "  training_iteration: 2497\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2497</td><td style=\"text-align: right;\">          199191</td><td style=\"text-align: right;\">9988000</td><td style=\"text-align: right;\">-0.489501</td><td style=\"text-align: right;\">           0.0411146</td><td style=\"text-align: right;\">            -2.11426</td><td style=\"text-align: right;\">            117.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39968000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03073000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.620505485286237\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.1080607658354351\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6680325983935506\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-30-20\n",
      "  done: false\n",
      "  episode_len_mean: 112.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.19519172979715904\n",
      "  episode_reward_mean: -0.4544664303943551\n",
      "  episode_reward_min: -2.0017598642956105\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 66328\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6291224918365479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01907535897940397\n",
      "          policy_loss: -0.11417422899603843\n",
      "          total_loss: 0.22887692180275918\n",
      "          vf_explained_var: 0.31983450055122375\n",
      "          vf_loss: 0.3140804514884949\n",
      "    num_agent_steps_sampled: 39968000\n",
      "    num_agent_steps_trained: 39968000\n",
      "    num_steps_sampled: 9992000\n",
      "    num_steps_trained: 9992000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.768932038834954\n",
      "    gpu_util_percent0: 0.1063106796116505\n",
      "    ram_util_percent: 45.78737864077673\n",
      "    vram_util_percent0: 0.16833430772130578\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5743584501849799\n",
      "  policy_reward_mean:\n",
      "    default: -0.11361660759858877\n",
      "  policy_reward_min:\n",
      "    default: -1.8339825695517997\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22405569548392307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.20714651602246\n",
      "    mean_inference_ms: 3.2900774170390705\n",
      "    mean_raw_obs_processing_ms: 0.9240412680086323\n",
      "  time_since_restore: 1498.1808636188507\n",
      "  time_this_iter_s: 77.5707995891571\n",
      "  time_total_s: 199268.4592409134\n",
      "  timers:\n",
      "    learn_throughput: 77.623\n",
      "    learn_time_ms: 51531.356\n",
      "    sample_throughput: 151.523\n",
      "    sample_time_ms: 26398.571\n",
      "  timestamp: 1638552620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9992000\n",
      "  training_iteration: 2498\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2498</td><td style=\"text-align: right;\">          199268</td><td style=\"text-align: right;\">9992000</td><td style=\"text-align: right;\">-0.454466</td><td style=\"text-align: right;\">            0.195192</td><td style=\"text-align: right;\">            -2.00176</td><td style=\"text-align: right;\">            112.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 39984000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.031560000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6317769865945186\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07021151719721819\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6680325983935506\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 120.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.19519172979715904\n",
      "  episode_reward_mean: -0.4769678082314416\n",
      "  episode_reward_min: -2.0017598642956105\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 66357\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6193946361541748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01912125466018915\n",
      "          policy_loss: -0.11559930013120175\n",
      "          total_loss: 0.22244297125935555\n",
      "          vf_explained_var: 0.29435113072395325\n",
      "          vf_loss: 0.30900186717510225\n",
      "    num_agent_steps_sampled: 39984000\n",
      "    num_agent_steps_trained: 39984000\n",
      "    num_steps_sampled: 9996000\n",
      "    num_steps_trained: 9996000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.075630252100837\n",
      "    gpu_util_percent0: 0.09302521008403362\n",
      "    ram_util_percent: 45.84621848739496\n",
      "    vram_util_percent0: 0.1678667518340012\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5755193765474984\n",
      "  policy_reward_mean:\n",
      "    default: -0.11924195205786042\n",
      "  policy_reward_min:\n",
      "    default: -1.8339825695517997\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22387972294574202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.19770630521867\n",
      "    mean_inference_ms: 3.2849142651036334\n",
      "    mean_raw_obs_processing_ms: 0.9251108060901686\n",
      "  time_since_restore: 1575.5602042675018\n",
      "  time_this_iter_s: 77.37934064865112\n",
      "  time_total_s: 199345.83858156204\n",
      "  timers:\n",
      "    learn_throughput: 77.612\n",
      "    learn_time_ms: 51538.751\n",
      "    sample_throughput: 151.544\n",
      "    sample_time_ms: 26395.054\n",
      "  timestamp: 1638552710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9996000\n",
      "  training_iteration: 2499\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2499</td><td style=\"text-align: right;\">          199346</td><td style=\"text-align: right;\">9996000</td><td style=\"text-align: right;\">-0.476968</td><td style=\"text-align: right;\">            0.195192</td><td style=\"text-align: right;\">            -2.00176</td><td style=\"text-align: right;\">            120.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40000000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03260000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18600000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6990673858526775\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08851641251741274\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7615557029152958\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 120.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.19519172979715904\n",
      "  episode_reward_mean: -0.48505814210247244\n",
      "  episode_reward_min: -2.0017598642956105\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 66393\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6285166749954224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019864089652895926\n",
      "          policy_loss: -0.11864605337381363\n",
      "          total_loss: 0.28240299654006956\n",
      "          vf_explained_var: 0.3269239068031311\n",
      "          vf_loss: 0.37088046717643736\n",
      "    num_agent_steps_sampled: 40000000\n",
      "    num_agent_steps_trained: 40000000\n",
      "    num_steps_sampled: 10000000\n",
      "    num_steps_trained: 10000000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.69223300970874\n",
      "    gpu_util_percent0: 0.10553398058252432\n",
      "    ram_util_percent: 45.867961165048534\n",
      "    vram_util_percent0: 0.16767619650050966\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6337353022928212\n",
      "  policy_reward_mean:\n",
      "    default: -0.12126453552561817\n",
      "  policy_reward_min:\n",
      "    default: -1.8601405179623576\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.223604103164642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.1812188423411\n",
      "    mean_inference_ms: 3.278137729007993\n",
      "    mean_raw_obs_processing_ms: 0.925530468873088\n",
      "  time_since_restore: 1652.8487272262573\n",
      "  time_this_iter_s: 77.2885229587555\n",
      "  time_total_s: 199423.1271045208\n",
      "  timers:\n",
      "    learn_throughput: 77.668\n",
      "    learn_time_ms: 51501.135\n",
      "    sample_throughput: 151.553\n",
      "    sample_time_ms: 26393.34\n",
      "  timestamp: 1638552787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000000\n",
      "  training_iteration: 2500\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2500</td><td style=\"text-align: right;\">          199423</td><td style=\"text-align: right;\">10000000</td><td style=\"text-align: right;\">-0.485058</td><td style=\"text-align: right;\">            0.195192</td><td style=\"text-align: right;\">            -2.00176</td><td style=\"text-align: right;\">            120.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40016000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03855000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3050000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6990673858526775\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03062034844539262\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7615557029152958\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-34-25\n",
      "  done: false\n",
      "  episode_len_mean: 133.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00673367950858017\n",
      "  episode_reward_mean: -0.5500460382436597\n",
      "  episode_reward_min: -3.2350150322974938\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 66418\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6306175770759582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019213067136704922\n",
      "          policy_loss: -0.1140707382261753\n",
      "          total_loss: 0.22856618037819862\n",
      "          vf_explained_var: 0.3353315591812134\n",
      "          vf_loss: 0.3134570721387863\n",
      "    num_agent_steps_sampled: 40016000\n",
      "    num_agent_steps_trained: 40016000\n",
      "    num_steps_sampled: 10004000\n",
      "    num_steps_trained: 10004000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.21359223300971\n",
      "    gpu_util_percent0: 0.10300970873786411\n",
      "    ram_util_percent: 45.74660194174755\n",
      "    vram_util_percent0: 0.16500381768402353\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6337353022928212\n",
      "  policy_reward_mean:\n",
      "    default: -0.13751150956091493\n",
      "  policy_reward_min:\n",
      "    default: -2.1483935282561477\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22346510686314935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.17148803091276\n",
      "    mean_inference_ms: 3.274798203333719\n",
      "    mean_raw_obs_processing_ms: 0.9249003315732106\n",
      "  time_since_restore: 1730.4208376407623\n",
      "  time_this_iter_s: 77.572110414505\n",
      "  time_total_s: 199500.6992149353\n",
      "  timers:\n",
      "    learn_throughput: 77.683\n",
      "    learn_time_ms: 51491.311\n",
      "    sample_throughput: 151.482\n",
      "    sample_time_ms: 26405.838\n",
      "  timestamp: 1638552865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10004000\n",
      "  training_iteration: 2501\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2501</td><td style=\"text-align: right;\">          199501</td><td style=\"text-align: right;\">10004000</td><td style=\"text-align: right;\">-0.550046</td><td style=\"text-align: right;\">          0.00673368</td><td style=\"text-align: right;\">            -3.23502</td><td style=\"text-align: right;\">            133.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40032000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.039990000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3050000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7173475568661247\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03188645440581579\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7615557029152958\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 129.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.28831834785670063\n",
      "  episode_reward_mean: -0.5360400825484976\n",
      "  episode_reward_min: -3.2350150322974938\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 66449\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6249040851593017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01925559350848198\n",
      "          policy_loss: -0.11500726119056344\n",
      "          total_loss: 0.2530948460400105\n",
      "          vf_explained_var: 0.3332565724849701\n",
      "          vf_loss: 0.3388576731681824\n",
      "    num_agent_steps_sampled: 40032000\n",
      "    num_agent_steps_trained: 40032000\n",
      "    num_steps_sampled: 10008000\n",
      "    num_steps_trained: 10008000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.106722689075628\n",
      "    gpu_util_percent0: 0.0939495798319328\n",
      "    ram_util_percent: 46.09159663865543\n",
      "    vram_util_percent0: 0.16255347153668967\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6337353022928212\n",
      "  policy_reward_mean:\n",
      "    default: -0.1340100206371244\n",
      "  policy_reward_min:\n",
      "    default: -2.1483935282561477\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22329000420386635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.159654628871905\n",
      "    mean_inference_ms: 3.2703631974702\n",
      "    mean_raw_obs_processing_ms: 0.9237825494595233\n",
      "  time_since_restore: 1807.7944388389587\n",
      "  time_this_iter_s: 77.37360119819641\n",
      "  time_total_s: 199578.0728161335\n",
      "  timers:\n",
      "    learn_throughput: 77.68\n",
      "    learn_time_ms: 51493.164\n",
      "    sample_throughput: 151.522\n",
      "    sample_time_ms: 26398.798\n",
      "  timestamp: 1638552955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10008000\n",
      "  training_iteration: 2502\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2502</td><td style=\"text-align: right;\">          199578</td><td style=\"text-align: right;\">10008000</td><td style=\"text-align: right;\">-0.53604</td><td style=\"text-align: right;\">            0.288318</td><td style=\"text-align: right;\">            -3.23502</td><td style=\"text-align: right;\">            129.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40048000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03472000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3050000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7173475568661247\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0063956299215136994\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7615557029152958\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 121.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.28831834785670063\n",
      "  episode_reward_mean: -0.5160158504254688\n",
      "  episode_reward_min: -3.2350150322974938\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 66490\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.619774477481842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019245871387422085\n",
      "          policy_loss: -0.11669064764678479\n",
      "          total_loss: 0.3081578879356384\n",
      "          vf_explained_var: 0.3175954818725586\n",
      "          vf_loss: 0.39561886763572696\n",
      "    num_agent_steps_sampled: 40048000\n",
      "    num_agent_steps_trained: 40048000\n",
      "    num_steps_sampled: 10012000\n",
      "    num_steps_trained: 10012000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.049038461538462\n",
      "    gpu_util_percent0: 0.10499999999999998\n",
      "    ram_util_percent: 45.96442307692307\n",
      "    vram_util_percent0: 0.16255347153668967\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5666728539579813\n",
      "  policy_reward_mean:\n",
      "    default: -0.12900396260636726\n",
      "  policy_reward_min:\n",
      "    default: -2.1483935282561477\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22310047323549573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.14930353125921\n",
      "    mean_inference_ms: 3.266618428924554\n",
      "    mean_raw_obs_processing_ms: 0.9240790103377844\n",
      "  time_since_restore: 1885.4408087730408\n",
      "  time_this_iter_s: 77.64636993408203\n",
      "  time_total_s: 199655.71918606758\n",
      "  timers:\n",
      "    learn_throughput: 77.667\n",
      "    learn_time_ms: 51502.209\n",
      "    sample_throughput: 151.454\n",
      "    sample_time_ms: 26410.708\n",
      "  timestamp: 1638553032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10012000\n",
      "  training_iteration: 2503\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2503</td><td style=\"text-align: right;\">          199656</td><td style=\"text-align: right;\">10012000</td><td style=\"text-align: right;\">-0.516016</td><td style=\"text-align: right;\">            0.288318</td><td style=\"text-align: right;\">            -3.23502</td><td style=\"text-align: right;\">            121.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40064000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03874000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7173475568661247\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.016923527106143383\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6506102587594709\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 131.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.28831834785670063\n",
      "  episode_reward_mean: -0.5387442305925224\n",
      "  episode_reward_min: -2.199459452524616\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 66511\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.631174747467041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018461720049381257\n",
      "          policy_loss: -0.11188334856927395\n",
      "          total_loss: 0.14421870094537734\n",
      "          vf_explained_var: 0.29646965861320496\n",
      "          vf_loss: 0.22806331157684326\n",
      "    num_agent_steps_sampled: 40064000\n",
      "    num_agent_steps_trained: 40064000\n",
      "    num_steps_sampled: 10016000\n",
      "    num_steps_trained: 10016000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.821568627450983\n",
      "    gpu_util_percent0: 0.10617647058823532\n",
      "    ram_util_percent: 45.85980392156862\n",
      "    vram_util_percent0: 0.16255347153668967\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5666728539579813\n",
      "  policy_reward_mean:\n",
      "    default: -0.1346860576481306\n",
      "  policy_reward_min:\n",
      "    default: -1.915763604669532\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22300701321702587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.143457913405836\n",
      "    mean_inference_ms: 3.2642511820478233\n",
      "    mean_raw_obs_processing_ms: 0.9238597845602249\n",
      "  time_since_restore: 1962.6523370742798\n",
      "  time_this_iter_s: 77.21152830123901\n",
      "  time_total_s: 199732.93071436882\n",
      "  timers:\n",
      "    learn_throughput: 77.883\n",
      "    learn_time_ms: 51359.144\n",
      "    sample_throughput: 151.642\n",
      "    sample_time_ms: 26377.92\n",
      "  timestamp: 1638553110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10016000\n",
      "  training_iteration: 2504\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2504</td><td style=\"text-align: right;\">          199733</td><td style=\"text-align: right;\">10016000</td><td style=\"text-align: right;\">-0.538744</td><td style=\"text-align: right;\">            0.288318</td><td style=\"text-align: right;\">            -2.19946</td><td style=\"text-align: right;\">            131.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40080000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03539000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2960000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6065535531532584\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0017746620082111698\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6361968775430288\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 126.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.05289471748223118\n",
      "  episode_reward_mean: -0.5392487858858422\n",
      "  episode_reward_min: -2.199459452524616\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 66538\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6330689339637756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019160012535750864\n",
      "          policy_loss: -0.1151452667787671\n",
      "          total_loss: 0.20492659947276115\n",
      "          vf_explained_var: 0.303879052400589\n",
      "          vf_loss: 0.2909725991487503\n",
      "    num_agent_steps_sampled: 40080000\n",
      "    num_agent_steps_trained: 40080000\n",
      "    num_steps_sampled: 10020000\n",
      "    num_steps_trained: 10020000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.82135922330097\n",
      "    gpu_util_percent0: 0.10766990291262135\n",
      "    ram_util_percent: 46.10873786407767\n",
      "    vram_util_percent0: 0.16024688754923855\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5666728539579813\n",
      "  policy_reward_mean:\n",
      "    default: -0.13481219647146056\n",
      "  policy_reward_min:\n",
      "    default: -1.881511890833293\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2229100665799162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.13825030646134\n",
      "    mean_inference_ms: 3.261128986843819\n",
      "    mean_raw_obs_processing_ms: 0.9238654146514006\n",
      "  time_since_restore: 2040.1772408485413\n",
      "  time_this_iter_s: 77.52490377426147\n",
      "  time_total_s: 199810.45561814308\n",
      "  timers:\n",
      "    learn_throughput: 77.985\n",
      "    learn_time_ms: 51291.804\n",
      "    sample_throughput: 151.792\n",
      "    sample_time_ms: 26351.844\n",
      "  timestamp: 1638553187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10020000\n",
      "  training_iteration: 2505\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2505</td><td style=\"text-align: right;\">          199810</td><td style=\"text-align: right;\">10020000</td><td style=\"text-align: right;\">-0.539249</td><td style=\"text-align: right;\">           0.0528947</td><td style=\"text-align: right;\">            -2.19946</td><td style=\"text-align: right;\">            126.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40096000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.040180000000000035\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2960000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6065535531532584\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03501445646418347\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7492020982766776\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 142.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.05289471748223118\n",
      "  episode_reward_mean: -0.5872584037333999\n",
      "  episode_reward_min: -2.2248774704612004\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 66566\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6205497074127198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019550991244614124\n",
      "          policy_loss: -0.11697174386680126\n",
      "          total_loss: 0.20656540527939796\n",
      "          vf_explained_var: 0.3641904890537262\n",
      "          vf_loss: 0.29384408247470856\n",
      "    num_agent_steps_sampled: 40096000\n",
      "    num_agent_steps_trained: 40096000\n",
      "    num_steps_sampled: 10024000\n",
      "    num_steps_trained: 10024000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.131092436974786\n",
      "    gpu_util_percent0: 0.09252100840336137\n",
      "    ram_util_percent: 46.24285714285716\n",
      "    vram_util_percent0: 0.15943712134409532\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5643861699214032\n",
      "  policy_reward_mean:\n",
      "    default: -0.14681460093334997\n",
      "  policy_reward_min:\n",
      "    default: -2.0179235032447633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2228152873284606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.131414524900578\n",
      "    mean_inference_ms: 3.257913550471617\n",
      "    mean_raw_obs_processing_ms: 0.9233962288636056\n",
      "  time_since_restore: 2117.426207780838\n",
      "  time_this_iter_s: 77.24896693229675\n",
      "  time_total_s: 199887.70458507538\n",
      "  timers:\n",
      "    learn_throughput: 78.122\n",
      "    learn_time_ms: 51202.075\n",
      "    sample_throughput: 151.97\n",
      "    sample_time_ms: 26320.979\n",
      "  timestamp: 1638553277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10024000\n",
      "  training_iteration: 2506\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2506</td><td style=\"text-align: right;\">          199888</td><td style=\"text-align: right;\">10024000</td><td style=\"text-align: right;\">-0.587258</td><td style=\"text-align: right;\">           0.0528947</td><td style=\"text-align: right;\">            -2.22488</td><td style=\"text-align: right;\">            142.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40112000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.04722000000000004\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2960000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6065535531532584\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.10095889010793624\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7492020982766776\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-42-36\n",
      "  done: false\n",
      "  episode_len_mean: 160.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.05289471748223118\n",
      "  episode_reward_mean: -0.6457622428409928\n",
      "  episode_reward_min: -2.2248774704612004\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 66590\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6261135869026184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019615451343357564\n",
      "          policy_loss: -0.11718684513866902\n",
      "          total_loss: 0.1974333729222417\n",
      "          vf_explained_var: 0.3322334289550781\n",
      "          vf_loss: 0.2848292506933212\n",
      "    num_agent_steps_sampled: 40112000\n",
      "    num_agent_steps_trained: 40112000\n",
      "    num_steps_sampled: 10028000\n",
      "    num_steps_trained: 10028000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.486666666666665\n",
      "    gpu_util_percent0: 0.11038095238095239\n",
      "    ram_util_percent: 46.290476190476205\n",
      "    vram_util_percent0: 0.16804713329886084\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5643861699214032\n",
      "  policy_reward_mean:\n",
      "    default: -0.16144056071024815\n",
      "  policy_reward_min:\n",
      "    default: -2.0179235032447633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2227436172234799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.12955498365847\n",
      "    mean_inference_ms: 3.2553652696063398\n",
      "    mean_raw_obs_processing_ms: 0.9225813583586767\n",
      "  time_since_restore: 2196.389904499054\n",
      "  time_this_iter_s: 78.96369671821594\n",
      "  time_total_s: 199966.6682817936\n",
      "  timers:\n",
      "    learn_throughput: 78.189\n",
      "    learn_time_ms: 51157.856\n",
      "    sample_throughput: 151.793\n",
      "    sample_time_ms: 26351.662\n",
      "  timestamp: 1638553356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10028000\n",
      "  training_iteration: 2507\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2507</td><td style=\"text-align: right;\">          199967</td><td style=\"text-align: right;\">10028000</td><td style=\"text-align: right;\">-0.645762</td><td style=\"text-align: right;\">           0.0528947</td><td style=\"text-align: right;\">            -2.22488</td><td style=\"text-align: right;\">            160.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40128000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.037720000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2630000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6065535531532584\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.09781808605180627\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7492020982766776\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 139.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09857651622702246\n",
      "  episode_reward_mean: -0.5602206723798916\n",
      "  episode_reward_min: -2.2248774704612004\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 66627\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6224676671028138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01914529321342707\n",
      "          policy_loss: -0.11548202345520257\n",
      "          total_loss: 0.25982552337646486\n",
      "          vf_explained_var: 0.31632575392723083\n",
      "          vf_loss: 0.3462306363582611\n",
      "    num_agent_steps_sampled: 40128000\n",
      "    num_agent_steps_trained: 40128000\n",
      "    num_steps_sampled: 10032000\n",
      "    num_steps_trained: 10032000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.940384615384612\n",
      "    gpu_util_percent0: 0.10317307692307692\n",
      "    ram_util_percent: 46.32115384615386\n",
      "    vram_util_percent0: 0.16549124205836943\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5184616826572124\n",
      "  policy_reward_mean:\n",
      "    default: -0.14005516809497287\n",
      "  policy_reward_min:\n",
      "    default: -2.0179235032447633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22268606211230377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.132419350529073\n",
      "    mean_inference_ms: 3.253039152323138\n",
      "    mean_raw_obs_processing_ms: 0.9232737305396589\n",
      "  time_since_restore: 2274.939264535904\n",
      "  time_this_iter_s: 78.54936003684998\n",
      "  time_total_s: 200045.21764183044\n",
      "  timers:\n",
      "    learn_throughput: 78.094\n",
      "    learn_time_ms: 51220.478\n",
      "    sample_throughput: 151.591\n",
      "    sample_time_ms: 26386.842\n",
      "  timestamp: 1638553434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10032000\n",
      "  training_iteration: 2508\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2508</td><td style=\"text-align: right;\">          200045</td><td style=\"text-align: right;\">10032000</td><td style=\"text-align: right;\">-0.560221</td><td style=\"text-align: right;\">           0.0985765</td><td style=\"text-align: right;\">            -2.22488</td><td style=\"text-align: right;\">            139.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40144000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03533000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2980000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6482986017047074\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.035593212788625414\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7492020982766776\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-45-25\n",
      "  done: false\n",
      "  episode_len_mean: 130.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09857651622702246\n",
      "  episode_reward_mean: -0.5156694192701464\n",
      "  episode_reward_min: -2.2248774704612004\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 66662\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6358457732200623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019395124435424804\n",
      "          policy_loss: -0.11769042503833771\n",
      "          total_loss: 0.2302743330001831\n",
      "          vf_explained_var: 0.36625149846076965\n",
      "          vf_loss: 0.3185084116458893\n",
      "    num_agent_steps_sampled: 40144000\n",
      "    num_agent_steps_trained: 40144000\n",
      "    num_steps_sampled: 10036000\n",
      "    num_steps_trained: 10036000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.039669421487606\n",
      "    gpu_util_percent0: 0.08735537190082644\n",
      "    ram_util_percent: 46.392561983471076\n",
      "    vram_util_percent0: 0.16059273521357337\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5184616826572124\n",
      "  policy_reward_mean:\n",
      "    default: -0.1289173548175366\n",
      "  policy_reward_min:\n",
      "    default: -2.0179235032447633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22271846322173972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.141386196877566\n",
      "    mean_inference_ms: 3.2521836008127445\n",
      "    mean_raw_obs_processing_ms: 0.9254489758500705\n",
      "  time_since_restore: 2353.625886440277\n",
      "  time_this_iter_s: 78.68662190437317\n",
      "  time_total_s: 200123.90426373482\n",
      "  timers:\n",
      "    learn_throughput: 78.031\n",
      "    learn_time_ms: 51261.529\n",
      "    sample_throughput: 151.078\n",
      "    sample_time_ms: 26476.38\n",
      "  timestamp: 1638553525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10036000\n",
      "  training_iteration: 2509\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2509</td><td style=\"text-align: right;\">          200124</td><td style=\"text-align: right;\">10036000</td><td style=\"text-align: right;\">-0.515669</td><td style=\"text-align: right;\">           0.0985765</td><td style=\"text-align: right;\">            -2.22488</td><td style=\"text-align: right;\">            130.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40160000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.027210000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2980000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6482986017047074\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.006029594989708462\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6711470088283435\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 101.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09857651622702246\n",
      "  episode_reward_mean: -0.45236560904509415\n",
      "  episode_reward_min: -1.9968332817161387\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 66707\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6170732650756836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019994205474853515\n",
      "          policy_loss: -0.119460807710886\n",
      "          total_loss: 0.32902382481098175\n",
      "          vf_explained_var: 0.3561112582683563\n",
      "          vf_loss: 0.41811843633651735\n",
      "    num_agent_steps_sampled: 40160000\n",
      "    num_agent_steps_trained: 40160000\n",
      "    num_steps_sampled: 10040000\n",
      "    num_steps_trained: 10040000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.408490566037738\n",
      "    gpu_util_percent0: 0.11075471698113208\n",
      "    ram_util_percent: 46.370754716981125\n",
      "    vram_util_percent0: 0.16128691786647792\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5070212827305793\n",
      "  policy_reward_mean:\n",
      "    default: -0.11309140226127354\n",
      "  policy_reward_min:\n",
      "    default: -1.8656676294459826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22277249587615303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.150170625405313\n",
      "    mean_inference_ms: 3.2511967923233738\n",
      "    mean_raw_obs_processing_ms: 0.9286100671617732\n",
      "  time_since_restore: 2432.9471023082733\n",
      "  time_this_iter_s: 79.32121586799622\n",
      "  time_total_s: 200203.2254796028\n",
      "  timers:\n",
      "    learn_throughput: 77.805\n",
      "    learn_time_ms: 51410.599\n",
      "    sample_throughput: 150.78\n",
      "    sample_time_ms: 26528.798\n",
      "  timestamp: 1638553605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10040000\n",
      "  training_iteration: 2510\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2510</td><td style=\"text-align: right;\">          200203</td><td style=\"text-align: right;\">10040000</td><td style=\"text-align: right;\">-0.452366</td><td style=\"text-align: right;\">           0.0985765</td><td style=\"text-align: right;\">            -1.99683</td><td style=\"text-align: right;\">            101.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40176000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029480000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2980000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6482986017047074\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.030831284707123992\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6711470088283435\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 112.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.049440138090282915\n",
      "  episode_reward_mean: -0.4720570938316017\n",
      "  episode_reward_min: -1.9968332817161387\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 66738\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6357998905181885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01941908724606037\n",
      "          policy_loss: -0.11653127845749259\n",
      "          total_loss: 0.2798343698382378\n",
      "          vf_explained_var: 0.3192005753517151\n",
      "          vf_loss: 0.3668729090690613\n",
      "    num_agent_steps_sampled: 40176000\n",
      "    num_agent_steps_trained: 40176000\n",
      "    num_steps_sampled: 10044000\n",
      "    num_steps_trained: 10044000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.20970873786408\n",
      "    gpu_util_percent0: 0.1066990291262136\n",
      "    ram_util_percent: 46.3514563106796\n",
      "    vram_util_percent0: 0.1692959168351879\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5359343570536104\n",
      "  policy_reward_mean:\n",
      "    default: -0.11801427345790043\n",
      "  policy_reward_min:\n",
      "    default: -1.8656676294459826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22276753249194242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.153920762892504\n",
      "    mean_inference_ms: 3.250063675836651\n",
      "    mean_raw_obs_processing_ms: 0.9297085731277825\n",
      "  time_since_restore: 2510.535659313202\n",
      "  time_this_iter_s: 77.58855700492859\n",
      "  time_total_s: 200280.81403660774\n",
      "  timers:\n",
      "    learn_throughput: 77.81\n",
      "    learn_time_ms: 51407.381\n",
      "    sample_throughput: 150.753\n",
      "    sample_time_ms: 26533.468\n",
      "  timestamp: 1638553683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10044000\n",
      "  training_iteration: 2511\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2511</td><td style=\"text-align: right;\">          200281</td><td style=\"text-align: right;\">10044000</td><td style=\"text-align: right;\">-0.472057</td><td style=\"text-align: right;\">           0.0494401</td><td style=\"text-align: right;\">            -1.99683</td><td style=\"text-align: right;\">            112.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40192000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.026490000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6305108848986919\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.006055514512876683\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6711470088283435\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-49-32\n",
      "  done: false\n",
      "  episode_len_mean: 110.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1414422020535251\n",
      "  episode_reward_mean: -0.4552302036349009\n",
      "  episode_reward_min: -1.6333257455416046\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 66768\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.628933961391449\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019429420441389084\n",
      "          policy_loss: -0.11663461050391197\n",
      "          total_loss: 0.21779837802052499\n",
      "          vf_explained_var: 0.3554251194000244\n",
      "          vf_loss: 0.30492456018924713\n",
      "    num_agent_steps_sampled: 40192000\n",
      "    num_agent_steps_trained: 40192000\n",
      "    num_steps_sampled: 10048000\n",
      "    num_steps_trained: 10048000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.935294117647064\n",
      "    gpu_util_percent0: 0.0889915966386555\n",
      "    ram_util_percent: 46.5739495798319\n",
      "    vram_util_percent0: 0.16633761105626846\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5359343570536104\n",
      "  policy_reward_mean:\n",
      "    default: -0.11380755090872519\n",
      "  policy_reward_min:\n",
      "    default: -1.8656676294459826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22271713795277734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.15286617204898\n",
      "    mean_inference_ms: 3.2488742152129877\n",
      "    mean_raw_obs_processing_ms: 0.9294320087022726\n",
      "  time_since_restore: 2587.8774592876434\n",
      "  time_this_iter_s: 77.34179997444153\n",
      "  time_total_s: 200358.15583658218\n",
      "  timers:\n",
      "    learn_throughput: 77.833\n",
      "    learn_time_ms: 51392.177\n",
      "    sample_throughput: 150.687\n",
      "    sample_time_ms: 26545.049\n",
      "  timestamp: 1638553772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10048000\n",
      "  training_iteration: 2512\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2512</td><td style=\"text-align: right;\">          200358</td><td style=\"text-align: right;\">10048000</td><td style=\"text-align: right;\">-0.45523</td><td style=\"text-align: right;\">            0.141442</td><td style=\"text-align: right;\">            -1.63333</td><td style=\"text-align: right;\">            110.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40208000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029710000000000014\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6867680587360543\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0428471074902974\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6341489619409673\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-50-49\n",
      "  done: false\n",
      "  episode_len_mean: 124.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1414422020535251\n",
      "  episode_reward_mean: -0.4983705515698213\n",
      "  episode_reward_min: -1.8931473774307424\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 66802\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.518750000000001\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6287149286270142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020076404623687268\n",
      "          policy_loss: -0.11853102798759937\n",
      "          total_loss: 0.2782974508404732\n",
      "          vf_explained_var: 0.27249374985694885\n",
      "          vf_loss: 0.36633743572235106\n",
      "    num_agent_steps_sampled: 40208000\n",
      "    num_agent_steps_trained: 40208000\n",
      "    num_steps_sampled: 10052000\n",
      "    num_steps_trained: 10052000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.90686274509804\n",
      "    gpu_util_percent0: 0.10500000000000004\n",
      "    ram_util_percent: 46.46568627450982\n",
      "    vram_util_percent0: 0.16337611056268508\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5359343570536104\n",
      "  policy_reward_mean:\n",
      "    default: -0.12459263789245532\n",
      "  policy_reward_min:\n",
      "    default: -1.8584546548634346\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22264930496430535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.147078989143605\n",
      "    mean_inference_ms: 3.2473089256687153\n",
      "    mean_raw_obs_processing_ms: 0.9286427523971679\n",
      "  time_since_restore: 2665.1964139938354\n",
      "  time_this_iter_s: 77.31895470619202\n",
      "  time_total_s: 200435.47479128838\n",
      "  timers:\n",
      "    learn_throughput: 77.837\n",
      "    learn_time_ms: 51389.365\n",
      "    sample_throughput: 150.865\n",
      "    sample_time_ms: 26513.813\n",
      "  timestamp: 1638553849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10052000\n",
      "  training_iteration: 2513\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2513</td><td style=\"text-align: right;\">          200435</td><td style=\"text-align: right;\">10052000</td><td style=\"text-align: right;\">-0.498371</td><td style=\"text-align: right;\">            0.141442</td><td style=\"text-align: right;\">            -1.89315</td><td style=\"text-align: right;\">            124.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40224000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028520000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23200000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6867680587360543\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04257918326853292\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6698515059120933\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 117.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1414422020535251\n",
      "  episode_reward_mean: -0.4728228289572567\n",
      "  episode_reward_min: -1.8931473774307424\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 66835\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6191677508354188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013715079694986343\n",
      "          policy_loss: -0.10459829875826836\n",
      "          total_loss: 0.26355809411406517\n",
      "          vf_explained_var: 0.3218136429786682\n",
      "          vf_loss: 0.33691172432899474\n",
      "    num_agent_steps_sampled: 40224000\n",
      "    num_agent_steps_trained: 40224000\n",
      "    num_steps_sampled: 10056000\n",
      "    num_steps_trained: 10056000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.951376146788988\n",
      "    gpu_util_percent0: 0.13972477064220182\n",
      "    ram_util_percent: 46.733944954128425\n",
      "    vram_util_percent0: 0.16815345463108036\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7066448244039547\n",
      "  policy_reward_mean:\n",
      "    default: -0.11820570723931421\n",
      "  policy_reward_min:\n",
      "    default: -1.8584546548634346\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22265596041156477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.147139858366746\n",
      "    mean_inference_ms: 3.247981736028029\n",
      "    mean_raw_obs_processing_ms: 0.9280750905387863\n",
      "  time_since_restore: 2746.892378091812\n",
      "  time_this_iter_s: 81.69596409797668\n",
      "  time_total_s: 200517.17075538635\n",
      "  timers:\n",
      "    learn_throughput: 77.339\n",
      "    learn_time_ms: 51720.559\n",
      "    sample_throughput: 150.203\n",
      "    sample_time_ms: 26630.694\n",
      "  timestamp: 1638553931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10056000\n",
      "  training_iteration: 2514\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2514</td><td style=\"text-align: right;\">          200517</td><td style=\"text-align: right;\">10056000</td><td style=\"text-align: right;\">-0.472823</td><td style=\"text-align: right;\">            0.141442</td><td style=\"text-align: right;\">            -1.89315</td><td style=\"text-align: right;\">            117.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40240000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028930000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.1550000000000001\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6867680587360543\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.030123080664442806\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6698515059120933\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-53-41\n",
      "  done: false\n",
      "  episode_len_mean: 116.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2325060207295535\n",
      "  episode_reward_mean: -0.49028324062829015\n",
      "  episode_reward_min: -1.8931473774307424\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 66868\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6262412109375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013817265368998051\n",
      "          policy_loss: -0.10600638034194708\n",
      "          total_loss: 0.2609675117433071\n",
      "          vf_explained_var: 0.30869626998901367\n",
      "          vf_loss: 0.33549643158912656\n",
      "    num_agent_steps_sampled: 40240000\n",
      "    num_agent_steps_trained: 40240000\n",
      "    num_steps_sampled: 10060000\n",
      "    num_steps_trained: 10060000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.139495798319327\n",
      "    gpu_util_percent0: 0.09218487394957985\n",
      "    ram_util_percent: 46.71680672268907\n",
      "    vram_util_percent0: 0.16654776421921177\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7066448244039547\n",
      "  policy_reward_mean:\n",
      "    default: -0.12257081015707254\n",
      "  policy_reward_min:\n",
      "    default: -1.8584546548634346\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2226493101414946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.14691888567177\n",
      "    mean_inference_ms: 3.2479416291150778\n",
      "    mean_raw_obs_processing_ms: 0.9282084125482961\n",
      "  time_since_restore: 2824.3455233573914\n",
      "  time_this_iter_s: 77.45314526557922\n",
      "  time_total_s: 200594.62390065193\n",
      "  timers:\n",
      "    learn_throughput: 77.322\n",
      "    learn_time_ms: 51731.991\n",
      "    sample_throughput: 150.308\n",
      "    sample_time_ms: 26611.979\n",
      "  timestamp: 1638554021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10060000\n",
      "  training_iteration: 2515\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2515</td><td style=\"text-align: right;\">          200595</td><td style=\"text-align: right;\">10060000</td><td style=\"text-align: right;\">-0.490283</td><td style=\"text-align: right;\">            0.232506</td><td style=\"text-align: right;\">            -1.89315</td><td style=\"text-align: right;\">            116.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40256000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028400000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17800000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6867680587360543\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.024153191549186968\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7928248020875908\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 119.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2325060207295535\n",
      "  episode_reward_mean: -0.49015179372186163\n",
      "  episode_reward_min: -2.484311974468998\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 66896\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6334635720252991\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013616982564330101\n",
      "          policy_loss: -0.10628553561866283\n",
      "          total_loss: 0.21782755095511674\n",
      "          vf_explained_var: 0.35744985938072205\n",
      "          vf_loss: 0.2930918971300125\n",
      "    num_agent_steps_sampled: 40256000\n",
      "    num_agent_steps_trained: 40256000\n",
      "    num_steps_sampled: 10064000\n",
      "    num_steps_trained: 10064000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.902912621359228\n",
      "    gpu_util_percent0: 0.1067961165048544\n",
      "    ram_util_percent: 46.275728155339834\n",
      "    vram_util_percent0: 0.16401185878083308\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7066448244039547\n",
      "  policy_reward_mean:\n",
      "    default: -0.12253794843046541\n",
      "  policy_reward_min:\n",
      "    default: -1.922956038320301\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22264561937492455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.147119064362588\n",
      "    mean_inference_ms: 3.2491608150445392\n",
      "    mean_raw_obs_processing_ms: 0.9284230283000267\n",
      "  time_since_restore: 2901.90775847435\n",
      "  time_this_iter_s: 77.56223511695862\n",
      "  time_total_s: 200672.1861357689\n",
      "  timers:\n",
      "    learn_throughput: 77.302\n",
      "    learn_time_ms: 51745.054\n",
      "    sample_throughput: 150.205\n",
      "    sample_time_ms: 26630.288\n",
      "  timestamp: 1638554099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10064000\n",
      "  training_iteration: 2516\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2516</td><td style=\"text-align: right;\">          200672</td><td style=\"text-align: right;\">10064000</td><td style=\"text-align: right;\">-0.490152</td><td style=\"text-align: right;\">            0.232506</td><td style=\"text-align: right;\">            -2.48431</td><td style=\"text-align: right;\">            119.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40272000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030690000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17800000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6124236581994801\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05603528942755316\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7928248020875908\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 123.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2325060207295535\n",
      "  episode_reward_mean: -0.506969926825862\n",
      "  episode_reward_min: -2.484311974468998\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 66934\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.622818365573883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014158022500574588\n",
      "          policy_loss: -0.10839250458776951\n",
      "          total_loss: 0.2772978748083115\n",
      "          vf_explained_var: 0.3879207372665405\n",
      "          vf_loss: 0.3534366343021393\n",
      "    num_agent_steps_sampled: 40272000\n",
      "    num_agent_steps_trained: 40272000\n",
      "    num_steps_sampled: 10068000\n",
      "    num_steps_trained: 10068000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.25865384615384\n",
      "    gpu_util_percent0: 0.10759615384615384\n",
      "    ram_util_percent: 46.23846153846155\n",
      "    vram_util_percent0: 0.16370516617308326\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5477079641124476\n",
      "  policy_reward_mean:\n",
      "    default: -0.12674248170646546\n",
      "  policy_reward_min:\n",
      "    default: -1.922956038320301\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22258354710524245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.14179293966682\n",
      "    mean_inference_ms: 3.248760073892529\n",
      "    mean_raw_obs_processing_ms: 0.9291081995955827\n",
      "  time_since_restore: 2979.6199645996094\n",
      "  time_this_iter_s: 77.7122061252594\n",
      "  time_total_s: 200749.89834189415\n",
      "  timers:\n",
      "    learn_throughput: 77.423\n",
      "    learn_time_ms: 51664.34\n",
      "    sample_throughput: 150.458\n",
      "    sample_time_ms: 26585.576\n",
      "  timestamp: 1638554177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10068000\n",
      "  training_iteration: 2517\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2517</td><td style=\"text-align: right;\">          200750</td><td style=\"text-align: right;\">10068000</td><td style=\"text-align: right;\">-0.50697</td><td style=\"text-align: right;\">            0.232506</td><td style=\"text-align: right;\">            -2.48431</td><td style=\"text-align: right;\">            123.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40288000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03286000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18800000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7064167422843373\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05082473067671131\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7928248020875908\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 124.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07186439685722767\n",
      "  episode_reward_mean: -0.4898335673016848\n",
      "  episode_reward_min: -2.484311974468998\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 66966\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6331533231735229\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013851331904530525\n",
      "          policy_loss: -0.10580387277901172\n",
      "          total_loss: 0.28184326928853987\n",
      "          vf_explained_var: 0.3347041606903076\n",
      "          vf_loss: 0.35609207701683043\n",
      "    num_agent_steps_sampled: 40288000\n",
      "    num_agent_steps_trained: 40288000\n",
      "    num_steps_sampled: 10072000\n",
      "    num_steps_trained: 10072000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.905825242718453\n",
      "    gpu_util_percent0: 0.1046601941747573\n",
      "    ram_util_percent: 46.21941747572819\n",
      "    vram_util_percent0: 0.1637051661730833\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5521054500940095\n",
      "  policy_reward_mean:\n",
      "    default: -0.12245839182542126\n",
      "  policy_reward_min:\n",
      "    default: -1.922956038320301\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2225208552098647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.137690617388138\n",
      "    mean_inference_ms: 3.2479318523075835\n",
      "    mean_raw_obs_processing_ms: 0.9302115284901546\n",
      "  time_since_restore: 3057.0951561927795\n",
      "  time_this_iter_s: 77.47519159317017\n",
      "  time_total_s: 200827.37353348732\n",
      "  timers:\n",
      "    learn_throughput: 77.528\n",
      "    learn_time_ms: 51594.118\n",
      "    sample_throughput: 150.67\n",
      "    sample_time_ms: 26548.119\n",
      "  timestamp: 1638554254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10072000\n",
      "  training_iteration: 2518\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2518</td><td style=\"text-align: right;\">          200827</td><td style=\"text-align: right;\">10072000</td><td style=\"text-align: right;\">-0.489834</td><td style=\"text-align: right;\">           0.0718644</td><td style=\"text-align: right;\">            -2.48431</td><td style=\"text-align: right;\">            124.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40304000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03467000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20300000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7064167422843373\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.021666038523005922\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.652121011682947\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_14-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 123.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07186439685722767\n",
      "  episode_reward_mean: -0.48974109606242927\n",
      "  episode_reward_min: -1.7685464470159207\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 66996\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6286440625190735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014177182108163834\n",
      "          policy_loss: -0.10808664659410715\n",
      "          total_loss: 0.22602784202992915\n",
      "          vf_explained_var: 0.33488157391548157\n",
      "          vf_loss: 0.3018170952796936\n",
      "    num_agent_steps_sampled: 40304000\n",
      "    num_agent_steps_trained: 40304000\n",
      "    num_steps_sampled: 10076000\n",
      "    num_steps_trained: 10076000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.02521008403361\n",
      "    gpu_util_percent0: 0.09176470588235294\n",
      "    ram_util_percent: 46.780672268907566\n",
      "    vram_util_percent0: 0.16329177278018803\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5521054500940095\n",
      "  policy_reward_mean:\n",
      "    default: -0.12243527401560739\n",
      "  policy_reward_min:\n",
      "    default: -1.8544844639930202\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22245121023163572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.133758602893632\n",
      "    mean_inference_ms: 3.2454366407210187\n",
      "    mean_raw_obs_processing_ms: 0.9309249248763019\n",
      "  time_since_restore: 3134.376989364624\n",
      "  time_this_iter_s: 77.28183317184448\n",
      "  time_total_s: 200904.65536665916\n",
      "  timers:\n",
      "    learn_throughput: 77.607\n",
      "    learn_time_ms: 51541.741\n",
      "    sample_throughput: 151.171\n",
      "    sample_time_ms: 26460.049\n",
      "  timestamp: 1638554344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10076000\n",
      "  training_iteration: 2519\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2519</td><td style=\"text-align: right;\">          200905</td><td style=\"text-align: right;\">10076000</td><td style=\"text-align: right;\">-0.489741</td><td style=\"text-align: right;\">           0.0718644</td><td style=\"text-align: right;\">            -1.76855</td><td style=\"text-align: right;\">            123.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40320000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.04109000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8086478555864062\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.020462312001486578\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.685244552926725\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 133.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.04791453932345924\n",
      "  episode_reward_mean: -0.5403275509733999\n",
      "  episode_reward_min: -1.7685464470159207\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 67020\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6307186269760132\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013589151620864869\n",
      "          policy_loss: -0.10523818600177764\n",
      "          total_loss: 0.1803320437222719\n",
      "          vf_explained_var: 0.3769644796848297\n",
      "          vf_loss: 0.25461244094371793\n",
      "    num_agent_steps_sampled: 40320000\n",
      "    num_agent_steps_trained: 40320000\n",
      "    num_steps_sampled: 10080000\n",
      "    num_steps_trained: 10080000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.17184466019418\n",
      "    gpu_util_percent0: 0.10592233009708737\n",
      "    ram_util_percent: 46.84077669902912\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.578896023029884\n",
      "  policy_reward_mean:\n",
      "    default: -0.13508188774335\n",
      "  policy_reward_min:\n",
      "    default: -1.8758411018475631\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22241430820987396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.130608963229907\n",
      "    mean_inference_ms: 3.2434917940112418\n",
      "    mean_raw_obs_processing_ms: 0.9308852878947625\n",
      "  time_since_restore: 3211.996190071106\n",
      "  time_this_iter_s: 77.61920070648193\n",
      "  time_total_s: 200982.27456736565\n",
      "  timers:\n",
      "    learn_throughput: 77.803\n",
      "    learn_time_ms: 51412.142\n",
      "    sample_throughput: 151.394\n",
      "    sample_time_ms: 26421.079\n",
      "  timestamp: 1638554421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10080000\n",
      "  training_iteration: 2520\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2520</td><td style=\"text-align: right;\">          200982</td><td style=\"text-align: right;\">10080000</td><td style=\"text-align: right;\">-0.540328</td><td style=\"text-align: right;\">          -0.0479145</td><td style=\"text-align: right;\">            -1.76855</td><td style=\"text-align: right;\">            133.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40336000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.04567000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8086478555864062\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.005447363184424555\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.685244552926725\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-01-39\n",
      "  done: false\n",
      "  episode_len_mean: 139.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.04791453932345924\n",
      "  episode_reward_mean: -0.5672403722559235\n",
      "  episode_reward_min: -1.5897684213741143\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 67048\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6252320652008057\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013476924926042556\n",
      "          policy_loss: -0.10238231872022152\n",
      "          total_loss: 0.19796214124560357\n",
      "          vf_explained_var: 0.3278856873512268\n",
      "          vf_loss: 0.26964233791828157\n",
      "    num_agent_steps_sampled: 40336000\n",
      "    num_agent_steps_trained: 40336000\n",
      "    num_steps_sampled: 10084000\n",
      "    num_steps_trained: 10084000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.770873786407762\n",
      "    gpu_util_percent0: 0.10621359223300973\n",
      "    ram_util_percent: 46.79417475728154\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.578896023029884\n",
      "  policy_reward_mean:\n",
      "    default: -0.14181009306398093\n",
      "  policy_reward_min:\n",
      "    default: -1.8758411018475631\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22237264276439483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.126089711368923\n",
      "    mean_inference_ms: 3.24140116313455\n",
      "    mean_raw_obs_processing_ms: 0.930576887989969\n",
      "  time_since_restore: 3289.340378522873\n",
      "  time_this_iter_s: 77.34418845176697\n",
      "  time_total_s: 201059.6187558174\n",
      "  timers:\n",
      "    learn_throughput: 77.81\n",
      "    learn_time_ms: 51407.495\n",
      "    sample_throughput: 151.509\n",
      "    sample_time_ms: 26401.079\n",
      "  timestamp: 1638554499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10084000\n",
      "  training_iteration: 2521\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2521</td><td style=\"text-align: right;\">          201060</td><td style=\"text-align: right;\">10084000</td><td style=\"text-align: right;\">-0.56724</td><td style=\"text-align: right;\">          -0.0479145</td><td style=\"text-align: right;\">            -1.58977</td><td style=\"text-align: right;\">            139.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40352000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.044620000000000035\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8086478555864062\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.009963675095178738\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6883989923873239\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-03-08\n",
      "  done: false\n",
      "  episode_len_mean: 146.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.03985801701131697\n",
      "  episode_reward_mean: -0.5931051793781091\n",
      "  episode_reward_min: -1.654828530548842\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 67077\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6274507265090943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013499849528074264\n",
      "          policy_loss: -0.10490907916426659\n",
      "          total_loss: 0.24078012472391128\n",
      "          vf_explained_var: 0.292175829410553\n",
      "          vf_loss: 0.3149348579645157\n",
      "    num_agent_steps_sampled: 40352000\n",
      "    num_agent_steps_trained: 40352000\n",
      "    num_steps_sampled: 10088000\n",
      "    num_steps_trained: 10088000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.088983050847457\n",
      "    gpu_util_percent0: 0.09305084745762712\n",
      "    ram_util_percent: 47.07118644067796\n",
      "    vram_util_percent0: 0.1609486282842817\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.578896023029884\n",
      "  policy_reward_mean:\n",
      "    default: -0.1482762948445273\n",
      "  policy_reward_min:\n",
      "    default: -1.8758411018475631\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22233681177893616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.120524018343232\n",
      "    mean_inference_ms: 3.2398313488049983\n",
      "    mean_raw_obs_processing_ms: 0.9297977351642663\n",
      "  time_since_restore: 3366.616157054901\n",
      "  time_this_iter_s: 77.2757785320282\n",
      "  time_total_s: 201136.89453434944\n",
      "  timers:\n",
      "    learn_throughput: 77.8\n",
      "    learn_time_ms: 51413.873\n",
      "    sample_throughput: 151.582\n",
      "    sample_time_ms: 26388.312\n",
      "  timestamp: 1638554588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10088000\n",
      "  training_iteration: 2522\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2522</td><td style=\"text-align: right;\">          201137</td><td style=\"text-align: right;\">10088000</td><td style=\"text-align: right;\">-0.593105</td><td style=\"text-align: right;\">           -0.039858</td><td style=\"text-align: right;\">            -1.65483</td><td style=\"text-align: right;\">            146.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40368000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03928000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7392353772291149\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.012374937269230366\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6883989923873239\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 139.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.03985801701131697\n",
      "  episode_reward_mean: -0.5578524179454795\n",
      "  episode_reward_min: -1.654828530548842\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 67103\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.631346342086792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013720233984291553\n",
      "          policy_loss: -0.1044156259931624\n",
      "          total_loss: 0.21916045290231706\n",
      "          vf_explained_var: 0.3024067282676697\n",
      "          vf_loss: 0.2923196699619293\n",
      "    num_agent_steps_sampled: 40368000\n",
      "    num_agent_steps_trained: 40368000\n",
      "    num_steps_sampled: 10092000\n",
      "    num_steps_trained: 10092000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.889423076923077\n",
      "    gpu_util_percent0: 0.10567307692307691\n",
      "    ram_util_percent: 46.82788461538461\n",
      "    vram_util_percent0: 0.1609588174247602\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5500980695648212\n",
      "  policy_reward_mean:\n",
      "    default: -0.13946310448636987\n",
      "  policy_reward_min:\n",
      "    default: -1.8729611560593384\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2222929342724555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.11655881564393\n",
      "    mean_inference_ms: 3.23835347319\n",
      "    mean_raw_obs_processing_ms: 0.929284066172338\n",
      "  time_since_restore: 3444.410177707672\n",
      "  time_this_iter_s: 77.794020652771\n",
      "  time_total_s: 201214.6885550022\n",
      "  timers:\n",
      "    learn_throughput: 77.743\n",
      "    learn_time_ms: 51451.317\n",
      "    sample_throughput: 151.523\n",
      "    sample_time_ms: 26398.578\n",
      "  timestamp: 1638554666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10092000\n",
      "  training_iteration: 2523\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2523</td><td style=\"text-align: right;\">          201215</td><td style=\"text-align: right;\">10092000</td><td style=\"text-align: right;\">-0.557852</td><td style=\"text-align: right;\">           -0.039858</td><td style=\"text-align: right;\">            -1.65483</td><td style=\"text-align: right;\">             139.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40384000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03606000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17800000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7622884845408203\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0022589116368717417\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6883989923873239\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-05-44\n",
      "  done: false\n",
      "  episode_len_mean: 140.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.0393706594004013\n",
      "  episode_reward_mean: -0.5488802259420443\n",
      "  episode_reward_min: -2.2457777151420633\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 67136\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6277493915557861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014079654276371003\n",
      "          policy_loss: -0.10819778192043304\n",
      "          total_loss: 0.24023070061206817\n",
      "          vf_explained_var: 0.3901812732219696\n",
      "          vf_loss: 0.31635326528549196\n",
      "    num_agent_steps_sampled: 40384000\n",
      "    num_agent_steps_trained: 40384000\n",
      "    num_steps_sampled: 10096000\n",
      "    num_steps_trained: 10096000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.511650485436892\n",
      "    gpu_util_percent0: 0.10728155339805825\n",
      "    ram_util_percent: 46.53009708737861\n",
      "    vram_util_percent0: 0.1609449327033356\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5500980695648212\n",
      "  policy_reward_mean:\n",
      "    default: -0.13722005648551108\n",
      "  policy_reward_min:\n",
      "    default: -2.008526749301061\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22225387949863426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.112966761828325\n",
      "    mean_inference_ms: 3.2369931840365678\n",
      "    mean_raw_obs_processing_ms: 0.9292074605241684\n",
      "  time_since_restore: 3522.254837989807\n",
      "  time_this_iter_s: 77.84466028213501\n",
      "  time_total_s: 201292.53321528435\n",
      "  timers:\n",
      "    learn_throughput: 78.221\n",
      "    learn_time_ms: 51137.251\n",
      "    sample_throughput: 151.931\n",
      "    sample_time_ms: 26327.817\n",
      "  timestamp: 1638554744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10096000\n",
      "  training_iteration: 2524\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2524</td><td style=\"text-align: right;\">          201293</td><td style=\"text-align: right;\">10096000</td><td style=\"text-align: right;\">-0.54888</td><td style=\"text-align: right;\">          -0.0393707</td><td style=\"text-align: right;\">            -2.24578</td><td style=\"text-align: right;\">            140.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40400000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02983000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.16000000000000011\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7686795894578305\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.021680989352382895\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6883989923873239\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-07-13\n",
      "  done: false\n",
      "  episode_len_mean: 128.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07374251073843419\n",
      "  episode_reward_mean: -0.5078712926069966\n",
      "  episode_reward_min: -2.2457777151420633\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 67173\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6341569876670837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01374511356651783\n",
      "          policy_loss: -0.10611203865706921\n",
      "          total_loss: 0.28334208315610887\n",
      "          vf_explained_var: 0.31190571188926697\n",
      "          vf_loss: 0.35814103412628173\n",
      "    num_agent_steps_sampled: 40400000\n",
      "    num_agent_steps_trained: 40400000\n",
      "    num_steps_sampled: 10100000\n",
      "    num_steps_trained: 10100000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.15210084033613\n",
      "    gpu_util_percent0: 0.09134453781512605\n",
      "    ram_util_percent: 47.12689075630249\n",
      "    vram_util_percent0: 0.16092754969707523\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6151893680141476\n",
      "  policy_reward_mean:\n",
      "    default: -0.12696782315174915\n",
      "  policy_reward_min:\n",
      "    default: -2.008526749301061\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22219931337384416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.109199379210196\n",
      "    mean_inference_ms: 3.2351700081517185\n",
      "    mean_raw_obs_processing_ms: 0.9300048898290153\n",
      "  time_since_restore: 3599.4848413467407\n",
      "  time_this_iter_s: 77.2300033569336\n",
      "  time_total_s: 201369.76321864128\n",
      "  timers:\n",
      "    learn_throughput: 78.252\n",
      "    learn_time_ms: 51116.634\n",
      "    sample_throughput: 151.941\n",
      "    sample_time_ms: 26326.053\n",
      "  timestamp: 1638554833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10100000\n",
      "  training_iteration: 2525\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2525</td><td style=\"text-align: right;\">          201370</td><td style=\"text-align: right;\">10100000</td><td style=\"text-align: right;\">-0.507871</td><td style=\"text-align: right;\">           0.0737425</td><td style=\"text-align: right;\">            -2.24578</td><td style=\"text-align: right;\">            128.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40416000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02637000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7686795894578305\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.049683283430819236\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6623312035013311\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 106.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07374251073843419\n",
      "  episode_reward_mean: -0.4545878104714688\n",
      "  episode_reward_min: -1.9046420329629645\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 67213\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.62488605594635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013949409179389477\n",
      "          policy_loss: -0.10528023041784763\n",
      "          total_loss: 0.294569531083107\n",
      "          vf_explained_var: 0.3266911506652832\n",
      "          vf_loss: 0.36807126450538635\n",
      "    num_agent_steps_sampled: 40416000\n",
      "    num_agent_steps_trained: 40416000\n",
      "    num_steps_sampled: 10104000\n",
      "    num_steps_trained: 10104000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.732038834951457\n",
      "    gpu_util_percent0: 0.10291262135922333\n",
      "    ram_util_percent: 47.09126213592233\n",
      "    vram_util_percent0: 0.16094173798867156\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6151893680141476\n",
      "  policy_reward_mean:\n",
      "    default: -0.1136469526178672\n",
      "  policy_reward_min:\n",
      "    default: -1.8545636433553088\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2221421953022791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.10512708478577\n",
      "    mean_inference_ms: 3.2338922701806307\n",
      "    mean_raw_obs_processing_ms: 0.9310581006635738\n",
      "  time_since_restore: 3677.0631268024445\n",
      "  time_this_iter_s: 77.57828545570374\n",
      "  time_total_s: 201447.34150409698\n",
      "  timers:\n",
      "    learn_throughput: 78.244\n",
      "    learn_time_ms: 51121.81\n",
      "    sample_throughput: 151.963\n",
      "    sample_time_ms: 26322.12\n",
      "  timestamp: 1638554911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10104000\n",
      "  training_iteration: 2526\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2526</td><td style=\"text-align: right;\">          201447</td><td style=\"text-align: right;\">10104000</td><td style=\"text-align: right;\">-0.454588</td><td style=\"text-align: right;\">           0.0737425</td><td style=\"text-align: right;\">            -1.90464</td><td style=\"text-align: right;\">             106.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40432000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.024930000000000018\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7686795894578305\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10697606764537873\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6623312035013311\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 110.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.04622661648147286\n",
      "  episode_reward_mean: -0.45834588247556335\n",
      "  episode_reward_min: -1.5733233221054848\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 67246\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6351683773994445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013873824529349803\n",
      "          policy_loss: -0.1064853219911456\n",
      "          total_loss: 0.2443153973519802\n",
      "          vf_explained_var: 0.3523949384689331\n",
      "          vf_loss: 0.3191944100856781\n",
      "    num_agent_steps_sampled: 40432000\n",
      "    num_agent_steps_trained: 40432000\n",
      "    num_steps_sampled: 10108000\n",
      "    num_steps_trained: 10108000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.796116504854368\n",
      "    gpu_util_percent0: 0.10805825242718448\n",
      "    ram_util_percent: 47.048543689320354\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6151893680141476\n",
      "  policy_reward_mean:\n",
      "    default: -0.11458647061889081\n",
      "  policy_reward_min:\n",
      "    default: -1.8545636433553088\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22207876214380756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.101828060995786\n",
      "    mean_inference_ms: 3.2330874136019703\n",
      "    mean_raw_obs_processing_ms: 0.9314407154187397\n",
      "  time_since_restore: 3754.6543939113617\n",
      "  time_this_iter_s: 77.59126710891724\n",
      "  time_total_s: 201524.9327712059\n",
      "  timers:\n",
      "    learn_throughput: 78.246\n",
      "    learn_time_ms: 51120.767\n",
      "    sample_throughput: 152.025\n",
      "    sample_time_ms: 26311.385\n",
      "  timestamp: 1638554988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10108000\n",
      "  training_iteration: 2527\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2527</td><td style=\"text-align: right;\">          201525</td><td style=\"text-align: right;\">10108000</td><td style=\"text-align: right;\">-0.458346</td><td style=\"text-align: right;\">           0.0462266</td><td style=\"text-align: right;\">            -1.57332</td><td style=\"text-align: right;\">            110.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40448000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028480000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6970371252312397\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08749212134091072\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6330070672709618\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-11-17\n",
      "  done: false\n",
      "  episode_len_mean: 114.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1436165781122225\n",
      "  episode_reward_mean: -0.45929217130592564\n",
      "  episode_reward_min: -1.5733233221054848\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 67277\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6321105422973633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01375456453114748\n",
      "          policy_loss: -0.10474440064281225\n",
      "          total_loss: 0.25444435289502143\n",
      "          vf_explained_var: 0.3005405366420746\n",
      "          vf_loss: 0.3278541352748871\n",
      "    num_agent_steps_sampled: 40448000\n",
      "    num_agent_steps_trained: 40448000\n",
      "    num_steps_sampled: 10112000\n",
      "    num_steps_trained: 10112000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.608474576271185\n",
      "    gpu_util_percent0: 0.09279661016949153\n",
      "    ram_util_percent: 47.202542372881354\n",
      "    vram_util_percent0: 0.16094862828428172\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.584785764786846\n",
      "  policy_reward_mean:\n",
      "    default: -0.11482304282648138\n",
      "  policy_reward_min:\n",
      "    default: -1.846853131375572\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22202454977136024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.098409062104793\n",
      "    mean_inference_ms: 3.2324162790061037\n",
      "    mean_raw_obs_processing_ms: 0.9313805250061616\n",
      "  time_since_restore: 3831.905559539795\n",
      "  time_this_iter_s: 77.25116562843323\n",
      "  time_total_s: 201602.18393683434\n",
      "  timers:\n",
      "    learn_throughput: 78.247\n",
      "    learn_time_ms: 51119.991\n",
      "    sample_throughput: 152.149\n",
      "    sample_time_ms: 26289.938\n",
      "  timestamp: 1638555077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10112000\n",
      "  training_iteration: 2528\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2528</td><td style=\"text-align: right;\">          201602</td><td style=\"text-align: right;\">10112000</td><td style=\"text-align: right;\">-0.459292</td><td style=\"text-align: right;\">            0.143617</td><td style=\"text-align: right;\">            -1.57332</td><td style=\"text-align: right;\">            114.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40464000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02735000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.16000000000000011\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6870430998261668\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.047908577557265755\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6993572468303935\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 117.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1436165781122225\n",
      "  episode_reward_mean: -0.44469281120701704\n",
      "  episode_reward_min: -1.5733233221054848\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 67315\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6181661112308502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014097873978316784\n",
      "          policy_loss: -0.10602770029008389\n",
      "          total_loss: 0.31546612572669985\n",
      "          vf_explained_var: 0.3091035485267639\n",
      "          vf_loss: 0.389377105474472\n",
      "    num_agent_steps_sampled: 40464000\n",
      "    num_agent_steps_trained: 40464000\n",
      "    num_steps_sampled: 10116000\n",
      "    num_steps_trained: 10116000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.959615384615386\n",
      "    gpu_util_percent0: 0.10451923076923078\n",
      "    ram_util_percent: 47.13750000000001\n",
      "    vram_util_percent0: 0.16093034145847573\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.54511701789328\n",
      "  policy_reward_mean:\n",
      "    default: -0.11117320280175426\n",
      "  policy_reward_min:\n",
      "    default: -1.846853131375572\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22197149116136763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.09420208254136\n",
      "    mean_inference_ms: 3.2323181587350884\n",
      "    mean_raw_obs_processing_ms: 0.9312380061774445\n",
      "  time_since_restore: 3909.6356143951416\n",
      "  time_this_iter_s: 77.73005485534668\n",
      "  time_total_s: 201679.91399168968\n",
      "  timers:\n",
      "    learn_throughput: 78.228\n",
      "    learn_time_ms: 51132.437\n",
      "    sample_throughput: 151.963\n",
      "    sample_time_ms: 26322.121\n",
      "  timestamp: 1638555155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10116000\n",
      "  training_iteration: 2529\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2529</td><td style=\"text-align: right;\">          201680</td><td style=\"text-align: right;\">10116000</td><td style=\"text-align: right;\">-0.444693</td><td style=\"text-align: right;\">            0.143617</td><td style=\"text-align: right;\">            -1.57332</td><td style=\"text-align: right;\">            117.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40480000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03063000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2700000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6870430998261668\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.010700958380566514\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6993572468303935\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 119.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1436165781122225\n",
      "  episode_reward_mean: -0.47081964383527863\n",
      "  episode_reward_min: -2.2353603211846638\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 67349\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.629869463443756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013885592848062515\n",
      "          policy_loss: -0.10650275584310293\n",
      "          total_loss: 0.27443409159779547\n",
      "          vf_explained_var: 0.3045595586299896\n",
      "          vf_loss: 0.3493037314414978\n",
      "    num_agent_steps_sampled: 40480000\n",
      "    num_agent_steps_trained: 40480000\n",
      "    num_steps_sampled: 10120000\n",
      "    num_steps_trained: 10120000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.91176470588235\n",
      "    gpu_util_percent0: 0.10754901960784316\n",
      "    ram_util_percent: 47.23137254901964\n",
      "    vram_util_percent0: 0.1609307757324714\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5772889638760543\n",
      "  policy_reward_mean:\n",
      "    default: -0.11770491095881966\n",
      "  policy_reward_min:\n",
      "    default: -1.8486873895795872\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22191173395908045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.089736117448957\n",
      "    mean_inference_ms: 3.231439549473633\n",
      "    mean_raw_obs_processing_ms: 0.9317684009586072\n",
      "  time_since_restore: 3986.985599040985\n",
      "  time_this_iter_s: 77.3499846458435\n",
      "  time_total_s: 201757.26397633553\n",
      "  timers:\n",
      "    learn_throughput: 78.246\n",
      "    learn_time_ms: 51120.81\n",
      "    sample_throughput: 152.052\n",
      "    sample_time_ms: 26306.752\n",
      "  timestamp: 1638555233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10120000\n",
      "  training_iteration: 2530\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2530</td><td style=\"text-align: right;\">          201757</td><td style=\"text-align: right;\">10120000</td><td style=\"text-align: right;\">-0.47082</td><td style=\"text-align: right;\">            0.143617</td><td style=\"text-align: right;\">            -2.23536</td><td style=\"text-align: right;\">            119.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40496000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02704000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2700000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7360035649182691\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03845205198285587\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7123455836807744\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 114.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08052636777951605\n",
      "  episode_reward_mean: -0.4714226335372503\n",
      "  episode_reward_min: -2.2353603211846638\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 67381\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6331039519309998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014317723833024501\n",
      "          policy_loss: -0.10921685383468867\n",
      "          total_loss: 0.3260279663205147\n",
      "          vf_explained_var: 0.34431177377700806\n",
      "          vf_loss: 0.4026272559165955\n",
      "    num_agent_steps_sampled: 40496000\n",
      "    num_agent_steps_trained: 40496000\n",
      "    num_steps_sampled: 10124000\n",
      "    num_steps_trained: 10124000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.142500000000002\n",
      "    gpu_util_percent0: 0.09191666666666666\n",
      "    ram_util_percent: 47.436666666666675\n",
      "    vram_util_percent0: 0.16094658330591205\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.589975232379973\n",
      "  policy_reward_mean:\n",
      "    default: -0.11785565838431257\n",
      "  policy_reward_min:\n",
      "    default: -1.8716810059175566\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22186467535173446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.086570437405395\n",
      "    mean_inference_ms: 3.2304574020656265\n",
      "    mean_raw_obs_processing_ms: 0.932232580899305\n",
      "  time_since_restore: 4064.4263141155243\n",
      "  time_this_iter_s: 77.44071507453918\n",
      "  time_total_s: 201834.70469141006\n",
      "  timers:\n",
      "    learn_throughput: 78.226\n",
      "    learn_time_ms: 51134.114\n",
      "    sample_throughput: 152.074\n",
      "    sample_time_ms: 26302.9\n",
      "  timestamp: 1638555322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10124000\n",
      "  training_iteration: 2531\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2531</td><td style=\"text-align: right;\">          201835</td><td style=\"text-align: right;\">10124000</td><td style=\"text-align: right;\">-0.471423</td><td style=\"text-align: right;\">           0.0805264</td><td style=\"text-align: right;\">            -2.23536</td><td style=\"text-align: right;\">            114.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40512000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02925000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2700000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7360035649182691\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.009351462094752686\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7198368268851357\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 119.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08052636777951605\n",
      "  episode_reward_mean: -0.5033482724706445\n",
      "  episode_reward_min: -2.2353603211846638\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 67412\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6324297232627869\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013639144197106361\n",
      "          policy_loss: -0.10470853419601918\n",
      "          total_loss: 0.257129485219717\n",
      "          vf_explained_var: 0.29614585638046265\n",
      "          vf_loss: 0.33076634240150454\n",
      "    num_agent_steps_sampled: 40512000\n",
      "    num_agent_steps_trained: 40512000\n",
      "    num_steps_sampled: 10128000\n",
      "    num_steps_trained: 10128000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.824271844660192\n",
      "    gpu_util_percent0: 0.10660194174757281\n",
      "    ram_util_percent: 47.28252427184464\n",
      "    vram_util_percent0: 0.16093055648734733\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.589975232379973\n",
      "  policy_reward_mean:\n",
      "    default: -0.12583706811766113\n",
      "  policy_reward_min:\n",
      "    default: -1.9367223866955552\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22182793674554346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.083966957297335\n",
      "    mean_inference_ms: 3.2296202943105357\n",
      "    mean_raw_obs_processing_ms: 0.9325150756545648\n",
      "  time_since_restore: 4141.981087446213\n",
      "  time_this_iter_s: 77.55477333068848\n",
      "  time_total_s: 201912.25946474075\n",
      "  timers:\n",
      "    learn_throughput: 78.239\n",
      "    learn_time_ms: 51125.182\n",
      "    sample_throughput: 151.862\n",
      "    sample_time_ms: 26339.626\n",
      "  timestamp: 1638555400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10128000\n",
      "  training_iteration: 2532\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2532</td><td style=\"text-align: right;\">          201912</td><td style=\"text-align: right;\">10128000</td><td style=\"text-align: right;\">-0.503348</td><td style=\"text-align: right;\">           0.0805264</td><td style=\"text-align: right;\">            -2.23536</td><td style=\"text-align: right;\">            119.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40528000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030710000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7360035649182691\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03325334231565175\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7198368268851357\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-17-58\n",
      "  done: false\n",
      "  episode_len_mean: 125.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08052636777951605\n",
      "  episode_reward_mean: -0.5168922084351618\n",
      "  episode_reward_min: -1.5738063250330363\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 67443\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6258925476074219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01339241798222065\n",
      "          policy_loss: -0.10236402575671673\n",
      "          total_loss: 0.2447123721241951\n",
      "          vf_explained_var: 0.2950729727745056\n",
      "          vf_loss: 0.3165667952299118\n",
      "    num_agent_steps_sampled: 40528000\n",
      "    num_agent_steps_trained: 40528000\n",
      "    num_steps_sampled: 10132000\n",
      "    num_steps_trained: 10132000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.834951456310677\n",
      "    gpu_util_percent0: 0.10650485436893206\n",
      "    ram_util_percent: 47.36601941747573\n",
      "    vram_util_percent0: 0.16094173798867156\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.589975232379973\n",
      "  policy_reward_mean:\n",
      "    default: -0.12922305210879048\n",
      "  policy_reward_min:\n",
      "    default: -1.9367223866955552\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22182564982157393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.081785641696296\n",
      "    mean_inference_ms: 3.2297097270252664\n",
      "    mean_raw_obs_processing_ms: 0.9323115426002326\n",
      "  time_since_restore: 4219.5146453380585\n",
      "  time_this_iter_s: 77.5335578918457\n",
      "  time_total_s: 201989.7930226326\n",
      "  timers:\n",
      "    learn_throughput: 78.286\n",
      "    learn_time_ms: 51094.425\n",
      "    sample_throughput: 151.829\n",
      "    sample_time_ms: 26345.382\n",
      "  timestamp: 1638555478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10132000\n",
      "  training_iteration: 2533\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2533</td><td style=\"text-align: right;\">          201990</td><td style=\"text-align: right;\">10132000</td><td style=\"text-align: right;\">-0.516892</td><td style=\"text-align: right;\">           0.0805264</td><td style=\"text-align: right;\">            -1.57381</td><td style=\"text-align: right;\">            125.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40544000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.034710000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6949357124820041\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05739441307657362\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7198368268851357\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 128.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.03483822997232178\n",
      "  episode_reward_mean: -0.5325627468720281\n",
      "  episode_reward_min: -1.6510785732240765\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 67473\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6371247224807739\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013644057556986808\n",
      "          policy_loss: -0.10541863436996937\n",
      "          total_loss: 0.23301277115941046\n",
      "          vf_explained_var: 0.2795788645744324\n",
      "          vf_loss: 0.30734853637218473\n",
      "    num_agent_steps_sampled: 40544000\n",
      "    num_agent_steps_trained: 40544000\n",
      "    num_steps_sampled: 10136000\n",
      "    num_steps_trained: 10136000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.77583333333333\n",
      "    gpu_util_percent0: 0.091\n",
      "    ram_util_percent: 47.56583333333333\n",
      "    vram_util_percent0: 0.1609383569156521\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5524728839845037\n",
      "  policy_reward_mean:\n",
      "    default: -0.13314068671800702\n",
      "  policy_reward_min:\n",
      "    default: -1.9367223866955552\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22181307885109533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.079600707769387\n",
      "    mean_inference_ms: 3.2300128746946695\n",
      "    mean_raw_obs_processing_ms: 0.9318334556693767\n",
      "  time_since_restore: 4296.707097291946\n",
      "  time_this_iter_s: 77.19245195388794\n",
      "  time_total_s: 202066.9854745865\n",
      "  timers:\n",
      "    learn_throughput: 78.325\n",
      "    learn_time_ms: 51069.012\n",
      "    sample_throughput: 152.06\n",
      "    sample_time_ms: 26305.388\n",
      "  timestamp: 1638555568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10136000\n",
      "  training_iteration: 2534\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2534</td><td style=\"text-align: right;\">          202067</td><td style=\"text-align: right;\">10136000</td><td style=\"text-align: right;\">-0.532563</td><td style=\"text-align: right;\">          -0.0348382</td><td style=\"text-align: right;\">            -1.65108</td><td style=\"text-align: right;\">            128.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40560000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.037880000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21500000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6949357124820041\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08236210326730171\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7137168497988988\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 127.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.008705578311156259\n",
      "  episode_reward_mean: -0.5308877575809577\n",
      "  episode_reward_min: -1.6510785732240765\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 67508\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.623937240600586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014094227708876132\n",
      "          policy_loss: -0.10685402941331268\n",
      "          total_loss: 0.296142530143261\n",
      "          vf_explained_var: 0.3118821680545807\n",
      "          vf_loss: 0.3708881471157074\n",
      "    num_agent_steps_sampled: 40560000\n",
      "    num_agent_steps_trained: 40560000\n",
      "    num_steps_sampled: 10140000\n",
      "    num_steps_trained: 10140000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.037864077669898\n",
      "    gpu_util_percent0: 0.10446601941747574\n",
      "    ram_util_percent: 47.43398058252427\n",
      "    vram_util_percent0: 0.1609433353460036\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5524728839845037\n",
      "  policy_reward_mean:\n",
      "    default: -0.13272193939523944\n",
      "  policy_reward_min:\n",
      "    default: -1.903523674879838\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22179198920437904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.077425030447884\n",
      "    mean_inference_ms: 3.229952178802043\n",
      "    mean_raw_obs_processing_ms: 0.9314087464999838\n",
      "  time_since_restore: 4374.484701871872\n",
      "  time_this_iter_s: 77.77760457992554\n",
      "  time_total_s: 202144.7630791664\n",
      "  timers:\n",
      "    learn_throughput: 78.282\n",
      "    learn_time_ms: 51097.433\n",
      "    sample_throughput: 151.908\n",
      "    sample_time_ms: 26331.684\n",
      "  timestamp: 1638555645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10140000\n",
      "  training_iteration: 2535\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2535</td><td style=\"text-align: right;\">          202145</td><td style=\"text-align: right;\">10140000</td><td style=\"text-align: right;\">-0.530888</td><td style=\"text-align: right;\">          0.00870558</td><td style=\"text-align: right;\">            -1.65108</td><td style=\"text-align: right;\">            127.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40576000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029050000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19000000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6034462738245409\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08356230911509459\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6706716886029352\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-22-03\n",
      "  done: false\n",
      "  episode_len_mean: 107.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.008705578311156259\n",
      "  episode_reward_mean: -0.4617448070667126\n",
      "  episode_reward_min: -1.6510785732240765\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 67546\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6305715527534484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013802761919796467\n",
      "          policy_loss: -0.10591119047999382\n",
      "          total_loss: 0.25890662634372713\n",
      "          vf_explained_var: 0.3133574426174164\n",
      "          vf_loss: 0.3333733983039856\n",
      "    num_agent_steps_sampled: 40576000\n",
      "    num_agent_steps_trained: 40576000\n",
      "    num_steps_sampled: 10144000\n",
      "    num_steps_trained: 10144000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.321359223300966\n",
      "    gpu_util_percent0: 0.10368932038834953\n",
      "    ram_util_percent: 47.4233009708738\n",
      "    vram_util_percent0: 0.16091937498602316\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5334555162769763\n",
      "  policy_reward_mean:\n",
      "    default: -0.11543620176667815\n",
      "  policy_reward_min:\n",
      "    default: -1.8297670954018057\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22176444285289332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.076607753387133\n",
      "    mean_inference_ms: 3.2296302317681636\n",
      "    mean_raw_obs_processing_ms: 0.93133721225476\n",
      "  time_since_restore: 4452.2827134132385\n",
      "  time_this_iter_s: 77.79801154136658\n",
      "  time_total_s: 202222.56109070778\n",
      "  timers:\n",
      "    learn_throughput: 78.29\n",
      "    learn_time_ms: 51092.258\n",
      "    sample_throughput: 151.751\n",
      "    sample_time_ms: 26359.053\n",
      "  timestamp: 1638555723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10144000\n",
      "  training_iteration: 2536\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2536</td><td style=\"text-align: right;\">          202223</td><td style=\"text-align: right;\">10144000</td><td style=\"text-align: right;\">-0.461745</td><td style=\"text-align: right;\">          0.00870558</td><td style=\"text-align: right;\">            -1.65108</td><td style=\"text-align: right;\">            107.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40592000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03728000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2860000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6034462738245409\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05416295802125468\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6706716886029352\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-23-21\n",
      "  done: false\n",
      "  episode_len_mean: 111.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03568754536768859\n",
      "  episode_reward_mean: -0.47341879811603893\n",
      "  episode_reward_min: -1.739326016106913\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 67578\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6315904693603516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013742250755429269\n",
      "          policy_loss: -0.10444441529363394\n",
      "          total_loss: 0.22175531193614006\n",
      "          vf_explained_var: 0.34287333488464355\n",
      "          vf_loss: 0.2948931641578674\n",
      "    num_agent_steps_sampled: 40592000\n",
      "    num_agent_steps_trained: 40592000\n",
      "    num_steps_sampled: 10148000\n",
      "    num_steps_trained: 10148000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.907766990291268\n",
      "    gpu_util_percent0: 0.10563106796116506\n",
      "    ram_util_percent: 47.53689320388346\n",
      "    vram_util_percent0: 0.16094014063133957\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5334555162769763\n",
      "  policy_reward_mean:\n",
      "    default: -0.11835469952900976\n",
      "  policy_reward_min:\n",
      "    default: -1.8126648285049742\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22174759787547843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.076548418662952\n",
      "    mean_inference_ms: 3.2291430385281514\n",
      "    mean_raw_obs_processing_ms: 0.9313141710488574\n",
      "  time_since_restore: 4529.643810510635\n",
      "  time_this_iter_s: 77.36109709739685\n",
      "  time_total_s: 202299.92218780518\n",
      "  timers:\n",
      "    learn_throughput: 78.304\n",
      "    learn_time_ms: 51082.979\n",
      "    sample_throughput: 151.831\n",
      "    sample_time_ms: 26345.094\n",
      "  timestamp: 1638555801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10148000\n",
      "  training_iteration: 2537\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2537</td><td style=\"text-align: right;\">          202300</td><td style=\"text-align: right;\">10148000</td><td style=\"text-align: right;\">-0.473419</td><td style=\"text-align: right;\">           0.0356875</td><td style=\"text-align: right;\">            -1.73933</td><td style=\"text-align: right;\">            111.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40608000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.038580000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3020000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7600304883803279\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.020399112719704928\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6706716886029352\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 117.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03568754536768859\n",
      "  episode_reward_mean: -0.493926034996968\n",
      "  episode_reward_min: -2.0506979777917005\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 67608\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6226493377685547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013678289435803891\n",
      "          policy_loss: -0.10393513264507055\n",
      "          total_loss: 0.2582496259957552\n",
      "          vf_explained_var: 0.2937830686569214\n",
      "          vf_loss: 0.3310239036083221\n",
      "    num_agent_steps_sampled: 40608000\n",
      "    num_agent_steps_trained: 40608000\n",
      "    num_steps_sampled: 10152000\n",
      "    num_steps_trained: 10152000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.197478991596643\n",
      "    gpu_util_percent0: 0.09176470588235296\n",
      "    ram_util_percent: 47.42521008403363\n",
      "    vram_util_percent0: 0.16094137556305835\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.560161301725925\n",
      "  policy_reward_mean:\n",
      "    default: -0.12348150874924205\n",
      "  policy_reward_min:\n",
      "    default: -1.8706967240755659\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22172212269654396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.076071720045373\n",
      "    mean_inference_ms: 3.2285143458155887\n",
      "    mean_raw_obs_processing_ms: 0.9311731929607387\n",
      "  time_since_restore: 4606.920064210892\n",
      "  time_this_iter_s: 77.27625370025635\n",
      "  time_total_s: 202377.19844150543\n",
      "  timers:\n",
      "    learn_throughput: 78.327\n",
      "    learn_time_ms: 51068.145\n",
      "    sample_throughput: 151.731\n",
      "    sample_time_ms: 26362.408\n",
      "  timestamp: 1638555890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10152000\n",
      "  training_iteration: 2538\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2538</td><td style=\"text-align: right;\">          202377</td><td style=\"text-align: right;\">10152000</td><td style=\"text-align: right;\">-0.493926</td><td style=\"text-align: right;\">           0.0356875</td><td style=\"text-align: right;\">             -2.0507</td><td style=\"text-align: right;\">            117.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40624000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.043160000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3020000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7600304883803279\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0199378860617468\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6415157412223105\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 127.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03568754536768859\n",
      "  episode_reward_mean: -0.5197588597817554\n",
      "  episode_reward_min: -2.0506979777917005\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 67642\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6280494165420533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013810713425278663\n",
      "          policy_loss: -0.10531323222070932\n",
      "          total_loss: 0.26327038833498956\n",
      "          vf_explained_var: 0.34021860361099243\n",
      "          vf_loss: 0.33712108731269835\n",
      "    num_agent_steps_sampled: 40624000\n",
      "    num_agent_steps_trained: 40624000\n",
      "    num_steps_sampled: 10156000\n",
      "    num_steps_trained: 10156000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.93592233009709\n",
      "    gpu_util_percent0: 0.10553398058252429\n",
      "    ram_util_percent: 47.18737864077671\n",
      "    vram_util_percent0: 0.16093055648734733\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.560161301725925\n",
      "  policy_reward_mean:\n",
      "    default: -0.12993971494543888\n",
      "  policy_reward_min:\n",
      "    default: -1.8706967240755659\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22167827647571575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.074551946287656\n",
      "    mean_inference_ms: 3.2273103684193676\n",
      "    mean_raw_obs_processing_ms: 0.9308940823539956\n",
      "  time_since_restore: 4684.350630998611\n",
      "  time_this_iter_s: 77.43056678771973\n",
      "  time_total_s: 202454.62900829315\n",
      "  timers:\n",
      "    learn_throughput: 78.364\n",
      "    learn_time_ms: 51043.835\n",
      "    sample_throughput: 151.763\n",
      "    sample_time_ms: 26356.948\n",
      "  timestamp: 1638555968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10156000\n",
      "  training_iteration: 2539\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2539</td><td style=\"text-align: right;\">          202455</td><td style=\"text-align: right;\">10156000</td><td style=\"text-align: right;\">-0.519759</td><td style=\"text-align: right;\">           0.0356875</td><td style=\"text-align: right;\">             -2.0507</td><td style=\"text-align: right;\">            127.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40640000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03574000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3020000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7600304883803279\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0035473697758201606\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8095084796902062\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 126.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.014120378018347424\n",
      "  episode_reward_mean: -0.5165195966928852\n",
      "  episode_reward_min: -2.0506979777917005\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 67676\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6250059323310853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013890436418354512\n",
      "          policy_loss: -0.10706603049486875\n",
      "          total_loss: 0.23456397733092307\n",
      "          vf_explained_var: 0.37776443362236023\n",
      "          vf_loss: 0.309985853433609\n",
      "    num_agent_steps_sampled: 40640000\n",
      "    num_agent_steps_trained: 40640000\n",
      "    num_steps_sampled: 10160000\n",
      "    num_steps_trained: 10160000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.86019417475729\n",
      "    gpu_util_percent0: 0.10844660194174757\n",
      "    ram_util_percent: 47.195145631067994\n",
      "    vram_util_percent0: 0.16093694591667546\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5929500348491672\n",
      "  policy_reward_mean:\n",
      "    default: -0.12912989917322132\n",
      "  policy_reward_min:\n",
      "    default: -1.9435850552892153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22163709761059003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.073449687978883\n",
      "    mean_inference_ms: 3.226474026115893\n",
      "    mean_raw_obs_processing_ms: 0.9309354417526524\n",
      "  time_since_restore: 4761.875724315643\n",
      "  time_this_iter_s: 77.52509331703186\n",
      "  time_total_s: 202532.15410161018\n",
      "  timers:\n",
      "    learn_throughput: 78.374\n",
      "    learn_time_ms: 51037.276\n",
      "    sample_throughput: 151.624\n",
      "    sample_time_ms: 26381.015\n",
      "  timestamp: 1638556045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10160000\n",
      "  training_iteration: 2540\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2540</td><td style=\"text-align: right;\">          202532</td><td style=\"text-align: right;\">10160000</td><td style=\"text-align: right;\">-0.51652</td><td style=\"text-align: right;\">          -0.0141204</td><td style=\"text-align: right;\">             -2.0507</td><td style=\"text-align: right;\">            126.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40656000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03496000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5983522731227479\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.012956660392222864\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8095084796902062\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 124.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.014120378018347424\n",
      "  episode_reward_mean: -0.5085773633819867\n",
      "  episode_reward_min: -1.6865654310985594\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 67704\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6281626625061035\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01377018814533949\n",
      "          policy_loss: -0.10587225403636694\n",
      "          total_loss: 0.2677780623435974\n",
      "          vf_explained_var: 0.2985825538635254\n",
      "          vf_loss: 0.34228010654449464\n",
      "    num_agent_steps_sampled: 40656000\n",
      "    num_agent_steps_trained: 40656000\n",
      "    num_steps_sampled: 10164000\n",
      "    num_steps_trained: 10164000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.188235294117643\n",
      "    gpu_util_percent0: 0.09218487394957983\n",
      "    ram_util_percent: 47.75294117647059\n",
      "    vram_util_percent0: 0.16093722780326342\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5929500348491672\n",
      "  policy_reward_mean:\n",
      "    default: -0.1271443408454967\n",
      "  policy_reward_min:\n",
      "    default: -1.9435850552892153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22161491075298312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.071930521593796\n",
      "    mean_inference_ms: 3.2259367472648353\n",
      "    mean_raw_obs_processing_ms: 0.9310308639063698\n",
      "  time_since_restore: 4839.248012542725\n",
      "  time_this_iter_s: 77.3722882270813\n",
      "  time_total_s: 202609.52638983727\n",
      "  timers:\n",
      "    learn_throughput: 78.375\n",
      "    learn_time_ms: 51036.984\n",
      "    sample_throughput: 151.66\n",
      "    sample_time_ms: 26374.764\n",
      "  timestamp: 1638556135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10164000\n",
      "  training_iteration: 2541\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2541</td><td style=\"text-align: right;\">          202610</td><td style=\"text-align: right;\">10164000</td><td style=\"text-align: right;\">-0.508577</td><td style=\"text-align: right;\">          -0.0141204</td><td style=\"text-align: right;\">            -1.68657</td><td style=\"text-align: right;\">            124.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40672000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.033820000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5983522731227479\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.00555487654553106\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8095084796902062\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-30-13\n",
      "  done: false\n",
      "  episode_len_mean: 123.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.014120378018347424\n",
      "  episode_reward_mean: -0.5170391915478713\n",
      "  episode_reward_min: -2.009252127143855\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 67738\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6282430396080018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013975343331694604\n",
      "          policy_loss: -0.10738603763282299\n",
      "          total_loss: 0.25407298454642296\n",
      "          vf_explained_var: 0.30249202251434326\n",
      "          vf_loss: 0.32962144231796264\n",
      "    num_agent_steps_sampled: 40672000\n",
      "    num_agent_steps_trained: 40672000\n",
      "    num_steps_sampled: 10168000\n",
      "    num_steps_trained: 10168000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.700000000000006\n",
      "    gpu_util_percent0: 0.10398058252427185\n",
      "    ram_util_percent: 47.668932038834946\n",
      "    vram_util_percent0: 0.16094173798867156\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5929500348491672\n",
      "  policy_reward_mean:\n",
      "    default: -0.12925979788696781\n",
      "  policy_reward_min:\n",
      "    default: -1.9435850552892153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22158455040516667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.06968642909893\n",
      "    mean_inference_ms: 3.2256010826428962\n",
      "    mean_raw_obs_processing_ms: 0.9311018374921332\n",
      "  time_since_restore: 4916.676776885986\n",
      "  time_this_iter_s: 77.42876434326172\n",
      "  time_total_s: 202686.95515418053\n",
      "  timers:\n",
      "    learn_throughput: 78.37\n",
      "    learn_time_ms: 51040.114\n",
      "    sample_throughput: 151.751\n",
      "    sample_time_ms: 26358.971\n",
      "  timestamp: 1638556213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10168000\n",
      "  training_iteration: 2542\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2542</td><td style=\"text-align: right;\">          202687</td><td style=\"text-align: right;\">10168000</td><td style=\"text-align: right;\">-0.517039</td><td style=\"text-align: right;\">          -0.0141204</td><td style=\"text-align: right;\">            -2.00925</td><td style=\"text-align: right;\">            123.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40688000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03781000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23000000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6601280451778934\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.004815681923660383\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6696315828381206\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 130.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.014120378018347424\n",
      "  episode_reward_mean: -0.5439130875910291\n",
      "  episode_reward_min: -2.009252127143855\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 67765\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6247115302085876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013473175838589669\n",
      "          policy_loss: -0.10294397655129432\n",
      "          total_loss: 0.2302480108588934\n",
      "          vf_explained_var: 0.3506854176521301\n",
      "          vf_loss: 0.3024984064102173\n",
      "    num_agent_steps_sampled: 40688000\n",
      "    num_agent_steps_trained: 40688000\n",
      "    num_steps_sampled: 10172000\n",
      "    num_steps_trained: 10172000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.819230769230767\n",
      "    gpu_util_percent0: 0.1064423076923077\n",
      "    ram_util_percent: 47.83173076923077\n",
      "    vram_util_percent0: 0.16093034145847576\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5147535537506776\n",
      "  policy_reward_mean:\n",
      "    default: -0.13597827189775727\n",
      "  policy_reward_min:\n",
      "    default: -1.9435850552892153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22156933948388924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.067361988393653\n",
      "    mean_inference_ms: 3.225453848604439\n",
      "    mean_raw_obs_processing_ms: 0.9308593993440657\n",
      "  time_since_restore: 4994.135949373245\n",
      "  time_this_iter_s: 77.45917248725891\n",
      "  time_total_s: 202764.4143266678\n",
      "  timers:\n",
      "    learn_throughput: 78.364\n",
      "    learn_time_ms: 51044.095\n",
      "    sample_throughput: 151.817\n",
      "    sample_time_ms: 26347.546\n",
      "  timestamp: 1638556290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10172000\n",
      "  training_iteration: 2543\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2543</td><td style=\"text-align: right;\">          202764</td><td style=\"text-align: right;\">10172000</td><td style=\"text-align: right;\">-0.543913</td><td style=\"text-align: right;\">          -0.0141204</td><td style=\"text-align: right;\">            -2.00925</td><td style=\"text-align: right;\">            130.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40704000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.045370000000000035\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2440000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6601280451778934\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0029134881554759105\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6696315828381206\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-32-48\n",
      "  done: false\n",
      "  episode_len_mean: 142.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.06748256127284957\n",
      "  episode_reward_mean: -0.5818422852520522\n",
      "  episode_reward_min: -2.177052542028356\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 67792\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6306842718124389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013872493751347066\n",
      "          policy_loss: -0.10420537928491831\n",
      "          total_loss: 0.21056965236365796\n",
      "          vf_explained_var: 0.3484770953655243\n",
      "          vf_loss: 0.28317176043987274\n",
      "    num_agent_steps_sampled: 40704000\n",
      "    num_agent_steps_trained: 40704000\n",
      "    num_steps_sampled: 10176000\n",
      "    num_steps_trained: 10176000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.965048543689317\n",
      "    gpu_util_percent0: 0.10485436893203885\n",
      "    ram_util_percent: 47.74271844660198\n",
      "    vram_util_percent0: 0.1609433353460036\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.548802181524857\n",
      "  policy_reward_mean:\n",
      "    default: -0.14546057131301307\n",
      "  policy_reward_min:\n",
      "    default: -1.832931275956655\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22155658963402847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.065086617015236\n",
      "    mean_inference_ms: 3.2258414256478614\n",
      "    mean_raw_obs_processing_ms: 0.9304892642175211\n",
      "  time_since_restore: 5071.64345240593\n",
      "  time_this_iter_s: 77.50750303268433\n",
      "  time_total_s: 202841.92182970047\n",
      "  timers:\n",
      "    learn_throughput: 78.343\n",
      "    learn_time_ms: 51057.236\n",
      "    sample_throughput: 151.716\n",
      "    sample_time_ms: 26365.091\n",
      "  timestamp: 1638556368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10176000\n",
      "  training_iteration: 2544\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2544</td><td style=\"text-align: right;\">          202842</td><td style=\"text-align: right;\">10176000</td><td style=\"text-align: right;\">-0.581842</td><td style=\"text-align: right;\">          -0.0674826</td><td style=\"text-align: right;\">            -2.17705</td><td style=\"text-align: right;\">            142.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40720000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03900000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2440000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6601280451778934\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.028305432276457262\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7415084555793924\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-34-17\n",
      "  done: false\n",
      "  episode_len_mean: 125.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.019430224533906193\n",
      "  episode_reward_mean: -0.49863932173815\n",
      "  episode_reward_min: -2.177052542028356\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 67834\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6280515937805176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013891608662903308\n",
      "          policy_loss: -0.10677050012350082\n",
      "          total_loss: 0.32527502727508545\n",
      "          vf_explained_var: 0.3437155485153198\n",
      "          vf_loss: 0.40039870524406435\n",
      "    num_agent_steps_sampled: 40720000\n",
      "    num_agent_steps_trained: 40720000\n",
      "    num_steps_sampled: 10180000\n",
      "    num_steps_trained: 10180000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.188135593220345\n",
      "    gpu_util_percent0: 0.09389830508474578\n",
      "    ram_util_percent: 47.86694915254237\n",
      "    vram_util_percent0: 0.1609388681602445\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.548802181524857\n",
      "  policy_reward_mean:\n",
      "    default: -0.1246598304345375\n",
      "  policy_reward_min:\n",
      "    default: -1.832931275956655\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22153307480186715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.061106077110324\n",
      "    mean_inference_ms: 3.2267446631972723\n",
      "    mean_raw_obs_processing_ms: 0.9300316320220656\n",
      "  time_since_restore: 5149.048987150192\n",
      "  time_this_iter_s: 77.4055347442627\n",
      "  time_total_s: 202919.32736444473\n",
      "  timers:\n",
      "    learn_throughput: 78.367\n",
      "    learn_time_ms: 51041.735\n",
      "    sample_throughput: 151.84\n",
      "    sample_time_ms: 26343.473\n",
      "  timestamp: 1638556457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10180000\n",
      "  training_iteration: 2545\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2545</td><td style=\"text-align: right;\">          202919</td><td style=\"text-align: right;\">10180000</td><td style=\"text-align: right;\">-0.498639</td><td style=\"text-align: right;\">           0.0194302</td><td style=\"text-align: right;\">            -2.17705</td><td style=\"text-align: right;\">            125.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40736000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03671000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2440000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6136477822274562\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03125555768370782\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7435080626674655\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 121.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.02318832967414508\n",
      "  episode_reward_mean: -0.5105515412300623\n",
      "  episode_reward_min: -2.177052542028356\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 67862\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6313005318641662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013260878100991249\n",
      "          policy_loss: -0.10159391197562218\n",
      "          total_loss: 0.24694641762971878\n",
      "          vf_explained_var: 0.3110353946685791\n",
      "          vf_loss: 0.3183303899765015\n",
      "    num_agent_steps_sampled: 40736000\n",
      "    num_agent_steps_trained: 40736000\n",
      "    num_steps_sampled: 10184000\n",
      "    num_steps_trained: 10184000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.595192307692304\n",
      "    gpu_util_percent0: 0.1064423076923077\n",
      "    ram_util_percent: 47.39615384615385\n",
      "    vram_util_percent0: 0.16094299744349103\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.548802181524857\n",
      "  policy_reward_mean:\n",
      "    default: -0.12763788530751558\n",
      "  policy_reward_min:\n",
      "    default: -1.9059872980115857\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22151392273764292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.060051260569036\n",
      "    mean_inference_ms: 3.227042210879696\n",
      "    mean_raw_obs_processing_ms: 0.9297375331931035\n",
      "  time_since_restore: 5226.857028722763\n",
      "  time_this_iter_s: 77.8080415725708\n",
      "  time_total_s: 202997.1354060173\n",
      "  timers:\n",
      "    learn_throughput: 78.357\n",
      "    learn_time_ms: 51048.607\n",
      "    sample_throughput: 151.874\n",
      "    sample_time_ms: 26337.561\n",
      "  timestamp: 1638556535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10184000\n",
      "  training_iteration: 2546\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2546</td><td style=\"text-align: right;\">          202997</td><td style=\"text-align: right;\">10184000</td><td style=\"text-align: right;\">-0.510552</td><td style=\"text-align: right;\">           0.0231883</td><td style=\"text-align: right;\">            -2.17705</td><td style=\"text-align: right;\">            121.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40752000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02951000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2930000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6756844605337295\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.03303407769158256\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7435080626674655\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-36-53\n",
      "  done: false\n",
      "  episode_len_mean: 110.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09116335675544285\n",
      "  episode_reward_mean: -0.48482126650751545\n",
      "  episode_reward_min: -3.5730479130619184\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 67900\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6254600286483765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013878103915601969\n",
      "          policy_loss: -0.10749129870533944\n",
      "          total_loss: 0.2718675208091736\n",
      "          vf_explained_var: 0.32652273774147034\n",
      "          vf_loss: 0.34774276113510133\n",
      "    num_agent_steps_sampled: 40752000\n",
      "    num_agent_steps_trained: 40752000\n",
      "    num_steps_sampled: 10188000\n",
      "    num_steps_trained: 10188000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.970873786407775\n",
      "    gpu_util_percent0: 0.10786407766990293\n",
      "    ram_util_percent: 47.4912621359223\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5524097208474152\n",
      "  policy_reward_mean:\n",
      "    default: -0.12120531662687885\n",
      "  policy_reward_min:\n",
      "    default: -1.9059872980115857\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22149055373778012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.060035184365756\n",
      "    mean_inference_ms: 3.2270713532440016\n",
      "    mean_raw_obs_processing_ms: 0.9297955231606666\n",
      "  time_since_restore: 5304.545849323273\n",
      "  time_this_iter_s: 77.68882060050964\n",
      "  time_total_s: 203074.8242266178\n",
      "  timers:\n",
      "    learn_throughput: 78.351\n",
      "    learn_time_ms: 51052.379\n",
      "    sample_throughput: 151.706\n",
      "    sample_time_ms: 26366.748\n",
      "  timestamp: 1638556613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10188000\n",
      "  training_iteration: 2547\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2547</td><td style=\"text-align: right;\">          203075</td><td style=\"text-align: right;\">10188000</td><td style=\"text-align: right;\">-0.484821</td><td style=\"text-align: right;\">           0.0911634</td><td style=\"text-align: right;\">            -3.57305</td><td style=\"text-align: right;\">            110.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40768000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028760000000000018\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2930000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6756844605337295\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05059876071431187\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.753228644976612\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-38-22\n",
      "  done: false\n",
      "  episode_len_mean: 108.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09116335675544285\n",
      "  episode_reward_mean: -0.4821774879365799\n",
      "  episode_reward_min: -3.5730479130619184\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 67936\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6272716584205628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014111378140747546\n",
      "          policy_loss: -0.1075230849608779\n",
      "          total_loss: 0.2708022644519806\n",
      "          vf_explained_var: 0.32662689685821533\n",
      "          vf_loss: 0.34617786383628846\n",
      "    num_agent_steps_sampled: 40768000\n",
      "    num_agent_steps_trained: 40768000\n",
      "    num_steps_sampled: 10192000\n",
      "    num_steps_trained: 10192000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.01764705882353\n",
      "    gpu_util_percent0: 0.09016806722689077\n",
      "    ram_util_percent: 48.00924369747899\n",
      "    vram_util_percent0: 0.16093722780326342\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5628344875894897\n",
      "  policy_reward_mean:\n",
      "    default: -0.12054437198414497\n",
      "  policy_reward_min:\n",
      "    default: -1.9059872980115857\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22147235501629897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.06008100221252\n",
      "    mean_inference_ms: 3.227042195819879\n",
      "    mean_raw_obs_processing_ms: 0.9297228730681966\n",
      "  time_since_restore: 5382.031186580658\n",
      "  time_this_iter_s: 77.48533725738525\n",
      "  time_total_s: 203152.3095638752\n",
      "  timers:\n",
      "    learn_throughput: 78.31\n",
      "    learn_time_ms: 51078.913\n",
      "    sample_throughput: 151.738\n",
      "    sample_time_ms: 26361.153\n",
      "  timestamp: 1638556702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10192000\n",
      "  training_iteration: 2548\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2548</td><td style=\"text-align: right;\">          203152</td><td style=\"text-align: right;\">10192000</td><td style=\"text-align: right;\">-0.482177</td><td style=\"text-align: right;\">           0.0911634</td><td style=\"text-align: right;\">            -3.57305</td><td style=\"text-align: right;\">            108.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40784000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03399000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2930000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7646797692963522\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.004819637774328055\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.753228644976612\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 113.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09116335675544285\n",
      "  episode_reward_mean: -0.4763552672140775\n",
      "  episode_reward_min: -3.5730479130619184\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 67975\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6245515036582947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01407662994414568\n",
      "          policy_loss: -0.1071949846148491\n",
      "          total_loss: 0.34042597794532775\n",
      "          vf_explained_var: 0.33785930275917053\n",
      "          vf_loss: 0.41555264019966126\n",
      "    num_agent_steps_sampled: 40784000\n",
      "    num_agent_steps_trained: 40784000\n",
      "    num_steps_sampled: 10196000\n",
      "    num_steps_trained: 10196000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.959615384615386\n",
      "    gpu_util_percent0: 0.10490384615384615\n",
      "    ram_util_percent: 47.974038461538456\n",
      "    vram_util_percent0: 0.16094141544536414\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5795755380658854\n",
      "  policy_reward_mean:\n",
      "    default: -0.11908881680351939\n",
      "  policy_reward_min:\n",
      "    default: -1.8815580601912911\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22147438032659672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.058294437737707\n",
      "    mean_inference_ms: 3.228377042390629\n",
      "    mean_raw_obs_processing_ms: 0.9299711373499252\n",
      "  time_since_restore: 5459.685242652893\n",
      "  time_this_iter_s: 77.65405607223511\n",
      "  time_total_s: 203229.96361994743\n",
      "  timers:\n",
      "    learn_throughput: 78.314\n",
      "    learn_time_ms: 51076.337\n",
      "    sample_throughput: 151.602\n",
      "    sample_time_ms: 26384.83\n",
      "  timestamp: 1638556780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10196000\n",
      "  training_iteration: 2549\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2549</td><td style=\"text-align: right;\">          203230</td><td style=\"text-align: right;\">10196000</td><td style=\"text-align: right;\">-0.476355</td><td style=\"text-align: right;\">           0.0911634</td><td style=\"text-align: right;\">            -3.57305</td><td style=\"text-align: right;\">            113.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40800000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03275000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2640000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7646797692963522\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.012823590772102182\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.753228644976612\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-40-58\n",
      "  done: false\n",
      "  episode_len_mean: 109.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.0424905095021606\n",
      "  episode_reward_mean: -0.4500807724762138\n",
      "  episode_reward_min: -1.7983837704030892\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 68010\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6291218433380127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014064474046230317\n",
      "          policy_loss: -0.10667902006208897\n",
      "          total_loss: 0.29149703902006147\n",
      "          vf_explained_var: 0.29638200998306274\n",
      "          vf_loss: 0.36613543033599855\n",
      "    num_agent_steps_sampled: 40800000\n",
      "    num_agent_steps_trained: 40800000\n",
      "    num_steps_sampled: 10200000\n",
      "    num_steps_trained: 10200000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.754368932038833\n",
      "    gpu_util_percent0: 0.10669902912621361\n",
      "    ram_util_percent: 48.067961165048516\n",
      "    vram_util_percent0: 0.16093375120201142\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6459057410022737\n",
      "  policy_reward_mean:\n",
      "    default: -0.11252019311905347\n",
      "  policy_reward_min:\n",
      "    default: -1.8815580601912911\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2214703606980946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.056791083811113\n",
      "    mean_inference_ms: 3.2292816899594414\n",
      "    mean_raw_obs_processing_ms: 0.9300064809247385\n",
      "  time_since_restore: 5537.276889324188\n",
      "  time_this_iter_s: 77.59164667129517\n",
      "  time_total_s: 203307.55526661873\n",
      "  timers:\n",
      "    learn_throughput: 78.29\n",
      "    learn_time_ms: 51092.23\n",
      "    sample_throughput: 151.656\n",
      "    sample_time_ms: 26375.477\n",
      "  timestamp: 1638556858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10200000\n",
      "  training_iteration: 2550\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2550</td><td style=\"text-align: right;\">          203308</td><td style=\"text-align: right;\">10200000</td><td style=\"text-align: right;\">-0.450081</td><td style=\"text-align: right;\">          -0.0424905</td><td style=\"text-align: right;\">            -1.79838</td><td style=\"text-align: right;\">            109.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40816000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03619000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2640000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7646797692963522\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.017675274286465762\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.664654751283798\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 122.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2653142803737094\n",
      "  episode_reward_mean: -0.4760558152575945\n",
      "  episode_reward_min: -1.7983837704030892\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 68034\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6346185722351074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013729339852929115\n",
      "          policy_loss: -0.1039581809565425\n",
      "          total_loss: 0.17550030142068862\n",
      "          vf_explained_var: 0.3078939616680145\n",
      "          vf_loss: 0.2481813303232193\n",
      "    num_agent_steps_sampled: 40816000\n",
      "    num_agent_steps_trained: 40816000\n",
      "    num_steps_sampled: 10204000\n",
      "    num_steps_trained: 10204000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.325210084033614\n",
      "    gpu_util_percent0: 0.09218487394957985\n",
      "    ram_util_percent: 48.226890756302524\n",
      "    vram_util_percent0: 0.1609482884960499\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6459057410022737\n",
      "  policy_reward_mean:\n",
      "    default: -0.11901395381439864\n",
      "  policy_reward_min:\n",
      "    default: -1.829664869961487\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22146046697764304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05599812362123\n",
      "    mean_inference_ms: 3.2298564119202076\n",
      "    mean_raw_obs_processing_ms: 0.9298684523941696\n",
      "  time_since_restore: 5614.7066786289215\n",
      "  time_this_iter_s: 77.42978930473328\n",
      "  time_total_s: 203384.98505592346\n",
      "  timers:\n",
      "    learn_throughput: 78.299\n",
      "    learn_time_ms: 51085.969\n",
      "    sample_throughput: 151.587\n",
      "    sample_time_ms: 26387.413\n",
      "  timestamp: 1638556947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10204000\n",
      "  training_iteration: 2551\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2551</td><td style=\"text-align: right;\">          203385</td><td style=\"text-align: right;\">10204000</td><td style=\"text-align: right;\">-0.476056</td><td style=\"text-align: right;\">            0.265314</td><td style=\"text-align: right;\">            -1.79838</td><td style=\"text-align: right;\">            122.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40832000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03531000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18000000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6837662817947376\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.008352491164221112\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6970782730832695\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 132.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2653142803737094\n",
      "  episode_reward_mean: -0.5133983150630618\n",
      "  episode_reward_min: -1.76870710791549\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 68061\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6262354898452759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013863287687301636\n",
      "          policy_loss: -0.10461457202583552\n",
      "          total_loss: 0.2077575521916151\n",
      "          vf_explained_var: 0.3175792992115021\n",
      "          vf_loss: 0.28078982079029086\n",
      "    num_agent_steps_sampled: 40832000\n",
      "    num_agent_steps_trained: 40832000\n",
      "    num_steps_sampled: 10208000\n",
      "    num_steps_trained: 10208000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.956862745098043\n",
      "    gpu_util_percent0: 0.10558823529411765\n",
      "    ram_util_percent: 48.13529411764705\n",
      "    vram_util_percent0: 0.16093238875016938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6459057410022737\n",
      "  policy_reward_mean:\n",
      "    default: -0.12834957876576547\n",
      "  policy_reward_min:\n",
      "    default: -1.8124913794282809\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22143422670667612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.054847033073074\n",
      "    mean_inference_ms: 3.229738758725407\n",
      "    mean_raw_obs_processing_ms: 0.9294958538184318\n",
      "  time_since_restore: 5691.962335109711\n",
      "  time_this_iter_s: 77.25565648078918\n",
      "  time_total_s: 203462.24071240425\n",
      "  timers:\n",
      "    learn_throughput: 78.315\n",
      "    learn_time_ms: 51075.765\n",
      "    sample_throughput: 151.627\n",
      "    sample_time_ms: 26380.456\n",
      "  timestamp: 1638557025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10208000\n",
      "  training_iteration: 2552\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2552</td><td style=\"text-align: right;\">          203462</td><td style=\"text-align: right;\">10208000</td><td style=\"text-align: right;\">-0.513398</td><td style=\"text-align: right;\">            0.265314</td><td style=\"text-align: right;\">            -1.76871</td><td style=\"text-align: right;\">            132.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40848000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03501000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2940000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.663065949414171\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.049206209987596686\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6970782730832695\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 141.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2653142803737094\n",
      "  episode_reward_mean: -0.5240058109131425\n",
      "  episode_reward_min: -2.090803677523529\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 68092\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6288181962966919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013679510541260242\n",
      "          policy_loss: -0.10515639895200729\n",
      "          total_loss: 0.2464147047251463\n",
      "          vf_explained_var: 0.2845861613750458\n",
      "          vf_loss: 0.3204074674844742\n",
      "    num_agent_steps_sampled: 40848000\n",
      "    num_agent_steps_trained: 40848000\n",
      "    num_steps_sampled: 10212000\n",
      "    num_steps_trained: 10212000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.17211538461538\n",
      "    gpu_util_percent0: 0.10538461538461538\n",
      "    ram_util_percent: 47.83269230769231\n",
      "    vram_util_percent0: 0.16093983344723722\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6459057410022737\n",
      "  policy_reward_mean:\n",
      "    default: -0.13100145272828562\n",
      "  policy_reward_min:\n",
      "    default: -1.8858017749799982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22141025788387567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.054790353835433\n",
      "    mean_inference_ms: 3.229834519255387\n",
      "    mean_raw_obs_processing_ms: 0.9289854097128537\n",
      "  time_since_restore: 5769.850213527679\n",
      "  time_this_iter_s: 77.88787841796875\n",
      "  time_total_s: 203540.12859082222\n",
      "  timers:\n",
      "    learn_throughput: 78.322\n",
      "    learn_time_ms: 51071.351\n",
      "    sample_throughput: 151.356\n",
      "    sample_time_ms: 26427.73\n",
      "  timestamp: 1638557103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10212000\n",
      "  training_iteration: 2553\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2553</td><td style=\"text-align: right;\">          203540</td><td style=\"text-align: right;\">10212000</td><td style=\"text-align: right;\">-0.524006</td><td style=\"text-align: right;\">            0.265314</td><td style=\"text-align: right;\">             -2.0908</td><td style=\"text-align: right;\">            141.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40864000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.032460000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2940000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7073469752098347\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08735336521133162\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6970782730832695\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 136.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.021091148930552084\n",
      "  episode_reward_mean: -0.5037889889744893\n",
      "  episode_reward_min: -2.090803677523529\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 68124\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6341434669494629\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013842131346464157\n",
      "          policy_loss: -0.10554428806900978\n",
      "          total_loss: 0.2454745770096779\n",
      "          vf_explained_var: 0.3314096927642822\n",
      "          vf_loss: 0.31948475766181944\n",
      "    num_agent_steps_sampled: 40864000\n",
      "    num_agent_steps_trained: 40864000\n",
      "    num_steps_sampled: 10216000\n",
      "    num_steps_trained: 10216000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.0344537815126\n",
      "    gpu_util_percent0: 0.0938655462184874\n",
      "    ram_util_percent: 48.33949579831936\n",
      "    vram_util_percent0: 0.16093722780326342\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6083560369499943\n",
      "  policy_reward_mean:\n",
      "    default: -0.12594724724362233\n",
      "  policy_reward_min:\n",
      "    default: -1.8858017749799982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22140430064215305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05373142772498\n",
      "    mean_inference_ms: 3.230205564184217\n",
      "    mean_raw_obs_processing_ms: 0.9284531214608642\n",
      "  time_since_restore: 5847.089904308319\n",
      "  time_this_iter_s: 77.23969078063965\n",
      "  time_total_s: 203617.36828160286\n",
      "  timers:\n",
      "    learn_throughput: 78.33\n",
      "    learn_time_ms: 51065.71\n",
      "    sample_throughput: 151.474\n",
      "    sample_time_ms: 26407.099\n",
      "  timestamp: 1638557192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10216000\n",
      "  training_iteration: 2554\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2554</td><td style=\"text-align: right;\">          203617</td><td style=\"text-align: right;\">10216000</td><td style=\"text-align: right;\">-0.503789</td><td style=\"text-align: right;\">           0.0210911</td><td style=\"text-align: right;\">             -2.0908</td><td style=\"text-align: right;\">            136.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40880000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.035890000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2940000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7073469752098347\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04470671208099931\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6909122493129196\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-47-50\n",
      "  done: false\n",
      "  episode_len_mean: 133.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.12772926302480725\n",
      "  episode_reward_mean: -0.4959527930756195\n",
      "  episode_reward_min: -2.090803677523529\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 68155\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6313530659675598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014029115982353688\n",
      "          policy_loss: -0.10625326755642892\n",
      "          total_loss: 0.29122792387753726\n",
      "          vf_explained_var: 0.3510822653770447\n",
      "          vf_loss: 0.36552111148834227\n",
      "    num_agent_steps_sampled: 40880000\n",
      "    num_agent_steps_trained: 40880000\n",
      "    num_steps_sampled: 10220000\n",
      "    num_steps_trained: 10220000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.756310679611655\n",
      "    gpu_util_percent0: 0.10456310679611652\n",
      "    ram_util_percent: 48.21553398058256\n",
      "    vram_util_percent0: 0.16094014063133952\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6083560369499943\n",
      "  policy_reward_mean:\n",
      "    default: -0.12398819826890485\n",
      "  policy_reward_min:\n",
      "    default: -1.8858017749799982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22141118702008494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.053450832260268\n",
      "    mean_inference_ms: 3.2307334448183513\n",
      "    mean_raw_obs_processing_ms: 0.9280728955365679\n",
      "  time_since_restore: 5924.689550638199\n",
      "  time_this_iter_s: 77.59964632987976\n",
      "  time_total_s: 203694.96792793274\n",
      "  timers:\n",
      "    learn_throughput: 78.339\n",
      "    learn_time_ms: 51059.842\n",
      "    sample_throughput: 151.329\n",
      "    sample_time_ms: 26432.392\n",
      "  timestamp: 1638557270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10220000\n",
      "  training_iteration: 2555\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2555</td><td style=\"text-align: right;\">          203695</td><td style=\"text-align: right;\">10220000</td><td style=\"text-align: right;\">-0.495953</td><td style=\"text-align: right;\">            0.127729</td><td style=\"text-align: right;\">             -2.0908</td><td style=\"text-align: right;\">            133.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40896000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.040960000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2940000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7073469752098347\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.01423852932184611\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6813979410675058\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 140.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.12772926302480725\n",
      "  episode_reward_mean: -0.5153292995972275\n",
      "  episode_reward_min: -2.454577381792306\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 68183\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.629424388885498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013710970565676689\n",
      "          policy_loss: -0.10403440415859222\n",
      "          total_loss: 0.2592653326690197\n",
      "          vf_explained_var: 0.30740615725517273\n",
      "          vf_loss: 0.33206442892551424\n",
      "    num_agent_steps_sampled: 40896000\n",
      "    num_agent_steps_trained: 40896000\n",
      "    num_steps_sampled: 10224000\n",
      "    num_steps_trained: 10224000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.893203883495147\n",
      "    gpu_util_percent0: 0.10514563106796118\n",
      "    ram_util_percent: 48.27087378640776\n",
      "    vram_util_percent0: 0.16093055648734733\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6083560369499943\n",
      "  policy_reward_mean:\n",
      "    default: -0.12883232489930688\n",
      "  policy_reward_min:\n",
      "    default: -1.8858017749799982\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22141041410343526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05233833942517\n",
      "    mean_inference_ms: 3.231014734613577\n",
      "    mean_raw_obs_processing_ms: 0.9277007172433622\n",
      "  time_since_restore: 6002.185333967209\n",
      "  time_this_iter_s: 77.49578332901001\n",
      "  time_total_s: 203772.46371126175\n",
      "  timers:\n",
      "    learn_throughput: 78.351\n",
      "    learn_time_ms: 51052.013\n",
      "    sample_throughput: 151.463\n",
      "    sample_time_ms: 26409.066\n",
      "  timestamp: 1638557347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10224000\n",
      "  training_iteration: 2556\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2556</td><td style=\"text-align: right;\">          203772</td><td style=\"text-align: right;\">10224000</td><td style=\"text-align: right;\">-0.515329</td><td style=\"text-align: right;\">            0.127729</td><td style=\"text-align: right;\">            -2.45458</td><td style=\"text-align: right;\">            140.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40912000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.04090000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2650000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7320209587094033\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06961577151712572\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6813979410675058\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 134.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.14681643981326697\n",
      "  episode_reward_mean: -0.517778523525128\n",
      "  episode_reward_min: -2.454577381792306\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 68210\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6304008107185364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013591096825897694\n",
      "          policy_loss: -0.10312924419343472\n",
      "          total_loss: 0.21421820889413357\n",
      "          vf_explained_var: 0.3339800238609314\n",
      "          vf_loss: 0.286385232925415\n",
      "    num_agent_steps_sampled: 40912000\n",
      "    num_agent_steps_trained: 40912000\n",
      "    num_steps_sampled: 10228000\n",
      "    num_steps_trained: 10228000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.636134453781512\n",
      "    gpu_util_percent0: 0.09168067226890758\n",
      "    ram_util_percent: 48.47226890756303\n",
      "    vram_util_percent0: 0.16093722780326342\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6135259477206005\n",
      "  policy_reward_mean:\n",
      "    default: -0.129444630881282\n",
      "  policy_reward_min:\n",
      "    default: -1.8436729401924705\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22140210819510459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05132382226395\n",
      "    mean_inference_ms: 3.2313810218559964\n",
      "    mean_raw_obs_processing_ms: 0.9273901090413216\n",
      "  time_since_restore: 6079.613160610199\n",
      "  time_this_iter_s: 77.42782664299011\n",
      "  time_total_s: 203849.89153790474\n",
      "  timers:\n",
      "    learn_throughput: 78.363\n",
      "    learn_time_ms: 51044.415\n",
      "    sample_throughput: 151.57\n",
      "    sample_time_ms: 26390.457\n",
      "  timestamp: 1638557437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10228000\n",
      "  training_iteration: 2557\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2557</td><td style=\"text-align: right;\">          203850</td><td style=\"text-align: right;\">10228000</td><td style=\"text-align: right;\">-0.517779</td><td style=\"text-align: right;\">            0.146816</td><td style=\"text-align: right;\">            -2.45458</td><td style=\"text-align: right;\">            134.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40928000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03994000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2650000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7320209587094033\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07308504406582203\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6340329968491969\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 134.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.14681643981326697\n",
      "  episode_reward_mean: -0.5269029341697885\n",
      "  episode_reward_min: -2.454577381792306\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 68244\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6310124378204346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013878557510674\n",
      "          policy_loss: -0.10570657962560653\n",
      "          total_loss: 0.31653682482242584\n",
      "          vf_explained_var: 0.3400861918926239\n",
      "          vf_loss: 0.3906263124942779\n",
      "    num_agent_steps_sampled: 40928000\n",
      "    num_agent_steps_trained: 40928000\n",
      "    num_steps_sampled: 10232000\n",
      "    num_steps_trained: 10232000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.087378640776702\n",
      "    gpu_util_percent0: 0.1075728155339806\n",
      "    ram_util_percent: 48.44368932038836\n",
      "    vram_util_percent0: 0.16093055648734736\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6135259477206005\n",
      "  policy_reward_mean:\n",
      "    default: -0.13172573354244713\n",
      "  policy_reward_min:\n",
      "    default: -1.8436729401924705\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22139023814755698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05033453467421\n",
      "    mean_inference_ms: 3.231994713883523\n",
      "    mean_raw_obs_processing_ms: 0.9272208890410528\n",
      "  time_since_restore: 6157.223516941071\n",
      "  time_this_iter_s: 77.61035633087158\n",
      "  time_total_s: 203927.5018942356\n",
      "  timers:\n",
      "    learn_throughput: 78.391\n",
      "    learn_time_ms: 51026.218\n",
      "    sample_throughput: 151.393\n",
      "    sample_time_ms: 26421.228\n",
      "  timestamp: 1638557515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10232000\n",
      "  training_iteration: 2558\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2558</td><td style=\"text-align: right;\">          203928</td><td style=\"text-align: right;\">10232000</td><td style=\"text-align: right;\">-0.526903</td><td style=\"text-align: right;\">            0.146816</td><td style=\"text-align: right;\">            -2.45458</td><td style=\"text-align: right;\">            134.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40944000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.038610000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2650000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7320209587094033\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07944271442346501\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6807547083721337\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-53-12\n",
      "  done: false\n",
      "  episode_len_mean: 127.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.14681643981326697\n",
      "  episode_reward_mean: -0.5242270318804401\n",
      "  episode_reward_min: -1.8141027000386303\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 68276\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6250242910385132\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01406865429878235\n",
      "          policy_loss: -0.10696528912335634\n",
      "          total_loss: 0.29171412965655324\n",
      "          vf_explained_var: 0.3002743124961853\n",
      "          vf_loss: 0.3666292593479156\n",
      "    num_agent_steps_sampled: 40944000\n",
      "    num_agent_steps_trained: 40944000\n",
      "    num_steps_sampled: 10236000\n",
      "    num_steps_trained: 10236000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.93592233009709\n",
      "    gpu_util_percent0: 0.10621359223300972\n",
      "    ram_util_percent: 48.43592233009709\n",
      "    vram_util_percent0: 0.16093055648734733\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6135259477206005\n",
      "  policy_reward_mean:\n",
      "    default: -0.13105675797011002\n",
      "  policy_reward_min:\n",
      "    default: -1.8436729401924705\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22138792560759996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.049241655815003\n",
      "    mean_inference_ms: 3.232628467940326\n",
      "    mean_raw_obs_processing_ms: 0.9271308788095506\n",
      "  time_since_restore: 6234.806679010391\n",
      "  time_this_iter_s: 77.58316206932068\n",
      "  time_total_s: 204005.08505630493\n",
      "  timers:\n",
      "    learn_throughput: 78.367\n",
      "    learn_time_ms: 51041.982\n",
      "    sample_throughput: 151.517\n",
      "    sample_time_ms: 26399.607\n",
      "  timestamp: 1638557592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10236000\n",
      "  training_iteration: 2559\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2559</td><td style=\"text-align: right;\">          204005</td><td style=\"text-align: right;\">10236000</td><td style=\"text-align: right;\">-0.524227</td><td style=\"text-align: right;\">            0.146816</td><td style=\"text-align: right;\">             -1.8141</td><td style=\"text-align: right;\">            127.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40960000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03257000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19900000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7320209587094033\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07977371218229617\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6948691392200939\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 123.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.020359732161267186\n",
      "  episode_reward_mean: -0.5135076029140734\n",
      "  episode_reward_min: -1.4892302469388121\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 68304\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.631334972858429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013840438336133958\n",
      "          policy_loss: -0.10670326764136553\n",
      "          total_loss: 0.2580067616701126\n",
      "          vf_explained_var: 0.34214821457862854\n",
      "          vf_loss: 0.33317978155612943\n",
      "    num_agent_steps_sampled: 40960000\n",
      "    num_agent_steps_trained: 40960000\n",
      "    num_steps_sampled: 10240000\n",
      "    num_steps_trained: 10240000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.073333333333334\n",
      "    gpu_util_percent0: 0.09258333333333332\n",
      "    ram_util_percent: 48.39416666666666\n",
      "    vram_util_percent0: 0.16094658330591205\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6135259477206005\n",
      "  policy_reward_mean:\n",
      "    default: -0.12837690072851834\n",
      "  policy_reward_min:\n",
      "    default: -1.8470254926427336\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22138339916171726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.047986728197586\n",
      "    mean_inference_ms: 3.2330736331949605\n",
      "    mean_raw_obs_processing_ms: 0.9270140562396969\n",
      "  time_since_restore: 6312.237641334534\n",
      "  time_this_iter_s: 77.43096232414246\n",
      "  time_total_s: 204082.51601862907\n",
      "  timers:\n",
      "    learn_throughput: 78.366\n",
      "    learn_time_ms: 51042.396\n",
      "    sample_throughput: 151.613\n",
      "    sample_time_ms: 26383.024\n",
      "  timestamp: 1638557682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10240000\n",
      "  training_iteration: 2560\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2560</td><td style=\"text-align: right;\">          204083</td><td style=\"text-align: right;\">10240000</td><td style=\"text-align: right;\">-0.513508</td><td style=\"text-align: right;\">           0.0203597</td><td style=\"text-align: right;\">            -1.48923</td><td style=\"text-align: right;\">            123.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40976000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.034460000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19900000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.628257087139157\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0037589433696191433\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8070109059864139\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 135.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.004009483699304095\n",
      "  episode_reward_mean: -0.5465150177471502\n",
      "  episode_reward_min: -1.4468299520643373\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 68331\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6270971508026123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01361142272502184\n",
      "          policy_loss: -0.10382334093004465\n",
      "          total_loss: 0.20736396661400794\n",
      "          vf_explained_var: 0.3303897976875305\n",
      "          vf_loss: 0.2801787848472595\n",
      "    num_agent_steps_sampled: 40976000\n",
      "    num_agent_steps_trained: 40976000\n",
      "    num_steps_sampled: 10244000\n",
      "    num_steps_trained: 10244000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.844660194174757\n",
      "    gpu_util_percent0: 0.10174757281553402\n",
      "    ram_util_percent: 48.14660194174759\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6840869668419893\n",
      "  policy_reward_mean:\n",
      "    default: -0.13662875443678757\n",
      "  policy_reward_min:\n",
      "    default: -1.8599650023914491\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213821925947304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04663414338791\n",
      "    mean_inference_ms: 3.233523467079779\n",
      "    mean_raw_obs_processing_ms: 0.9267003703695127\n",
      "  time_since_restore: 6389.962758779526\n",
      "  time_this_iter_s: 77.72511744499207\n",
      "  time_total_s: 204160.24113607407\n",
      "  timers:\n",
      "    learn_throughput: 78.335\n",
      "    learn_time_ms: 51063.001\n",
      "    sample_throughput: 151.56\n",
      "    sample_time_ms: 26392.137\n",
      "  timestamp: 1638557760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10244000\n",
      "  training_iteration: 2561\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2561</td><td style=\"text-align: right;\">          204160</td><td style=\"text-align: right;\">10244000</td><td style=\"text-align: right;\">-0.546515</td><td style=\"text-align: right;\">          0.00400948</td><td style=\"text-align: right;\">            -1.44683</td><td style=\"text-align: right;\">            135.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 40992000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03297000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19200000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6271902907688294\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.009777294969677254\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8070109059864139\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-57-17\n",
      "  done: false\n",
      "  episode_len_mean: 129.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.004009483699304095\n",
      "  episode_reward_mean: -0.502030449645917\n",
      "  episode_reward_min: -1.4468299520643373\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 68364\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6316352245807648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013998056344687938\n",
      "          policy_loss: -0.10717086188495159\n",
      "          total_loss: 0.29004795867204664\n",
      "          vf_explained_var: 0.31018519401550293\n",
      "          vf_loss: 0.36532950115203855\n",
      "    num_agent_steps_sampled: 40992000\n",
      "    num_agent_steps_trained: 40992000\n",
      "    num_steps_sampled: 10248000\n",
      "    num_steps_trained: 10248000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.092233009708732\n",
      "    gpu_util_percent0: 0.10330097087378642\n",
      "    ram_util_percent: 48.13203883495146\n",
      "    vram_util_percent0: 0.16094173798867156\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7072435805194948\n",
      "  policy_reward_mean:\n",
      "    default: -0.12550761241147926\n",
      "  policy_reward_min:\n",
      "    default: -1.8599650023914491\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22136881819423893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04551938723018\n",
      "    mean_inference_ms: 3.2339425906986823\n",
      "    mean_raw_obs_processing_ms: 0.9262797655971121\n",
      "  time_since_restore: 6467.567721605301\n",
      "  time_this_iter_s: 77.60496282577515\n",
      "  time_total_s: 204237.84609889984\n",
      "  timers:\n",
      "    learn_throughput: 78.321\n",
      "    learn_time_ms: 51072.035\n",
      "    sample_throughput: 151.412\n",
      "    sample_time_ms: 26418.025\n",
      "  timestamp: 1638557837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10248000\n",
      "  training_iteration: 2562\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2562</td><td style=\"text-align: right;\">          204238</td><td style=\"text-align: right;\">10248000</td><td style=\"text-align: right;\">-0.50203</td><td style=\"text-align: right;\">          0.00400948</td><td style=\"text-align: right;\">            -1.44683</td><td style=\"text-align: right;\">            129.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41008000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03361000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19200000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6271902907688294\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0808064707133627\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8070109059864139\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_15-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 128.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03133436343903706\n",
      "  episode_reward_mean: -0.4952597893622734\n",
      "  episode_reward_min: -1.4060286911895377\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 68398\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6333224372863769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0140095893740654\n",
      "          policy_loss: -0.10536603471636773\n",
      "          total_loss: 0.2998940896987915\n",
      "          vf_explained_var: 0.3072044253349304\n",
      "          vf_loss: 0.3733445293903351\n",
      "    num_agent_steps_sampled: 41008000\n",
      "    num_agent_steps_trained: 41008000\n",
      "    num_steps_sampled: 10252000\n",
      "    num_steps_trained: 10252000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.239495798319332\n",
      "    gpu_util_percent0: 0.09470588235294121\n",
      "    ram_util_percent: 48.646218487394925\n",
      "    vram_util_percent0: 0.16093861038986174\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7072435805194948\n",
      "  policy_reward_mean:\n",
      "    default: -0.12381494734056836\n",
      "  policy_reward_min:\n",
      "    default: -1.8599650023914491\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22135453319446755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04390867587702\n",
      "    mean_inference_ms: 3.2341729699590234\n",
      "    mean_raw_obs_processing_ms: 0.9261218244963755\n",
      "  time_since_restore: 6544.731296300888\n",
      "  time_this_iter_s: 77.16357469558716\n",
      "  time_total_s: 204315.00967359543\n",
      "  timers:\n",
      "    learn_throughput: 78.34\n",
      "    learn_time_ms: 51059.274\n",
      "    sample_throughput: 151.755\n",
      "    sample_time_ms: 26358.345\n",
      "  timestamp: 1638557927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10252000\n",
      "  training_iteration: 2563\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2563</td><td style=\"text-align: right;\">          204315</td><td style=\"text-align: right;\">10252000</td><td style=\"text-align: right;\">-0.49526</td><td style=\"text-align: right;\">           0.0313344</td><td style=\"text-align: right;\">            -1.40603</td><td style=\"text-align: right;\">            128.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41024000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030180000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19200000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6271902907688294\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.024612127308098287\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7255463705921389\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-00-05\n",
      "  done: false\n",
      "  episode_len_mean: 110.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.03133436343903706\n",
      "  episode_reward_mean: -0.44610582169543006\n",
      "  episode_reward_min: -1.3882046277598024\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 68439\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6337241826057434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013694385215640068\n",
      "          policy_loss: -0.10376631002873182\n",
      "          total_loss: 0.2680712777376175\n",
      "          vf_explained_var: 0.37666183710098267\n",
      "          vf_loss: 0.3406400656700134\n",
      "    num_agent_steps_sampled: 41024000\n",
      "    num_agent_steps_trained: 41024000\n",
      "    num_steps_sampled: 10256000\n",
      "    num_steps_trained: 10256000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.736538461538466\n",
      "    gpu_util_percent0: 0.10278846153846154\n",
      "    ram_util_percent: 48.64038461538462\n",
      "    vram_util_percent0: 0.16093034145847573\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5702803904437683\n",
      "  policy_reward_mean:\n",
      "    default: -0.1115264554238575\n",
      "  policy_reward_min:\n",
      "    default: -1.9006016771002574\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22137538813231436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04404391618665\n",
      "    mean_inference_ms: 3.235020593998271\n",
      "    mean_raw_obs_processing_ms: 0.9264938429825051\n",
      "  time_since_restore: 6622.7623295784\n",
      "  time_this_iter_s: 78.0310332775116\n",
      "  time_total_s: 204393.04070687294\n",
      "  timers:\n",
      "    learn_throughput: 78.342\n",
      "    learn_time_ms: 51058.292\n",
      "    sample_throughput: 151.293\n",
      "    sample_time_ms: 26438.726\n",
      "  timestamp: 1638558005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10256000\n",
      "  training_iteration: 2564\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2564</td><td style=\"text-align: right;\">          204393</td><td style=\"text-align: right;\">10256000</td><td style=\"text-align: right;\">-0.446106</td><td style=\"text-align: right;\">           0.0313344</td><td style=\"text-align: right;\">             -1.3882</td><td style=\"text-align: right;\">            110.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41040000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02927000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17700000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6258620032601859\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.018391043364642395\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7255463705921389\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-01-23\n",
      "  done: false\n",
      "  episode_len_mean: 102.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10637391770854931\n",
      "  episode_reward_mean: -0.4167950482988621\n",
      "  episode_reward_min: -1.4164479016041347\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 68478\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6230511622428894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01365779833495617\n",
      "          policy_loss: -0.10453372426331044\n",
      "          total_loss: 0.3261747053861618\n",
      "          vf_explained_var: 0.3589019775390625\n",
      "          vf_loss: 0.3995942578315735\n",
      "    num_agent_steps_sampled: 41040000\n",
      "    num_agent_steps_trained: 41040000\n",
      "    num_steps_sampled: 10260000\n",
      "    num_steps_trained: 10260000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.94271844660194\n",
      "    gpu_util_percent0: 0.10446601941747574\n",
      "    ram_util_percent: 48.664077669902916\n",
      "    vram_util_percent0: 0.16093055648734736\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5702803904437683\n",
      "  policy_reward_mean:\n",
      "    default: -0.10419876207471555\n",
      "  policy_reward_min:\n",
      "    default: -1.9006016771002574\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2214146596894407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04408089011924\n",
      "    mean_inference_ms: 3.236084149326426\n",
      "    mean_raw_obs_processing_ms: 0.9269421746678436\n",
      "  time_since_restore: 6700.424421310425\n",
      "  time_this_iter_s: 77.66209173202515\n",
      "  time_total_s: 204470.70279860497\n",
      "  timers:\n",
      "    learn_throughput: 78.329\n",
      "    learn_time_ms: 51066.38\n",
      "    sample_throughput: 151.304\n",
      "    sample_time_ms: 26436.848\n",
      "  timestamp: 1638558083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10260000\n",
      "  training_iteration: 2565\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2565</td><td style=\"text-align: right;\">          204471</td><td style=\"text-align: right;\">10260000</td><td style=\"text-align: right;\">-0.416795</td><td style=\"text-align: right;\">            0.106374</td><td style=\"text-align: right;\">            -1.41645</td><td style=\"text-align: right;\">            102.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41056000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030960000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17700000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7527905144196856\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08056207659102452\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7058955957744191\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-02-40\n",
      "  done: false\n",
      "  episode_len_mean: 106.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10637391770854931\n",
      "  episode_reward_mean: -0.4150380668866758\n",
      "  episode_reward_min: -1.4164479016041347\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 68509\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6343850674629211\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013849952131509781\n",
      "          policy_loss: -0.10543370674550533\n",
      "          total_loss: 0.24565682646632195\n",
      "          vf_explained_var: 0.3580104112625122\n",
      "          vf_loss: 0.31953860878944396\n",
      "    num_agent_steps_sampled: 41056000\n",
      "    num_agent_steps_trained: 41056000\n",
      "    num_steps_sampled: 10264000\n",
      "    num_steps_trained: 10264000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.90388349514563\n",
      "    gpu_util_percent0: 0.10572815533980585\n",
      "    ram_util_percent: 48.64368932038834\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5402069348407768\n",
      "  policy_reward_mean:\n",
      "    default: -0.10375951672166897\n",
      "  policy_reward_min:\n",
      "    default: -1.9469261847323724\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.221436134967719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.044428117236258\n",
      "    mean_inference_ms: 3.2367263733793616\n",
      "    mean_raw_obs_processing_ms: 0.9270185995282485\n",
      "  time_since_restore: 6777.87136054039\n",
      "  time_this_iter_s: 77.44693922996521\n",
      "  time_total_s: 204548.14973783493\n",
      "  timers:\n",
      "    learn_throughput: 78.349\n",
      "    learn_time_ms: 51053.483\n",
      "    sample_throughput: 151.258\n",
      "    sample_time_ms: 26444.801\n",
      "  timestamp: 1638558160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10264000\n",
      "  training_iteration: 2566\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2566</td><td style=\"text-align: right;\">          204548</td><td style=\"text-align: right;\">10264000</td><td style=\"text-align: right;\">-0.415038</td><td style=\"text-align: right;\">            0.106374</td><td style=\"text-align: right;\">            -1.41645</td><td style=\"text-align: right;\">            106.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41072000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03249000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17700000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8278450378856808\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06620849166810401\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7058955957744191\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-04-10\n",
      "  done: false\n",
      "  episode_len_mean: 115.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.10637391770854931\n",
      "  episode_reward_mean: -0.45275130085569\n",
      "  episode_reward_min: -1.4164479016041347\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 68543\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6293973917961121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01390136430412531\n",
      "          policy_loss: -0.1054157469905913\n",
      "          total_loss: 0.2768974991440773\n",
      "          vf_explained_var: 0.32575735449790955\n",
      "          vf_loss: 0.35064420008659364\n",
      "    num_agent_steps_sampled: 41072000\n",
      "    num_agent_steps_trained: 41072000\n",
      "    num_steps_sampled: 10268000\n",
      "    num_steps_trained: 10268000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.27372881355932\n",
      "    gpu_util_percent0: 0.08813559322033898\n",
      "    ram_util_percent: 48.7991525423729\n",
      "    vram_util_percent0: 0.16094444537398003\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.662276080490623\n",
      "  policy_reward_mean:\n",
      "    default: -0.1131878252139225\n",
      "  policy_reward_min:\n",
      "    default: -1.9469261847323724\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2214200823515583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.043401556080326\n",
      "    mean_inference_ms: 3.2363079871412173\n",
      "    mean_raw_obs_processing_ms: 0.9268402047252775\n",
      "  time_since_restore: 6855.08944106102\n",
      "  time_this_iter_s: 77.21808052062988\n",
      "  time_total_s: 204625.36781835556\n",
      "  timers:\n",
      "    learn_throughput: 78.334\n",
      "    learn_time_ms: 51063.697\n",
      "    sample_throughput: 151.436\n",
      "    sample_time_ms: 26413.718\n",
      "  timestamp: 1638558250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10268000\n",
      "  training_iteration: 2567\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2567</td><td style=\"text-align: right;\">          204625</td><td style=\"text-align: right;\">10268000</td><td style=\"text-align: right;\">-0.452751</td><td style=\"text-align: right;\">            0.106374</td><td style=\"text-align: right;\">            -1.41645</td><td style=\"text-align: right;\">            115.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41088000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.031280000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.22000000000000017\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8278450378856808\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06348191470406742\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6569462088170098\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 114.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0011887520635474047\n",
      "  episode_reward_mean: -0.4799061298089475\n",
      "  episode_reward_min: -2.248444002345953\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 68580\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6323196840286255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014088428534567356\n",
      "          policy_loss: -0.10841086206585168\n",
      "          total_loss: 0.3357983999252319\n",
      "          vf_explained_var: 0.31448692083358765\n",
      "          vf_loss: 0.41211405920982364\n",
      "    num_agent_steps_sampled: 41088000\n",
      "    num_agent_steps_trained: 41088000\n",
      "    num_steps_sampled: 10272000\n",
      "    num_steps_trained: 10272000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.844660194174757\n",
      "    gpu_util_percent0: 0.10524271844660196\n",
      "    ram_util_percent: 48.44077669902913\n",
      "    vram_util_percent0: 0.16128516981505797\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.662276080490623\n",
      "  policy_reward_mean:\n",
      "    default: -0.11997653245223687\n",
      "  policy_reward_min:\n",
      "    default: -1.9469261847323724\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22139212308663048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.042526118246098\n",
      "    mean_inference_ms: 3.2351841127262913\n",
      "    mean_raw_obs_processing_ms: 0.9266827102572214\n",
      "  time_since_restore: 6932.572555780411\n",
      "  time_this_iter_s: 77.48311471939087\n",
      "  time_total_s: 204702.85093307495\n",
      "  timers:\n",
      "    learn_throughput: 78.323\n",
      "    learn_time_ms: 51070.729\n",
      "    sample_throughput: 151.55\n",
      "    sample_time_ms: 26393.925\n",
      "  timestamp: 1638558327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10272000\n",
      "  training_iteration: 2568\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2568</td><td style=\"text-align: right;\">          204703</td><td style=\"text-align: right;\">10272000</td><td style=\"text-align: right;\">-0.479906</td><td style=\"text-align: right;\">          0.00118875</td><td style=\"text-align: right;\">            -2.24844</td><td style=\"text-align: right;\">            114.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41104000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02463000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.22500000000000017\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8278450378856808\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0031146177327794444\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6795813849393049\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 106.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1163559769279483\n",
      "  episode_reward_mean: -0.45232777429634785\n",
      "  episode_reward_min: -2.248444002345953\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 68619\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6243499975204467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014129186041653157\n",
      "          policy_loss: -0.10664722256362438\n",
      "          total_loss: 0.29884709358215333\n",
      "          vf_explained_var: 0.34250763058662415\n",
      "          vf_loss: 0.373306263923645\n",
      "    num_agent_steps_sampled: 41104000\n",
      "    num_agent_steps_trained: 41104000\n",
      "    num_steps_sampled: 10276000\n",
      "    num_steps_trained: 10276000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.0\n",
      "    gpu_util_percent0: 0.10817307692307693\n",
      "    ram_util_percent: 48.349999999999994\n",
      "    vram_util_percent0: 0.1612736350520161\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.662276080490623\n",
      "  policy_reward_mean:\n",
      "    default: -0.11308194357408695\n",
      "  policy_reward_min:\n",
      "    default: -1.928443707758736\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213644395384508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041991813530554\n",
      "    mean_inference_ms: 3.233979261907878\n",
      "    mean_raw_obs_processing_ms: 0.9268247494291332\n",
      "  time_since_restore: 7010.292384147644\n",
      "  time_this_iter_s: 77.71982836723328\n",
      "  time_total_s: 204780.57076144218\n",
      "  timers:\n",
      "    learn_throughput: 78.317\n",
      "    learn_time_ms: 51074.283\n",
      "    sample_throughput: 151.493\n",
      "    sample_time_ms: 26403.887\n",
      "  timestamp: 1638558405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10276000\n",
      "  training_iteration: 2569\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2569</td><td style=\"text-align: right;\">          204781</td><td style=\"text-align: right;\">10276000</td><td style=\"text-align: right;\">-0.452328</td><td style=\"text-align: right;\">            0.116356</td><td style=\"text-align: right;\">            -2.24844</td><td style=\"text-align: right;\">            106.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41120000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.026440000000000016\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.22500000000000017\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6144706578989778\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.04875002575055745\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6795813849393049\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 107.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1163559769279483\n",
      "  episode_reward_mean: -0.45559958968273084\n",
      "  episode_reward_min: -2.248444002345953\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 68657\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6294522185325623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013878974206745625\n",
      "          policy_loss: -0.10617931152135134\n",
      "          total_loss: 0.32025849652290345\n",
      "          vf_explained_var: 0.2946498990058899\n",
      "          vf_loss: 0.394819766998291\n",
      "    num_agent_steps_sampled: 41120000\n",
      "    num_agent_steps_trained: 41120000\n",
      "    num_steps_sampled: 10280000\n",
      "    num_steps_trained: 10280000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.030252100840343\n",
      "    gpu_util_percent0: 0.09277310924369749\n",
      "    ram_util_percent: 48.99579831932774\n",
      "    vram_util_percent0: 0.1612662834136617\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5721504552919128\n",
      "  policy_reward_mean:\n",
      "    default: -0.1138998974206827\n",
      "  policy_reward_min:\n",
      "    default: -1.8049549248571783\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22133769489773336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041304940087965\n",
      "    mean_inference_ms: 3.233055116100826\n",
      "    mean_raw_obs_processing_ms: 0.9271138054072777\n",
      "  time_since_restore: 7087.534751415253\n",
      "  time_this_iter_s: 77.24236726760864\n",
      "  time_total_s: 204857.8131287098\n",
      "  timers:\n",
      "    learn_throughput: 78.338\n",
      "    learn_time_ms: 51060.733\n",
      "    sample_throughput: 151.522\n",
      "    sample_time_ms: 26398.749\n",
      "  timestamp: 1638558495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10280000\n",
      "  training_iteration: 2570\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2570</td><td style=\"text-align: right;\">          204858</td><td style=\"text-align: right;\">10280000</td><td style=\"text-align: right;\"> -0.4556</td><td style=\"text-align: right;\">            0.116356</td><td style=\"text-align: right;\">            -2.24844</td><td style=\"text-align: right;\">            107.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41136000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.026810000000000018\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.22500000000000017\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6705763395363609\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.042818645927427304\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6795813849393049\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-09-32\n",
      "  done: false\n",
      "  episode_len_mean: 114.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1163559769279483\n",
      "  episode_reward_mean: -0.47080832581202253\n",
      "  episode_reward_min: -1.9417245760244455\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 68685\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6298278465270996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013398479849100113\n",
      "          policy_loss: -0.10315310283005237\n",
      "          total_loss: 0.22966944327950478\n",
      "          vf_explained_var: 0.34445643424987793\n",
      "          vf_loss: 0.3022991341352463\n",
      "    num_agent_steps_sampled: 41136000\n",
      "    num_agent_steps_trained: 41136000\n",
      "    num_steps_sampled: 10284000\n",
      "    num_steps_trained: 10284000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.403883495145635\n",
      "    gpu_util_percent0: 0.10728155339805828\n",
      "    ram_util_percent: 48.98155339805822\n",
      "    vram_util_percent0: 0.16109827900721047\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5558580772235644\n",
      "  policy_reward_mean:\n",
      "    default: -0.11770208145300563\n",
      "  policy_reward_min:\n",
      "    default: -1.8992911144980675\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213231237146585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041382312335816\n",
      "    mean_inference_ms: 3.2325842948264008\n",
      "    mean_raw_obs_processing_ms: 0.9273087338915856\n",
      "  time_since_restore: 7165.3427476882935\n",
      "  time_this_iter_s: 77.80799627304077\n",
      "  time_total_s: 204935.62112498283\n",
      "  timers:\n",
      "    learn_throughput: 78.364\n",
      "    learn_time_ms: 51044.155\n",
      "    sample_throughput: 151.38\n",
      "    sample_time_ms: 26423.553\n",
      "  timestamp: 1638558572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10284000\n",
      "  training_iteration: 2571\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2571</td><td style=\"text-align: right;\">          204936</td><td style=\"text-align: right;\">10284000</td><td style=\"text-align: right;\">-0.470808</td><td style=\"text-align: right;\">            0.116356</td><td style=\"text-align: right;\">            -1.94172</td><td style=\"text-align: right;\">             114.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41152000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029820000000000027\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7571894860585887\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.11433414246153939\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6859834890919654\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 120.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00020568062073467175\n",
      "  episode_reward_mean: -0.4955193766717541\n",
      "  episode_reward_min: -1.9417245760244455\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 68720\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6317211480140686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014260634005069732\n",
      "          policy_loss: -0.10867409971356391\n",
      "          total_loss: 0.2978682122528553\n",
      "          vf_explained_var: 0.28036636114120483\n",
      "          vf_loss: 0.37405480313301087\n",
      "    num_agent_steps_sampled: 41152000\n",
      "    num_agent_steps_trained: 41152000\n",
      "    num_steps_sampled: 10288000\n",
      "    num_steps_trained: 10288000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.793203883495153\n",
      "    gpu_util_percent0: 0.10533980582524273\n",
      "    ram_util_percent: 48.99514563106793\n",
      "    vram_util_percent0: 0.16093215384467938\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5977362792007845\n",
      "  policy_reward_mean:\n",
      "    default: -0.1238798441679385\n",
      "  policy_reward_min:\n",
      "    default: -1.8992911144980675\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22130935440154825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041119665608193\n",
      "    mean_inference_ms: 3.2319695254176097\n",
      "    mean_raw_obs_processing_ms: 0.9273838428064337\n",
      "  time_since_restore: 7242.863715648651\n",
      "  time_this_iter_s: 77.52096796035767\n",
      "  time_total_s: 205013.1420929432\n",
      "  timers:\n",
      "    learn_throughput: 78.347\n",
      "    learn_time_ms: 51054.772\n",
      "    sample_throughput: 151.489\n",
      "    sample_time_ms: 26404.49\n",
      "  timestamp: 1638558650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10288000\n",
      "  training_iteration: 2572\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2572</td><td style=\"text-align: right;\">          205013</td><td style=\"text-align: right;\">10288000</td><td style=\"text-align: right;\">-0.495519</td><td style=\"text-align: right;\">         0.000205681</td><td style=\"text-align: right;\">            -1.94172</td><td style=\"text-align: right;\">            120.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41168000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03268000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7571894860585887\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.0946174597813206\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7086124293090843\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 130.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00020568062073467175\n",
      "  episode_reward_mean: -0.5243570376291299\n",
      "  episode_reward_min: -1.9417245760244455\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 68750\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6353410892486572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013731206692755222\n",
      "          policy_loss: -0.10617652044072748\n",
      "          total_loss: 0.23659036515653134\n",
      "          vf_explained_var: 0.292311429977417\n",
      "          vf_loss: 0.3114854800701141\n",
      "    num_agent_steps_sampled: 41168000\n",
      "    num_agent_steps_trained: 41168000\n",
      "    num_steps_sampled: 10292000\n",
      "    num_steps_trained: 10292000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.806796116504852\n",
      "    gpu_util_percent0: 0.10533980582524274\n",
      "    ram_util_percent: 48.8854368932039\n",
      "    vram_util_percent0: 0.16093055648734733\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5977362792007845\n",
      "  policy_reward_mean:\n",
      "    default: -0.13108925940728247\n",
      "  policy_reward_min:\n",
      "    default: -1.8992911144980675\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22130890728085903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041566818950873\n",
      "    mean_inference_ms: 3.2317660753380255\n",
      "    mean_raw_obs_processing_ms: 0.9273704233824347\n",
      "  time_since_restore: 7320.52423286438\n",
      "  time_this_iter_s: 77.66051721572876\n",
      "  time_total_s: 205090.80261015892\n",
      "  timers:\n",
      "    learn_throughput: 78.316\n",
      "    learn_time_ms: 51074.927\n",
      "    sample_throughput: 151.32\n",
      "    sample_time_ms: 26434.036\n",
      "  timestamp: 1638558728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10292000\n",
      "  training_iteration: 2573\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2573</td><td style=\"text-align: right;\">          205091</td><td style=\"text-align: right;\">10292000</td><td style=\"text-align: right;\">-0.524357</td><td style=\"text-align: right;\">         0.000205681</td><td style=\"text-align: right;\">            -1.94172</td><td style=\"text-align: right;\">            130.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41184000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.034410000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23600000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7571894860585887\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.09230023655432808\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7086124293090843\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 133.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.00020568062073467175\n",
      "  episode_reward_mean: -0.5441628814563679\n",
      "  episode_reward_min: -1.8687809638993234\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 68777\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6290472741127014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013577836930751801\n",
      "          policy_loss: -0.10279088416695595\n",
      "          total_loss: 0.2554497002363205\n",
      "          vf_explained_var: 0.30090126395225525\n",
      "          vf_loss: 0.32730857574939726\n",
      "    num_agent_steps_sampled: 41184000\n",
      "    num_agent_steps_trained: 41184000\n",
      "    num_steps_sampled: 10296000\n",
      "    num_steps_trained: 10296000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.301599999999997\n",
      "    gpu_util_percent0: 0.11816000000000002\n",
      "    ram_util_percent: 49.2944\n",
      "    vram_util_percent0: 0.16526357354392893\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5977362792007845\n",
      "  policy_reward_mean:\n",
      "    default: -0.13604072036409195\n",
      "  policy_reward_min:\n",
      "    default: -1.8992911144980675\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22134951759779115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.045348498514244\n",
      "    mean_inference_ms: 3.232174531265221\n",
      "    mean_raw_obs_processing_ms: 0.9276928923376607\n",
      "  time_since_restore: 7401.972946405411\n",
      "  time_this_iter_s: 81.44871354103088\n",
      "  time_total_s: 205172.25132369995\n",
      "  timers:\n",
      "    learn_throughput: 77.968\n",
      "    learn_time_ms: 51302.908\n",
      "    sample_throughput: 150.672\n",
      "    sample_time_ms: 26547.81\n",
      "  timestamp: 1638558822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10296000\n",
      "  training_iteration: 2574\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2574</td><td style=\"text-align: right;\">          205172</td><td style=\"text-align: right;\">10296000</td><td style=\"text-align: right;\">-0.544163</td><td style=\"text-align: right;\">         0.000205681</td><td style=\"text-align: right;\">            -1.86878</td><td style=\"text-align: right;\">             133.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41200000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.032660000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20000000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7558064879928886\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.10420374400893298\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7086124293090843\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 127.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.04993079552781454\n",
      "  episode_reward_mean: -0.518415157168958\n",
      "  episode_reward_min: -1.852122505152362\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 68812\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6301050848960876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013751792974770069\n",
      "          policy_loss: -0.10681461144983768\n",
      "          total_loss: 0.2995037962794304\n",
      "          vf_explained_var: 0.32322588562965393\n",
      "          vf_loss: 0.37499010396003724\n",
      "    num_agent_steps_sampled: 41200000\n",
      "    num_agent_steps_trained: 41200000\n",
      "    num_steps_sampled: 10300000\n",
      "    num_steps_trained: 10300000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.323076923076925\n",
      "    gpu_util_percent0: 0.10192307692307694\n",
      "    ram_util_percent: 49.03942307692308\n",
      "    vram_util_percent0: 0.1622006859543878\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5858668748209155\n",
      "  policy_reward_mean:\n",
      "    default: -0.12960378929223945\n",
      "  policy_reward_min:\n",
      "    default: -1.9275769685641633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22139961005965728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.050233892380746\n",
      "    mean_inference_ms: 3.233170129409628\n",
      "    mean_raw_obs_processing_ms: 0.9282249750773931\n",
      "  time_since_restore: 7480.205347537994\n",
      "  time_this_iter_s: 78.23240113258362\n",
      "  time_total_s: 205250.48372483253\n",
      "  timers:\n",
      "    learn_throughput: 77.9\n",
      "    learn_time_ms: 51348.172\n",
      "    sample_throughput: 150.606\n",
      "    sample_time_ms: 26559.455\n",
      "  timestamp: 1638558900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10300000\n",
      "  training_iteration: 2575\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2575</td><td style=\"text-align: right;\">          205250</td><td style=\"text-align: right;\">10300000</td><td style=\"text-align: right;\">-0.518415</td><td style=\"text-align: right;\">          -0.0499308</td><td style=\"text-align: right;\">            -1.85212</td><td style=\"text-align: right;\">            127.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41216000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029580000000000023\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.16500000000000012\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7558064879928886\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.10393180771581674\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6770101541029636\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 118.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.04993079552781454\n",
      "  episode_reward_mean: -0.49885221794136875\n",
      "  episode_reward_min: -1.7609101793310615\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 68848\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6325254445075988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01404614532738924\n",
      "          policy_loss: -0.10714696380496025\n",
      "          total_loss: 0.29950693202018736\n",
      "          vf_explained_var: 0.33440449833869934\n",
      "          vf_loss: 0.3746550190448761\n",
      "    num_agent_steps_sampled: 41216000\n",
      "    num_agent_steps_trained: 41216000\n",
      "    num_steps_sampled: 10304000\n",
      "    num_steps_trained: 10304000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.906796116504847\n",
      "    gpu_util_percent0: 0.10485436893203884\n",
      "    ram_util_percent: 48.72815533980586\n",
      "    vram_util_percent0: 0.16209343262506506\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5858668748209155\n",
      "  policy_reward_mean:\n",
      "    default: -0.12471305448534223\n",
      "  policy_reward_min:\n",
      "    default: -1.9275769685641633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2214482571748262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05514778594111\n",
      "    mean_inference_ms: 3.233887172699123\n",
      "    mean_raw_obs_processing_ms: 0.9290369539966585\n",
      "  time_since_restore: 7557.709139585495\n",
      "  time_this_iter_s: 77.50379204750061\n",
      "  time_total_s: 205327.98751688004\n",
      "  timers:\n",
      "    learn_throughput: 77.89\n",
      "    learn_time_ms: 51354.555\n",
      "    sample_throughput: 150.611\n",
      "    sample_time_ms: 26558.552\n",
      "  timestamp: 1638558977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10304000\n",
      "  training_iteration: 2576\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2576</td><td style=\"text-align: right;\">          205328</td><td style=\"text-align: right;\">10304000</td><td style=\"text-align: right;\">-0.498852</td><td style=\"text-align: right;\">          -0.0499308</td><td style=\"text-align: right;\">            -1.76091</td><td style=\"text-align: right;\">             118.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41232000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02829000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18800000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7558064879928886\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07648219058684275\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8274390482115571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 114.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.0217360045846251\n",
      "  episode_reward_mean: -0.4841446630289461\n",
      "  episode_reward_min: -1.5855652586820277\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 68880\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6305186500549317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013599040009081364\n",
      "          policy_loss: -0.10423584628850222\n",
      "          total_loss: 0.2420389006435871\n",
      "          vf_explained_var: 0.36237451434135437\n",
      "          vf_loss: 0.31529442977905275\n",
      "    num_agent_steps_sampled: 41232000\n",
      "    num_agent_steps_trained: 41232000\n",
      "    num_steps_sampled: 10308000\n",
      "    num_steps_trained: 10308000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.01260504201681\n",
      "    gpu_util_percent0: 0.09134453781512607\n",
      "    ram_util_percent: 49.37310924369747\n",
      "    vram_util_percent0: 0.1620806269200671\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7420535001859372\n",
      "  policy_reward_mean:\n",
      "    default: -0.12103616575723657\n",
      "  policy_reward_min:\n",
      "    default: -1.9275769685641633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22143968744298953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05480361332543\n",
      "    mean_inference_ms: 3.2337904900587584\n",
      "    mean_raw_obs_processing_ms: 0.9293404551934543\n",
      "  time_since_restore: 7635.122186660767\n",
      "  time_this_iter_s: 77.4130470752716\n",
      "  time_total_s: 205405.4005639553\n",
      "  timers:\n",
      "    learn_throughput: 77.891\n",
      "    learn_time_ms: 51353.544\n",
      "    sample_throughput: 150.495\n",
      "    sample_time_ms: 26579.034\n",
      "  timestamp: 1638559067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10308000\n",
      "  training_iteration: 2577\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2577</td><td style=\"text-align: right;\">          205405</td><td style=\"text-align: right;\">10308000</td><td style=\"text-align: right;\">-0.484145</td><td style=\"text-align: right;\">           -0.021736</td><td style=\"text-align: right;\">            -1.58557</td><td style=\"text-align: right;\">            114.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41248000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03207000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2610000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5772458591507919\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06166313313184266\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8274390482115571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 114.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.15365926658214368\n",
      "  episode_reward_mean: -0.5041489541221182\n",
      "  episode_reward_min: -2.033467241708707\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 68914\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.627364643573761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014088054910302163\n",
      "          policy_loss: -0.1068369140252471\n",
      "          total_loss: 0.2683886759877205\n",
      "          vf_explained_var: 0.2982160449028015\n",
      "          vf_loss: 0.3431312379837036\n",
      "    num_agent_steps_sampled: 41248000\n",
      "    num_agent_steps_trained: 41248000\n",
      "    num_steps_sampled: 10312000\n",
      "    num_steps_trained: 10312000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.875728155339804\n",
      "    gpu_util_percent0: 0.10553398058252428\n",
      "    ram_util_percent: 49.35922330097088\n",
      "    vram_util_percent0: 0.16151199455620618\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7420535001859372\n",
      "  policy_reward_mean:\n",
      "    default: -0.1260372385305296\n",
      "  policy_reward_min:\n",
      "    default: -1.9633124893904\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22141590356719695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.054061089640605\n",
      "    mean_inference_ms: 3.2327922429713203\n",
      "    mean_raw_obs_processing_ms: 0.9296271599364401\n",
      "  time_since_restore: 7712.743764400482\n",
      "  time_this_iter_s: 77.62157773971558\n",
      "  time_total_s: 205483.02214169502\n",
      "  timers:\n",
      "    learn_throughput: 77.86\n",
      "    learn_time_ms: 51374.171\n",
      "    sample_throughput: 150.533\n",
      "    sample_time_ms: 26572.172\n",
      "  timestamp: 1638559145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10312000\n",
      "  training_iteration: 2578\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2578</td><td style=\"text-align: right;\">          205483</td><td style=\"text-align: right;\">10312000</td><td style=\"text-align: right;\">-0.504149</td><td style=\"text-align: right;\">            0.153659</td><td style=\"text-align: right;\">            -2.03347</td><td style=\"text-align: right;\">            114.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41264000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03524000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2750000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6956866018105003\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.008302390183496811\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8274390482115571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 124.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.15365926658214368\n",
      "  episode_reward_mean: -0.5216077963024651\n",
      "  episode_reward_min: -2.033467241708707\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 68946\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6326858706474304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014211826950311661\n",
      "          policy_loss: -0.10934979331493377\n",
      "          total_loss: 0.28033032941818237\n",
      "          vf_explained_var: 0.3184916377067566\n",
      "          vf_loss: 0.35730380392074584\n",
      "    num_agent_steps_sampled: 41264000\n",
      "    num_agent_steps_trained: 41264000\n",
      "    num_steps_sampled: 10316000\n",
      "    num_steps_trained: 10316000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.98365384615385\n",
      "    gpu_util_percent0: 0.1071153846153846\n",
      "    ram_util_percent: 49.3048076923077\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7420535001859372\n",
      "  policy_reward_mean:\n",
      "    default: -0.13040194907561628\n",
      "  policy_reward_min:\n",
      "    default: -2.147719098164754\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2214218852229672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.0545285156909\n",
      "    mean_inference_ms: 3.232209247490557\n",
      "    mean_raw_obs_processing_ms: 0.9296502476274157\n",
      "  time_since_restore: 7790.611849069595\n",
      "  time_this_iter_s: 77.86808466911316\n",
      "  time_total_s: 205560.89022636414\n",
      "  timers:\n",
      "    learn_throughput: 77.89\n",
      "    learn_time_ms: 51354.552\n",
      "    sample_throughput: 150.339\n",
      "    sample_time_ms: 26606.467\n",
      "  timestamp: 1638559223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10316000\n",
      "  training_iteration: 2579\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2579</td><td style=\"text-align: right;\">          205561</td><td style=\"text-align: right;\">10316000</td><td style=\"text-align: right;\">-0.521608</td><td style=\"text-align: right;\">            0.153659</td><td style=\"text-align: right;\">            -2.03347</td><td style=\"text-align: right;\">            124.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41280000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03812000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2750000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6956866018105003\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.02523539262970858\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6973136465448438\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 131.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.15365926658214368\n",
      "  episode_reward_mean: -0.5396512664031407\n",
      "  episode_reward_min: -2.033467241708707\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 68976\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6385836238861085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013853552602231502\n",
      "          policy_loss: -0.10675363601744176\n",
      "          total_loss: 0.23028055825829505\n",
      "          vf_explained_var: 0.3494124710559845\n",
      "          vf_loss: 0.30547406923770903\n",
      "    num_agent_steps_sampled: 41280000\n",
      "    num_agent_steps_trained: 41280000\n",
      "    num_steps_sampled: 10320000\n",
      "    num_steps_trained: 10320000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.13697478991597\n",
      "    gpu_util_percent0: 0.09126050420168068\n",
      "    ram_util_percent: 49.39747899159664\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5627592457604567\n",
      "  policy_reward_mean:\n",
      "    default: -0.13491281660078522\n",
      "  policy_reward_min:\n",
      "    default: -2.147719098164754\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22142824717805126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.054824745400566\n",
      "    mean_inference_ms: 3.2317316635734255\n",
      "    mean_raw_obs_processing_ms: 0.9295333080726991\n",
      "  time_since_restore: 7867.849080562592\n",
      "  time_this_iter_s: 77.23723149299622\n",
      "  time_total_s: 205638.12745785713\n",
      "  timers:\n",
      "    learn_throughput: 77.897\n",
      "    learn_time_ms: 51349.874\n",
      "    sample_throughput: 150.316\n",
      "    sample_time_ms: 26610.574\n",
      "  timestamp: 1638559313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10320000\n",
      "  training_iteration: 2580\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2580</td><td style=\"text-align: right;\">          205638</td><td style=\"text-align: right;\">10320000</td><td style=\"text-align: right;\">-0.539651</td><td style=\"text-align: right;\">            0.153659</td><td style=\"text-align: right;\">            -2.03347</td><td style=\"text-align: right;\">            131.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41296000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.032690000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2750000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6956866018105003\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.022712130824112924\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6993531628689216\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 131.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1366136837852272\n",
      "  episode_reward_mean: -0.5209101885306626\n",
      "  episode_reward_min: -1.9759032437584745\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 69009\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6303056712150574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01354690035432577\n",
      "          policy_loss: -0.10518885596841573\n",
      "          total_loss: 0.2417510772049427\n",
      "          vf_explained_var: 0.34081146121025085\n",
      "          vf_loss: 0.31607840085029604\n",
      "    num_agent_steps_sampled: 41296000\n",
      "    num_agent_steps_trained: 41296000\n",
      "    num_steps_sampled: 10324000\n",
      "    num_steps_trained: 10324000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.833009708737865\n",
      "    gpu_util_percent0: 0.1066990291262136\n",
      "    ram_util_percent: 49.45631067961166\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5339057432694547\n",
      "  policy_reward_mean:\n",
      "    default: -0.13022754713266568\n",
      "  policy_reward_min:\n",
      "    default: -2.147719098164754\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22144595318956217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05472009622047\n",
      "    mean_inference_ms: 3.2318064129736195\n",
      "    mean_raw_obs_processing_ms: 0.9294089280946656\n",
      "  time_since_restore: 7945.245704650879\n",
      "  time_this_iter_s: 77.39662408828735\n",
      "  time_total_s: 205715.52408194542\n",
      "  timers:\n",
      "    learn_throughput: 77.914\n",
      "    learn_time_ms: 51338.448\n",
      "    sample_throughput: 150.486\n",
      "    sample_time_ms: 26580.555\n",
      "  timestamp: 1638559390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10324000\n",
      "  training_iteration: 2581\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2581</td><td style=\"text-align: right;\">          205716</td><td style=\"text-align: right;\">10324000</td><td style=\"text-align: right;\">-0.52091</td><td style=\"text-align: right;\">            0.136614</td><td style=\"text-align: right;\">             -1.9759</td><td style=\"text-align: right;\">            131.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41312000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.027860000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20000000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6499648706403196\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.007648230753875983\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8887509837320812\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 119.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1366136837852272\n",
      "  episode_reward_mean: -0.48447997789435676\n",
      "  episode_reward_min: -1.8419701976467837\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 69041\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6303313336372376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014024150535464287\n",
      "          policy_loss: -0.10750484971702098\n",
      "          total_loss: 0.24463145595788954\n",
      "          vf_explained_var: 0.3375578820705414\n",
      "          vf_loss: 0.3201875379085541\n",
      "    num_agent_steps_sampled: 41312000\n",
      "    num_agent_steps_trained: 41312000\n",
      "    num_steps_sampled: 10328000\n",
      "    num_steps_trained: 10328000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.882524271844655\n",
      "    gpu_util_percent0: 0.10563106796116506\n",
      "    ram_util_percent: 49.32233009708737\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6487356322963391\n",
      "  policy_reward_mean:\n",
      "    default: -0.12111999447358915\n",
      "  policy_reward_min:\n",
      "    default: -1.9709986830423705\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22143397571014273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05365231171922\n",
      "    mean_inference_ms: 3.231537779841598\n",
      "    mean_raw_obs_processing_ms: 0.9292034628732868\n",
      "  time_since_restore: 8022.839921236038\n",
      "  time_this_iter_s: 77.5942165851593\n",
      "  time_total_s: 205793.11829853058\n",
      "  timers:\n",
      "    learn_throughput: 77.899\n",
      "    learn_time_ms: 51348.451\n",
      "    sample_throughput: 150.502\n",
      "    sample_time_ms: 26577.741\n",
      "  timestamp: 1638559468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10328000\n",
      "  training_iteration: 2582\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2582</td><td style=\"text-align: right;\">          205793</td><td style=\"text-align: right;\">10328000</td><td style=\"text-align: right;\">-0.48448</td><td style=\"text-align: right;\">            0.136614</td><td style=\"text-align: right;\">            -1.84197</td><td style=\"text-align: right;\">             119.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41328000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.032650000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.32200000000000023\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6182953978846084\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.007689305472830683\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8887509837320812\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 125.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.01712034023945641\n",
      "  episode_reward_mean: -0.5122303094842673\n",
      "  episode_reward_min: -2.400761935417463\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 69073\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6342083830833435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014132613956928252\n",
      "          policy_loss: -0.10671769952774048\n",
      "          total_loss: 0.24821403899788858\n",
      "          vf_explained_var: 0.37088271975517273\n",
      "          vf_loss: 0.3227358770370483\n",
      "    num_agent_steps_sampled: 41328000\n",
      "    num_agent_steps_trained: 41328000\n",
      "    num_steps_sampled: 10332000\n",
      "    num_steps_trained: 10332000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.976470588235294\n",
      "    gpu_util_percent0: 0.10490196078431376\n",
      "    ram_util_percent: 49.089215686274514\n",
      "    vram_util_percent0: 0.15860480421191178\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6487356322963391\n",
      "  policy_reward_mean:\n",
      "    default: -0.12805757737106682\n",
      "  policy_reward_min:\n",
      "    default: -2.003652508717849\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22141494668409295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05287315131068\n",
      "    mean_inference_ms: 3.231091692349781\n",
      "    mean_raw_obs_processing_ms: 0.9290822291232985\n",
      "  time_since_restore: 8100.131826400757\n",
      "  time_this_iter_s: 77.29190516471863\n",
      "  time_total_s: 205870.4102036953\n",
      "  timers:\n",
      "    learn_throughput: 77.94\n",
      "    learn_time_ms: 51321.574\n",
      "    sample_throughput: 150.558\n",
      "    sample_time_ms: 26567.786\n",
      "  timestamp: 1638559545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10332000\n",
      "  training_iteration: 2583\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2583</td><td style=\"text-align: right;\">          205870</td><td style=\"text-align: right;\">10332000</td><td style=\"text-align: right;\">-0.51223</td><td style=\"text-align: right;\">           0.0171203</td><td style=\"text-align: right;\">            -2.40076</td><td style=\"text-align: right;\">            125.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41344000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03494000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.32200000000000023\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5921439912153563\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.024879158687836835\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8887509837320812\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 117.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1610263846175286\n",
      "  episode_reward_mean: -0.48146502827738785\n",
      "  episode_reward_min: -2.400761935417463\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 69112\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6287812089920044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013938415594398975\n",
      "          policy_loss: -0.10604212643206119\n",
      "          total_loss: 0.2913033921867609\n",
      "          vf_explained_var: 0.34439411759376526\n",
      "          vf_loss: 0.3655920660495758\n",
      "    num_agent_steps_sampled: 41344000\n",
      "    num_agent_steps_trained: 41344000\n",
      "    num_steps_sampled: 10336000\n",
      "    num_steps_trained: 10336000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.990909090909092\n",
      "    gpu_util_percent0: 0.09148760330578512\n",
      "    ram_util_percent: 49.64958677685951\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6487356322963391\n",
      "  policy_reward_mean:\n",
      "    default: -0.12036625706934691\n",
      "  policy_reward_min:\n",
      "    default: -2.003652508717849\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.221391095878727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.051595204259876\n",
      "    mean_inference_ms: 3.2307154724178635\n",
      "    mean_raw_obs_processing_ms: 0.9288902836247803\n",
      "  time_since_restore: 8177.4802277088165\n",
      "  time_this_iter_s: 77.34840130805969\n",
      "  time_total_s: 205947.75860500336\n",
      "  timers:\n",
      "    learn_throughput: 78.275\n",
      "    learn_time_ms: 51101.612\n",
      "    sample_throughput: 151.643\n",
      "    sample_time_ms: 26377.771\n",
      "  timestamp: 1638559636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10336000\n",
      "  training_iteration: 2584\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2584</td><td style=\"text-align: right;\">          205948</td><td style=\"text-align: right;\">10336000</td><td style=\"text-align: right;\">-0.481465</td><td style=\"text-align: right;\">            0.161026</td><td style=\"text-align: right;\">            -2.40076</td><td style=\"text-align: right;\">            117.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41360000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.032710000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.32200000000000023\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.580133612421011\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.04910233211952035\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7343326517436584\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 109.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1610263846175286\n",
      "  episode_reward_mean: -0.46279084377592405\n",
      "  episode_reward_min: -2.400761935417463\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 69152\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6237540621757507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013770958371460439\n",
      "          policy_loss: -0.1056324879527092\n",
      "          total_loss: 0.3180665552616119\n",
      "          vf_explained_var: 0.32263249158859253\n",
      "          vf_loss: 0.39232708048820497\n",
      "    num_agent_steps_sampled: 41360000\n",
      "    num_agent_steps_trained: 41360000\n",
      "    num_steps_sampled: 10340000\n",
      "    num_steps_trained: 10340000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.99223300970874\n",
      "    gpu_util_percent0: 0.10533980582524274\n",
      "    ram_util_percent: 49.67184466019418\n",
      "    vram_util_percent0: 0.15861598571323599\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5346168181762576\n",
      "  policy_reward_mean:\n",
      "    default: -0.115697710943981\n",
      "  policy_reward_min:\n",
      "    default: -2.003652508717849\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22136465008125633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.050053927066173\n",
      "    mean_inference_ms: 3.230387038259674\n",
      "    mean_raw_obs_processing_ms: 0.9291609813938845\n",
      "  time_since_restore: 8254.885806322098\n",
      "  time_this_iter_s: 77.40557861328125\n",
      "  time_total_s: 206025.16418361664\n",
      "  timers:\n",
      "    learn_throughput: 78.367\n",
      "    learn_time_ms: 51041.861\n",
      "    sample_throughput: 151.775\n",
      "    sample_time_ms: 26354.838\n",
      "  timestamp: 1638559713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10340000\n",
      "  training_iteration: 2585\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2585</td><td style=\"text-align: right;\">          206025</td><td style=\"text-align: right;\">10340000</td><td style=\"text-align: right;\">-0.462791</td><td style=\"text-align: right;\">            0.161026</td><td style=\"text-align: right;\">            -2.40076</td><td style=\"text-align: right;\">             109.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41376000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02269000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23800000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6335604927295831\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.015625377020285543\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7343326517436584\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 93.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1610263846175286\n",
      "  episode_reward_mean: -0.417113897782988\n",
      "  episode_reward_min: -1.5579068431670595\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 69195\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6261751613616944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014183128654956818\n",
      "          policy_loss: -0.1086238936111331\n",
      "          total_loss: 0.34881930458545685\n",
      "          vf_explained_var: 0.3147393465042114\n",
      "          vf_loss: 0.42513225483894346\n",
      "    num_agent_steps_sampled: 41376000\n",
      "    num_agent_steps_trained: 41376000\n",
      "    num_steps_sampled: 10344000\n",
      "    num_steps_trained: 10344000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.728155339805824\n",
      "    gpu_util_percent0: 0.10514563106796118\n",
      "    ram_util_percent: 49.67961165048547\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5129038989872605\n",
      "  policy_reward_mean:\n",
      "    default: -0.10427847444574705\n",
      "  policy_reward_min:\n",
      "    default: -1.7628448191877548\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213452181336126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.048977594860546\n",
      "    mean_inference_ms: 3.2300612125584354\n",
      "    mean_raw_obs_processing_ms: 0.9297967959074349\n",
      "  time_since_restore: 8332.486946105957\n",
      "  time_this_iter_s: 77.60113978385925\n",
      "  time_total_s: 206102.7653234005\n",
      "  timers:\n",
      "    learn_throughput: 78.367\n",
      "    learn_time_ms: 51041.97\n",
      "    sample_throughput: 151.718\n",
      "    sample_time_ms: 26364.625\n",
      "  timestamp: 1638559791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10344000\n",
      "  training_iteration: 2586\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2586</td><td style=\"text-align: right;\">          206103</td><td style=\"text-align: right;\">10344000</td><td style=\"text-align: right;\">-0.417114</td><td style=\"text-align: right;\">            0.161026</td><td style=\"text-align: right;\">            -1.55791</td><td style=\"text-align: right;\">             93.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41392000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.027530000000000013\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.23800000000000018\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7284563968401802\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.024644728248441005\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6416389316583455\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-31-08\n",
      "  done: false\n",
      "  episode_len_mean: 103.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.030303320109465126\n",
      "  episode_reward_mean: -0.43714425075605307\n",
      "  episode_reward_min: -1.6388843327082605\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 69229\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6298440675735474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013834212079644203\n",
      "          policy_loss: -0.10731125268340111\n",
      "          total_loss: 0.26313540279865266\n",
      "          vf_explained_var: 0.3384013772010803\n",
      "          vf_loss: 0.3389305894374847\n",
      "    num_agent_steps_sampled: 41392000\n",
      "    num_agent_steps_trained: 41392000\n",
      "    num_steps_sampled: 10348000\n",
      "    num_steps_trained: 10348000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.03398058252427\n",
      "    gpu_util_percent0: 0.10417475728155344\n",
      "    ram_util_percent: 49.708737864077705\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5140611645918993\n",
      "  policy_reward_mean:\n",
      "    default: -0.10928606268901328\n",
      "  policy_reward_min:\n",
      "    default: -1.8803821630426933\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22134250594781985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.048266089187845\n",
      "    mean_inference_ms: 3.2297944378294914\n",
      "    mean_raw_obs_processing_ms: 0.9303120724732088\n",
      "  time_since_restore: 8409.88439154625\n",
      "  time_this_iter_s: 77.39744544029236\n",
      "  time_total_s: 206180.1627688408\n",
      "  timers:\n",
      "    learn_throughput: 78.375\n",
      "    learn_time_ms: 51036.69\n",
      "    sample_throughput: 151.697\n",
      "    sample_time_ms: 26368.436\n",
      "  timestamp: 1638559868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10348000\n",
      "  training_iteration: 2587\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2587</td><td style=\"text-align: right;\">          206180</td><td style=\"text-align: right;\">10348000</td><td style=\"text-align: right;\">-0.437144</td><td style=\"text-align: right;\">           0.0303033</td><td style=\"text-align: right;\">            -1.63888</td><td style=\"text-align: right;\">            103.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41408000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.027940000000000017\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2870000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7284563968401802\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03100718164626851\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6547192559414274\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-32-39\n",
      "  done: false\n",
      "  episode_len_mean: 102.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.13320130009450448\n",
      "  episode_reward_mean: -0.4299049817906265\n",
      "  episode_reward_min: -1.6388843327082605\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 69271\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6319000988006592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014094156295061111\n",
      "          policy_loss: -0.10803337159007788\n",
      "          total_loss: 0.3099480234384537\n",
      "          vf_explained_var: 0.3259460926055908\n",
      "          vf_loss: 0.3858731417655945\n",
      "    num_agent_steps_sampled: 41408000\n",
      "    num_agent_steps_trained: 41408000\n",
      "    num_steps_sampled: 10352000\n",
      "    num_steps_trained: 10352000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.098333333333333\n",
      "    gpu_util_percent0: 0.0915\n",
      "    ram_util_percent: 49.82416666666668\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.523003769425857\n",
      "  policy_reward_mean:\n",
      "    default: -0.10747624544765662\n",
      "  policy_reward_min:\n",
      "    default: -1.8803821630426933\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22133831139594598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.046914896095743\n",
      "    mean_inference_ms: 3.229535444928703\n",
      "    mean_raw_obs_processing_ms: 0.9307165328137015\n",
      "  time_since_restore: 8487.235276222229\n",
      "  time_this_iter_s: 77.35088467597961\n",
      "  time_total_s: 206257.51365351677\n",
      "  timers:\n",
      "    learn_throughput: 78.413\n",
      "    learn_time_ms: 51012.189\n",
      "    sample_throughput: 151.712\n",
      "    sample_time_ms: 26365.771\n",
      "  timestamp: 1638559959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10352000\n",
      "  training_iteration: 2588\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2588</td><td style=\"text-align: right;\">          206258</td><td style=\"text-align: right;\">10352000</td><td style=\"text-align: right;\">-0.429905</td><td style=\"text-align: right;\">            0.133201</td><td style=\"text-align: right;\">            -1.63888</td><td style=\"text-align: right;\">            102.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41424000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03155000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2870000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7284563968401802\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06155818086409874\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7169820188724193\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 114.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.13320130009450448\n",
      "  episode_reward_mean: -0.46431775711321455\n",
      "  episode_reward_min: -1.6388843327082605\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 69296\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6386399383544922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013732638359069824\n",
      "          policy_loss: -0.10752011176943779\n",
      "          total_loss: 0.23287519457936287\n",
      "          vf_explained_var: 0.3074241578578949\n",
      "          vf_loss: 0.30911064040660857\n",
      "    num_agent_steps_sampled: 41424000\n",
      "    num_agent_steps_trained: 41424000\n",
      "    num_steps_sampled: 10356000\n",
      "    num_steps_trained: 10356000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.00485436893204\n",
      "    gpu_util_percent0: 0.10553398058252429\n",
      "    ram_util_percent: 49.8223300970874\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5659293280866111\n",
      "  policy_reward_mean:\n",
      "    default: -0.1160794392783036\n",
      "  policy_reward_min:\n",
      "    default: -1.8803821630426933\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213348190138487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04610027435413\n",
      "    mean_inference_ms: 3.229302300906087\n",
      "    mean_raw_obs_processing_ms: 0.9307066857263129\n",
      "  time_since_restore: 8564.890637397766\n",
      "  time_this_iter_s: 77.65536117553711\n",
      "  time_total_s: 206335.1690146923\n",
      "  timers:\n",
      "    learn_throughput: 78.37\n",
      "    learn_time_ms: 51040.148\n",
      "    sample_throughput: 151.992\n",
      "    sample_time_ms: 26317.15\n",
      "  timestamp: 1638560036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10356000\n",
      "  training_iteration: 2589\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2589</td><td style=\"text-align: right;\">          206335</td><td style=\"text-align: right;\">10356000</td><td style=\"text-align: right;\">-0.464318</td><td style=\"text-align: right;\">            0.133201</td><td style=\"text-align: right;\">            -1.63888</td><td style=\"text-align: right;\">            114.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41440000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03212000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2870000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6715292351208841\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.019063271119834156\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7169820188724193\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 113.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.016071717325893076\n",
      "  episode_reward_mean: -0.4789613144342389\n",
      "  episode_reward_min: -2.2221571860461973\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 69335\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6271162405014038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013879070557653904\n",
      "          policy_loss: -0.10523376914858817\n",
      "          total_loss: 0.26228770732879636\n",
      "          vf_explained_var: 0.3294312655925751\n",
      "          vf_loss: 0.3359032187461853\n",
      "    num_agent_steps_sampled: 41440000\n",
      "    num_agent_steps_trained: 41440000\n",
      "    num_steps_sampled: 10360000\n",
      "    num_steps_trained: 10360000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.00485436893204\n",
      "    gpu_util_percent0: 0.10621359223300972\n",
      "    ram_util_percent: 49.37184466019417\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5748104959568643\n",
      "  policy_reward_mean:\n",
      "    default: -0.11974032860855972\n",
      "  policy_reward_min:\n",
      "    default: -1.8639538949014076\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22133120410522739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.045049855227045\n",
      "    mean_inference_ms: 3.2293006069168535\n",
      "    mean_raw_obs_processing_ms: 0.9305114883915835\n",
      "  time_since_restore: 8642.332906961441\n",
      "  time_this_iter_s: 77.44226956367493\n",
      "  time_total_s: 206412.61128425598\n",
      "  timers:\n",
      "    learn_throughput: 78.366\n",
      "    learn_time_ms: 51042.472\n",
      "    sample_throughput: 151.886\n",
      "    sample_time_ms: 26335.461\n",
      "  timestamp: 1638560114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10360000\n",
      "  training_iteration: 2590\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2590</td><td style=\"text-align: right;\">          206413</td><td style=\"text-align: right;\">10360000</td><td style=\"text-align: right;\">-0.478961</td><td style=\"text-align: right;\">           0.0160717</td><td style=\"text-align: right;\">            -2.22216</td><td style=\"text-align: right;\">            113.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41456000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03994000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3100000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7090023921249765\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03415681972591207\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7169820188724193\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-36-31\n",
      "  done: false\n",
      "  episode_len_mean: 132.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.016071717325893076\n",
      "  episode_reward_mean: -0.5412719246976727\n",
      "  episode_reward_min: -2.2221571860461973\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 69357\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6366138243675232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013687654331326484\n",
      "          policy_loss: -0.10335539217293263\n",
      "          total_loss: 0.17392473262548447\n",
      "          vf_explained_var: 0.3176145553588867\n",
      "          vf_loss: 0.2460979368686676\n",
      "    num_agent_steps_sampled: 41456000\n",
      "    num_agent_steps_trained: 41456000\n",
      "    num_steps_sampled: 10364000\n",
      "    num_steps_trained: 10364000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.79514563106796\n",
      "    gpu_util_percent0: 0.10495145631067965\n",
      "    ram_util_percent: 49.53398058252424\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5748104959568643\n",
      "  policy_reward_mean:\n",
      "    default: -0.13531798117441818\n",
      "  policy_reward_min:\n",
      "    default: -1.9200603813274149\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22133301937874855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.044522207362196\n",
      "    mean_inference_ms: 3.229369338502193\n",
      "    mean_raw_obs_processing_ms: 0.9302945108904848\n",
      "  time_since_restore: 8719.764699220657\n",
      "  time_this_iter_s: 77.43179225921631\n",
      "  time_total_s: 206490.0430765152\n",
      "  timers:\n",
      "    learn_throughput: 78.353\n",
      "    learn_time_ms: 51051.133\n",
      "    sample_throughput: 151.914\n",
      "    sample_time_ms: 26330.766\n",
      "  timestamp: 1638560191\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10364000\n",
      "  training_iteration: 2591\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2591</td><td style=\"text-align: right;\">          206490</td><td style=\"text-align: right;\">10364000</td><td style=\"text-align: right;\">-0.541272</td><td style=\"text-align: right;\">           0.0160717</td><td style=\"text-align: right;\">            -2.22216</td><td style=\"text-align: right;\">            132.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41472000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03366000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3100000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7090023921249765\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.021954487590471174\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7010543122153214\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 124.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08543672745934627\n",
      "  episode_reward_mean: -0.5083587659818491\n",
      "  episode_reward_min: -2.2221571860461973\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 69396\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6300873312950134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013905662424862384\n",
      "          policy_loss: -0.1069834734275937\n",
      "          total_loss: 0.307004695892334\n",
      "          vf_explained_var: 0.2829662263393402\n",
      "          vf_loss: 0.38230933499336245\n",
      "    num_agent_steps_sampled: 41472000\n",
      "    num_agent_steps_trained: 41472000\n",
      "    num_steps_sampled: 10368000\n",
      "    num_steps_trained: 10368000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.126050420168067\n",
      "    gpu_util_percent0: 0.09453781512605042\n",
      "    ram_util_percent: 49.9\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5748104959568643\n",
      "  policy_reward_mean:\n",
      "    default: -0.1270896914954623\n",
      "  policy_reward_min:\n",
      "    default: -1.9200603813274149\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22132007237933912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.043056387976694\n",
      "    mean_inference_ms: 3.2290126879336833\n",
      "    mean_raw_obs_processing_ms: 0.9300558871915172\n",
      "  time_since_restore: 8796.996891021729\n",
      "  time_this_iter_s: 77.23219180107117\n",
      "  time_total_s: 206567.27526831627\n",
      "  timers:\n",
      "    learn_throughput: 78.382\n",
      "    learn_time_ms: 51032.028\n",
      "    sample_throughput: 152.012\n",
      "    sample_time_ms: 26313.644\n",
      "  timestamp: 1638560281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10368000\n",
      "  training_iteration: 2592\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2592</td><td style=\"text-align: right;\">          206567</td><td style=\"text-align: right;\">10368000</td><td style=\"text-align: right;\">-0.508359</td><td style=\"text-align: right;\">           0.0854367</td><td style=\"text-align: right;\">            -2.22216</td><td style=\"text-align: right;\">            124.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41488000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03724000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3100000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7090023921249765\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.023820562250733816\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6273681583119959\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 128.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08543672745934627\n",
      "  episode_reward_mean: -0.502164641753458\n",
      "  episode_reward_min: -1.975553440988025\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 69426\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6289676008224487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013731886364519596\n",
      "          policy_loss: -0.10646340494602918\n",
      "          total_loss: 0.2379485557228327\n",
      "          vf_explained_var: 0.33186015486717224\n",
      "          vf_loss: 0.3131290076971054\n",
      "    num_agent_steps_sampled: 41488000\n",
      "    num_agent_steps_trained: 41488000\n",
      "    num_steps_sampled: 10372000\n",
      "    num_steps_trained: 10372000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.766990291262132\n",
      "    gpu_util_percent0: 0.10601941747572817\n",
      "    ram_util_percent: 49.961165048543656\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6710135175100485\n",
      "  policy_reward_mean:\n",
      "    default: -0.12554116043836452\n",
      "  policy_reward_min:\n",
      "    default: -1.9200603813274149\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22130689565214376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04212745160877\n",
      "    mean_inference_ms: 3.2287421651010018\n",
      "    mean_raw_obs_processing_ms: 0.9298509231970218\n",
      "  time_since_restore: 8874.309666395187\n",
      "  time_this_iter_s: 77.31277537345886\n",
      "  time_total_s: 206644.58804368973\n",
      "  timers:\n",
      "    learn_throughput: 78.388\n",
      "    learn_time_ms: 51027.926\n",
      "    sample_throughput: 151.983\n",
      "    sample_time_ms: 26318.732\n",
      "  timestamp: 1638560358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10372000\n",
      "  training_iteration: 2593\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2593</td><td style=\"text-align: right;\">          206645</td><td style=\"text-align: right;\">10372000</td><td style=\"text-align: right;\">-0.502165</td><td style=\"text-align: right;\">           0.0854367</td><td style=\"text-align: right;\">            -1.97555</td><td style=\"text-align: right;\">            128.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41504000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03780000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2740000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7287822883741338\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.036713587551963925\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.65887377607766\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 130.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08543672745934627\n",
      "  episode_reward_mean: -0.5174232215050908\n",
      "  episode_reward_min: -1.975553440988025\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 69448\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6370083389282226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013461100213229656\n",
      "          policy_loss: -0.10392561307549476\n",
      "          total_loss: 0.1989912572801113\n",
      "          vf_explained_var: 0.32497599720954895\n",
      "          vf_loss: 0.2722507990598679\n",
      "    num_agent_steps_sampled: 41504000\n",
      "    num_agent_steps_trained: 41504000\n",
      "    num_steps_sampled: 10376000\n",
      "    num_steps_trained: 10376000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.890196078431373\n",
      "    gpu_util_percent0: 0.1054901960784314\n",
      "    ram_util_percent: 50.00392156862742\n",
      "    vram_util_percent0: 0.15860480421191178\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6710135175100485\n",
      "  policy_reward_mean:\n",
      "    default: -0.1293558053762727\n",
      "  policy_reward_min:\n",
      "    default: -1.861196768092366\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2212971847956996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041532706561885\n",
      "    mean_inference_ms: 3.2285849453087674\n",
      "    mean_raw_obs_processing_ms: 0.9296344928918601\n",
      "  time_since_restore: 8951.646125078201\n",
      "  time_this_iter_s: 77.33645868301392\n",
      "  time_total_s: 206721.92450237274\n",
      "  timers:\n",
      "    learn_throughput: 78.413\n",
      "    learn_time_ms: 51011.725\n",
      "    sample_throughput: 151.896\n",
      "    sample_time_ms: 26333.819\n",
      "  timestamp: 1638560436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10376000\n",
      "  training_iteration: 2594\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2594</td><td style=\"text-align: right;\">          206722</td><td style=\"text-align: right;\">10376000</td><td style=\"text-align: right;\">-0.517423</td><td style=\"text-align: right;\">           0.0854367</td><td style=\"text-align: right;\">            -1.97555</td><td style=\"text-align: right;\">            130.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41520000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03673000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2740000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7287822883741338\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05544071974459553\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.827942313859847\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 133.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.09258268899201161\n",
      "  episode_reward_mean: -0.5120792123268783\n",
      "  episode_reward_min: -2.842807680759061\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 69486\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6207337870597839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013504640616476537\n",
      "          policy_loss: -0.1018479069173336\n",
      "          total_loss: 0.2934010772109032\n",
      "          vf_explained_var: 0.3020654618740082\n",
      "          vf_loss: 0.36448372387886047\n",
      "    num_agent_steps_sampled: 41520000\n",
      "    num_agent_steps_trained: 41520000\n",
      "    num_steps_sampled: 10380000\n",
      "    num_steps_trained: 10380000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.993333333333332\n",
      "    gpu_util_percent0: 0.09083333333333334\n",
      "    ram_util_percent: 50.19666666666667\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6710135175100485\n",
      "  policy_reward_mean:\n",
      "    default: -0.12801980308171954\n",
      "  policy_reward_min:\n",
      "    default: -1.9690041271543464\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2212946602218138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04070912401527\n",
      "    mean_inference_ms: 3.2285993908456505\n",
      "    mean_raw_obs_processing_ms: 0.9292992925895465\n",
      "  time_since_restore: 9028.914665699005\n",
      "  time_this_iter_s: 77.26854062080383\n",
      "  time_total_s: 206799.19304299355\n",
      "  timers:\n",
      "    learn_throughput: 78.426\n",
      "    learn_time_ms: 51003.345\n",
      "    sample_throughput: 151.927\n",
      "    sample_time_ms: 26328.431\n",
      "  timestamp: 1638560525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10380000\n",
      "  training_iteration: 2595\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2595</td><td style=\"text-align: right;\">          206799</td><td style=\"text-align: right;\">10380000</td><td style=\"text-align: right;\">-0.512079</td><td style=\"text-align: right;\">           0.0925827</td><td style=\"text-align: right;\">            -2.84281</td><td style=\"text-align: right;\">            133.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41536000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03904000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2740000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8291923107623477\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.08230940999221589\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.827942313859847\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-43-23\n",
      "  done: false\n",
      "  episode_len_mean: 134.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1410696750272602\n",
      "  episode_reward_mean: -0.515557852593984\n",
      "  episode_reward_min: -2.842807680759061\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 69518\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6299887046813964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014101923897862434\n",
      "          policy_loss: -0.10574983783066273\n",
      "          total_loss: 0.2257156696021557\n",
      "          vf_explained_var: 0.34156784415245056\n",
      "          vf_loss: 0.29933956515789034\n",
      "    num_agent_steps_sampled: 41536000\n",
      "    num_agent_steps_trained: 41536000\n",
      "    num_steps_sampled: 10384000\n",
      "    num_steps_trained: 10384000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.933980582524274\n",
      "    gpu_util_percent0: 0.10582524271844661\n",
      "    ram_util_percent: 50.075728155339775\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5400783987723383\n",
      "  policy_reward_mean:\n",
      "    default: -0.128889463148496\n",
      "  policy_reward_min:\n",
      "    default: -1.9690041271543464\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22129337665815654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.0400757901641\n",
      "    mean_inference_ms: 3.2284247497271816\n",
      "    mean_raw_obs_processing_ms: 0.9290314864523702\n",
      "  time_since_restore: 9106.298323631287\n",
      "  time_this_iter_s: 77.3836579322815\n",
      "  time_total_s: 206876.57670092583\n",
      "  timers:\n",
      "    learn_throughput: 78.429\n",
      "    learn_time_ms: 51001.746\n",
      "    sample_throughput: 152.045\n",
      "    sample_time_ms: 26308.057\n",
      "  timestamp: 1638560603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10384000\n",
      "  training_iteration: 2596\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2596</td><td style=\"text-align: right;\">          206877</td><td style=\"text-align: right;\">10384000</td><td style=\"text-align: right;\">-0.515558</td><td style=\"text-align: right;\">             0.14107</td><td style=\"text-align: right;\">            -2.84281</td><td style=\"text-align: right;\">            134.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41552000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028180000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2660000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8291923107623477\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.033175760897093384\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6762244963782401\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 106.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1410696750272602\n",
      "  episode_reward_mean: -0.4133357210075394\n",
      "  episode_reward_min: -2.2433929996382265\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 69559\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.632550696849823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013862322106957436\n",
      "          policy_loss: -0.10733852015435695\n",
      "          total_loss: 0.28952581125497817\n",
      "          vf_explained_var: 0.33914750814437866\n",
      "          vf_loss: 0.36528422594070437\n",
      "    num_agent_steps_sampled: 41552000\n",
      "    num_agent_steps_trained: 41552000\n",
      "    num_steps_sampled: 10388000\n",
      "    num_steps_trained: 10388000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.14423076923077\n",
      "    gpu_util_percent0: 0.10259615384615382\n",
      "    ram_util_percent: 49.877884615384616\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5585647159193132\n",
      "  policy_reward_mean:\n",
      "    default: -0.10333393025188481\n",
      "  policy_reward_min:\n",
      "    default: -1.8707551960967095\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22129609173686604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.040637852353512\n",
      "    mean_inference_ms: 3.228126398925259\n",
      "    mean_raw_obs_processing_ms: 0.9292368754230291\n",
      "  time_since_restore: 9184.721494197845\n",
      "  time_this_iter_s: 78.42317056655884\n",
      "  time_total_s: 206954.9998714924\n",
      "  timers:\n",
      "    learn_throughput: 78.355\n",
      "    learn_time_ms: 51049.755\n",
      "    sample_throughput: 151.731\n",
      "    sample_time_ms: 26362.489\n",
      "  timestamp: 1638560681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10388000\n",
      "  training_iteration: 2597\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2597</td><td style=\"text-align: right;\">          206955</td><td style=\"text-align: right;\">10388000</td><td style=\"text-align: right;\">-0.413336</td><td style=\"text-align: right;\">             0.14107</td><td style=\"text-align: right;\">            -2.24339</td><td style=\"text-align: right;\">             106.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41568000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.025880000000000025\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2660000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8291923107623477\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06084339509989796\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7089848968198156\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-45-59\n",
      "  done: false\n",
      "  episode_len_mean: 110.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1410696750272602\n",
      "  episode_reward_mean: -0.44227968162344816\n",
      "  episode_reward_min: -2.2433929996382265\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 69593\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6395245471000671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013738967560231685\n",
      "          policy_loss: -0.10525284334272146\n",
      "          total_loss: 0.2834544968008995\n",
      "          vf_explained_var: 0.2866297960281372\n",
      "          vf_loss: 0.3574082546234131\n",
      "    num_agent_steps_sampled: 41568000\n",
      "    num_agent_steps_trained: 41568000\n",
      "    num_steps_sampled: 10392000\n",
      "    num_steps_trained: 10392000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.75825242718447\n",
      "    gpu_util_percent0: 0.10514563106796117\n",
      "    ram_util_percent: 49.692233009708765\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5585647159193132\n",
      "  policy_reward_mean:\n",
      "    default: -0.11056992040586205\n",
      "  policy_reward_min:\n",
      "    default: -1.8707551960967095\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22129875529148316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041159293889567\n",
      "    mean_inference_ms: 3.228068794856272\n",
      "    mean_raw_obs_processing_ms: 0.9294590077611582\n",
      "  time_since_restore: 9262.115984201431\n",
      "  time_this_iter_s: 77.39449000358582\n",
      "  time_total_s: 207032.39436149597\n",
      "  timers:\n",
      "    learn_throughput: 78.369\n",
      "    learn_time_ms: 51040.853\n",
      "    sample_throughput: 151.654\n",
      "    sample_time_ms: 26375.769\n",
      "  timestamp: 1638560759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10392000\n",
      "  training_iteration: 2598\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2598</td><td style=\"text-align: right;\">          207032</td><td style=\"text-align: right;\">10392000</td><td style=\"text-align: right;\">-0.44228</td><td style=\"text-align: right;\">             0.14107</td><td style=\"text-align: right;\">            -2.24339</td><td style=\"text-align: right;\">            110.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41584000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.02844000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.17500000000000013\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6098003054464456\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07961472657801573\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7089848968198156\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 114.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.132235417267334\n",
      "  episode_reward_mean: -0.472328768393695\n",
      "  episode_reward_min: -1.5959115688519052\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 69621\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6388883695602418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013757891140878201\n",
      "          policy_loss: -0.10526397211849689\n",
      "          total_loss: 0.22562174853682518\n",
      "          vf_explained_var: 0.3167068660259247\n",
      "          vf_loss: 0.2995435242652893\n",
      "    num_agent_steps_sampled: 41584000\n",
      "    num_agent_steps_trained: 41584000\n",
      "    num_steps_sampled: 10396000\n",
      "    num_steps_trained: 10396000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.011764705882353\n",
      "    gpu_util_percent0: 0.09075630252100841\n",
      "    ram_util_percent: 50.247899159663866\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5585647159193132\n",
      "  policy_reward_mean:\n",
      "    default: -0.11808219209842374\n",
      "  policy_reward_min:\n",
      "    default: -1.841773136838647\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22130006964821725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.041137444752557\n",
      "    mean_inference_ms: 3.2282365887227717\n",
      "    mean_raw_obs_processing_ms: 0.9295480784206118\n",
      "  time_since_restore: 9339.194416761398\n",
      "  time_this_iter_s: 77.07843255996704\n",
      "  time_total_s: 207109.47279405594\n",
      "  timers:\n",
      "    learn_throughput: 78.444\n",
      "    learn_time_ms: 50991.927\n",
      "    sample_throughput: 151.708\n",
      "    sample_time_ms: 26366.503\n",
      "  timestamp: 1638560848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10396000\n",
      "  training_iteration: 2599\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2599</td><td style=\"text-align: right;\">          207109</td><td style=\"text-align: right;\">10396000</td><td style=\"text-align: right;\">-0.472329</td><td style=\"text-align: right;\">            0.132235</td><td style=\"text-align: right;\">            -1.59591</td><td style=\"text-align: right;\">            114.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41600000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03453000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3810000000000003\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6098003054464456\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0405679354176991\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7089848968198156\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 128.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.132235417267334\n",
      "  episode_reward_mean: -0.5260745930824493\n",
      "  episode_reward_min: -2.099393039784925\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 69645\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6386401710510254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013536902643740177\n",
      "          policy_loss: -0.10293189656734467\n",
      "          total_loss: 0.19445726948976516\n",
      "          vf_explained_var: 0.27810806035995483\n",
      "          vf_loss: 0.26655040895938875\n",
      "    num_agent_steps_sampled: 41600000\n",
      "    num_agent_steps_trained: 41600000\n",
      "    num_steps_sampled: 10400000\n",
      "    num_steps_trained: 10400000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.905825242718443\n",
      "    gpu_util_percent0: 0.10563106796116506\n",
      "    ram_util_percent: 50.23203883495149\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5143338623502203\n",
      "  policy_reward_mean:\n",
      "    default: -0.13151864827061233\n",
      "  policy_reward_min:\n",
      "    default: -1.841773136838647\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22129082052361335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.04050671293654\n",
      "    mean_inference_ms: 3.2280179366512596\n",
      "    mean_raw_obs_processing_ms: 0.9293126986215695\n",
      "  time_since_restore: 9416.669962644577\n",
      "  time_this_iter_s: 77.47554588317871\n",
      "  time_total_s: 207186.94833993912\n",
      "  timers:\n",
      "    learn_throughput: 78.404\n",
      "    learn_time_ms: 51017.682\n",
      "    sample_throughput: 151.838\n",
      "    sample_time_ms: 26343.784\n",
      "  timestamp: 1638560926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10400000\n",
      "  training_iteration: 2600\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2600</td><td style=\"text-align: right;\">          207187</td><td style=\"text-align: right;\">10400000</td><td style=\"text-align: right;\">-0.526075</td><td style=\"text-align: right;\">            0.132235</td><td style=\"text-align: right;\">            -2.09939</td><td style=\"text-align: right;\">            128.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41616000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.042840000000000024\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3810000000000003\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6755013400760613\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.030762652336208892\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8499868936746766\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 136.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.06531239955631385\n",
      "  episode_reward_mean: -0.5699163113301231\n",
      "  episode_reward_min: -3.273787704683513\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 69678\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.638377788066864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014012986071407794\n",
      "          policy_loss: -0.10743653615564108\n",
      "          total_loss: 0.2705207479596138\n",
      "          vf_explained_var: 0.30287638306617737\n",
      "          vf_loss: 0.3460339524745941\n",
      "    num_agent_steps_sampled: 41616000\n",
      "    num_agent_steps_trained: 41616000\n",
      "    num_steps_sampled: 10404000\n",
      "    num_steps_trained: 10404000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.031372549019608\n",
      "    gpu_util_percent0: 0.10460784313725491\n",
      "    ram_util_percent: 50.28235294117649\n",
      "    vram_util_percent0: 0.15860480421191178\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.617125716293443\n",
      "  policy_reward_mean:\n",
      "    default: -0.14247907783253078\n",
      "  policy_reward_min:\n",
      "    default: -2.1359972838516814\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22127806149883344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.03963638201222\n",
      "    mean_inference_ms: 3.2277186347218128\n",
      "    mean_raw_obs_processing_ms: 0.9289561214216937\n",
      "  time_since_restore: 9494.167930364609\n",
      "  time_this_iter_s: 77.49796772003174\n",
      "  time_total_s: 207264.44630765915\n",
      "  timers:\n",
      "    learn_throughput: 78.41\n",
      "    learn_time_ms: 51014.089\n",
      "    sample_throughput: 151.78\n",
      "    sample_time_ms: 26353.942\n",
      "  timestamp: 1638561003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10404000\n",
      "  training_iteration: 2601\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2601</td><td style=\"text-align: right;\">          207264</td><td style=\"text-align: right;\">10404000</td><td style=\"text-align: right;\">-0.569916</td><td style=\"text-align: right;\">           0.0653124</td><td style=\"text-align: right;\">            -3.27379</td><td style=\"text-align: right;\">            136.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41632000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.04311000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.3810000000000003\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7526320962514916\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07679844920814378\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8499868936746766\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 137.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07392594260920582\n",
      "  episode_reward_mean: -0.5492852532427763\n",
      "  episode_reward_min: -3.273787704683513\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 69706\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6342827649116516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013996815398335458\n",
      "          policy_loss: -0.10709036345034838\n",
      "          total_loss: 0.2682311974018812\n",
      "          vf_explained_var: 0.26812025904655457\n",
      "          vf_loss: 0.3434350652694702\n",
      "    num_agent_steps_sampled: 41632000\n",
      "    num_agent_steps_trained: 41632000\n",
      "    num_steps_sampled: 10408000\n",
      "    num_steps_trained: 10408000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.832038834951454\n",
      "    gpu_util_percent0: 0.10310679611650488\n",
      "    ram_util_percent: 50.22912621359227\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.617125716293443\n",
      "  policy_reward_mean:\n",
      "    default: -0.13732131331069405\n",
      "  policy_reward_min:\n",
      "    default: -2.1359972838516814\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22126345953470897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.039162371335124\n",
      "    mean_inference_ms: 3.227195875837122\n",
      "    mean_raw_obs_processing_ms: 0.9286148544409383\n",
      "  time_since_restore: 9571.465764045715\n",
      "  time_this_iter_s: 77.29783368110657\n",
      "  time_total_s: 207341.74414134026\n",
      "  timers:\n",
      "    learn_throughput: 78.427\n",
      "    learn_time_ms: 51002.937\n",
      "    sample_throughput: 151.678\n",
      "    sample_time_ms: 26371.704\n",
      "  timestamp: 1638561081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10408000\n",
      "  training_iteration: 2602\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2602</td><td style=\"text-align: right;\">          207342</td><td style=\"text-align: right;\">10408000</td><td style=\"text-align: right;\">-0.549285</td><td style=\"text-align: right;\">           0.0739259</td><td style=\"text-align: right;\">            -3.27379</td><td style=\"text-align: right;\">            137.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41648000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.041090000000000015\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2920000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7526320962514916\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.12094303869106174\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8499868936746766\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 136.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07392594260920582\n",
      "  episode_reward_mean: -0.5411274186822905\n",
      "  episode_reward_min: -3.273787704683513\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 69734\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6432248334884644\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013533013686537743\n",
      "          policy_loss: -0.10317281886190176\n",
      "          total_loss: 0.21205365409702062\n",
      "          vf_explained_var: 0.27799829840660095\n",
      "          vf_loss: 0.28439657735824586\n",
      "    num_agent_steps_sampled: 41648000\n",
      "    num_agent_steps_trained: 41648000\n",
      "    num_steps_sampled: 10412000\n",
      "    num_steps_trained: 10412000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.105833333333333\n",
      "    gpu_util_percent0: 0.09075000000000001\n",
      "    ram_util_percent: 50.4675\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.617125716293443\n",
      "  policy_reward_mean:\n",
      "    default: -0.1352818546705726\n",
      "  policy_reward_min:\n",
      "    default: -2.1359972838516814\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22125458629108594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.03868529238657\n",
      "    mean_inference_ms: 3.2267909917912245\n",
      "    mean_raw_obs_processing_ms: 0.9284060197940976\n",
      "  time_since_restore: 9649.178262472153\n",
      "  time_this_iter_s: 77.71249842643738\n",
      "  time_total_s: 207419.4566397667\n",
      "  timers:\n",
      "    learn_throughput: 78.347\n",
      "    learn_time_ms: 51054.955\n",
      "    sample_throughput: 151.741\n",
      "    sample_time_ms: 26360.661\n",
      "  timestamp: 1638561171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10412000\n",
      "  training_iteration: 2603\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2603</td><td style=\"text-align: right;\">          207419</td><td style=\"text-align: right;\">10412000</td><td style=\"text-align: right;\">-0.541127</td><td style=\"text-align: right;\">           0.0739259</td><td style=\"text-align: right;\">            -3.27379</td><td style=\"text-align: right;\">            136.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41664000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03763000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2920000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7526320962514916\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.062229926383431104\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6954560450306507\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 134.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07392594260920582\n",
      "  episode_reward_mean: -0.5332527361545217\n",
      "  episode_reward_min: -2.771519311041403\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 69763\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6347415809631347\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013112124145030976\n",
      "          policy_loss: -0.09977310894429683\n",
      "          total_loss: 0.19552072820067407\n",
      "          vf_explained_var: 0.326363742351532\n",
      "          vf_loss: 0.26542277753353116\n",
      "    num_agent_steps_sampled: 41664000\n",
      "    num_agent_steps_trained: 41664000\n",
      "    num_steps_sampled: 10416000\n",
      "    num_steps_trained: 10416000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.861165048543686\n",
      "    gpu_util_percent0: 0.10504854368932041\n",
      "    ram_util_percent: 50.32524271844661\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.617125716293443\n",
      "  policy_reward_mean:\n",
      "    default: -0.13331318403863038\n",
      "  policy_reward_min:\n",
      "    default: -1.8967032923988127\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22124516345585202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.038212254933914\n",
      "    mean_inference_ms: 3.226440051213783\n",
      "    mean_raw_obs_processing_ms: 0.9281749073103921\n",
      "  time_since_restore: 9726.82265162468\n",
      "  time_this_iter_s: 77.64438915252686\n",
      "  time_total_s: 207497.10102891922\n",
      "  timers:\n",
      "    learn_throughput: 78.286\n",
      "    learn_time_ms: 51094.692\n",
      "    sample_throughput: 151.793\n",
      "    sample_time_ms: 26351.659\n",
      "  timestamp: 1638561249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10416000\n",
      "  training_iteration: 2604\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2604</td><td style=\"text-align: right;\">          207497</td><td style=\"text-align: right;\">10416000</td><td style=\"text-align: right;\">-0.533253</td><td style=\"text-align: right;\">           0.0739259</td><td style=\"text-align: right;\">            -2.77152</td><td style=\"text-align: right;\">            134.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41680000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03630000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2920000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5990315247033329\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.022523007218630466\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6954560450306507\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 134.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.13827778586810302\n",
      "  episode_reward_mean: -0.5248108950902227\n",
      "  episode_reward_min: -2.771519311041403\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 69797\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6318070001602173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014189198240637779\n",
      "          policy_loss: -0.10746865949034691\n",
      "          total_loss: 0.28809173077344896\n",
      "          vf_explained_var: 0.3491322696208954\n",
      "          vf_loss: 0.3632356224060059\n",
      "    num_agent_steps_sampled: 41680000\n",
      "    num_agent_steps_trained: 41680000\n",
      "    num_steps_sampled: 10420000\n",
      "    num_steps_trained: 10420000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.957281553398055\n",
      "    gpu_util_percent0: 0.10669902912621362\n",
      "    ram_util_percent: 49.99417475728152\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5351791572846283\n",
      "  policy_reward_mean:\n",
      "    default: -0.1312027237725556\n",
      "  policy_reward_min:\n",
      "    default: -1.9034818240310827\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2212327437602438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.037681474348105\n",
      "    mean_inference_ms: 3.226083493075837\n",
      "    mean_raw_obs_processing_ms: 0.927979146661198\n",
      "  time_since_restore: 9804.349391698837\n",
      "  time_this_iter_s: 77.52674007415771\n",
      "  time_total_s: 207574.62776899338\n",
      "  timers:\n",
      "    learn_throughput: 78.263\n",
      "    learn_time_ms: 51109.813\n",
      "    sample_throughput: 151.73\n",
      "    sample_time_ms: 26362.547\n",
      "  timestamp: 1638561326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10420000\n",
      "  training_iteration: 2605\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2605</td><td style=\"text-align: right;\">          207575</td><td style=\"text-align: right;\">10420000</td><td style=\"text-align: right;\">-0.524811</td><td style=\"text-align: right;\">            0.138278</td><td style=\"text-align: right;\">            -2.77152</td><td style=\"text-align: right;\">            134.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41696000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03505000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.2750000000000002\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5990315247033329\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06943796473288658\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7544232127183711\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 133.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.13827778586810302\n",
      "  episode_reward_mean: -0.5378609147301053\n",
      "  episode_reward_min: -1.9294352197422955\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 69822\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.637295503616333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01339183259010315\n",
      "          policy_loss: -0.10235041387379169\n",
      "          total_loss: 0.21699218672513962\n",
      "          vf_explained_var: 0.30992528796195984\n",
      "          vf_loss: 0.28883433187007906\n",
      "    num_agent_steps_sampled: 41696000\n",
      "    num_agent_steps_trained: 41696000\n",
      "    num_steps_sampled: 10424000\n",
      "    num_steps_trained: 10424000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.165546218487393\n",
      "    gpu_util_percent0: 0.0897478991596639\n",
      "    ram_util_percent: 50.58739495798316\n",
      "    vram_util_percent0: 0.15860480421191175\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5351791572846283\n",
      "  policy_reward_mean:\n",
      "    default: -0.1344652286825263\n",
      "  policy_reward_min:\n",
      "    default: -1.9034818240310827\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22122440441652436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.037030492694367\n",
      "    mean_inference_ms: 3.226068284811073\n",
      "    mean_raw_obs_processing_ms: 0.9277480456813021\n",
      "  time_since_restore: 9881.52776002884\n",
      "  time_this_iter_s: 77.17836833000183\n",
      "  time_total_s: 207651.80613732338\n",
      "  timers:\n",
      "    learn_throughput: 78.283\n",
      "    learn_time_ms: 51096.577\n",
      "    sample_throughput: 151.771\n",
      "    sample_time_ms: 26355.446\n",
      "  timestamp: 1638561416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10424000\n",
      "  training_iteration: 2606\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2606</td><td style=\"text-align: right;\">          207652</td><td style=\"text-align: right;\">10424000</td><td style=\"text-align: right;\">-0.537861</td><td style=\"text-align: right;\">            0.138278</td><td style=\"text-align: right;\">            -1.92944</td><td style=\"text-align: right;\">            133.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41712000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03610000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21000000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7026305253025955\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.07289109594849895\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7544232127183711\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 135.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.13827778586810302\n",
      "  episode_reward_mean: -0.535837300081303\n",
      "  episode_reward_min: -1.9294352197422955\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 69852\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.632762098312378\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013621400237083435\n",
      "          policy_loss: -0.10383937335014343\n",
      "          total_loss: 0.21230929589271547\n",
      "          vf_explained_var: 0.3830178380012512\n",
      "          vf_loss: 0.2851174156665802\n",
      "    num_agent_steps_sampled: 41712000\n",
      "    num_agent_steps_trained: 41712000\n",
      "    num_steps_sampled: 10428000\n",
      "    num_steps_trained: 10428000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.865714285714287\n",
      "    gpu_util_percent0: 0.11276190476190474\n",
      "    ram_util_percent: 50.58190476190476\n",
      "    vram_util_percent0: 0.1605258622040458\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5442519932513037\n",
      "  policy_reward_mean:\n",
      "    default: -0.13395932502032573\n",
      "  policy_reward_min:\n",
      "    default: -1.9034818240310827\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22122235033637053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.037276522323587\n",
      "    mean_inference_ms: 3.226056512981488\n",
      "    mean_raw_obs_processing_ms: 0.9275004105641451\n",
      "  time_since_restore: 9960.843616962433\n",
      "  time_this_iter_s: 79.31585693359375\n",
      "  time_total_s: 207731.12199425697\n",
      "  timers:\n",
      "    learn_throughput: 78.129\n",
      "    learn_time_ms: 51197.262\n",
      "    sample_throughput: 151.838\n",
      "    sample_time_ms: 26343.819\n",
      "  timestamp: 1638561495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10428000\n",
      "  training_iteration: 2607\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2607</td><td style=\"text-align: right;\">          207731</td><td style=\"text-align: right;\">10428000</td><td style=\"text-align: right;\">-0.535837</td><td style=\"text-align: right;\">            0.138278</td><td style=\"text-align: right;\">            -1.92944</td><td style=\"text-align: right;\">             135.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41728000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03329000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.21000000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7026305253025955\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.06300567833423709\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7544232127183711\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_16-59-40\n",
      "  done: false\n",
      "  episode_len_mean: 127.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08079145363947227\n",
      "  episode_reward_mean: -0.5084356158382842\n",
      "  episode_reward_min: -1.9016190505056274\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 69890\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6251474270820617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01389083931595087\n",
      "          policy_loss: -0.10563713394105434\n",
      "          total_loss: 0.2961847324371338\n",
      "          vf_explained_var: 0.2933284342288971\n",
      "          vf_loss: 0.37017679572105405\n",
      "    num_agent_steps_sampled: 41728000\n",
      "    num_agent_steps_trained: 41728000\n",
      "    num_steps_sampled: 10432000\n",
      "    num_steps_trained: 10432000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.37456140350877\n",
      "    gpu_util_percent0: 0.15061403508771928\n",
      "    ram_util_percent: 51.00526315789473\n",
      "    vram_util_percent0: 0.16374124683211813\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.582640748038022\n",
      "  policy_reward_mean:\n",
      "    default: -0.12710890395957106\n",
      "  policy_reward_min:\n",
      "    default: -1.8747954958084108\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22129026829431597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.0430083001409\n",
      "    mean_inference_ms: 3.227198105293101\n",
      "    mean_raw_obs_processing_ms: 0.9278139328434897\n",
      "  time_since_restore: 10046.231583833694\n",
      "  time_this_iter_s: 85.3879668712616\n",
      "  time_total_s: 207816.50996112823\n",
      "  timers:\n",
      "    learn_throughput: 77.293\n",
      "    learn_time_ms: 51751.168\n",
      "    sample_throughput: 150.448\n",
      "    sample_time_ms: 26587.304\n",
      "  timestamp: 1638561580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10432000\n",
      "  training_iteration: 2608\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2608</td><td style=\"text-align: right;\">          207817</td><td style=\"text-align: right;\">10432000</td><td style=\"text-align: right;\">-0.508436</td><td style=\"text-align: right;\">           0.0807915</td><td style=\"text-align: right;\">            -1.90162</td><td style=\"text-align: right;\">             127.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41744000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03051000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20700000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8589570972208173\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05652101325900091\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7254137422478788\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-01-19\n",
      "  done: false\n",
      "  episode_len_mean: 111.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.08079145363947227\n",
      "  episode_reward_mean: -0.44073669678421196\n",
      "  episode_reward_min: -1.1700385908059228\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 69926\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6312793092727661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014035461448132992\n",
      "          policy_loss: -0.10660925753414631\n",
      "          total_loss: 0.3052518800497055\n",
      "          vf_explained_var: 0.3062836825847626\n",
      "          vf_loss: 0.3798866012096405\n",
      "    num_agent_steps_sampled: 41744000\n",
      "    num_agent_steps_trained: 41744000\n",
      "    num_steps_sampled: 10436000\n",
      "    num_steps_trained: 10436000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.183076923076925\n",
      "    gpu_util_percent0: 0.1306923076923077\n",
      "    ram_util_percent: 52.090769230769226\n",
      "    vram_util_percent0: 0.16711721973321184\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7001920432021327\n",
      "  policy_reward_mean:\n",
      "    default: -0.11018417419605299\n",
      "  policy_reward_min:\n",
      "    default: -1.901072436568758\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2213855039606969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.05188415745946\n",
      "    mean_inference_ms: 3.2285747804477545\n",
      "    mean_raw_obs_processing_ms: 0.9283624026497956\n",
      "  time_since_restore: 10131.756478786469\n",
      "  time_this_iter_s: 85.52489495277405\n",
      "  time_total_s: 207902.034856081\n",
      "  timers:\n",
      "    learn_throughput: 76.274\n",
      "    learn_time_ms: 52442.364\n",
      "    sample_throughput: 149.588\n",
      "    sample_time_ms: 26740.171\n",
      "  timestamp: 1638561679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10436000\n",
      "  training_iteration: 2609\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2609</td><td style=\"text-align: right;\">          207902</td><td style=\"text-align: right;\">10436000</td><td style=\"text-align: right;\">-0.440737</td><td style=\"text-align: right;\">           0.0807915</td><td style=\"text-align: right;\">            -1.17004</td><td style=\"text-align: right;\">            111.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41760000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030560000000000018\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20700000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8589570972208173\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.011613351435426815\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7254137422478788\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 112.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.11755307410085969\n",
      "  episode_reward_mean: -0.44017811168731547\n",
      "  episode_reward_min: -1.5916696579924214\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 69960\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6291514706611633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014088954605162143\n",
      "          policy_loss: -0.10704447154700757\n",
      "          total_loss: 0.2960289095342159\n",
      "          vf_explained_var: 0.3554708659648895\n",
      "          vf_loss: 0.3709769797325134\n",
      "    num_agent_steps_sampled: 41760000\n",
      "    num_agent_steps_trained: 41760000\n",
      "    num_steps_sampled: 10440000\n",
      "    num_steps_trained: 10440000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.262385321100915\n",
      "    gpu_util_percent0: 0.14330275229357795\n",
      "    ram_util_percent: 56.302752293577974\n",
      "    vram_util_percent0: 0.1811013400714262\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7001920432021327\n",
      "  policy_reward_mean:\n",
      "    default: -0.11004452792182887\n",
      "  policy_reward_min:\n",
      "    default: -1.901072436568758\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22150501544357556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.06244720231335\n",
      "    mean_inference_ms: 3.230607666812136\n",
      "    mean_raw_obs_processing_ms: 0.9290092044921243\n",
      "  time_since_restore: 10213.814136505127\n",
      "  time_this_iter_s: 82.05765771865845\n",
      "  time_total_s: 207984.09251379967\n",
      "  timers:\n",
      "    learn_throughput: 75.942\n",
      "    learn_time_ms: 52671.673\n",
      "    sample_throughput: 148.319\n",
      "    sample_time_ms: 26968.843\n",
      "  timestamp: 1638561762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10440000\n",
      "  training_iteration: 2610\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2610</td><td style=\"text-align: right;\">          207984</td><td style=\"text-align: right;\">10440000</td><td style=\"text-align: right;\">-0.440178</td><td style=\"text-align: right;\">            0.117553</td><td style=\"text-align: right;\">            -1.59167</td><td style=\"text-align: right;\">             112.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41776000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.030950000000000023\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.18400000000000014\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8589570972208173\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.007070118019116629\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7254137422478788\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 110.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.11755307410085969\n",
      "  episode_reward_mean: -0.43180148561668547\n",
      "  episode_reward_min: -1.5916696579924214\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 69997\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6297236037254333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013618522636592387\n",
      "          policy_loss: -0.10509762158989906\n",
      "          total_loss: 0.24091393676400186\n",
      "          vf_explained_var: 0.3250553011894226\n",
      "          vf_loss: 0.31498686254024505\n",
      "    num_agent_steps_sampled: 41776000\n",
      "    num_agent_steps_trained: 41776000\n",
      "    num_steps_sampled: 10444000\n",
      "    num_steps_trained: 10444000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.12616822429906\n",
      "    gpu_util_percent0: 0.1361682242990654\n",
      "    ram_util_percent: 56.04205607476635\n",
      "    vram_util_percent0: 0.18057464795662623\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.7001920432021327\n",
      "  policy_reward_mean:\n",
      "    default: -0.10795037140417132\n",
      "  policy_reward_min:\n",
      "    default: -1.901072436568758\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22160814331145345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.071229847368272\n",
      "    mean_inference_ms: 3.2323125247076643\n",
      "    mean_raw_obs_processing_ms: 0.9293824940039859\n",
      "  time_since_restore: 10294.492412090302\n",
      "  time_this_iter_s: 80.67827558517456\n",
      "  time_total_s: 208064.77078938484\n",
      "  timers:\n",
      "    learn_throughput: 75.618\n",
      "    learn_time_ms: 52897.206\n",
      "    sample_throughput: 147.813\n",
      "    sample_time_ms: 27061.212\n",
      "  timestamp: 1638561842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10444000\n",
      "  training_iteration: 2611\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2611</td><td style=\"text-align: right;\">          208065</td><td style=\"text-align: right;\">10444000</td><td style=\"text-align: right;\">-0.431801</td><td style=\"text-align: right;\">            0.117553</td><td style=\"text-align: right;\">            -1.59167</td><td style=\"text-align: right;\">            110.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41792000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.036840000000000026\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.36300000000000027\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7692157193081476\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07398761631106038\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5952652364035179\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-05-23\n",
      "  done: false\n",
      "  episode_len_mean: 123.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.11755307410085969\n",
      "  episode_reward_mean: -0.48231809390249153\n",
      "  episode_reward_min: -1.8761764908501244\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 70025\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6395214686393738\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013481668815016747\n",
      "          policy_loss: -0.10460619812458753\n",
      "          total_loss: 0.21260850229859352\n",
      "          vf_explained_var: 0.30747321248054504\n",
      "          vf_loss: 0.2865017731189728\n",
      "    num_agent_steps_sampled: 41792000\n",
      "    num_agent_steps_trained: 41792000\n",
      "    num_steps_sampled: 10448000\n",
      "    num_steps_trained: 10448000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.92616822429906\n",
      "    gpu_util_percent0: 0.13037383177570092\n",
      "    ram_util_percent: 55.930841121495334\n",
      "    vram_util_percent0: 0.17664443234831922\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5522424533456478\n",
      "  policy_reward_mean:\n",
      "    default: -0.12057952347562287\n",
      "  policy_reward_min:\n",
      "    default: -1.994273732679763\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2216847776244789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.07795915191821\n",
      "    mean_inference_ms: 3.233520407907534\n",
      "    mean_raw_obs_processing_ms: 0.9297490786804167\n",
      "  time_since_restore: 10374.799258232117\n",
      "  time_this_iter_s: 80.30684614181519\n",
      "  time_total_s: 208145.07763552666\n",
      "  timers:\n",
      "    learn_throughput: 75.379\n",
      "    learn_time_ms: 53065.297\n",
      "    sample_throughput: 147.095\n",
      "    sample_time_ms: 27193.321\n",
      "  timestamp: 1638561923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10448000\n",
      "  training_iteration: 2612\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2612</td><td style=\"text-align: right;\">          208145</td><td style=\"text-align: right;\">10448000</td><td style=\"text-align: right;\">-0.482318</td><td style=\"text-align: right;\">            0.117553</td><td style=\"text-align: right;\">            -1.87618</td><td style=\"text-align: right;\">            123.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41808000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03980000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.36300000000000027\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7692157193081476\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.05679941039452662\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7609068944421405\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 126.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.07692681234649301\n",
      "  episode_reward_mean: -0.5187525298093809\n",
      "  episode_reward_min: -1.8761764908501244\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 70055\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6335787858963012\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013917271465063096\n",
      "          policy_loss: -0.10584405276179314\n",
      "          total_loss: 0.24891859394311905\n",
      "          vf_explained_var: 0.36776629090309143\n",
      "          vf_loss: 0.3230573625564575\n",
      "    num_agent_steps_sampled: 41808000\n",
      "    num_agent_steps_trained: 41808000\n",
      "    num_steps_sampled: 10452000\n",
      "    num_steps_trained: 10452000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.956910569105695\n",
      "    gpu_util_percent0: 0.12317073170731704\n",
      "    ram_util_percent: 56.63170731707315\n",
      "    vram_util_percent0: 0.17565148997985539\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.5353247190847048\n",
      "  policy_reward_mean:\n",
      "    default: -0.1296881324523452\n",
      "  policy_reward_min:\n",
      "    default: -1.994273732679763\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2217429578408951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.083122361921298\n",
      "    mean_inference_ms: 3.2343822148029817\n",
      "    mean_raw_obs_processing_ms: 0.9301509228371184\n",
      "  time_since_restore: 10454.805909395218\n",
      "  time_this_iter_s: 80.0066511631012\n",
      "  time_total_s: 208225.08428668976\n",
      "  timers:\n",
      "    learn_throughput: 75.203\n",
      "    learn_time_ms: 53189.595\n",
      "    sample_throughput: 146.529\n",
      "    sample_time_ms: 27298.392\n",
      "  timestamp: 1638562015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10452000\n",
      "  training_iteration: 2613\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2613</td><td style=\"text-align: right;\">          208225</td><td style=\"text-align: right;\">10452000</td><td style=\"text-align: right;\">-0.518753</td><td style=\"text-align: right;\">           0.0769268</td><td style=\"text-align: right;\">            -1.87618</td><td style=\"text-align: right;\">            126.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41824000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03963000000000003\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.36300000000000027\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6858241851528817\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.04900595885371163\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7705691104558529\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-08-16\n",
      "  done: false\n",
      "  episode_len_mean: 121.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.23252741518076636\n",
      "  episode_reward_mean: -0.5287717411364778\n",
      "  episode_reward_min: -1.8761764908501244\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 70096\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6290916681289673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013864626914262771\n",
      "          policy_loss: -0.10631767032295465\n",
      "          total_loss: 0.26739944180846215\n",
      "          vf_explained_var: 0.3427611291408539\n",
      "          vf_loss: 0.3421317610740662\n",
      "    num_agent_steps_sampled: 41824000\n",
      "    num_agent_steps_trained: 41824000\n",
      "    num_steps_sampled: 10456000\n",
      "    num_steps_trained: 10456000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.266037735849054\n",
      "    gpu_util_percent0: 0.13839622641509433\n",
      "    ram_util_percent: 56.71886792452831\n",
      "    vram_util_percent0: 0.17588488020513204\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.561362650529169\n",
      "  policy_reward_mean:\n",
      "    default: -0.13219293528411946\n",
      "  policy_reward_min:\n",
      "    default: -1.994273732679763\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2218136151054555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.09061081007113\n",
      "    mean_inference_ms: 3.2352400318890635\n",
      "    mean_raw_obs_processing_ms: 0.930829169821277\n",
      "  time_since_restore: 10534.935571432114\n",
      "  time_this_iter_s: 80.12966203689575\n",
      "  time_total_s: 208305.21394872665\n",
      "  timers:\n",
      "    learn_throughput: 75.009\n",
      "    learn_time_ms: 53327.166\n",
      "    sample_throughput: 145.938\n",
      "    sample_time_ms: 27408.823\n",
      "  timestamp: 1638562096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10456000\n",
      "  training_iteration: 2614\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2614</td><td style=\"text-align: right;\">          208305</td><td style=\"text-align: right;\">10456000</td><td style=\"text-align: right;\">-0.528772</td><td style=\"text-align: right;\">            0.232527</td><td style=\"text-align: right;\">            -1.87618</td><td style=\"text-align: right;\">            121.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41840000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.03225000000000002\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.19900000000000015\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.627181971874151\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.043916652142175557\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7705691104558529\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-09-36\n",
      "  done: false\n",
      "  episode_len_mean: 110.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.23252741518076636\n",
      "  episode_reward_mean: -0.49666125769361097\n",
      "  episode_reward_min: -1.7550897804917425\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 70131\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6253948106765747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013822163589298725\n",
      "          policy_loss: -0.10480037917196751\n",
      "          total_loss: 0.27500849443674086\n",
      "          vf_explained_var: 0.3418087661266327\n",
      "          vf_loss: 0.34832025599479677\n",
      "    num_agent_steps_sampled: 41840000\n",
      "    num_agent_steps_trained: 41840000\n",
      "    num_steps_sampled: 10460000\n",
      "    num_steps_trained: 10460000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.572641509433964\n",
      "    gpu_util_percent0: 0.13764150943396222\n",
      "    ram_util_percent: 56.80000000000001\n",
      "    vram_util_percent0: 0.17499239446938233\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.561362650529169\n",
      "  policy_reward_mean:\n",
      "    default: -0.12416531442340274\n",
      "  policy_reward_min:\n",
      "    default: -1.845676410645185\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22186337994862917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.095462433983094\n",
      "    mean_inference_ms: 3.235940406587206\n",
      "    mean_raw_obs_processing_ms: 0.9312292744438676\n",
      "  time_since_restore: 10614.900562524796\n",
      "  time_this_iter_s: 79.96499109268188\n",
      "  time_total_s: 208385.17893981934\n",
      "  timers:\n",
      "    learn_throughput: 74.78\n",
      "    learn_time_ms: 53490.14\n",
      "    sample_throughput: 145.51\n",
      "    sample_time_ms: 27489.426\n",
      "  timestamp: 1638562176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10460000\n",
      "  training_iteration: 2615\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2615</td><td style=\"text-align: right;\">          208385</td><td style=\"text-align: right;\">10460000</td><td style=\"text-align: right;\">-0.496661</td><td style=\"text-align: right;\">            0.232527</td><td style=\"text-align: right;\">            -1.75509</td><td style=\"text-align: right;\">            110.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41856000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.029020000000000018\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20600000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7648601077499572\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02744403548194295\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7705691104558529\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 108.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.23252741518076636\n",
      "  episode_reward_mean: -0.467313664185941\n",
      "  episode_reward_min: -1.5677932639272845\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 70160\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6329724006652832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013908966101706027\n",
      "          policy_loss: -0.10559327682107687\n",
      "          total_loss: 0.25051435056328775\n",
      "          vf_explained_var: 0.2997889816761017\n",
      "          vf_loss: 0.3244212640523911\n",
      "    num_agent_steps_sampled: 41856000\n",
      "    num_agent_steps_trained: 41856000\n",
      "    num_steps_sampled: 10464000\n",
      "    num_steps_trained: 10464000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.7352\n",
      "    gpu_util_percent0: 0.12615999999999997\n",
      "    ram_util_percent: 57.00000000000001\n",
      "    vram_util_percent0: 0.17210924646265224\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6302021237586044\n",
      "  policy_reward_mean:\n",
      "    default: -0.11682841604648528\n",
      "  policy_reward_min:\n",
      "    default: -1.845676410645185\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22189654166712103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.099114970868786\n",
      "    mean_inference_ms: 3.236393078531715\n",
      "    mean_raw_obs_processing_ms: 0.9313745177839141\n",
      "  time_since_restore: 10694.959290027618\n",
      "  time_this_iter_s: 80.05872750282288\n",
      "  time_total_s: 208465.23766732216\n",
      "  timers:\n",
      "    learn_throughput: 74.501\n",
      "    learn_time_ms: 53690.441\n",
      "    sample_throughput: 145.048\n",
      "    sample_time_ms: 27577.058\n",
      "  timestamp: 1638562270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10464000\n",
      "  training_iteration: 2616\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2616</td><td style=\"text-align: right;\">          208465</td><td style=\"text-align: right;\">10464000</td><td style=\"text-align: right;\">-0.467314</td><td style=\"text-align: right;\">            0.232527</td><td style=\"text-align: right;\">            -1.56779</td><td style=\"text-align: right;\">            108.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_491c2_00000:\n",
      "  agent_timesteps_total: 41872000\n",
      "  custom_metrics:\n",
      "    agent_0_total_agent_position_to_ball_reward_max: 0.0\n",
      "    agent_0_total_agent_position_to_ball_reward_mean: -0.028220000000000012\n",
      "    agent_0_total_agent_position_to_ball_reward_min: -0.20600000000000016\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7648601077499572\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.022743142413394354\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7382406555875302\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-03_17-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 114.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.1217560268613278\n",
      "  episode_reward_mean: -0.4917774515933698\n",
      "  episode_reward_min: -1.5677932639272845\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 70194\n",
      "  experiment_id: 51c83eec84bd4de89e8eaf92d7325d10\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2781249999999993\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 0.6331188950538635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013836203545331955\n",
      "          policy_loss: -0.10500671007484198\n",
      "          total_loss: 0.2519441811442375\n",
      "          vf_explained_var: 0.31007274985313416\n",
      "          vf_loss: 0.3254302887916565\n",
      "    num_agent_steps_sampled: 41872000\n",
      "    num_agent_steps_trained: 41872000\n",
      "    num_steps_sampled: 10468000\n",
      "    num_steps_trained: 10468000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.108\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.345283018867924\n",
      "    gpu_util_percent0: 0.1321698113207547\n",
      "    ram_util_percent: 56.867924528301906\n",
      "    vram_util_percent0: 0.16973526544854006\n",
      "  pid: 95010\n",
      "  policy_reward_max:\n",
      "    default: 1.6302021237586044\n",
      "  policy_reward_mean:\n",
      "    default: -0.12294436289834247\n",
      "  policy_reward_min:\n",
      "    default: -1.845676410645185\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.22193681876739993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.10344247685846\n",
      "    mean_inference_ms: 3.2371943934818783\n",
      "    mean_raw_obs_processing_ms: 0.9314761257804619\n",
      "  time_since_restore: 10774.867249011993\n",
      "  time_this_iter_s: 79.907958984375\n",
      "  time_total_s: 208545.14562630653\n",
      "  timers:\n",
      "    learn_throughput: 74.505\n",
      "    learn_time_ms: 53687.479\n",
      "    sample_throughput: 144.722\n",
      "    sample_time_ms: 27639.217\n",
      "  timestamp: 1638562350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10468000\n",
      "  training_iteration: 2617\n",
      "  trial_id: 491c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.55 GiB heap, 0.0/3.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_491c2_00000</td><td>RUNNING </td><td>192.168.0.108:95010</td><td style=\"text-align: right;\">  2617</td><td style=\"text-align: right;\">          208545</td><td style=\"text-align: right;\">10468000</td><td style=\"text-align: right;\">-0.491777</td><td style=\"text-align: right;\">            0.121756</td><td style=\"text-align: right;\">            -1.56779</td><td style=\"text-align: right;\">            114.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        name=\"PPO_multiagent_player_custom_rewards\",\n",
    "        # name=\"Measuring_rewards\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=100,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"../../ray_results\",\n",
    "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_path /home/bruno/Workspace/soccer-tows-player/src/experiments/ppo_multiagent\n",
      "/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards/PPO_Soccer_491c2_00000_0_2021-11-30_01-16-26 /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards/PPO_Soccer_491c2_00000_0_2021-11-30_01-16-26/checkpoint_002617/checkpoint-2617\n",
      "TRIALLL /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards/PPO_Soccer_491c2_00000_0_2021-11-30_01-16-26\n"
     ]
    }
   ],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "print('this_path', this_path)\n",
    "\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"my_ray_soccer_agent\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(\n",
    "        agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"default\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return Analysis(experiment)\n",
    "\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\n",
    "        \"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards\")\n",
    "\n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "\n",
    "export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
