{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiente da competição\n",
    "# !pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "# !pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "# !pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "# !pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install torch > /dev/null 2>&1\n",
    "# !pip install lz4 > /dev/null 2>&1\n",
    "# !pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# # Dependências necessárias para gravar os vídeos\n",
    "# !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "# !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "# !pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Analysis\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.utils.typing import PolicyID\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "MAX_STEPS = 1000\n",
    "MATCH_STEPS = 4000\n",
    "\n",
    "def get_scalar_projection(x, y):\n",
    "    return np.dot(x, y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
    "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
    "min_ball_position_x, max_ball_position_x = - \\\n",
    "    15.563264846801758, 15.682827949523926\n",
    "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
    "min_player_position_x, max_player_position_x = - \\\n",
    "    17.26804542541504, 17.16301727294922\n",
    "min_player_position_y, max_player_position_y = - \\\n",
    "    7.399587631225586, 7.406457424163818\n",
    "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
    "    -23.366606239568615, 23.749571761530724\n",
    "\n",
    "max_ball_abs_velocity = 78.25721740722656\n",
    "max_goals_one_team = -9999999\n",
    "max_goals_one_match = -9999999\n",
    "max_steps = -999999\n",
    "\n",
    "max_diff_reward = -np.inf\n",
    "\n",
    "# Infered\n",
    "max_ball_abs_avg_velocity = max(\n",
    "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
    "\n",
    "\n",
    "SPEED_IMPORTANCE = 1.0 / (14.0)\n",
    "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
    "\n",
    "AFTER_BALL_STEP_PENALTY = 1 / MAX_STEPS #0.001\n",
    "\n",
    "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
    "# min_ball_to_goal_avg_velocity e\n",
    "# max_ball_to_goal_avg_velocity:\n",
    "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
    "\n",
    "\n",
    "def is_after_the_ball(player_id: int, player_pos: np.array, ball_pos: np.array):\n",
    "    if player_id in range(2):\n",
    "        return player_pos[0] > ball_pos[0]\n",
    "    elif player_id in [2, 3]:\n",
    "        return player_pos[0] < ball_pos[0]\n",
    "\n",
    "\n",
    "def get_center_of_goal_pos(player_id):\n",
    "    global min_ball_position_x, max_ball_position_x, \\\n",
    "        min_ball_position_y, max_ball_position_y, \\\n",
    "        min_player_position_x, max_player_position_x, \\\n",
    "        min_player_position_y, max_player_position_y\n",
    "    if player_id in [0, 1]:\n",
    "        return np.array([max_ball_position_x, 0.0])\n",
    "    elif player_id in [2, 3]:\n",
    "        return np.array([min_ball_position_x, 0.0])\n",
    "\n",
    "\n",
    "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict):\n",
    "    goal_pos = get_center_of_goal_pos(player_id)\n",
    "    # print(f\"goal_pos: {goal_pos}\")\n",
    "    ball_pos = info[\"ball_info\"][\"position\"]\n",
    "    # print(f\"ball_pos: {ball_pos}\")\n",
    "    direction_to_center_of_goal = goal_pos - ball_pos\n",
    "    # print(f\"direction_to_center_of_goal: {direction_to_center_of_goal}\")\n",
    "\n",
    "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
    "\n",
    "    # global max_ball_abs_velocity\n",
    "    # if np.linalg.norm(ball_velocity) > max_ball_abs_velocity:\n",
    "    #     max_ball_abs_velocity = np.linalg.norm(ball_velocity)\n",
    "\n",
    "    # print(f\"ball_velocity: {ball_velocity}\")\n",
    "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
    "        ball_velocity, direction_to_center_of_goal)\n",
    "    # print(f\"ball_velocity_to_center_of_goal: {ball_velocity_to_center_of_goal}\")\n",
    "    return ball_velocity_to_center_of_goal\n",
    "\n",
    "# print('ball_velocity_to_center_of_goal', calculate_ball_to_goal_scalar_velocity(0, { \"ball_info\": { \"position\": np.array([3.0, 2.0]), \"velocity\": np.array([0.0, 0.0]) }}))\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    # def __init__(self, env):\n",
    "    #     gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "\n",
    "        # print(info)\n",
    "        # if rewards[0] > 0.0:\n",
    "        #     assert False\n",
    "\n",
    "        if type(action) is dict:\n",
    "            new_rewards = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k]) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        if type(action) is dict:\n",
    "            splitted_rets = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k], splitted_returns=True) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "\n",
    "        info = {\n",
    "            i: {\n",
    "                **info[i],\n",
    "                \"ep_metrics\": {\n",
    "                    # \"total_timesteps\": np.array([0.0008], dtype=np.float32)\n",
    "                    \"total_timesteps\": self.n_step + 1,\n",
    "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
    "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
    "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
    "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
    "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
    "                    \"episode_ended\": done[\"__all__\"],\n",
    "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
    "                    \"env_reward\": splitted_rets[i][0],\n",
    "                    \"ball_to_goal_speed_reward\": splitted_rets[i][1],\n",
    "                    \"agent_position_to_ball_reward\": splitted_rets[i][2],\n",
    "                }\n",
    "            } for i in info.keys()\n",
    "        }\n",
    "\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y, \\\n",
    "        #     max_goals_one_team, max_goals_one_match\n",
    "        # if done:\n",
    "        #     print(f'min_ball_position_x: {min_ball_position_x}')\n",
    "        #     print(f'max_ball_position_x: {max_ball_position_x}')\n",
    "        #     print(f'min_ball_position_y: {min_ball_position_y}')\n",
    "        #     print(f'max_ball_position_y: {max_ball_position_y}')\n",
    "        #     print(f'min_player_position_x: {min_player_position_x}')\n",
    "        #     print(f'max_player_position_x: {max_player_position_x}')\n",
    "        #     print(f'min_player_position_y: {min_player_position_y}')\n",
    "        #     print(f'max_player_position_y: {max_player_position_y}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_goals_one_team: {max_goals_one_team}')\n",
    "        #     print(f'max_goals_one_match: {max_goals_one_match}')\n",
    "        #     print(self.scoreboard)\n",
    "        #     print(f'Done... last n_step: {self.n_step}')\n",
    "        #     if self.scoreboard[\"team_0\"] > 0 or self.scoreboard[\"team_1\"] > 0:\n",
    "        #         input(\"Press Enter to continue...\")\n",
    "\n",
    "        # global max_steps\n",
    "        # if done:\n",
    "        #     if self.n_step + 1 > max_steps:\n",
    "        #         max_steps = self.n_step + 1\n",
    "        #     print('max_steps', max_steps)\n",
    "\n",
    "        # global max_diff_reward\n",
    "        # if done:\n",
    "        #     print(f'max_diff_reward: {max_diff_reward}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'max_ball_abs_velocity: {max_ball_abs_velocity}')\n",
    "\n",
    "        self.n_step += 1\n",
    "        return obs, new_rewards, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.n_step = 0\n",
    "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
    "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
    "        self.await_press = False\n",
    "        # print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        # print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        return obs\n",
    "\n",
    "    def _calculate_reward(self, reward: float, player_id: int, info: Dict, splitted_returns=False) -> float:\n",
    "        # print('calculating reward')\n",
    "        if reward != 0.0:\n",
    "            # print('Goal was made!', reward, info)\n",
    "            self._update_scoreboard(player_id, reward)\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y\n",
    "        # print(f\"info: {info}\")\n",
    "        # if info[\"ball_info\"][\"position\"][0] < min_ball_position_x:\n",
    "        #     min_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][0] > max_ball_position_x:\n",
    "        #     max_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][1] < min_ball_position_y:\n",
    "        #     min_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"ball_info\"][\"position\"][1] > max_ball_position_y:\n",
    "        #     max_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][0] < min_player_position_x:\n",
    "        #     min_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][0] > max_player_position_x:\n",
    "        #     max_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][1] < min_player_position_y:\n",
    "        #     min_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][1] > max_player_position_y:\n",
    "        #     max_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "\n",
    "        self._update_avg_ball_speed_to_goal(\n",
    "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
    "        # global max_diff_reward\n",
    "        # if (np.abs(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity) > max_diff_reward):\n",
    "        #     max_diff_reward = SPEED_IMPORTANCE * \\\n",
    "        #         self.last_ball_speed_mean_per_player[player_id] / \\\n",
    "        #         max_ball_abs_avg_velocity\n",
    "\n",
    "        ball_pos = info[\"ball_info\"][\"position\"]\n",
    "        player_pos = info[\"player_info\"][\"position\"]\n",
    "\n",
    "        env_reward = reward\n",
    "        ball_to_goal_speed_reward = np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE,\n",
    "                               SPEED_IMPORTANCE) if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE else SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
    "        agent_position_to_ball_reward = is_after_the_ball(player_id, player_pos,\n",
    "                                  ball_pos) * (-AFTER_BALL_STEP_PENALTY)\n",
    "\n",
    "        if splitted_returns:\n",
    "            return (env_reward, ball_to_goal_speed_reward, agent_position_to_ball_reward)\n",
    "        return env_reward + ball_to_goal_speed_reward + agent_position_to_ball_reward\n",
    "        if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE:\n",
    "            # print(reward + np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE))\n",
    "            return reward + \\\n",
    "                np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE) + \\\n",
    "                is_after_the_ball(player_id, player_pos,\n",
    "                                  ball_pos) * AFTER_BALL_STEP_PENALTY\n",
    "        return reward + \\\n",
    "            SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity + \\\n",
    "            is_after_the_ball(player_id, player_pos,\n",
    "                              ball_pos) * AFTER_BALL_STEP_PENALTY\n",
    "\n",
    "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
    "        assert player_id in [0, 1, 2, 3]\n",
    "        global min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity\n",
    "\n",
    "        # Getting min/max ball to goal speed forr normalization\n",
    "        # print(f'player_id: {player_id}')\n",
    "        # print(f'self.last_ball_speed_mean_per_player: {self.last_ball_speed_mean_per_player}')\n",
    "        # print(f'self.n_step: {self.n_step}')\n",
    "        # print(f'ball_speed: {ball_speed}')\n",
    "\n",
    "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
    "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
    "        # if avg < min_ball_to_goal_avg_velocity:\n",
    "        #     min_ball_to_goal_avg_velocity = avg\n",
    "        # elif avg > max_ball_to_goal_avg_velocity:\n",
    "        #     max_ball_to_goal_avg_velocity = avg\n",
    "\n",
    "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
    "\n",
    "    def _update_scoreboard(self, player_id, reward):\n",
    "        global max_goals_one_team, max_goals_one_match\n",
    "\n",
    "        if player_id == 0 and reward == -1.0:\n",
    "            self.scoreboard[\"team_1\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_1\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_1\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n",
    "        elif player_id == 2 and reward == -1.0:\n",
    "            self.scoreboard[\"team_0\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_0\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_0\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)\n",
    "\n",
    "\n",
    "def create_custom_env(env_config: dict = {}):\n",
    "    env = create_rllib_env(env_config)\n",
    "    return CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(DefaultCallbacks):\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker: \"RolloutWorker\",\n",
    "                        base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index: Optional[int] = None,\n",
    "                        **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker: \"RolloutWorker\",\n",
    "                       base_env: BaseEnv,\n",
    "                       policies: Dict[PolicyID, Policy],\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index: Optional[int] = None,\n",
    "                       **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    # \"episodes_total\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENVS_PER_WORKER = 1\n",
    "# NUM_ENVS_PER_WORKER = 4\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "}\n",
    "\n",
    "\n",
    "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": 1,\n",
    "    # \"num_workers\": 3,\n",
    "    \"num_workers\": 0,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 8,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_gpus_per_worker\": 1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"default\": (None, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": tune.function(lambda _: \"default\"),\n",
    "        \"policies_to_train\": [\"default\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": ENVIRONMENT_CONFIG,\n",
    "    \"callbacks\": Callback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        name=\"PPO_multiagent_player_custom_rewards\",\n",
    "        # name=\"Measuring_rewards\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=1,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"../../ray_results\",\n",
    "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        # resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "print('this_path', this_path)\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"my_ray_soccer_agent\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"default\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return Analysis(experiment)\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test\")\n",
    "\n",
    "    \n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "export()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
