{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente da competição\n",
    "!pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "!pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "!pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "!pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "!pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "!pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "!pip install torch > /dev/null 2>&1\n",
    "!pip install lz4 > /dev/null 2>&1\n",
    "!pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# Dependências necessárias para gravar os vídeos\n",
    "!apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "!pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "!pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/soccer-twos/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Analysis\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.utils.typing import PolicyID\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalar_projection(x, y):\n",
    "    return np.dot(x, y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
    "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
    "min_ball_position_x, max_ball_position_x = - \\\n",
    "    15.563264846801758, 15.682827949523926\n",
    "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
    "min_player_position_x, max_player_position_x = - \\\n",
    "    17.26804542541504, 17.16301727294922\n",
    "min_player_position_y, max_player_position_y = - \\\n",
    "    7.399587631225586, 7.406457424163818\n",
    "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
    "    -23.366606239568615, 23.749571761530724\n",
    "\n",
    "max_ball_abs_velocity = 78.25721740722656\n",
    "max_goals_one_team = -9999999\n",
    "max_goals_one_match = -9999999\n",
    "max_steps = -999999\n",
    "\n",
    "max_diff_reward = -np.inf\n",
    "\n",
    "# Infered\n",
    "max_ball_abs_avg_velocity = max(\n",
    "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
    "\n",
    "\n",
    "SPEED_IMPORTANCE = 1.0\n",
    "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
    "\n",
    "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
    "# min_ball_to_goal_avg_velocity e\n",
    "# max_ball_to_goal_avg_velocity:\n",
    "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
    "\n",
    "\n",
    "def get_center_of_goal_pos(player_id):\n",
    "    global min_ball_position_x, max_ball_position_x, \\\n",
    "        min_ball_position_y, max_ball_position_y, \\\n",
    "        min_player_position_x, max_player_position_x, \\\n",
    "        min_player_position_y, max_player_position_y\n",
    "    if player_id in [0, 1]:\n",
    "        return np.array([max_ball_position_x, 0.0])\n",
    "    elif player_id in [2, 3]:\n",
    "        return np.array([min_ball_position_x, 0.0])\n",
    "\n",
    "\n",
    "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict):\n",
    "    goal_pos = get_center_of_goal_pos(player_id)\n",
    "    # print(f\"goal_pos: {goal_pos}\")\n",
    "    ball_pos = info[\"ball_info\"][\"position\"]\n",
    "    # print(f\"ball_pos: {ball_pos}\")\n",
    "    direction_to_center_of_goal = goal_pos - ball_pos\n",
    "    # print(f\"direction_to_center_of_goal: {direction_to_center_of_goal}\")\n",
    "\n",
    "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
    "\n",
    "    # global max_ball_abs_velocity\n",
    "    # if np.linalg.norm(ball_velocity) > max_ball_abs_velocity:\n",
    "    #     max_ball_abs_velocity = np.linalg.norm(ball_velocity)\n",
    "\n",
    "    # print(f\"ball_velocity: {ball_velocity}\")\n",
    "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
    "        ball_velocity, direction_to_center_of_goal)\n",
    "    # print(f\"ball_velocity_to_center_of_goal: {ball_velocity_to_center_of_goal}\")\n",
    "    return ball_velocity_to_center_of_goal\n",
    "\n",
    "# print('ball_velocity_to_center_of_goal', calculate_ball_to_goal_scalar_velocity(0, { \"ball_info\": { \"position\": np.array([3.0, 2.0]), \"velocity\": np.array([0.0, 0.0]) }}))\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    # def __init__(self, env):\n",
    "    #     gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "\n",
    "        # print(info)\n",
    "        # if rewards[0] > 0.0:\n",
    "        #     assert False\n",
    "\n",
    "        if type(action) is dict:\n",
    "            rewards = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k]) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        info = {\n",
    "            i: {\n",
    "                **info[i],\n",
    "                \"ep_metrics\": {\n",
    "                    # \"total_timesteps\": np.array([0.0008], dtype=np.float32)\n",
    "                    \"total_timesteps\": self.n_step + 1,\n",
    "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
    "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
    "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
    "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
    "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
    "                    \"episode_ended\": done[\"__all__\"],\n",
    "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
    "                }\n",
    "            } for i in info.keys()\n",
    "        }\n",
    "\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y, \\\n",
    "        #     max_goals_one_team, max_goals_one_match\n",
    "        # if done:\n",
    "        #     print(f'min_ball_position_x: {min_ball_position_x}')\n",
    "        #     print(f'max_ball_position_x: {max_ball_position_x}')\n",
    "        #     print(f'min_ball_position_y: {min_ball_position_y}')\n",
    "        #     print(f'max_ball_position_y: {max_ball_position_y}')\n",
    "        #     print(f'min_player_position_x: {min_player_position_x}')\n",
    "        #     print(f'max_player_position_x: {max_player_position_x}')\n",
    "        #     print(f'min_player_position_y: {min_player_position_y}')\n",
    "        #     print(f'max_player_position_y: {max_player_position_y}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_goals_one_team: {max_goals_one_team}')\n",
    "        #     print(f'max_goals_one_match: {max_goals_one_match}')\n",
    "        #     print(self.scoreboard)\n",
    "        #     print(f'Done... last n_step: {self.n_step}')\n",
    "        #     if self.scoreboard[\"team_0\"] > 0 or self.scoreboard[\"team_1\"] > 0:\n",
    "        #         input(\"Press Enter to continue...\")\n",
    "\n",
    "        # global max_steps\n",
    "        # if done:\n",
    "        #     if self.n_step + 1 > max_steps:\n",
    "        #         max_steps = self.n_step + 1\n",
    "        #     print('max_steps', max_steps)\n",
    "\n",
    "        # global max_diff_reward\n",
    "        # if done:\n",
    "        #     print(f'max_diff_reward: {max_diff_reward}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'max_ball_abs_velocity: {max_ball_abs_velocity}')\n",
    "\n",
    "        self.n_step += 1\n",
    "        return obs, rewards, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.n_step = 0\n",
    "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
    "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
    "        self.await_press = False\n",
    "        # print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        # print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        return obs\n",
    "\n",
    "    def _calculate_reward(self, reward: float, player_id: int, info: Dict) -> float:\n",
    "        # print('calculating reward')\n",
    "        if reward != 0.0:\n",
    "            # print('Goal was made!', reward, info)\n",
    "            self._update_scoreboard(player_id, reward)\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y\n",
    "        # print(f\"info: {info}\")\n",
    "        # if info[\"ball_info\"][\"position\"][0] < min_ball_position_x:\n",
    "        #     min_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][0] > max_ball_position_x:\n",
    "        #     max_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][1] < min_ball_position_y:\n",
    "        #     min_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"ball_info\"][\"position\"][1] > max_ball_position_y:\n",
    "        #     max_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][0] < min_player_position_x:\n",
    "        #     min_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][0] > max_player_position_x:\n",
    "        #     max_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][1] < min_player_position_y:\n",
    "        #     min_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][1] > max_player_position_y:\n",
    "        #     max_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "\n",
    "        self._update_avg_ball_speed_to_goal(\n",
    "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
    "        # global max_diff_reward\n",
    "        # if (np.abs(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity) > max_diff_reward):\n",
    "        #     max_diff_reward = SPEED_IMPORTANCE * \\\n",
    "        #         self.last_ball_speed_mean_per_player[player_id] / \\\n",
    "        #         max_ball_abs_avg_velocity\n",
    "        if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE:\n",
    "            # print(reward + np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE))\n",
    "            return reward + np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE, SPEED_IMPORTANCE)\n",
    "        return reward + SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
    "\n",
    "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
    "        assert player_id in [0, 1, 2, 3]\n",
    "        global min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity\n",
    "\n",
    "        # Getting min/max ball to goal speed forr normalization\n",
    "        # print(f'player_id: {player_id}')\n",
    "        # print(f'self.last_ball_speed_mean_per_player: {self.last_ball_speed_mean_per_player}')\n",
    "        # print(f'self.n_step: {self.n_step}')\n",
    "        # print(f'ball_speed: {ball_speed}')\n",
    "\n",
    "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
    "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
    "        # if avg < min_ball_to_goal_avg_velocity:\n",
    "        #     min_ball_to_goal_avg_velocity = avg\n",
    "        # elif avg > max_ball_to_goal_avg_velocity:\n",
    "        #     max_ball_to_goal_avg_velocity = avg\n",
    "\n",
    "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
    "\n",
    "    def _update_scoreboard(self, player_id, reward):\n",
    "        global max_goals_one_team, max_goals_one_match\n",
    "\n",
    "        if player_id == 0 and reward == -1.0:\n",
    "            self.scoreboard[\"team_1\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_1\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_1\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n",
    "        elif player_id == 2 and reward == -1.0:\n",
    "            self.scoreboard[\"team_0\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_0\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_0\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)\n",
    "\n",
    "\n",
    "def create_custom_env(env_config: dict = {}):\n",
    "    env = create_rllib_env(env_config)\n",
    "    return CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 1000\n",
    "MATCH_STEPS = 4000\n",
    "\n",
    "class Callback(DefaultCallbacks):\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker: \"RolloutWorker\",\n",
    "                       base_env: BaseEnv,\n",
    "                       policies: Dict[PolicyID, Policy],\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index: Optional[int] = None,\n",
    "                       **kwargs):\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(total_timesteps) if total_goals > 0 else 9999.0\n",
    "        episode.custom_metrics = {\n",
    "            \"total_timesteps\": total_timesteps,\n",
    "            \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    # \"episodes_total\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "WARNING:ray.tune.sample:DeprecationWarning: wrapping <function <lambda> at 0x7f15f97518b0> with tune.function() is no longer needed\n"
     ]
    }
   ],
   "source": [
    "NUM_ENVS_PER_WORKER = 1\n",
    "# NUM_ENVS_PER_WORKER = 4\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "}\n",
    "\n",
    "\n",
    "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": 1,\n",
    "    # \"num_workers\": 3,\n",
    "    \"num_workers\": 0,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 8,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_gpus_per_worker\": 1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"default\": (None, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": tune.function(lambda _: \"default\"),\n",
    "        \"policies_to_train\": [\"default\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": ENVIRONMENT_CONFIG,\n",
    "    \"callbacks\": Callback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.09 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_93e21_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:29:40,641\tINFO torch_policy.py:148 -- TorchPolicy (worker=local) running on 1 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:00,493\tINFO rollout_worker.py:1199 -- Built policy map: {'default': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f37fc0e6be0>}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:00,494\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'default': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f37fc0e6130>}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:00,494\tINFO rollout_worker.py:583 -- Built filter map: {'default': <ray.rllib.utils.filter.NoFilter object at 0x7f37f8818550>}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,575\tINFO trainable.py:101 -- Trainable.setup took 23.810 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,701\tINFO rollout_worker.py:723 -- Generating sample batch of size 800\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,722\tINFO sampler.py:590 -- Raw obs from env: { 0: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   1: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   2: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   3: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)}}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,722\tINFO sampler.py:592 -- Info return from env: { 0: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   1: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   2: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   3: {0: {}, 1: {}, 2: {}, 3: {}}}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,722\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,722\tINFO sampler.py:817 -- Filtered obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,727\tINFO sampler.py:1004 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m { 'default': [ { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                            'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:02,759\tINFO sampler.py:1022 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m { 'default': ( np.ndarray((16, 3), dtype=int64, min=0.0, max=2.0, mean=1.062),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                [],\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                { 'action_dist_inputs': np.ndarray((16, 9), dtype=float32, min=-0.008, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'action_logp': np.ndarray((16,), dtype=float32, min=-3.305, max=-3.29, mean=-3.297),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'action_prob': np.ndarray((16,), dtype=float32, min=0.037, max=0.037, mean=0.037),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                  'vf_preds': np.ndarray((16,), dtype=float32, min=-0.003, max=0.001, mean=-0.001)})}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:07,330\tINFO simple_list_collector.py:659 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m { 0: { 'action_dist_inputs': np.ndarray((182, 9), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'action_logp': np.ndarray((182,), dtype=float32, min=-3.308, max=-3.283, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'actions': np.ndarray((182, 3), dtype=int64, min=0.0, max=2.0, mean=0.947),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'advantages': np.ndarray((182,), dtype=float32, min=-3.305, max=-0.143, mean=-1.79),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'agent_index': np.ndarray((182,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'dones': np.ndarray((182,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'eps_id': np.ndarray((182,), dtype=int64, min=1040827809.0, max=1040827809.0, mean=1040827809.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'infos': np.ndarray((182,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.532, max=1.161, mean=-4.186), 'rotation_y': 78.31834, 'velocity': np.ndarray((2,), dtype=float32, min=-7.998, max=-0.84, mean=-4.419)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'new_obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.16),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.16),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'rewards': np.ndarray((182,), dtype=float32, min=-1.095, max=0.041, mean=-0.028),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'unroll_id': np.ndarray((182,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'value_targets': np.ndarray((182,), dtype=float32, min=-3.303, max=-0.139, mean=-1.788),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'vf_preds': np.ndarray((182,), dtype=float32, min=-0.003, max=0.009, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   1: { 'action_dist_inputs': np.ndarray((182, 9), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'action_logp': np.ndarray((182,), dtype=float32, min=-3.309, max=-3.285, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'actions': np.ndarray((182, 3), dtype=int64, min=0.0, max=2.0, mean=1.016),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'advantages': np.ndarray((182,), dtype=float32, min=-3.309, max=-0.141, mean=-1.79),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'agent_index': np.ndarray((182,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'dones': np.ndarray((182,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'eps_id': np.ndarray((182,), dtype=int64, min=1040827809.0, max=1040827809.0, mean=1040827809.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'infos': np.ndarray((182,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-6.24, max=-1.338, mean=-3.789), 'rotation_y': 97.002495, 'velocity': np.ndarray((2,), dtype=float32, min=-2.212, max=-0.058, mean=-1.135)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'new_obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'rewards': np.ndarray((182,), dtype=float32, min=-1.095, max=0.041, mean=-0.028),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'unroll_id': np.ndarray((182,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'value_targets': np.ndarray((182,), dtype=float32, min=-3.303, max=-0.139, mean=-1.788),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'vf_preds': np.ndarray((182,), dtype=float32, min=-0.003, max=0.008, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   2: { 'action_dist_inputs': np.ndarray((182, 9), dtype=float32, min=-0.01, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'action_logp': np.ndarray((182,), dtype=float32, min=-3.31, max=-3.281, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'actions': np.ndarray((182, 3), dtype=int64, min=0.0, max=2.0, mean=1.029),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'advantages': np.ndarray((182,), dtype=float32, min=0.007, max=3.246, mean=1.658),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'agent_index': np.ndarray((182,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'dones': np.ndarray((182,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'eps_id': np.ndarray((182,), dtype=int64, min=1040827809.0, max=1040827809.0, mean=1040827809.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'infos': np.ndarray((182,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=1.247, max=5.953, mean=3.6), 'rotation_y': 275.33014, 'velocity': np.ndarray((2,), dtype=float32, min=-8.017, max=0.748, mean=-3.634)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'new_obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.188),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.188),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'rewards': np.ndarray((182,), dtype=float32, min=-0.038, max=0.883, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'unroll_id': np.ndarray((182,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'value_targets': np.ndarray((182,), dtype=float32, min=0.008, max=3.248, mean=1.659),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'vf_preds': np.ndarray((182,), dtype=float32, min=-0.005, max=0.007, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   3: { 'action_dist_inputs': np.ndarray((182, 9), dtype=float32, min=-0.009, max=0.011, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'action_logp': np.ndarray((182,), dtype=float32, min=-3.309, max=-3.284, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'actions': np.ndarray((182, 3), dtype=int64, min=0.0, max=2.0, mean=0.945),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'advantages': np.ndarray((182,), dtype=float32, min=0.007, max=3.249, mean=1.658),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'agent_index': np.ndarray((182,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'dones': np.ndarray((182,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'eps_id': np.ndarray((182,), dtype=int64, min=1040827809.0, max=1040827809.0, mean=1040827809.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'infos': np.ndarray((182,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-1.225, max=7.166, mean=2.971), 'rotation_y': 280.03668, 'velocity': np.ndarray((2,), dtype=float32, min=-0.611, max=8.018, mean=3.704)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'new_obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.172),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'obs': np.ndarray((182, 336), dtype=float32, min=0.0, max=1.0, mean=0.172),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'rewards': np.ndarray((182,), dtype=float32, min=-0.038, max=0.883, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'unroll_id': np.ndarray((182,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'value_targets': np.ndarray((182,), dtype=float32, min=0.008, max=3.248, mean=1.659),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m        'vf_preds': np.ndarray((182,), dtype=float32, min=-0.004, max=0.007, mean=0.0)}}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:07,816\tINFO rollout_worker.py:761 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   'policy_batches': { 'default': { 'action_dist_inputs': np.ndarray((3200, 9), dtype=float32, min=-0.011, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'action_logp': np.ndarray((3200,), dtype=float32, min=-3.311, max=-3.281, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'actions': np.ndarray((3200, 3), dtype=int64, min=0.0, max=2.0, mean=1.007),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'advantages': np.ndarray((3200,), dtype=float32, min=-4.472, max=3.649, mean=-0.109),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'agent_index': np.ndarray((3200,), dtype=int64, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'dones': np.ndarray((3200,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'eps_id': np.ndarray((3200,), dtype=int64, min=162434314.0, max=1506253365.0, mean=860272722.327),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'infos': np.ndarray((3200,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.035, max=1.338, mean=-3.849), 'rotation_y': 88.31835, 'velocity': np.ndarray((2,), dtype=float32, min=-0.065, max=2.214, mean=1.075)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'new_obs': np.ndarray((3200, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'obs': np.ndarray((3200, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'rewards': np.ndarray((3200,), dtype=float32, min=-1.095, max=0.883, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'unroll_id': np.ndarray((3200,), dtype=int64, min=0.0, max=19.0, mean=7.86),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'value_targets': np.ndarray((3200,), dtype=float32, min=-4.469, max=3.647, mean=-0.107),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'vf_preds': np.ndarray((3200,), dtype=float32, min=-0.006, max=0.011, mean=0.002)}},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m 2021-11-29 00:30:26,665\tINFO rollout_worker.py:901 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   'policy_batches': { 'default': { 'action_dist_inputs': np.ndarray((128, 9), dtype=float32, min=-0.01, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'action_logp': np.ndarray((128,), dtype=float32, min=-3.309, max=-3.284, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'actions': np.ndarray((128, 3), dtype=int64, min=0.0, max=2.0, mean=1.044),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'advantages': np.ndarray((128,), dtype=float32, min=-4.212, max=5.094, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.742),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'eps_id': np.ndarray((128,), dtype=int64, min=162434314.0, max=1548411722.0, mean=797703407.18),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'infos': np.ndarray((128,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-3.672, max=7.796, mean=2.062), 'rotation_y': 30.036724, 'velocity': np.ndarray((2,), dtype=float32, min=-0.537, max=10.904, mean=5.184)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=-10.779, max=-6.637, mean=-8.708), 'velocity': np.ndarray((2,), dtype=float32, min=-0.0, max=0.0, mean=-0.0)}, 'ep_metrics': {'total_timesteps': 886, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False}}),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'new_obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'rewards': np.ndarray((128,), dtype=float32, min=-0.128, max=0.095, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'unroll_id': np.ndarray((128,), dtype=int64, min=1.0, max=86.0, mean=46.617),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'value_targets': np.ndarray((128,), dtype=float32, min=-4.38, max=5.174, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m                                    'vf_preds': np.ndarray((128,), dtype=float32, min=-0.005, max=0.007, mean=0.002)}},\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3467)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_93e21_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-29_00-31-10\n",
      "  done: true\n",
      "  episode_len_mean: 784.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5300339193162786\n",
      "  episode_reward_mean: -2.7610312928913894\n",
      "  episode_reward_min: -5.256192151694677\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 4\n",
      "  experiment_id: 39fb015add7c4fbfb00e6a6e49b6581a\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      default:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0e-05\n",
      "          entropy: 3.269902194976807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026177214577794074\n",
      "          policy_loss: -0.046513454262167214\n",
      "          total_loss: 0.9102762937545776\n",
      "          vf_explained_var: 0.09366738796234131\n",
      "          vf_loss: 0.9515542953014374\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.1.1\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.963736263736266\n",
      "    gpu_util_percent0: 0.14340659340659337\n",
      "    ram_util_percent: 33.36043956043956\n",
      "    vram_util_percent0: 0.15767006931863797\n",
      "  pid: 3467\n",
      "  policy_reward_max:\n",
      "    default: 5.178564324790221\n",
      "  policy_reward_mean:\n",
      "    default: -0.6902578232228468\n",
      "  policy_reward_min:\n",
      "    default: -7.748411192350716\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.21183455026114023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.879205839021818\n",
      "    mean_inference_ms: 3.140441902153023\n",
      "    mean_raw_obs_processing_ms: 0.6248436488590755\n",
      "  time_since_restore: 68.27390360832214\n",
      "  time_this_iter_s: 68.27390360832214\n",
      "  time_total_s: 68.27390360832214\n",
      "  timers:\n",
      "    learn_throughput: 90.195\n",
      "    learn_time_ms: 44348.524\n",
      "    sample_throughput: 167.196\n",
      "    sample_time_ms: 23924.038\n",
      "  timestamp: 1638156670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 93e21_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/8.09 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_93e21_00000</td><td>RUNNING </td><td>127.0.1.1:3467</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.2739</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-2.76103</td><td style=\"text-align: right;\">           -0.530034</td><td style=\"text-align: right;\">            -5.25619</td><td style=\"text-align: right;\">               784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.09 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_93e21_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.2739</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-2.76103</td><td style=\"text-align: right;\">           -0.530034</td><td style=\"text-align: right;\">            -5.25619</td><td style=\"text-align: right;\">               784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 00:31:13,257\tINFO tune.py:549 -- Total run time: 97.09 seconds (94.43 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO_Soccer_93e21_00000\n",
      "/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test/PPO_Soccer_93e21_00000_0_2021-11-29_00-29-36/checkpoint_000001/checkpoint-1\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        # name=\"PPO_multiagent_player\",\n",
    "        name=\"Testing_pickle\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=1,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"./src/ray_results\",\n",
    "        # restore=\"./src/ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        # resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_path /home/bruno/Workspace/soccer-tows-player/src/experiments/ppo_multiagent\n",
      "/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test/PPO_Soccer_93e21_00000_0_2021-11-29_00-29-36 /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test/PPO_Soccer_93e21_00000_0_2021-11-29_00-29-36/checkpoint_000001/checkpoint-1\n",
      "TRIALLL /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test/PPO_Soccer_93e21_00000_0_2021-11-29_00-29-36\n"
     ]
    }
   ],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "print('this_path', this_path)\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"my_ray_soccer_agent\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"default\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return Analysis(experiment)\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiplayer_agent_test\")\n",
    "\n",
    "    \n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "export()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
