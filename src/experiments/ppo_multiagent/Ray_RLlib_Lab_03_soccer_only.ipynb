{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oJ6ZI3_DNkR"
   },
   "source": [
    "# Biblioteca de Algoritmos - Lab 03\n",
    "\n",
    "Nos últimos anos, muitas bibliotecas RL foram desenvolvidas. Essas bibliotecas foram projetadas para ter todas as ferramentas necessárias para implementar e testar agentes de Aprendizado por Reforço .\n",
    "\n",
    "Ainda assim, elas se diferem muito. É por isso que é importante escolher uma biblioteca que seja rápida, confiável e relevante para sua tarefa de RL. Do ponto de vista técnico, existem algumas coisas a se ter em mente ao considerar uma bilioteca para RL.\n",
    "\n",
    "- **Suporte para bibliotecas de aprendizado de máquina existentes:** Como o RL normalmente usa algoritmos baseados em gradiente para aprender e ajustar funções de política, você vai querer que ele suporte sua biblioteca favorita (Tensorflow, Keras, Pytorch, etc.)\n",
    "- **Escalabilidade:** RL é computacionalmente intensivo e ter a opção de executar de forma distribuída torna-se importante ao atacar ambientes complexos.\n",
    "- **Composibilidade:** Os algoritmos de RL normalmente envolvem simulações e muitos outros componentes. Você vai querer uma biblioteca que permita reutilizar componentes de algoritmos de RL, que seja compatível com várias estruturas de aprendizado profundo.\n",
    "\n",
    "[Aqui](https://docs.google.com/spreadsheets/d/1ZWhViAwCpRqupA5E_xFHSaBaaBZ1wAjO6PvmmEEpXGI/edit#gid=0) você consegue visualizar uma lista com algumas bibliotecas existentes.\n",
    "\n",
    "<img src=\"https://i1.wp.com/neptune.ai/wp-content/uploads/RL-tools.png?resize=1024%2C372&ssl=1\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lV7pvJEIz7w"
   },
   "source": [
    "## Ray RLlib\n",
    "\n",
    "[Ray](https://docs.ray.io/en/latest/) é uma plataforma de execução distribuída que fornece bases para paralelismo e escalabilidade que são simples de usar e permitem que os programas Python sejam escalados em qualquer lugar, de um notebook a um grande cluster. Além disso, construída sobre o Ray, temos a [RLlib](https://docs.ray.io/en/latest/rllib.html), que fornece uma API unificada que pode ser aproveitada em uma ampla gama de aplicações.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*_bomm09XtiZfQ52Kfz9Ciw.png\" width=600>\n",
    "\n",
    "\n",
    "A RLlib foi projetada para oferecer suporte a várias estruturas de aprendizado profundo (TensorFlow e PyTorch) e pode ser acessada por meio de uma API Python simples. Atualmente, ela vem com uma [série de algoritmos RL](https://docs.ray.io/en/latest/rllib-algorithms.html#available-algorithms-overview).\n",
    "\n",
    "Em particular, a RLlib permite um desenvolvimento rápido porque torna mais fácil construir algoritmos RL escaláveis ​​por meio da reutilização e montagem de implementações existentes. A RLlib também permite que os desenvolvedores usem redes neurais criadas com várias estruturas de aprendizado profundo e se integra facilmente a simuladores de terceiros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE7inIG6QBNH"
   },
   "source": [
    "## (Iniciar Colab) Configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYuZ0_Yfpf7_"
   },
   "source": [
    "Você precisará fazer uma cópia deste notebook em seu Google Drive antes de editar. Você pode fazer isso com **Arquivo → Salvar uma cópia no Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "ZB7IKKg9peTw",
    "outputId": "6e7ee3df-f320-450d-938f-b657c162bae0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "isColab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO84pMtApgxS"
   },
   "outputs": [],
   "source": [
    "# Seu trabalho será armazenado em uma pasta chamada `minicurso_rl` por padrão \n",
    "# para evitar que o tempo limite da instância do Colab exclua suas edições\n",
    "\n",
    "DRIVE_PATH = \"/content/gdrive/MyDrive/minicurso_rl/lab03\"\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace(\"\\\\\", \"\")\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir -p $DRIVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWh9m8C7Oxhh",
    "outputId": "80f5cea3-99cb-4def-b152-03f0321dcf4c"
   },
   "outputs": [],
   "source": [
    "! wget http://www.atarimania.com/roms/Roms.rar\n",
    "! mkdir /content/ROM/\n",
    "! unrar e /content/Roms.rar /content/ROM/ -y\n",
    "! python -m atari_py.import_roms /content/ROM/ > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o44ofLXFDUph"
   },
   "source": [
    "## (Iniciar somente local, fora do COLAB) Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SFGsxqn5DUph"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T8c7Qt4WDUpi"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Seu trabalho será armazenado em uma pasta chamada `minicurso_rl` por padrão \n",
    "# para evitar que o tempo limite da instância do Colab exclua suas edições\n",
    "CONTENT_PATH = \"./content\"\n",
    "if not os.path.exists(CONTENT_PATH):\n",
    "    %mkdir $CONTENT_PATH\n",
    "\n",
    "CKPT_PATH = \"./ckpt\"\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    %mkdir $CKPT_PATH\n",
    "\n",
    "if not isColab:\n",
    "    DRIVE_PATH = copy.deepcopy(CONTENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQumDBnyDUpi",
    "outputId": "2add62e5-962d-4780-8cc6-e03e13451fad"
   },
   "outputs": [],
   "source": [
    "# ! wget http://www.atarimania.com/roms/Roms.rar\n",
    "# ! mkdir ./content/ROM/\n",
    "# ! mv ./Roms.rar ./content/\n",
    "# ! unrar e ./content/Roms.rar ./content/ROM/ -y\n",
    "# ! python -m atari_py.import_roms ./content/ROM/ > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyxQHLgRDUpk"
   },
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eSTA7tLoNFqP"
   },
   "outputs": [],
   "source": [
    "# Ambiente da competição\n",
    "!pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "!pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "!pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "!pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "!pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "!pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "!pip install torch > /dev/null 2>&1\n",
    "!pip install lz4 > /dev/null 2>&1\n",
    "!pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# Dependências necessárias para gravar os vídeos\n",
    "!apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "!pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "!pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SpUW4lSC6bwH"
   },
   "outputs": [],
   "source": [
    "# Carrega a extensão do notebook TensorBoard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm_F9Zsb60fi"
   },
   "source": [
    "# Bônus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJm60-Zu69eH"
   },
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ORYtmYf823f"
   },
   "source": [
    "Utilize o ambiente instanciado abaixo para executar o algoritmo de treinamento. Ao final da execução, a recompensa do seu agente por episódio deve tender a +2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN7ucdZ1DUpy"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as pg\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwotOfEt81mq",
    "outputId": "4c741cea-388b-4a5a-f2a9-e3dc5b5dfa29"
   },
   "outputs": [],
   "source": [
    "import soccer_twos\n",
    "\n",
    "# Fecha o ambiente caso tenha sido aberto anteriormente\n",
    "try: env.close()\n",
    "except: pass\n",
    "\n",
    "env = soccer_twos.make(\n",
    "    variation=soccer_twos.EnvType.team_vs_policy,\n",
    "    flatten_branched=True, # converte o action_space de MultiDiscrete para Discrete\n",
    "    single_player=True, # controla um dos jogadores enquanto os outros ficam parados\n",
    "    opponent_policy=lambda *_: 0,  # faz os oponentes ficarem parados\n",
    ")\n",
    "\n",
    "environment_id = \"soccer-v0\"\n",
    "\n",
    "# Obtem tamanhos de estado e ação\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"Tamanho do estado: {}, tamanho da ação: {}\".format(state_size, action_size))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=4, ignore_reinit_error=True, include_dashboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWfhclM8-xRy"
   },
   "outputs": [],
   "source": [
    "def create_rllib_env(env_config: dict = {}):\n",
    "    # suporte a múltiplas instâncias do ambiente na mesma máquina\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    return soccer_twos.make(**env_config)\n",
    "\n",
    "# registra ambiente no Ray\n",
    "tune.registry.register_env(environment_id, create_rllib_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9ESDaHxDUpz"
   },
   "outputs": [],
   "source": [
    "NUM_ENVS_PER_WORKER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsrDqXA31mt"
   },
   "source": [
    "Utilize a configuração abaixo como ponto de partida para seus testes. \n",
    "\n",
    "A parte mais imporante é a chave `env_config`, que configura o ambiente para ser compatível com o agente disponibilizado para exportação do seu agente. Neste ponto do curso você já deve conseguir testar as outras variações do ambiente e utilizar as APIs do Ray para treinar um agente próximo (ou melhor) do que o [ceia_baseline_agent](https://drive.google.com/file/d/1WEjr48D7QG9uVy1tf4GJAZTpimHtINzE/view). Exemplos de como utilizar as outras variações podem ser encontrados [aqui](https://github.com/dlb-rl/rl-tournament-starter/). Ao utilizar essas variações, você deve utilizar também outras definições de agente para lidar com os diferentes espaços de observação e ação (que também estão presentes nos exemplos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JU6FFjY3-vsS"
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    num_samples=1,\n",
    "    config={\n",
    "        # system settings\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 3,\n",
    "        \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"framework\": \"torch\",\n",
    "        # RL setup\n",
    "        \"env\": environment_id,\n",
    "        \"env_config\": {\n",
    "            \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "            \"variation\": soccer_twos.EnvType.team_vs_policy,\n",
    "            \"single_player\": True,\n",
    "            \"flatten_branched\": True,\n",
    "            \"opponent_policy\": lambda *_: 0,\n",
    "        },\n",
    "    },\n",
    "    stop={\n",
    "        # 10000000 (10M) de steps podem ser necessários para aprender uma política útil\n",
    "        \"timesteps_total\": int(15e6),\n",
    "        # você também pode limitar por tempo, de acordo com o tempo limite do colab\n",
    "#         \"time_total_s\": 14400, # 4h\n",
    "        \"time_total_s\": 86400, # 24h\n",
    "    },\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True,\n",
    "    local_dir=os.path.join(DRIVE_PATH, \"results\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmnTZpqAPiEE"
   },
   "source": [
    "## Exportando seu agente treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXgLarb2Qp4A"
   },
   "source": [
    "Assim como no Lab 02, você pode exportar seu agente treinado para ser executado como competidor no ambiente da competição ou simplesmente assistí-lo. Para isso, devemos definir uma classe de agente que implemente a interface e trate as observações/ações para o formato da competição. Abaixo, configuramos qual experimento/checkpoint exportar e guardamos a implementação em uma variável para salvá-la em um arquivo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4avcVx4RMpE",
    "outputId": "1386794a-a2b5-4461-d39b-c7e420d2cfa3"
   },
   "outputs": [],
   "source": [
    "ALGORITHM = \"PPO\"\n",
    "TRIAL = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
    "CHECKPOINT = analysis.get_best_checkpoint(\n",
    "  TRIAL,\n",
    "  \"training_iteration\",\n",
    "  \"max\",\n",
    ")\n",
    "TRIAL, CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnppiHT6Q5gK"
   },
   "outputs": [],
   "source": [
    "agent_file = f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import gym\n",
    "from gym_unity.envs import ActionFlattener\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface, DummyEnv\n",
    "\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"LAB_03/\")[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        self.flattener = ActionFlattener(env.action_space.nvec)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        obs_space = env.observation_space\n",
    "        act_space = self.flattener.action_space\n",
    "        tune.registry.register_env(\n",
    "            \"DummyEnv\",\n",
    "            lambda *_: DummyEnv(obs_space, act_space),\n",
    "        )\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get default policy for evaluation\n",
    "        self.policy = agent.get_policy()\n",
    "\n",
    "    def act(self, observation):\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id] = self.flattener.lookup_action(\n",
    "                self.policy.compute_single_action(observation[player_id])[0]\n",
    "            )\n",
    "        return actions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yo3UPK8Pjsg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "agent_name = \"my_ray_soccer_agent\"\n",
    "agent_path = os.path.join(\n",
    "    DRIVE_PATH, agent_name, agent_name) if isColab else os.path.join(DRIVE_PATH, agent_name)\n",
    "os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "shutil.rmtree(agent_path)\n",
    "os.makedirs(agent_path)\n",
    "\n",
    "# salva a classe do agente\n",
    "with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "    f.write(agent_file)\n",
    "\n",
    "# salva um __init__ para criar o módulo Python\n",
    "with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "    f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "# copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "shutil.copytree(TRIAL, os.path.join(agent_path, TRIAL.split(\"LAB_03/\")[1]))\n",
    "\n",
    "# empacota tudo num arquivo .zip\n",
    "if isColab:\n",
    "    shutil.make_archive(os.path.join(DRIVE_PATH, agent_name),\n",
    "                        \"zip\", os.path.join(DRIVE_PATH, agent_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef7DM2J46OFF"
   },
   "source": [
    "Após empacotar todos os arquivos necessários para a execução do seu agente, será criado um arquivo `minicurso_rl/lab03/my_ray_soccer_agent.zip` nos arquivos do Colab e na pasta correspondente no Google Drive. Baixe o arquivo e extraia-o para alguma pasta no seu computador. \n",
    "\n",
    "Assumindo que o ambiente Python já está configurado (e.g. os pacotes no [requirements.txt](https://github.com/dlb-rl/rl-tournament-starter/blob/main/requirements.txt) estão instalados), rode `python -m soccer_twos.watch -m my_ray_soccer_agent` para assistir seu agente jogando contra si mesmo. \n",
    "\n",
    "Você também pode testar dois agentes diferentes jogando um contra o outro. Utilize o seguinte comando: `python -m soccer_twos.watch -m1 my_ray_soccer_agent -m2 ceia_baseline_agent`. Você pode baixar o agente *ceia_baseline_agent* [aqui](https://drive.google.com/file/d/1WEjr48D7QG9uVy1tf4GJAZTpimHtINzE/view)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ray_RLlib_Lab_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
