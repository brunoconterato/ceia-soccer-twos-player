{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkzGc9wTHFcj"
      },
      "source": [
        "# Iniciar ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-beBzFHFcn"
      },
      "source": [
        "## Iniciar Local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX_GPU7oHFco"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "isColab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os-pRq4fH-qC",
        "outputId": "4ccf45ee-f725-44d8-ce13-dbad21439f13"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6obbGU7InFQ",
        "outputId": "bf7b831e-ce55-4d0d-c873-4ec546d5f726"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3KVZOMFHFcr"
      },
      "source": [
        "## (Sempre) Outras configurações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3me5zmeHFcr"
      },
      "outputs": [],
      "source": [
        "# # # Ambiente da competição\n",
        "# !pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
        "# # # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
        "# !pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
        "# !pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
        "# !pip install 'ray==1.4.0' > /dev/null 2>&1\n",
        "# !pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
        "# !pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
        "# !pip install torch > /dev/null 2>&1\n",
        "# !pip install lz4 > /dev/null 2>&1\n",
        "# !pip install GPUtil > /dev/null 2>&1\n",
        "\n",
        "# # # Dependências necessárias para gravar os vídeos\n",
        "# # !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
        "# # !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
        "# # !pip install tensorboard > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO1Vtg93HFct",
        "outputId": "66998f6b-f56a-4e29-8e93-30e60dc9356a"
      },
      "outputs": [],
      "source": [
        "!pip show ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tlk7dgHHFcu"
      },
      "outputs": [],
      "source": [
        "RAY_RESULTS_PATH = \"/content/gdrive/MyDrive/minicurso_rl/ray_results\" if isColab else '../../ray_results'\n",
        "RAY_RESULTS_PYTHON_PATH = RAY_RESULTS_PATH.replace(\"\\\\\", \"\")\n",
        "if not os.path.exists(RAY_RESULTS_PYTHON_PATH):\n",
        "  %mkdir -p $RAY_RESULTS_PATH\n",
        "print(RAY_RESULTS_PATH)\n",
        "\n",
        "AGENTS_PATH = \"/content/gdrive/MyDrive/minicurso_rl/agents\" if isColab else './agents'\n",
        "AGENTS_PATH_PYTHON_PATH = AGENTS_PATH.replace(\"\\\\\", \"\")\n",
        "if not os.path.exists(AGENTS_PATH_PYTHON_PATH):\n",
        "  %mkdir -p $AGENTS_PATH\n",
        "print(AGENTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H8yDF4XHFcv"
      },
      "source": [
        "# Soccer Twos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjyHV5C9HFcw"
      },
      "source": [
        "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
        "\n",
        "> Visualização do ambiente\n",
        "\n",
        "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
        "\n",
        "\n",
        "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNL8OU2eHFcx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4F3kNZkHFcy",
        "outputId": "d159d8f4-78a3-47cb-c809-85fa587760be"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import Analysis\n",
        "# from ray.rllib.agents.ppo import PPOTrainer\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
        "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
        "from ray.rllib.policy import Policy\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "# from ray.rllib.utils.typing import PolicyID\n",
        "# from ray.tune.registry import get_trainable_cls\n",
        "# from ray.rllib.policy.policy import PolicySpec\n",
        "\n",
        "import numpy as np\n",
        "from typing import Any, Dict, List, Union, Optional\n",
        "from collections import deque\n",
        "# import pickle\n",
        "import math\n",
        "from pprint import pprint\n",
        "\n",
        "import soccer_twos\n",
        "from soccer_twos import EnvType\n",
        "\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KFXRTMWHFcz"
      },
      "source": [
        "## Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-cPLP1EHFcz"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "MAX_STEPS = 1000\n",
        "MATCH_STEPS = 4000\n",
        "\n",
        "\n",
        "def get_scalar_projection(x, y):\n",
        "    assert np.linalg.norm(y) > 0.000001\n",
        "    return np.dot(x, y) / np.linalg.norm(y)\n",
        "\n",
        "\n",
        "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
        "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
        "min_ball_position_x, max_ball_position_x = - \\\n",
        "    15.563264846801758, 15.682827949523926\n",
        "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
        "min_player_position_x, max_player_position_x = - \\\n",
        "    17.26804542541504, 17.16301727294922\n",
        "min_player_position_y, max_player_position_y = - \\\n",
        "    7.399587631225586, 7.406457424163818\n",
        "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
        "    -23.366606239568615, 23.749571761530724\n",
        "\n",
        "max_ball_abs_velocity = 78.25721740722656\n",
        "max_goals_one_team = -9999999\n",
        "max_goals_one_match = -9999999\n",
        "max_steps = -999999\n",
        "\n",
        "max_diff_reward = -np.inf\n",
        "\n",
        "# Infered\n",
        "max_ball_abs_avg_velocity = max(\n",
        "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
        "\n",
        "\n",
        "SPEED_IMPORTANCE = 1.0 / (14.0)\n",
        "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
        "\n",
        "AFTER_BALL_STEP_PENALTY = 1 / MAX_STEPS  # 0.001\n",
        "\n",
        "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
        "# min_ball_to_goal_avg_velocity e\n",
        "# max_ball_to_goal_avg_velocity:\n",
        "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
        "\n",
        "\n",
        "def is_after_the_ball(player_id: int, player_pos: np.array, ball_pos: np.array):\n",
        "    if player_id in range(2):\n",
        "        return player_pos[0] > ball_pos[0]\n",
        "    elif player_id in [2, 3]:\n",
        "        return player_pos[0] < ball_pos[0]\n",
        "\n",
        "\n",
        "def get_center_of_goal_pos(player_id):\n",
        "    global min_ball_position_x, max_ball_position_x, \\\n",
        "        min_ball_position_y, max_ball_position_y, \\\n",
        "        min_player_position_x, max_player_position_x, \\\n",
        "        min_player_position_y, max_player_position_y\n",
        "    if player_id in [0, 1]:\n",
        "        return np.array([max_ball_position_x, 0.0])\n",
        "    elif player_id in [2, 3]:\n",
        "        return np.array([min_ball_position_x, 0.0])\n",
        "\n",
        "\n",
        "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict, x_axis_only=True):\n",
        "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
        "    if x_axis_only and player_id in [0, 1]:\n",
        "        return ball_velocity[0]\n",
        "    elif x_axis_only and player_id in [2, 3]:\n",
        "        return -ball_velocity[0]\n",
        "\n",
        "    goal_pos = get_center_of_goal_pos(player_id)\n",
        "    ball_pos = info[\"ball_info\"][\"position\"]\n",
        "\n",
        "    direction_to_center_of_goal = goal_pos - ball_pos\n",
        "\n",
        "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
        "        ball_velocity, direction_to_center_of_goal)\n",
        "    return ball_velocity_to_center_of_goal\n",
        "\n",
        "\n",
        "def calculate_distance(pt1: np.ndarray, pt2: np.ndarray):\n",
        "    assert pt1.shape == (2,) and pt2.shape == (2,)\n",
        "    return np.linalg.norm(pt1 - pt2)\n",
        "\n",
        "\n",
        "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
        "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
        "        obs, rewards, done, info = super().step(action)\n",
        "\n",
        "        ball_pos = info[0][\"ball_info\"][\"position\"]\n",
        "        ball_velocity = info[0][\"ball_info\"][\"velocity\"]\n",
        "        player0_pos = info[0][\"player_info\"][\"position\"]\n",
        "        player1_pos = info[1][\"player_info\"][\"position\"]\n",
        "        player2_pos = info[2][\"player_info\"][\"position\"]\n",
        "        player3_pos = info[3][\"player_info\"][\"position\"]\n",
        "\n",
        "        if self._was_ball_effective_touched(self.prev_ball_velocity, ball_velocity):\n",
        "            self._get_ball_toucher(\n",
        "                ball_velocity, ball_pos, player0_pos, player1_pos, player2_pos, player3_pos)\n",
        "\n",
        "        if type(action) is dict:\n",
        "            new_rewards = {k: self._calculate_reward(\n",
        "                rewards[k], k, info[k]) for k in info.keys()}\n",
        "        else:\n",
        "            raise NotImplementedError('Necessário implementar!')\n",
        "\n",
        "        if type(action) is dict:\n",
        "            splitted_rets = {k: self._calculate_reward(\n",
        "                rewards[k], k, info[k], splitted_returns=True) for k in info.keys()}\n",
        "        else:\n",
        "            raise NotImplementedError('Necessário implementar!')\n",
        "\n",
        "        info = {\n",
        "            i: {\n",
        "                **info[i],\n",
        "                \"ep_metrics\": {\n",
        "                    \"total_timesteps\": self.n_step + 1,\n",
        "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
        "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
        "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
        "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
        "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
        "                    \"episode_ended\": done[\"__all__\"],\n",
        "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
        "                    \"env_reward\": splitted_rets[i][0],\n",
        "                    \"ball_to_goal_speed_reward\": splitted_rets[i][1],\n",
        "                }\n",
        "            } for i in info.keys()\n",
        "        }\n",
        "\n",
        "        self.n_step += 1\n",
        "        self.prev_ball_velocity = ball_velocity.copy()\n",
        "\n",
        "        return obs, new_rewards, done, info\n",
        "            \n",
        "    def reset(self, **kwargs):\n",
        "        obs = super().reset(**kwargs)\n",
        "        self.n_step = 0\n",
        "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
        "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
        "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
        "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
        "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
        "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
        "        self.await_press = False\n",
        "        self.prev_ball_velocity = np.array([0.0, 0.0])\n",
        "        self.last_ball_toucher = -1\n",
        "        \n",
        "        return obs\n",
        "\n",
        "    def _was_ball_effective_touched(self, prev_ball_velocity: np.ndarray, curr_ball_velocity: np.ndarray):\n",
        "        \"\"\"Get if ball was touched (either by player or wall)\n",
        "\n",
        "        Args:\n",
        "            prev_ball_velocity (np.ndarray): Previous ball coordinates\n",
        "            curr_ball_velocity (np.ndarray): Current ball coordinates\n",
        "        \"\"\"\n",
        "        assert prev_ball_velocity.shape == (\n",
        "            2,) and curr_ball_velocity.shape == (2,)\n",
        "        percentual_scalar_thresold = 0.2  # 20%\n",
        "        diff = curr_ball_velocity - prev_ball_velocity\n",
        "\n",
        "\n",
        "        if np.linalg.norm(curr_ball_velocity) < 1.0:\n",
        "            self.last_ball_toucher = -1\n",
        "\n",
        "        if np.linalg.norm(prev_ball_velocity) > 0.0000001:\n",
        "            return np.linalg.norm(diff) / np.linalg.norm(prev_ball_velocity) > percentual_scalar_thresold\n",
        "        return np.linalg.norm(curr_ball_velocity) > np.linalg.norm(prev_ball_velocity)\n",
        "\n",
        "    def _get_ball_toucher(self,\n",
        "                          ball_velocity: np.ndarray,\n",
        "                          ball_position: np.ndarray,\n",
        "                          player_0_pos: np.ndarray,\n",
        "                          player_1_pos: np.ndarray,\n",
        "                          player_2_pos: np.ndarray,\n",
        "                          player_3_pos: np.ndarray):\n",
        "        assert ball_position.shape == (2,) and \\\n",
        "            player_0_pos.shape == (2,) and \\\n",
        "            player_1_pos.shape == (2,) and \\\n",
        "            player_2_pos.shape == (2,) and \\\n",
        "            player_3_pos.shape == (2,)\n",
        "        top_wall_y = max_ball_position_y\n",
        "        bottom_wall_y = min_ball_position_y\n",
        "        left_wall_x = min_ball_position_x\n",
        "        right_wall_x = max_ball_position_x\n",
        "\n",
        "        if np.linalg.norm(ball_velocity) > 0.000001:\n",
        "            distances = np.array([\n",
        "                calculate_distance(ball_position, player_0_pos),\n",
        "                calculate_distance(ball_position, player_1_pos),\n",
        "                calculate_distance(ball_position, player_2_pos),\n",
        "                calculate_distance(ball_position, player_3_pos),\n",
        "                np.abs(ball_position[1] - top_wall_y),\n",
        "                np.abs(ball_position[1] - bottom_wall_y),\n",
        "                np.abs(ball_position[0] - left_wall_x),\n",
        "                np.abs(ball_position[0] - right_wall_x)\n",
        "            ])\n",
        "\n",
        "            nearest = np.argmin(distances)\n",
        "            if nearest < 4:\n",
        "                self.last_ball_toucher = nearest\n",
        "\n",
        "        return self.last_ball_toucher\n",
        "\n",
        "    def _calculate_reward(self, reward: float, player_id: int, info: Dict, splitted_returns=False) -> float:\n",
        "        # print('calculating reward')\n",
        "        if reward != 0.0:\n",
        "            self._update_scoreboard(player_id, reward)\n",
        "\n",
        "\n",
        "        self._update_avg_ball_speed_to_goal(\n",
        "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
        "\n",
        "        env_reward = reward\n",
        "        \n",
        "        ball_to_goal_speed_reward = np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE,\n",
        "                                            SPEED_IMPORTANCE) if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE else SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
        "        ball_to_goal_speed_reward = (\n",
        "            player_id == self.last_ball_toucher) * ball_to_goal_speed_reward\n",
        "\n",
        "        if splitted_returns:\n",
        "            return (env_reward, ball_to_goal_speed_reward)\n",
        "        return env_reward + ball_to_goal_speed_reward\n",
        "\n",
        "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
        "        assert player_id in [0, 1, 2, 3]\n",
        "\n",
        "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
        "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
        "\n",
        "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
        "\n",
        "    def _update_scoreboard(self, player_id, reward):\n",
        "        # global max_goals_one_team, max_goals_one_match\n",
        "\n",
        "        if player_id == 0 and reward == -1.0:\n",
        "            self.scoreboard[\"team_1\"] += 1\n",
        "\n",
        "        elif player_id == 2 and reward == -1.0:\n",
        "            self.scoreboard[\"team_0\"] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2x2iXcDHFc4"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2gGEpeBHFc7"
      },
      "outputs": [],
      "source": [
        "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
        "    \"\"\"\n",
        "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def create_rllib_env(env_config: dict = {}):\n",
        "    \"\"\"\n",
        "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
        "    Args:\n",
        "        env_config: configuration for the environment.\n",
        "            You may specify the following keys:\n",
        "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
        "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
        "    \"\"\"\n",
        "    if hasattr(env_config, \"worker_index\"):\n",
        "        env_config[\"worker_id\"] = (\n",
        "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
        "            + env_config.vector_index\n",
        "        )\n",
        "    env = soccer_twos.make(**env_config)\n",
        "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
        "        # is multiagent by default, is only disabled if explicitly set to False\n",
        "        return env\n",
        "    return RLLibWrapper(env)\n",
        "\n",
        "\n",
        "def create_custom_env(env_config: dict = {}):\n",
        "    env = create_rllib_env(env_config)\n",
        "    return CustomRewardWrapper(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlHplu3tHFc8"
      },
      "source": [
        "## Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMSEjetMHFc8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from gym.spaces import Box\n",
        "\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.typing import ModelWeights\n",
        "\n",
        "\n",
        "class RandomPolicy(Policy):\n",
        "    \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Whether for compute_actions, the bounds given in action_space\n",
        "        # should be ignored (default: False). This is to test action-clipping\n",
        "        # and any Env's reaction to boon_episode_stepunds breaches.\n",
        "        if self.config.get(\"ignore_action_bounds\", False) and \\\n",
        "                isinstance(self.action_space, Box):\n",
        "            self.action_space_for_sampling = Box(\n",
        "                -float(\"inf\"),\n",
        "                float(\"inf\"),\n",
        "                shape=self.action_space.shape,\n",
        "                dtype=self.action_space.dtype)\n",
        "        else:\n",
        "            self.action_space_for_sampling = self.action_space\n",
        "\n",
        "    @override(Policy)\n",
        "    def compute_actions(self,\n",
        "                        obs_batch,\n",
        "                        state_batches=None,\n",
        "                        prev_action_batch=None,\n",
        "                        prev_reward_batch=None,\n",
        "                        **kwargs):\n",
        "        # Alternatively, a numpy array would work here as well.\n",
        "        # e.g.: np.array([random.choice([0, 1])] * len(obs_batch))\n",
        "        return [self.action_space_for_sampling.sample() for _ in obs_batch], \\\n",
        "               [], {}\n",
        "\n",
        "    @override(Policy)\n",
        "    def learn_on_batch(self, samples):\n",
        "        \"\"\"No learning.\"\"\"\n",
        "        return {}\n",
        "\n",
        "    @override(Policy)\n",
        "    def compute_log_likelihoods(self,\n",
        "                                actions,\n",
        "                                obs_batch,\n",
        "                                state_batches=None,\n",
        "                                prev_action_batch=None,\n",
        "                                prev_reward_batch=None):\n",
        "        return np.array([random.random()] * len(obs_batch))\n",
        "\n",
        "    @override(Policy)\n",
        "    def get_weights(self) -> ModelWeights:\n",
        "        \"\"\"No weights to save.\"\"\"\n",
        "        return {}\n",
        "\n",
        "    @override(Policy)\n",
        "    def set_weights(self, weights: ModelWeights) -> None:\n",
        "        \"\"\"No weights to set.\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRaY-huAHFc9"
      },
      "outputs": [],
      "source": [
        "class OpponentSelector:\n",
        "    def __init__(self):\n",
        "        self.opponent_policy = 'random'\n",
        "\n",
        "    def choose_new_policy(self, n_opponents):\n",
        "        if n_opponents == 0:\n",
        "            self.opponent_policy = 'random'\n",
        "        elif n_opponents > 0:\n",
        "            self.opponent_policy = f'main_v{np.random.choice(list(range(1, n_opponents + 1)))}'\n",
        "        return self.opponent_policy\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYRWUg_GHFc9"
      },
      "outputs": [],
      "source": [
        "MAX_POLICIES = 40\n",
        "\n",
        "@ray.remote\n",
        "class PolicyHandler:\n",
        "    def __init__(self) -> None:\n",
        "        self.n_opponents = 0\n",
        "        self.policies = ['main', 'random']\n",
        "        self.opponnent_selector = OpponentSelector()\n",
        "        self.main_left_side = True\n",
        "        self.opponent_policy = 'random'\n",
        "        self.last_policy_n = 0\n",
        "\n",
        "    def add_policy(self):\n",
        "        \"\"\"Add a new Policy to handler\n",
        "\n",
        "        Returns:\n",
        "            str: policy_id of the policy updated\n",
        "        \"\"\"\n",
        "        self.n_opponents = np.min([self.n_opponents + 1, MAX_POLICIES])\n",
        "        print(f'PolicyHandler add_policy: {self.n_opponents}')\n",
        "        pol_id = f'main_v{self.n_opponents}'\n",
        "        if pol_id not in self.policies:\n",
        "            self.policies.append(pol_id)\n",
        "        print(f'Policies: {self.policies}')\n",
        "        self.last_policy_n %= MAX_POLICIES\n",
        "        self.last_policy_n += 1\n",
        "        return f'main_v{self.last_policy_n}'\n",
        "\n",
        "    def _step_agent(self, agent_id):\n",
        "        if agent_id == 0:\n",
        "            self.main_left_side = not self.main_left_side\n",
        "            self.opponent_policy = self.opponnent_selector.choose_new_policy(self.n_opponents)\n",
        "\n",
        "    def _mapping_fn(self, agent_id):\n",
        "        self._step_agent(agent_id)\n",
        "        \n",
        "        if self.main_left_side and agent_id in [0,1]:\n",
        "            selected_pol = \"main\"\n",
        "\n",
        "        elif self.main_left_side and agent_id in [2,3]:\n",
        "            selected_pol = self.opponent_policy\n",
        "\n",
        "        elif not self.main_left_side and agent_id in [0,1]:\n",
        "            selected_pol = self.opponent_policy\n",
        "        \n",
        "        elif not self.main_left_side and agent_id in [2,3]:\n",
        "            selected_pol = \"main\"\n",
        "        \n",
        "        print(f'_mapping_fn agent_id: {agent_id}, n_oppoonents: {self.n_opponents}')\n",
        "        print(f'policy_mapping_fn selected_pol: {selected_pol}')\n",
        "        return selected_pol\n",
        "\n",
        "\n",
        "ray.init(num_cpus=cores, include_dashboard=False, ignore_reinit_error=True)\n",
        "ph_actor = PolicyHandler.options(name='policy_handler').remote()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwExUrlsHFc-"
      },
      "outputs": [],
      "source": [
        "WIN_RATE_THEWSHOLD = .7\n",
        "\n",
        "class SelfPlayCallback(DefaultCallbacks):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def on_episode_step(self,\n",
        "                        *,\n",
        "                        worker: \"RolloutWorker\",\n",
        "                        base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode,\n",
        "                        env_index: Optional[int] = None,\n",
        "                        **kwargs) -> None:\n",
        "        total_timesteps = episode.last_info_for(\n",
        "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
        "        total_goals = float(episode.last_info_for(0)[\n",
        "                            \"ep_metrics\"][\"total_goals\"])\n",
        "\n",
        "        if not episode.user_data:\n",
        "            episode.user_data = {\n",
        "                0: {\n",
        "                    \"total_env_reward\": 0.0,\n",
        "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
        "                },\n",
        "                1: {\n",
        "                    \"total_env_reward\": 0.0,\n",
        "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
        "                },\n",
        "                2: {\n",
        "                    \"total_env_reward\": 0.0,\n",
        "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
        "                },\n",
        "                3: {\n",
        "                    \"total_env_reward\": 0.0,\n",
        "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
        "                }\n",
        "            }\n",
        "\n",
        "        episode.user_data = {\n",
        "            **episode.user_data,\n",
        "            0: {\n",
        "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
        "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
        "            },\n",
        "            1: {\n",
        "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
        "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
        "            },\n",
        "            2: {\n",
        "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
        "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
        "            },\n",
        "            3: {\n",
        "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
        "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
        "            }\n",
        "        }\n",
        "\n",
        "        episode.custom_metrics = {\n",
        "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
        "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
        "        }\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 0=RandomPolicy, 1=1st main policy snapshot,\n",
        "        # 2=2nd main policy snapshot, etc..\n",
        "        self.current_opponent = 0\n",
        "\n",
        "    def on_train_result(self, *, trainer, result, **kwargs):\n",
        "        # Get the win rate for the train batch.\n",
        "        # Note that normally, one should set up a proper evaluation config,\n",
        "        # such that evaluation always happens on the already updated policy,\n",
        "        # instead of on the already used train_batch.\n",
        "        main_rew = result[\"hist_stats\"].pop(\"policy_main_reward\")\n",
        "        \n",
        "        result[\"hist_stats\"].pop('episode_reward')\n",
        "        result[\"hist_stats\"].pop('episode_lengths')\n",
        "\n",
        "        opponent_rew_lists = list(result[\"hist_stats\"].values())\n",
        "        opponent_rew = []\n",
        "        for rew_list in opponent_rew_lists:\n",
        "            opponent_rew += rew_list\n",
        "\n",
        "        assert len(main_rew) == len(opponent_rew)\n",
        "        won = 0\n",
        "        for r_main, r_opponent in zip(main_rew, opponent_rew):\n",
        "            if r_main > r_opponent:\n",
        "                won += 1\n",
        "        win_rate = won / len(main_rew)\n",
        "        result[\"win_rate\"] = win_rate\n",
        "        print(f\"Iter={trainer.iteration} win-rate={win_rate} -> \", end=\"\")\n",
        "        # If win rate is good -> Snapshot current policy and play against\n",
        "        # it next, keeping the snapshot fixed and only improving the \"main\"\n",
        "        # policy.\n",
        "        if win_rate > WIN_RATE_THEWSHOLD:\n",
        "            self.current_opponent %= MAX_POLICIES\n",
        "            self.current_opponent += 1\n",
        "            new_pol_id = ray.get(ph_actor.add_policy.remote())\n",
        "            print(f\"adding new opponent to the mix ({new_pol_id}).\")\n",
        "\n",
        "            # Set the weights of the new policy to the main policy.\n",
        "            # We'll keep training the main policy, whereas `new_pol_id` will\n",
        "            # remain fixed.\n",
        "            main_state = trainer.get_policy(\"main\").get_state()\n",
        "            trainer.get_policy(new_pol_id).set_state(main_state)\n",
        "            # We need to sync the just copied local weights (from main policy)\n",
        "            # to all the remote workers as well.\n",
        "            trainer.workers.sync_weights()\n",
        "        else:\n",
        "            print(\"not good enough; will keep learning ...\")\n",
        "\n",
        "        # +2 = main + random\n",
        "        result[\"league_size\"] = self.current_opponent + 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAfJFrDEHFc_"
      },
      "source": [
        "## Stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lre1W_gHFc_"
      },
      "outputs": [],
      "source": [
        "stop = {\n",
        "    \"timesteps_total\": 15000000,  # 15M\n",
        "    # \"time_total_s\": 14400, # 4h\n",
        "    # \"episodes_total\": 10,\n",
        "    # \"training_iteration\": 1,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSLBTOIgHFc_"
      },
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ46560iHFdA",
        "outputId": "f4d382e1-d766-458d-83cc-253fae20bdef"
      },
      "outputs": [],
      "source": [
        "NUM_ENVS_PER_WORKER = 4\n",
        "ENVIRONMENT_ID = \"Soccer\"\n",
        "\n",
        "ENVIRONMENT_CONFIG = {\n",
        "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
        "    \"variation\": EnvType.multiagent_player,\n",
        "}\n",
        "\n",
        "\n",
        "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "temp_env.close()\n",
        "\n",
        "def policy_mapping_fn(agent_id):\n",
        "    actor = ray.get_actor('policy_handler')\n",
        "    return ray.get(actor._mapping_fn.remote(agent_id))\n",
        "\n",
        "def create_policy_pool(max_policies=MAX_POLICIES, obs_space=obs_space, act_space=act_space):\n",
        "    pool = {\n",
        "        \"main\": (None, obs_space, act_space, {}),\n",
        "        \"random\": (RandomPolicy, obs_space, act_space, {}),\n",
        "    }\n",
        "\n",
        "    for i in range(1, max_policies+1):\n",
        "        pool[f'main_v{i}'] = None, obs_space, act_space, {},\n",
        "\n",
        "    return pool\n",
        "\n",
        "gpu_count = 1\n",
        "num_workers = 3\n",
        "num_cpus_for_driver = 2\n",
        "\n",
        "num_gpus_for_driver = 1 / (num_workers + 1) # Driver GPU\n",
        "num_gpus_per_worker = (gpu_count - num_gpus_for_driver) / num_workers if num_workers > 0 else 0\n",
        "num_cpu_per_worker = math.floor(cores - num_cpus_for_driver) / num_workers\n",
        "\n",
        "config = {\n",
        "    # system settings\n",
        "    \"num_gpus\": num_gpus_for_driver,\n",
        "    \"num_workers\": num_workers,\n",
        "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
        "    \"num_cpus_for_driver\": num_cpus_for_driver,\n",
        "    \"num_cpus_per_worker\": num_cpu_per_worker,\n",
        "    \"num_gpus_per_worker\": num_gpus_per_worker,\n",
        "    \"log_level\": \"INFO\",\n",
        "    \"framework\": \"torch\",\n",
        "    # RL setup\n",
        "    \"multiagent\": {\n",
        "        \"policies\": create_policy_pool(),\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "        \"policies_to_train\": [\"main\"],\n",
        "    },\n",
        "    \"env\": ENVIRONMENT_ID,\n",
        "    \"env_config\": {\n",
        "        **ENVIRONMENT_CONFIG,\n",
        "    },\n",
        "    \"callbacks\": SelfPlayCallback,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dFg_VR0HFdA"
      },
      "source": [
        "## Run experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgDwsVo3HFdA"
      },
      "source": [
        "### Train PPO SelfPlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DKn5nTSGHFdB",
        "outputId": "5c0f8859-5f0e-4a1d-baf5-65a6317092a0"
      },
      "outputs": [],
      "source": [
        "def run_experiment():\n",
        "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
        "\n",
        "    analysis = tune.run(\n",
        "        \"PPO\",\n",
        "        num_samples=1,\n",
        "        name=\"PPO_multiagent_league_and_rewards_1.4\",\n",
        "        config=config,\n",
        "        stop=stop,\n",
        "        checkpoint_freq=50,\n",
        "        checkpoint_at_end=True,\n",
        "        local_dir=RAY_RESULTS_PATH,\n",
        "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
        "        # resume=True\n",
        "    )\n",
        "\n",
        "    # Gets best trial based on max accuracy across all training iterations.\n",
        "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
        "    print(best_trial)\n",
        "    # Gets best checkpoint for trial based on accuracy.\n",
        "    best_checkpoint = analysis.get_best_checkpoint(\n",
        "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
        "    )\n",
        "    print(best_checkpoint)\n",
        "    print(\"Done training\")\n",
        "    return analysis, best_trial, best_checkpoint\n",
        "\n",
        "run_experiment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvgui0HyHFdB"
      },
      "source": [
        "## Export agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z939As16HFdB"
      },
      "outputs": [],
      "source": [
        "# this_path = os.path.dirname(os.path.realpath(\"__file__\"))0\n",
        "\n",
        "def export_agent(agent_file: str, TRIAL, agent_name=\"bajai_belzonte\", makeZip=False):\n",
        "    agent_path = os.path.join(AGENTS_PATH, agent_name)\n",
        "    os.makedirs(agent_path, exist_ok=True)\n",
        "\n",
        "\n",
        "    shutil.rmtree(agent_path)\n",
        "    os.makedirs(agent_path)\n",
        "\n",
        "    # salva a classe do agente\n",
        "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
        "        f.write(agent_file)\n",
        "\n",
        "    # salva um __init__ para criar o módulo Python\n",
        "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
        "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
        "\n",
        "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
        "    print(f\"TRIALLL {TRIAL}\")\n",
        "    shutil.copytree(TRIAL, os.path.join(\n",
        "        agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
        "\n",
        "    # empacota tudo num arquivo .zip\n",
        "    if makeZip:\n",
        "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
        "                            \"zip\", os.path.join(agent_path, agent_name))\n",
        "\n",
        "\n",
        "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"main\"):\n",
        "    return f\"\"\"\n",
        "import pickle\n",
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env.base_env import BaseEnv\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "\n",
        "from soccer_twos import AgentInterface\n",
        "\n",
        "ALGORITHM = \"{ALGORITHM}\"\n",
        "CHECKPOINT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.abspath(__file__)), \n",
        "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
        ")\n",
        "POLICY_NAME = \"{POLICY_NAME}\"\n",
        "\n",
        "\n",
        "class MyRaySoccerAgent(AgentInterface):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__()\n",
        "        ray.init(ignore_reinit_error=True)\n",
        "\n",
        "        # Load configuration from checkpoint file.\n",
        "        config_path = \"\"\n",
        "        if CHECKPOINT_PATH:\n",
        "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
        "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
        "            # Try parent directory.\n",
        "            if not os.path.exists(config_path):\n",
        "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
        "\n",
        "        # Load the config from pickled.\n",
        "        if os.path.exists(config_path):\n",
        "            with open(config_path, \"rb\") as f:\n",
        "                config = pickle.load(f)\n",
        "        else:\n",
        "            # If no config in given checkpoint -> Error.\n",
        "            raise ValueError(\n",
        "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
        "                \"its parent directory!\"\n",
        "            )\n",
        "\n",
        "        # no need for parallelism on evaluation\n",
        "        config[\"num_workers\"] = 0\n",
        "        config[\"num_gpus\"] = 0\n",
        "\n",
        "        # create a dummy env since it's required but we only care about the policy\n",
        "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
        "        config[\"env\"] = \"DummyEnv\"\n",
        "\n",
        "        # create the Trainer from config\n",
        "        cls = get_trainable_cls(ALGORITHM)\n",
        "        agent = cls(env=config[\"env\"], config=config)\n",
        "        # load state from checkpoint\n",
        "        agent.restore(CHECKPOINT_PATH)\n",
        "        # get policy for evaluation\n",
        "        self.policy = agent.get_policy(POLICY_NAME)\n",
        "\n",
        "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
        "        actions = {{}}\n",
        "        for player_id in observation:\n",
        "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
        "            # as we only need the action, we discard the other elements\n",
        "            actions[player_id], *_ = self.policy.compute_single_action(\n",
        "                observation[player_id]\n",
        "            )\n",
        "        return actions\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def getAnalysis(experiment: str):\n",
        "    return Analysis(experiment)\n",
        "\n",
        "\n",
        "def export():\n",
        "    analysis = getAnalysis(\n",
        "        \"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_league_and_rewards_1.4\")\n",
        "\n",
        "    ALGORITHM = \"PPO\"\n",
        "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
        "    CHECKPOINT = analysis.get_best_checkpoint(\n",
        "        TRIAL,\n",
        "        \"training_iteration\",\n",
        "        \"max\",\n",
        "    )\n",
        "\n",
        "    print(TRIAL, CHECKPOINT)\n",
        "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
        "    export_agent(agent_file, TRIAL)\n",
        "\n",
        "\n",
        "export()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "experiment.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
