{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiente da competição\n",
    "# !pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "# !pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "# !pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "# !pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install torch > /dev/null 2>&1\n",
    "# !pip install lz4 > /dev/null 2>&1\n",
    "# !pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# # Dependências necessárias para gravar os vídeos\n",
    "# !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "# !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "# !pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ray\n",
      "Version: 1.4.0\n",
      "Summary: Ray provides a simple, universal API for building distributed applications.\n",
      "Home-page: https://github.com/ray-project/ray\n",
      "Author: Ray Team\n",
      "Author-email: ray-dev@googlegroups.com\n",
      "License: Apache 2.0\n",
      "Location: /home/bruno/anaconda3/envs/soccer-twos/lib/python3.8/site-packages\n",
      "Requires: pyyaml, py-spy, gpustat, numpy, opencensus, filelock, pydantic, jsonschema, redis, aiohttp-cors, aioredis, colorama, click, grpcio, msgpack, prometheus-client, requests, aiohttp, protobuf\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import ExperimentAnalysis\n",
    "# from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "# from ray.rllib.utils.typing import PolicyID\n",
    "# from ray.tune.registry import get_trainable_cls\n",
    "# from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "# import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "MAX_STEPS = 1000\n",
    "MATCH_STEPS = 5000\n",
    "\n",
    "\n",
    "def get_scalar_projection(x, y):\n",
    "    assert np.linalg.norm(y) > 0.000001\n",
    "    return np.dot(x, y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
    "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
    "min_ball_position_x, max_ball_position_x = - \\\n",
    "    15.563264846801758, 15.682827949523926\n",
    "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
    "min_player_position_x, max_player_position_x = - \\\n",
    "    17.26804542541504, 17.16301727294922\n",
    "min_player_position_y, max_player_position_y = - \\\n",
    "    7.399587631225586, 7.406457424163818\n",
    "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
    "    -23.366606239568615, 23.749571761530724\n",
    "\n",
    "max_ball_abs_velocity = 78.25721740722656\n",
    "max_goals_one_team = -9999999\n",
    "max_goals_one_match = -9999999\n",
    "max_steps = -999999\n",
    "\n",
    "max_diff_reward = -np.inf\n",
    "\n",
    "# Infered\n",
    "max_ball_abs_avg_velocity = max(\n",
    "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
    "\n",
    "\n",
    "SPEED_IMPORTANCE = 1.0 / (14.0)\n",
    "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
    "\n",
    "AFTER_BALL_STEP_PENALTY = 1 / MAX_STEPS  # 0.001\n",
    "\n",
    "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
    "# min_ball_to_goal_avg_velocity e\n",
    "# max_ball_to_goal_avg_velocity:\n",
    "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
    "\n",
    "\n",
    "def is_after_the_ball(player_id: int, player_pos: np.array, ball_pos: np.array):\n",
    "    if player_id in range(2):\n",
    "        return player_pos[0] > ball_pos[0]\n",
    "    elif player_id in [2, 3]:\n",
    "        return player_pos[0] < ball_pos[0]\n",
    "\n",
    "\n",
    "def get_center_of_goal_pos(player_id):\n",
    "    global min_ball_position_x, max_ball_position_x, \\\n",
    "        min_ball_position_y, max_ball_position_y, \\\n",
    "        min_player_position_x, max_player_position_x, \\\n",
    "        min_player_position_y, max_player_position_y\n",
    "    if player_id in [0, 1]:\n",
    "        return np.array([max_ball_position_x, 0.0])\n",
    "    elif player_id in [2, 3]:\n",
    "        return np.array([min_ball_position_x, 0.0])\n",
    "\n",
    "\n",
    "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict, x_axis_only=True):\n",
    "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
    "    if x_axis_only and player_id in [0, 1]:\n",
    "        return ball_velocity[0]\n",
    "    elif x_axis_only and player_id in [2, 3]:\n",
    "        return -ball_velocity[0]\n",
    "\n",
    "    goal_pos = get_center_of_goal_pos(player_id)\n",
    "    ball_pos = info[\"ball_info\"][\"position\"]\n",
    "\n",
    "    # print(f\"ball_pos: {ball_pos}\")\n",
    "    direction_to_center_of_goal = goal_pos - ball_pos\n",
    "    # print(f\"direction_to_center_of_goal: {direction_to_center_of_goal}\")\n",
    "\n",
    "    # global max_ball_abs_velocity\n",
    "    # if np.linalg.norm(ball_velocity) > max_ball_abs_velocity:\n",
    "    #     max_ball_abs_velocity = np.linalg.norm(ball_velocity)\n",
    "\n",
    "    # print(f\"ball_velocity: {ball_velocity}\")\n",
    "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
    "        ball_velocity, direction_to_center_of_goal)\n",
    "    # print(f\"ball_velocity_to_center_of_goal: {ball_velocity_to_center_of_goal}\")\n",
    "    return ball_velocity_to_center_of_goal\n",
    "\n",
    "# print('ball_velocity_to_center_of_goal', calculate_ball_to_goal_scalar_velocity(0, { \"ball_info\": { \"position\": np.array([3.0, 2.0]), \"velocity\": np.array([0.0, 0.0]) }}))\n",
    "\n",
    "\n",
    "def calculate_distance(pt1: np.ndarray, pt2: np.ndarray):\n",
    "    assert pt1.shape == (2,) and pt2.shape == (2,)\n",
    "    return np.linalg.norm(pt1 - pt2)\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    # def __init__(self, env):\n",
    "    #     gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "\n",
    "        # print(info)\n",
    "        # if rewards[0] > 0.0:\n",
    "        #     assert False\n",
    "\n",
    "        ball_pos = info[0][\"ball_info\"][\"position\"]\n",
    "        ball_velocity = info[0][\"ball_info\"][\"velocity\"]\n",
    "        player0_pos = info[0][\"player_info\"][\"position\"]\n",
    "        player1_pos = info[1][\"player_info\"][\"position\"]\n",
    "        player2_pos = info[2][\"player_info\"][\"position\"]\n",
    "        player3_pos = info[3][\"player_info\"][\"position\"]\n",
    "\n",
    "        # print('ball_velocity', ball_velocity)\n",
    "        if self._was_ball_effective_touched(self.prev_ball_velocity, ball_velocity):\n",
    "            ball_toucher = self._get_ball_toucher(\n",
    "                ball_velocity, ball_pos, player0_pos, player1_pos, player2_pos, player3_pos)\n",
    "            # self.ball_touchers.append(ball_toucher)\n",
    "\n",
    "        if type(action) is dict:\n",
    "            new_rewards = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k]) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        if type(action) is dict:\n",
    "            splitted_rets = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k], splitted_returns=True) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        info = {\n",
    "            i: {\n",
    "                **info[i],\n",
    "                \"ep_metrics\": {\n",
    "                    # \"total_timesteps\": np.array([0.0008], dtype=np.float32)\n",
    "                    \"total_timesteps\": self.n_step + 1,\n",
    "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
    "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
    "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
    "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
    "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
    "                    \"episode_ended\": done[\"__all__\"],\n",
    "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
    "                    \"env_reward\": splitted_rets[i][0],\n",
    "                    \"ball_to_goal_speed_reward\": splitted_rets[i][1],\n",
    "                    # \"agent_position_to_ball_reward\": splitted_rets[i][2],\n",
    "                }\n",
    "            } for i in info.keys()\n",
    "        }\n",
    "\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y, \\\n",
    "        #     max_goals_one_team, max_goals_one_match\n",
    "        # if done:\n",
    "        #     print(f'min_ball_position_x: {min_ball_position_x}')\n",
    "        #     print(f'max_ball_position_x: {max_ball_position_x}')\n",
    "        #     print(f'min_ball_position_y: {min_ball_position_y}')\n",
    "        #     print(f'max_ball_position_y: {max_ball_position_y}')\n",
    "        #     print(f'min_player_position_x: {min_player_position_x}')\n",
    "        #     print(f'max_player_position_x: {max_player_position_x}')\n",
    "        #     print(f'min_player_position_y: {min_player_position_y}')\n",
    "        #     print(f'max_player_position_y: {max_player_position_y}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_goals_one_team: {max_goals_one_team}')\n",
    "        #     print(f'max_goals_one_match: {max_goals_one_match}')\n",
    "        #     print(self.scoreboard)\n",
    "        #     print(f'Done... last n_step: {self.n_step}')\n",
    "        #     if self.scoreboard[\"team_0\"] > 0 or self.scoreboard[\"team_1\"] > 0:\n",
    "        #         input(\"Press Enter to continue...\")\n",
    "\n",
    "        # global max_steps\n",
    "        # if done:\n",
    "        #     if self.n_step + 1 > max_steps:\n",
    "        #         max_steps = self.n_step + 1\n",
    "        #     print('max_steps', max_steps)\n",
    "\n",
    "        # global max_diff_reward\n",
    "        # if done:\n",
    "        #     print(f'max_diff_reward: {max_diff_reward}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'max_ball_abs_velocity: {max_ball_abs_velocity}')\n",
    "        # if done:\n",
    "        #     print('self.ball_touched', self.ball_touched)\n",
    "        #     print('self.ball_touchers', self.ball_touchers)\n",
    "\n",
    "        self.n_step += 1\n",
    "        self.prev_ball_velocity = ball_velocity.copy()\n",
    "\n",
    "        return obs, new_rewards, done, info\n",
    "            \n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.n_step = 0\n",
    "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
    "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
    "        self.await_press = False\n",
    "        self.prev_ball_velocity = np.array([0.0, 0.0])\n",
    "        self.last_ball_toucher = -1\n",
    "        # self.ball_touched = []\n",
    "        # self.ball_touchers = []\n",
    "        # print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        # print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        return obs\n",
    "\n",
    "    def _was_ball_effective_touched(self, prev_ball_velocity: np.ndarray, curr_ball_velocity: np.ndarray):\n",
    "        \"\"\"Get if ball was touched (either by player or wall)\n",
    "\n",
    "        Args:\n",
    "            prev_ball_velocity (np.ndarray): Previous ball coordinates\n",
    "            curr_ball_velocity (np.ndarray): Current ball coordinates\n",
    "        \"\"\"\n",
    "        assert prev_ball_velocity.shape == (\n",
    "            2,) and curr_ball_velocity.shape == (2,)\n",
    "        percentual_scalar_thresold = 0.2  # 20%\n",
    "        diff = curr_ball_velocity - prev_ball_velocity\n",
    "\n",
    "\n",
    "        if np.linalg.norm(curr_ball_velocity) < 1.0:\n",
    "            self.last_ball_toucher = -1\n",
    "\n",
    "        if np.linalg.norm(prev_ball_velocity) > 0.0000001:\n",
    "            return np.linalg.norm(diff) / np.linalg.norm(prev_ball_velocity) > percentual_scalar_thresold\n",
    "        return np.linalg.norm(curr_ball_velocity) > np.linalg.norm(prev_ball_velocity)\n",
    "\n",
    "    def _get_ball_toucher(self,\n",
    "                          ball_velocity: np.ndarray,\n",
    "                          ball_position: np.ndarray,\n",
    "                          player_0_pos: np.ndarray,\n",
    "                          player_1_pos: np.ndarray,\n",
    "                          player_2_pos: np.ndarray,\n",
    "                          player_3_pos: np.ndarray):\n",
    "        assert ball_position.shape == (2,) and \\\n",
    "            player_0_pos.shape == (2,) and \\\n",
    "            player_1_pos.shape == (2,) and \\\n",
    "            player_2_pos.shape == (2,) and \\\n",
    "            player_3_pos.shape == (2,)\n",
    "        top_wall_y = max_ball_position_y\n",
    "        bottom_wall_y = min_ball_position_y\n",
    "        left_wall_x = min_ball_position_x\n",
    "        right_wall_x = max_ball_position_x\n",
    "\n",
    "        if np.linalg.norm(ball_velocity) > 0.000001:\n",
    "            distances = np.array([\n",
    "                calculate_distance(ball_position, player_0_pos),\n",
    "                calculate_distance(ball_position, player_1_pos),\n",
    "                calculate_distance(ball_position, player_2_pos),\n",
    "                calculate_distance(ball_position, player_3_pos),\n",
    "                np.abs(ball_position[1] - top_wall_y),\n",
    "                np.abs(ball_position[1] - bottom_wall_y),\n",
    "                np.abs(ball_position[0] - left_wall_x),\n",
    "                np.abs(ball_position[0] - right_wall_x)\n",
    "            ])\n",
    "\n",
    "            # print(distances)\n",
    "            nearest = np.argmin(distances)\n",
    "            # print(nearest)\n",
    "            if nearest < 4:\n",
    "                self.last_ball_toucher = nearest\n",
    "\n",
    "        return self.last_ball_toucher\n",
    "\n",
    "    def _calculate_reward(self, reward: float, player_id: int, info: Dict, splitted_returns=False) -> float:\n",
    "        # print('calculating reward')\n",
    "        if reward != 0.0:\n",
    "            # print('Goal was made!', reward, info)\n",
    "            self._update_scoreboard(player_id, reward)\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y\n",
    "        # print(f\"info: {info}\")\n",
    "        # if info[\"ball_info\"][\"position\"][0] < min_ball_position_x:\n",
    "        #     min_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][0] > max_ball_position_x:\n",
    "        #     max_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][1] < min_ball_position_y:\n",
    "        #     min_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"ball_info\"][\"position\"][1] > max_ball_position_y:\n",
    "        #     max_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][0] < min_player_position_x:\n",
    "        #     min_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][0] > max_player_position_x:\n",
    "        #     max_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][1] < min_player_position_y:\n",
    "        #     min_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][1] > max_player_position_y:\n",
    "        #     max_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "\n",
    "        self._update_avg_ball_speed_to_goal(\n",
    "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
    "        # global max_diff_reward\n",
    "        # if (np.abs(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity) > max_diff_reward):\n",
    "        #     max_diff_reward = SPEED_IMPORTANCE * \\\n",
    "        #         self.last_ball_speed_mean_per_player[player_id] / \\\n",
    "        #         max_ball_abs_avg_velocity\n",
    "\n",
    "        # ball_pos = info[\"ball_info\"][\"position\"]\n",
    "        # player_pos = info[\"player_info\"][\"position\"]\n",
    "\n",
    "        env_reward = reward\n",
    "        \n",
    "        ball_to_goal_speed_reward = np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE,\n",
    "                                            SPEED_IMPORTANCE) if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE else SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
    "        ball_to_goal_speed_reward = (\n",
    "            player_id == self.last_ball_toucher) * ball_to_goal_speed_reward\n",
    "        # agent_position_to_ball_reward = is_after_the_ball(player_id, player_pos,\n",
    "        #                                                   ball_pos) * (-AFTER_BALL_STEP_PENALTY)\n",
    "\n",
    "        # if splitted_returns:\n",
    "        #     return (env_reward, ball_to_goal_speed_reward, agent_position_to_ball_reward)\n",
    "        # return env_reward + ball_to_goal_speed_reward + agent_position_to_ball_reward\n",
    "        if splitted_returns:\n",
    "            return (env_reward, ball_to_goal_speed_reward)\n",
    "        return env_reward + ball_to_goal_speed_reward\n",
    "\n",
    "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
    "        assert player_id in [0, 1, 2, 3]\n",
    "        # global min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity\n",
    "\n",
    "        # Getting min/max ball to goal speed forr normalization\n",
    "        # print(f'player_id: {player_id}')\n",
    "        # print(f'self.last_ball_speed_mean_per_player: {self.last_ball_speed_mean_per_player}')\n",
    "        # print(f'self.n_step: {self.n_step}')\n",
    "        # print(f'ball_speed: {ball_speed}')\n",
    "\n",
    "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
    "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
    "        # if avg < min_ball_to_goal_avg_velocity:\n",
    "        #     min_ball_to_goal_avg_velocity = avg\n",
    "        # elif avg > max_ball_to_goal_avg_velocity:\n",
    "        #     max_ball_to_goal_avg_velocity = avg\n",
    "\n",
    "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
    "\n",
    "    def _update_scoreboard(self, player_id, reward):\n",
    "        # global max_goals_one_team, max_goals_one_match\n",
    "\n",
    "        if player_id == 0 and reward == -1.0:\n",
    "            self.scoreboard[\"team_1\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_1\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_1\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n",
    "        elif player_id == 2 and reward == -1.0:\n",
    "            self.scoreboard[\"team_0\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_0\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_0\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)\n",
    "\n",
    "\n",
    "def create_custom_env(env_config: dict = {}):\n",
    "    env = create_rllib_env(env_config)\n",
    "    return CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.typing import ModelWeights\n",
    "\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Whether for compute_actions, the bounds given in action_space\n",
    "        # should be ignored (default: False). This is to test action-clipping\n",
    "        # and any Env's reaction to boon_episode_stepunds breaches.\n",
    "        if self.config.get(\"ignore_action_bounds\", False) and \\\n",
    "                isinstance(self.action_space, Box):\n",
    "            self.action_space_for_sampling = Box(\n",
    "                -float(\"inf\"),\n",
    "                float(\"inf\"),\n",
    "                shape=self.action_space.shape,\n",
    "                dtype=self.action_space.dtype)\n",
    "        else:\n",
    "            self.action_space_for_sampling = self.action_space\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        **kwargs):\n",
    "        # Alternatively, a numpy array would work here as well.\n",
    "        # e.g.: np.array([random.choice([0, 1])] * len(obs_batch))\n",
    "        return [self.action_space_for_sampling.sample() for _ in obs_batch], \\\n",
    "               [], {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        \"\"\"No learning.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_log_likelihoods(self,\n",
    "                                actions,\n",
    "                                obs_batch,\n",
    "                                state_batches=None,\n",
    "                                prev_action_batch=None,\n",
    "                                prev_reward_batch=None):\n",
    "        return np.array([random.random()] * len(obs_batch))\n",
    "\n",
    "    @override(Policy)\n",
    "    def get_weights(self) -> ModelWeights:\n",
    "        \"\"\"No weights to save.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def set_weights(self, weights: ModelWeights) -> None:\n",
    "        \"\"\"No weights to set.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alternator:\n",
    "    def __init__(self) -> None:\n",
    "        self.value = -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.value = -1\n",
    "\n",
    "    def step_value(self):\n",
    "        self.value += 1\n",
    "        self.value %= 2\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_RATE_THEWSHOLD = .2\n",
    "\n",
    "class SelfPlayCallback(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker: \"RolloutWorker\",\n",
    "                        base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index: Optional[int] = None,\n",
    "                        **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            # \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n",
    "\n",
    "    # def on_episode_end(self,\n",
    "    #                    *,\n",
    "    #                    worker: \"RolloutWorker\",\n",
    "    #                    base_env: BaseEnv,\n",
    "    #                    policies: Dict[PolicyID, Policy],\n",
    "    #                    episode: MultiAgentEpisode,\n",
    "    #                    env_index: Optional[int] = None,\n",
    "    #                    **kwargs) -> None:\n",
    "    #     total_timesteps = episode.last_info_for(\n",
    "    #         0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "    #     total_goals = float(episode.last_info_for(0)[\n",
    "    #                         \"ep_metrics\"][\"total_goals\"])\n",
    "    #     estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "    #         float(total_timesteps) if total_goals > 0 else 0.0\n",
    "    #     timesteps_to_goal = float(\n",
    "    #         total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "    #     episode.user_data = {\n",
    "    #         **episode.user_data,\n",
    "    #         0: {\n",
    "    #             \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         1: {\n",
    "    #             \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         2: {\n",
    "    #             \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         3: {\n",
    "    #             \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         }\n",
    "    #     }\n",
    "\n",
    "    #     episode.custom_metrics = {\n",
    "    #         # \"total_timesteps\": total_timesteps,\n",
    "    #         # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "    #         # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "    #         # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "    #         # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "    #         # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "    #         \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "    #         \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "    #         # \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "    #     }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 0=RandomPolicy, 1=1st main policy snapshot,\n",
    "        # 2=2nd main policy snapshot, etc..\n",
    "        self.current_opponent = 0\n",
    "\n",
    "    def on_train_result(self, *, trainer, result, **kwargs):\n",
    "        # Get the win rate for the train batch.\n",
    "        # Note that normally, one should set up a proper evaluation config,\n",
    "        # such that evaluation always happens on the already updated policy,\n",
    "        # instead of on the already used train_batch.\n",
    "        # print(\"result\", result)\n",
    "        # print(\"result[hist_stats]\", result[\"hist_stats\"])\n",
    "        main_rew = result[\"hist_stats\"].pop(\"policy_main_reward\")\n",
    "        opponent_rew = result[\"hist_stats\"].pop(\"policy_random_reward\")\n",
    "        # opponent_rew = list(result[\"hist_stats\"].values())[0]\n",
    "        # print('len(main_rew)', len(main_rew))\n",
    "        # print(\"len(opponent_rew)\", len(opponent_rew))\n",
    "        assert len(main_rew) == len(opponent_rew)\n",
    "        won = 0\n",
    "        for r_main, r_opponent in zip(main_rew, opponent_rew):\n",
    "            if r_main > r_opponent:\n",
    "                won += 1\n",
    "        win_rate = won / len(main_rew)\n",
    "        result[\"win_rate\"] = win_rate\n",
    "        print(f\"Iter={trainer.iteration} win-rate={win_rate} -> \", end=\"\")\n",
    "        # If win rate is good -> Snapshot current policy and play against\n",
    "        # it next, keeping the snapshot fixed and only improving the \"main\"\n",
    "        # policy.\n",
    "        if win_rate > WIN_RATE_THEWSHOLD:\n",
    "            self.current_opponent += 1\n",
    "            new_pol_id = f\"main_v{self.current_opponent}\"\n",
    "            print(f\"adding new opponent to the mix ({new_pol_id}).\")\n",
    "\n",
    "            # Re-define the mapping function, such that \"main\" is forced\n",
    "            # to play against any of the previously played policies\n",
    "            # (excluding \"random\").\n",
    "            alternator = Alternator()\n",
    "            def new_policy_mapping_fn(agent_id, **kwargs):\n",
    "                # agent_id = [0|1] -> policy depends on episode ID\n",
    "                # This way, we make sure that both policies sometimes play\n",
    "                # (start player) and sometimes agent1 (player to move 2nd).\n",
    "                selected_pol = \"main\" if alternator.step_value() == agent_id \\\n",
    "                    else \"main_v{}\".format(np.random.choice(\n",
    "                        list(range(1, self.current_opponent + 1))))\n",
    "                print(f'policy_mapping_fn selected_pol: {selected_pol}\\nself.current_opponent: {self.current_opponent}')\n",
    "                return selected_pol\n",
    "\n",
    "            # new_policy = trainer.add_policy(\n",
    "            #     policy_id=new_pol_id,\n",
    "            #     policy_cls=type(trainer.get_policy(\"main\")),\n",
    "            #     policy_mapping_fn=policy_mapping_fn,\n",
    "            # )\n",
    "           \n",
    "            trainer.workers.local_worker().policy_config[\"multiagent\"][\"policy_mapping_fn\"] = new_policy_mapping_fn\n",
    "            trainer.workers.local_worker().policy_mapping_fn = new_policy_mapping_fn\n",
    "            \n",
    "            trainer.workers.local_worker().policy_map[new_pol_id] = trainer.get_policy(\"main\")\n",
    "\n",
    "            # for r in trainer.workers.remote_workers():\n",
    "            #     # r.policy_config[\"multiagent\"][\"policy_mapping_fn\"] = policy_mapping_fn\n",
    "            #     # r.policy_mapping_fn = policy_mapping_fn\n",
    "            #     r.policy_map[new_pol_id] = trainer.get_policy(\"main\")\n",
    "            #     # r.policy_map[new_pol_id].set_state(main_state)\n",
    "\n",
    "            # Set the weights of the new policy to the main policy.\n",
    "            # We'll keep training the main policy, whereas `new_pol_id` will\n",
    "            # remain fixed.\n",
    "            main_state = trainer.get_policy(\"main\").get_state()\n",
    "            # new_policy.set_state(main_state)\n",
    "            trainer.workers.local_worker().policy_map[new_pol_id].set_state(main_state)\n",
    "            # We need to sync the just copied local weights (from main policy)\n",
    "            # to all the remote workers as well.\n",
    "            # trainer.workers.sync_weights()\n",
    "        else:\n",
    "            print(\"not good enough; will keep learning ...\")\n",
    "\n",
    "        # +2 = main + random\n",
    "        result[\"league_size\"] = self.current_opponent + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    # \"episodes_total\": 10,\n",
    "    \"training_iteration\": 100,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    }
   ],
   "source": [
    "NUM_ENVS_PER_WORKER = 8\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "}\n",
    "\n",
    "\n",
    "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "alternator = Alternator()\n",
    "def policy_mapping_fn(agent_id, **kwargs):\n",
    "    print('chamando policy_mapping_fn original')\n",
    "    # agent_id = [0|1] -> policy depends on episode ID\n",
    "    # This way, we make sure that both policies sometimes play agent0\n",
    "    # (start player) and sometimes agent1 (player to move 2nd).\n",
    "    return \"main\" if alternator.step_value() == agent_id else \"random\"\n",
    "\n",
    "gpu_count = 1\n",
    "num_workers = 0\n",
    "num_gpus_for_driver = 1 / (num_workers + 1) # Driver GPU\n",
    "num_gpus_per_worker = (gpu_count - num_gpus_for_driver) / num_workers if num_workers > 0 else 0\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": num_gpus_for_driver,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 8,\n",
    "    \"num_cpus_per_worker\": 0,\n",
    "    \"num_gpus_per_worker\": num_gpus_per_worker,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"main\": (None, obs_space, act_space, {}),\n",
    "            \"random\": (RandomPolicy, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        \"policies_to_train\": [\"main\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": {\n",
    "        **ENVIRONMENT_CONFIG,\n",
    "        # \"render\": True,\n",
    "        # \"time_scale\": 1,\n",
    "    },\n",
    "    \"callbacks\": SelfPlayCallback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PPO SelfPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/7.07 GiB heap, 0.0/3.54 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_ed9e4_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:50,368\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=8 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 500.\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:51,240\tINFO torch_policy.py:148 -- TorchPolicy (worker=local) running on 1.0 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:53,761\tINFO rollout_worker.py:1199 -- Built policy map: {'main': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7fc4a2090dc0>, 'random': <__main__.RandomPolicy object at 0x7fc4a212e820>}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:53,761\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'main': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc4a2090b50>, 'random': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc4a20a2520>}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:53,761\tINFO rollout_worker.py:583 -- Built filter map: {'main': <ray.rllib.utils.filter.NoFilter object at 0x7fc4a21485e0>, 'random': <ray.rllib.utils.filter.NoFilter object at 0x7fc4a2090b20>}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,931\tINFO rollout_worker.py:723 -- Generating sample batch of size 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,967\tINFO sampler.py:590 -- Raw obs from env: { 0: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   1: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   2: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   3: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   4: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   5: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   6: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   7: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)}}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,967\tINFO sampler.py:592 -- Info return from env: { 0: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   1: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   2: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   3: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   4: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   5: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   6: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   7: {0: {}, 1: {}, 2: {}, 3: {}}}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,968\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,968\tINFO sampler.py:817 -- Filtered obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,977\tINFO sampler.py:1004 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m { 'main': [ { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'type': 'PolicyEvalData'}],\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'random': [ { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                           'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:36:58,989\tINFO sampler.py:1022 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m { 'main': ( np.ndarray((16, 3), dtype=int64, min=0.0, max=2.0, mean=0.896),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             [],\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m             { 'action_dist_inputs': np.ndarray((16, 9), dtype=float32, min=-0.008, max=0.005, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'action_logp': np.ndarray((16,), dtype=float32, min=-3.303, max=-3.287, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'action_prob': np.ndarray((16,), dtype=float32, min=0.037, max=0.037, mean=0.037),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               'vf_preds': np.ndarray((16,), dtype=float32, min=-0.001, max=0.003, mean=0.001)}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'random': ( [ np.ndarray((3,), dtype=int64, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=0.667),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=0.667),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=1.0, mean=0.667),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=1.0, max=2.0, mean=1.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=1.0, max=2.0, mean=1.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                 np.ndarray((3,), dtype=int64, min=1.0, max=2.0, mean=1.333)],\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               [],\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m               {})}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:37:13,152\tINFO simple_list_collector.py:659 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m { 0: { 'action_dist_inputs': np.ndarray((255, 9), dtype=float32, min=-0.009, max=0.008, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'action_logp': np.ndarray((255,), dtype=float32, min=-3.31, max=-3.285, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=0.987),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'advantages': np.ndarray((255,), dtype=float32, min=0.177, max=0.77, mean=0.351),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'agent_index': np.ndarray((255,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'dones': np.ndarray((255,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'eps_id': np.ndarray((255,), dtype=int64, min=488914350.0, max=488914350.0, mean=488914350.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'infos': np.ndarray((255,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.027, max=1.062, mean=-3.983), 'rotation_y': 88.31835, 'velocity': np.ndarray((2,), dtype=float32, min=-2.214, max=0.065, mean=-1.075)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'new_obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'rewards': np.ndarray((255,), dtype=float32, min=0.0, max=0.711, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'unroll_id': np.ndarray((255,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'value_targets': np.ndarray((255,), dtype=float32, min=0.176, max=0.769, mean=0.35),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'vf_preds': np.ndarray((255,), dtype=float32, min=-0.007, max=0.003, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   1: { 'action_dist_inputs': np.ndarray((255, 9), dtype=float32, min=-0.009, max=0.009, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'action_logp': np.ndarray((255,), dtype=float32, min=-3.307, max=-3.284, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=1.014),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'advantages': np.ndarray((255,), dtype=float32, min=0.054, max=0.751, mean=0.271),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'agent_index': np.ndarray((255,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'dones': np.ndarray((255,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'eps_id': np.ndarray((255,), dtype=int64, min=488914350.0, max=488914350.0, mean=488914350.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'infos': np.ndarray((255,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-6.24, max=-1.338, mean=-3.789), 'rotation_y': 97.002495, 'velocity': np.ndarray((2,), dtype=float32, min=-2.212, max=-0.058, mean=-1.135)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'new_obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.185),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.185),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'rewards': np.ndarray((255,), dtype=float32, min=0.0, max=0.745, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'unroll_id': np.ndarray((255,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'value_targets': np.ndarray((255,), dtype=float32, min=0.058, max=0.745, mean=0.27),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'vf_preds': np.ndarray((255,), dtype=float32, min=-0.007, max=0.005, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   2: { 'actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=0.973),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'agent_index': np.ndarray((255,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'dones': np.ndarray((255,), dtype=float32, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'eps_id': np.ndarray((255,), dtype=int64, min=488914350.0, max=488914350.0, mean=488914350.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'infos': np.ndarray((255,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=1.153, max=6.955, mean=4.054), 'rotation_y': 275.33014, 'velocity': np.ndarray((2,), dtype=float32, min=-0.748, max=8.017, mean=3.634)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'new_obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.176),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.176),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'prev_actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=0.969),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'prev_rewards': np.ndarray((255,), dtype=float32, min=-0.034, max=0.041, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'rewards': np.ndarray((255,), dtype=float32, min=-1.0, max=0.041, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        't': np.ndarray((255,), dtype=int64, min=0.0, max=254.0, mean=127.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'unroll_id': np.ndarray((255,), dtype=int64, min=2.0, max=2.0, mean=2.0)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   3: { 'actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'agent_index': np.ndarray((255,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'dones': np.ndarray((255,), dtype=float32, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'eps_id': np.ndarray((255,), dtype=int64, min=488914350.0, max=488914350.0, mean=488914350.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'infos': np.ndarray((255,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-1.338, max=6.657, mean=2.66), 'rotation_y': 280.03668, 'velocity': np.ndarray((2,), dtype=float32, min=-2.205, max=-0.175, mean=-1.19)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'new_obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'obs': np.ndarray((255, 336), dtype=float32, min=0.0, max=1.0, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'prev_actions': np.ndarray((255, 3), dtype=int64, min=0.0, max=2.0, mean=0.996),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'prev_rewards': np.ndarray((255,), dtype=float32, min=-0.011, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'rewards': np.ndarray((255,), dtype=float32, min=-1.0, max=0.004, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        't': np.ndarray((255,), dtype=int64, min=0.0, max=254.0, mean=127.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m        'unroll_id': np.ndarray((255,), dtype=int64, min=3.0, max=3.0, mean=3.0)}}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:37:26,634\tINFO rollout_worker.py:761 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m { 'count': 4000,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'policy_batches': { 'main': { 'action_dist_inputs': np.ndarray((8000, 9), dtype=float32, min=-0.011, max=0.012, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'action_logp': np.ndarray((8000,), dtype=float32, min=-3.31, max=-3.28, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'actions': np.ndarray((8000, 3), dtype=int64, min=0.0, max=2.0, mean=1.004),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'advantages': np.ndarray((8000,), dtype=float32, min=-1.0, max=0.77, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'agent_index': np.ndarray((8000,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'dones': np.ndarray((8000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'eps_id': np.ndarray((8000,), dtype=int64, min=74494647.0, max=1676983340.0, mean=718524092.558),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'infos': np.ndarray((8000,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-8.529, max=1.19, mean=-3.669), 'rotation_y': 98.31837, 'velocity': np.ndarray((2,), dtype=float32, min=-0.37, max=8.033, mean=3.832)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'new_obs': np.ndarray((8000, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'obs': np.ndarray((8000, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'rewards': np.ndarray((8000,), dtype=float32, min=-1.0, max=0.745, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'unroll_id': np.ndarray((8000,), dtype=int64, min=0.0, max=37.0, mean=19.981),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'value_targets': np.ndarray((8000,), dtype=float32, min=-1.0, max=0.769, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'vf_preds': np.ndarray((8000,), dtype=float32, min=-0.01, max=0.008, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                       'random': { 'actions': np.ndarray((8000, 3), dtype=int64, min=0.0, max=2.0, mean=0.996),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'agent_index': np.ndarray((8000,), dtype=int64, min=2.0, max=3.0, mean=2.5),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'dones': np.ndarray((8000,), dtype=float32, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'eps_id': np.ndarray((8000,), dtype=int64, min=74494647.0, max=1676983340.0, mean=718524092.558),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'infos': np.ndarray((8000,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=1.2, max=6.454, mean=3.827), 'rotation_y': 265.33014, 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'new_obs': np.ndarray((8000, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'obs': np.ndarray((8000, 336), dtype=float32, min=0.0, max=1.0, mean=0.178),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'prev_actions': np.ndarray((8000, 3), dtype=int64, min=0.0, max=2.0, mean=0.993),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'prev_rewards': np.ndarray((8000,), dtype=float32, min=-0.063, max=0.071, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'rewards': np.ndarray((8000,), dtype=float32, min=-1.0, max=0.977, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   't': np.ndarray((8000,), dtype=int64, min=0.0, max=499.0, mean=232.362),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                   'unroll_id': np.ndarray((8000,), dtype=int64, min=2.0, max=39.0, mean=21.981)}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m 2021-12-07 21:37:26,644\tINFO rollout_worker.py:901 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'policy_batches': { 'main': { 'action_dist_inputs': np.ndarray((128, 9), dtype=float32, min=-0.008, max=0.008, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'action_logp': np.ndarray((128,), dtype=float32, min=-3.307, max=-3.288, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'actions': np.ndarray((128, 3), dtype=int64, min=0.0, max=2.0, mean=1.021),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'advantages': np.ndarray((128,), dtype=float32, min=-6.884, max=4.601, mean=-0.008),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.391),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'eps_id': np.ndarray((128,), dtype=int64, min=74494647.0, max=1676983340.0, mean=649863036.656),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'infos': np.ndarray((128,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-16.852, max=-2.14, mean=-9.496), 'rotation_y': 288.31784, 'velocity': np.ndarray((2,), dtype=float32, min=-8.334, max=4.733, mean=-1.801)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=-3.251, max=-1.682, mean=-2.466), 'velocity': np.ndarray((2,), dtype=float32, min=-0.189, max=0.136, mean=-0.026)}, 'ep_metrics': {'total_timesteps': 208, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': -0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'new_obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'rewards': np.ndarray((128,), dtype=float32, min=0.0, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'unroll_id': np.ndarray((128,), dtype=int64, min=0.0, max=37.0, mean=18.797),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'value_targets': np.ndarray((128,), dtype=float32, min=-0.794, max=0.557, mean=0.015),\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m                                 'vf_preds': np.ndarray((128,), dtype=float32, min=-0.006, max=0.008, mean=-0.001)}},\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m Iter=1 win-rate=0.5 -> adding new opponent to the mix (main_v1).Result for PPO_Soccer_ed9e4_00000:\n",
      "\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.35569555610097214\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.17784777805048607\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: 0.0\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-07_21-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 141.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5345264327538932\n",
      "  episode_reward_mean: 0.13965102424657916\n",
      "  episode_reward_min: -0.2552243842607349\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: f72239e32aca45bbbf67f8edfb25bfdc\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 5.000000000000002e-05\n",
      "          entropy: 3.2762351149604436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01970064276385875\n",
      "          policy_loss: -0.05041301336198572\n",
      "          total_loss: -0.03720985367775909\n",
      "          vf_explained_var: 0.2875194251537323\n",
      "          vf_loss: 0.009263032792313467\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  league_size: 3\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.117910447761194\n",
      "    gpu_util_percent0: 0.13761194029850748\n",
      "    ram_util_percent: 46.19253731343282\n",
      "    vram_util_percent0: 0.18263814196539516\n",
      "  pid: 17689\n",
      "  policy_reward_max:\n",
      "    main: 1.066464126157209\n",
      "    random: 1.5619264495385612\n",
      "  policy_reward_mean:\n",
      "    main: -0.04703396557583295\n",
      "    random: 0.11685947769912264\n",
      "  policy_reward_min:\n",
      "    main: -1.0\n",
      "    random: -1.0623808618702595\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4757635607690868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.35931778715518\n",
      "    mean_inference_ms: 4.044631284154104\n",
      "    mean_raw_obs_processing_ms: 1.345391283016243\n",
      "  time_since_restore: 50.31712293624878\n",
      "  time_this_iter_s: 50.31712293624878\n",
      "  time_total_s: 50.31712293624878\n",
      "  timers:\n",
      "    learn_throughput: 176.893\n",
      "    learn_time_ms: 22612.538\n",
      "    sample_throughput: 144.389\n",
      "    sample_time_ms: 27702.905\n",
      "  timestamp: 1638923869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: ed9e4_00000\n",
      "  win_rate: 0.5\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.07 GiB heap, 0.0/3.54 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_ed9e4_00000</td><td>RUNNING </td><td>192.168.0.104:17689</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         50.3171</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">0.139651</td><td style=\"text-align: right;\">            0.534526</td><td style=\"text-align: right;\">           -0.255224</td><td style=\"text-align: right;\">             141.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "Result for PPO_Soccer_ed9e4_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.35569555610097214\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0386948557850119\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.19013650200370572\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-07_21-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 798.8888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5345264327538932\n",
      "  episode_reward_mean: -0.13658938601473475\n",
      "  episode_reward_min: -1.7185449999628042\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 9\n",
      "  experiment_id: f72239e32aca45bbbf67f8edfb25bfdc\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 5.000000000000002e-05\n",
      "          entropy: 3.2513044599502807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021228765773158225\n",
      "          policy_loss: -0.05323056650481054\n",
      "          total_loss: -0.0387765813382372\n",
      "          vf_explained_var: 0.23533469438552856\n",
      "          vf_loss: 0.0102082325685178\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  league_size: 4\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.654794520547945\n",
      "    gpu_util_percent0: 0.12493150684931509\n",
      "    ram_util_percent: 46.84931506849315\n",
      "    vram_util_percent0: 0.18255148818780512\n",
      "  pid: 17689\n",
      "  policy_reward_max:\n",
      "    main: 1.066464126157209\n",
      "    random: 1.5619264495385612\n",
      "  policy_reward_mean:\n",
      "    main: -0.10958226124737772\n",
      "    random: 0.041287568240010335\n",
      "  policy_reward_min:\n",
      "    main: -1.1916689333677197\n",
      "    random: -1.0623808618702595\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.47438482651206165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.829242516811405\n",
      "    mean_inference_ms: 4.090697831608231\n",
      "    mean_raw_obs_processing_ms: 1.3169399955458232\n",
      "  time_since_restore: 106.21993088722229\n",
      "  time_this_iter_s: 55.90280795097351\n",
      "  time_total_s: 106.21993088722229\n",
      "  timers:\n",
      "    learn_throughput: 155.421\n",
      "    learn_time_ms: 25736.629\n",
      "    sample_throughput: 146.239\n",
      "    sample_time_ms: 27352.54\n",
      "  timestamp: 1638923925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: ed9e4_00000\n",
      "  win_rate: 0.5\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m Iter=2 win-rate=0.5 -> adding new opponent to the mix (main_v2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.07 GiB heap, 0.0/3.54 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_ed9e4_00000</td><td>RUNNING </td><td>192.168.0.104:17689</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          106.22</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-0.136589</td><td style=\"text-align: right;\">            0.534526</td><td style=\"text-align: right;\">            -1.71854</td><td style=\"text-align: right;\">           798.889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m Iter=3 win-rate=0.5 -> adding new opponent to the mix (main_v3).\n",
      "Result for PPO_Soccer_ed9e4_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.35569555610097214\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04601323519351992\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.19013650200370572\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-07_21-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 819.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5345264327538932\n",
      "  episode_reward_mean: -0.11782667370758189\n",
      "  episode_reward_min: -1.7185449999628042\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: f72239e32aca45bbbf67f8edfb25bfdc\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.30000000000000016\n",
      "          cur_lr: 5.000000000000002e-05\n",
      "          entropy: 3.2162178546663314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020684078838380558\n",
      "          policy_loss: -0.06361930132917469\n",
      "          total_loss: -0.05575625079550913\n",
      "          vf_explained_var: 0.3518986403942108\n",
      "          vf_loss: 0.0016578252405105602\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  league_size: 5\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.14666666666667\n",
      "    gpu_util_percent0: 0.132\n",
      "    ram_util_percent: 47.25466666666667\n",
      "    vram_util_percent0: 0.18640561588241744\n",
      "  pid: 17689\n",
      "  policy_reward_max:\n",
      "    main: 1.066464126157209\n",
      "    random: 1.5619264495385612\n",
      "  policy_reward_mean:\n",
      "    main: -0.09303010262913533\n",
      "    random: 0.03411676577534439\n",
      "  policy_reward_min:\n",
      "    main: -1.1916689333677197\n",
      "    random: -1.0623808618702595\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4751214306834669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.84703271243639\n",
      "    mean_inference_ms: 4.105096025743753\n",
      "    mean_raw_obs_processing_ms: 1.3198844144341213\n",
      "  time_since_restore: 163.69818544387817\n",
      "  time_this_iter_s: 57.478254556655884\n",
      "  time_total_s: 163.69818544387817\n",
      "  timers:\n",
      "    learn_throughput: 148.499\n",
      "    learn_time_ms: 26936.153\n",
      "    sample_throughput: 144.915\n",
      "    sample_time_ms: 27602.346\n",
      "  timestamp: 1638923982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: ed9e4_00000\n",
      "  win_rate: 0.5\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.07 GiB heap, 0.0/3.54 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_ed9e4_00000</td><td>RUNNING </td><td>192.168.0.104:17689</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         163.698</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.117827</td><td style=\"text-align: right;\">            0.534526</td><td style=\"text-align: right;\">            -1.71854</td><td style=\"text-align: right;\">               819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "Result for PPO_Soccer_ed9e4_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.4217437988184514\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.044484121955920324\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.19013650200370572\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-07_21-40-39\n",
      "  done: false\n",
      "  episode_len_mean: 802.4444444444445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5345264327538932\n",
      "  episode_reward_mean: -0.2688385476148634\n",
      "  episode_reward_min: -1.7185449999628042\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 18\n",
      "  experiment_id: f72239e32aca45bbbf67f8edfb25bfdc\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.45\n",
      "          cur_lr: 5.000000000000002e-05\n",
      "          entropy: 3.1939237534053744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017335570816482817\n",
      "          policy_loss: -0.058095778649051986\n",
      "          total_loss: -0.038941798909079464\n",
      "          vf_explained_var: 0.23033903539180756\n",
      "          vf_loss: 0.011352974736678696\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  league_size: 6\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.864\n",
      "    gpu_util_percent0: 0.11133333333333333\n",
      "    ram_util_percent: 47.592\n",
      "    vram_util_percent0: 0.18313919052319844\n",
      "  pid: 17689\n",
      "  policy_reward_max:\n",
      "    main: 1.3581309154433052\n",
      "    random: 1.5619264495385612\n",
      "  policy_reward_mean:\n",
      "    main: 0.0755858357944401\n",
      "    random: -0.21000510960187185\n",
      "  policy_reward_min:\n",
      "    main: -1.1916689333677197\n",
      "    random: -1.4497433980569356\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.47986956068196907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.0435104493834\n",
      "    mean_inference_ms: 4.164149569567718\n",
      "    mean_raw_obs_processing_ms: 1.345990490022177\n",
      "  time_since_restore: 220.37505197525024\n",
      "  time_this_iter_s: 56.67686653137207\n",
      "  time_total_s: 220.37505197525024\n",
      "  timers:\n",
      "    learn_throughput: 146.498\n",
      "    learn_time_ms: 27304.037\n",
      "    sample_throughput: 144.102\n",
      "    sample_time_ms: 27758.12\n",
      "  timestamp: 1638924039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: ed9e4_00000\n",
      "  win_rate: 0.5555555555555556\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m Iter=4 win-rate=0.5555555555555556 -> adding new opponent to the mix (main_v4).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.07 GiB heap, 0.0/3.54 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_ed9e4_00000</td><td>RUNNING </td><td>192.168.0.104:17689</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         220.375</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.268839</td><td style=\"text-align: right;\">            0.534526</td><td style=\"text-align: right;\">            -1.71854</td><td style=\"text-align: right;\">           802.444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n",
      "\u001b[2m\u001b[36m(pid=17689)\u001b[0m chamando policy_mapping_fn original\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        # name=\"PPO_multiagent_league\",\n",
    "        name=\"Teste_14\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=1,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"../../ray_results\",\n",
    "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        # resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "# print('this_path', this_path)\n",
    "\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"bajai_belzonte\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(\n",
    "        agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"main\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return ExperimentAnalysis(experiment)\n",
    "\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\n",
    "        \"/home/bruno/Workspace/soccer-tows-player/src/ray_results/Teste_14\")\n",
    "\n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "\n",
    "export()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
