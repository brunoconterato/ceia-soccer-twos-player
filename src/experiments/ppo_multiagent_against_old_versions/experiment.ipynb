{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiente da competição\n",
    "# !pip install --upgrade ceia-soccer-twos --force-reinstall > /dev/null 2>&1\n",
    "# # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "# !pip install 'aioredis' --upgrade > /dev/null 2>&1\n",
    "# !pip install 'aiohttp' --upgrade > /dev/null 2>&1\n",
    "# !pip install 'ray[default]' --upgrade > /dev/null 2>&1\n",
    "# !pip install 'ray[rllib]' --upgrade > /dev/null 2>&1\n",
    "# !pip install 'ray[tune]' --upgrade > /dev/null 2>&1\n",
    "# !pip install torch --upgrade > /dev/null 2>&1\n",
    "# !pip install lz4 --upgrade > /dev/null 2>&1\n",
    "# !pip install GPUtil --upgrade > /dev/null 2>&1\n",
    "# !pip install tensorboard --upgrade > /dev/null 2>&1\n",
    "\n",
    "# # Dependências necessárias para gravar os vídeos\n",
    "# # !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "# # !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.utils.typing import PolicyID\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.typing import ModelWeights\n",
    "\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Whether for compute_actions, the bounds given in action_space\n",
    "        # should be ignored (default: False). This is to test action-clipping\n",
    "        # and any Env's reaction to bounds breaches.\n",
    "        if self.config.get(\"ignore_action_bounds\", False) and \\\n",
    "                isinstance(self.action_space, Box):\n",
    "            self.action_space_for_sampling = Box(\n",
    "                -float(\"inf\"),\n",
    "                float(\"inf\"),\n",
    "                shape=self.action_space.shape,\n",
    "                dtype=self.action_space.dtype)\n",
    "        else:\n",
    "            self.action_space_for_sampling = self.action_space\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        **kwargs):\n",
    "        # Alternatively, a numpy array would work here as well.\n",
    "        # e.g.: np.array([random.choice([0, 1])] * len(obs_batch))\n",
    "        return [self.action_space_for_sampling.sample() for _ in obs_batch], \\\n",
    "               [], {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def learn_on_batch(self, samples):\n",
    "        \"\"\"No learning.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def compute_log_likelihoods(self,\n",
    "                                actions,\n",
    "                                obs_batch,\n",
    "                                state_batches=None,\n",
    "                                prev_action_batch=None,\n",
    "                                prev_reward_batch=None):\n",
    "        return np.array([random.random()] * len(obs_batch))\n",
    "\n",
    "    @override(Policy)\n",
    "    def get_weights(self) -> ModelWeights:\n",
    "        \"\"\"No weights to save.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @override(Policy)\n",
    "    def set_weights(self, weights: ModelWeights) -> None:\n",
    "        \"\"\"No weights to set.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_RATE_THEWSHOLD = .6\n",
    "\n",
    "class SelfPlayCallback(DefaultCallbacks):\n",
    "    # def on_episode_step(self,\n",
    "    #                     *,\n",
    "    #                     worker: \"RolloutWorker\",\n",
    "    #                     base_env: BaseEnv,\n",
    "    #                     episode: MultiAgentEpisode,\n",
    "    #                     env_index: Optional[int] = None,\n",
    "    #                     **kwargs) -> None:\n",
    "    #     total_timesteps = episode.last_info_for(\n",
    "    #         0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "    #     total_goals = float(episode.last_info_for(0)[\n",
    "    #                         \"ep_metrics\"][\"total_goals\"])\n",
    "    #     estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "    #         float(total_timesteps) if total_goals > 0 else 0.0\n",
    "    #     timesteps_to_goal = float(\n",
    "    #         total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "    #     if not episode.user_data:\n",
    "    #         episode.user_data = {\n",
    "    #             0: {\n",
    "    #                 \"total_env_reward\": 0.0,\n",
    "    #                 \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "    #                 \"total_agent_position_to_ball_reward\": 0.0,\n",
    "    #             },\n",
    "    #             1: {\n",
    "    #                 \"total_env_reward\": 0.0,\n",
    "    #                 \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "    #                 \"total_agent_position_to_ball_reward\": 0.0,\n",
    "    #             },\n",
    "    #             2: {\n",
    "    #                 \"total_env_reward\": 0.0,\n",
    "    #                 \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "    #                 \"total_agent_position_to_ball_reward\": 0.0,\n",
    "    #             },\n",
    "    #             3: {\n",
    "    #                 \"total_env_reward\": 0.0,\n",
    "    #                 \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "    #                 \"total_agent_position_to_ball_reward\": 0.0,\n",
    "    #             }\n",
    "    #         }\n",
    "\n",
    "    #     episode.user_data = {\n",
    "    #         **episode.user_data,\n",
    "    #         0: {\n",
    "    #             \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         1: {\n",
    "    #             \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         2: {\n",
    "    #             \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         3: {\n",
    "    #             \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         }\n",
    "    #     }\n",
    "\n",
    "    #     episode.custom_metrics = {\n",
    "    #         # \"total_timesteps\": total_timesteps,\n",
    "    #         # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "    #         # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "    #         # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "    #         # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "    #         # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "    #         \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "    #         \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "    #         \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "    #     }\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker: \"RolloutWorker\",\n",
    "                       base_env: BaseEnv,\n",
    "                       policies: Dict[PolicyID, Policy],\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index: Optional[int] = None,\n",
    "                       **kwargs) -> None:\n",
    "        print('on_episode_end worker:')\n",
    "        print(worker)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 0=RandomPolicy, 1=1st main policy snapshot,\n",
    "        # 2=2nd main policy snapshot, etc..\n",
    "        self.current_opponent = 0\n",
    "\n",
    "    def on_train_result(self, *, trainer, result, **kwargs):\n",
    "        # Get the win rate for the train batch.\n",
    "        # Note that normally, one should set up a proper evaluation config,\n",
    "        # such that evaluation always happens on the already updated policy,\n",
    "        # instead of on the already used train_batch.\n",
    "        print(result)\n",
    "        main_rew = result[\"hist_stats\"].pop(\"policy_main_reward\")\n",
    "        opponent_rew = list(result[\"hist_stats\"].values())[0]\n",
    "        assert len(main_rew) == len(opponent_rew)\n",
    "        won = 0\n",
    "        for r_main, r_opponent in zip(main_rew, opponent_rew):\n",
    "            if r_main > r_opponent:\n",
    "                won += 1\n",
    "        win_rate = won / len(main_rew)\n",
    "        result[\"win_rate\"] = win_rate\n",
    "        print(f\"Iter={trainer.iteration} win-rate={win_rate} -> \", end=\"\")\n",
    "        # If win rate is good -> Snapshot current policy and play against\n",
    "        # it next, keeping the snapshot fixed and only improving the \"main\"\n",
    "        # policy.\n",
    "        if win_rate > WIN_RATE_THEWSHOLD:\n",
    "            self.current_opponent += 1\n",
    "            new_pol_id = f\"main_v{self.current_opponent}\"\n",
    "            print(f\"adding new opponent to the mix ({new_pol_id}).\")\n",
    "\n",
    "            # Re-define the mapping function, such that \"main\" is forced\n",
    "            # to play against any of the previously played policies\n",
    "            # (excluding \"random\").\n",
    "            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "                # agent_id = [0|1] -> policy depends on episode ID\n",
    "                # This way, we make sure that both policies sometimes play\n",
    "                # (start player) and sometimes agent1 (player to move 2nd).\n",
    "                return \"main\" if episode.episode_id % 2 == agent_id \\\n",
    "                    else \"main_v{}\".format(np.random.choice(\n",
    "                        list(range(1, self.current_opponent + 1))))\n",
    "\n",
    "            new_policy = trainer.add_policy(\n",
    "                policy_id=new_pol_id,\n",
    "                policy_cls=type(trainer.get_policy(\"main\")),\n",
    "                policy_mapping_fn=policy_mapping_fn,\n",
    "            )\n",
    "\n",
    "            # Set the weights of the new policy to the main policy.\n",
    "            # We'll keep training the main policy, whereas `new_pol_id` will\n",
    "            # remain fixed.\n",
    "            main_state = trainer.get_policy(\"main\").get_state()\n",
    "            new_policy.set_state(main_state)\n",
    "            # We need to sync the just copied local weights (from main policy)\n",
    "            # to all the remote workers as well.\n",
    "            trainer.workers.sync_weights()\n",
    "        else:\n",
    "            print(\"not good enough; will keep learning ...\")\n",
    "\n",
    "        # +2 = main + random\n",
    "        result[\"league_size\"] = self.current_opponent + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    # \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    \"episodes_total\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_ENVS_PER_WORKER = 1\n",
    "NUM_ENVS_PER_WORKER = 4\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "}\n",
    "\n",
    "\n",
    "temp_env = create_rllib_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    # agent_id = [0|1] -> policy depends on episode ID\n",
    "    # This way, we make sure that both policies sometimes play agent0\n",
    "    # (start player) and sometimes agent1 (player to move 2nd).\n",
    "    return \"main\" if episode.episode_id % 2 == agent_id else \"random\"\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": 1,\n",
    "    # \"num_workers\": 3,\n",
    "    \"num_workers\": 7,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"num_gpus_per_worker\": 0,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"main\": (None, obs_space, act_space, {}),\n",
    "            \"random\": (RandomPolicy, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        \"policies_to_train\": [\"main\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": {**ENVIRONMENT_CONFIG},\n",
    "    \"callbacks\": SelfPlayCallback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PPO SelfPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_rllib_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        name=\"PPO_multiagent_league\",\n",
    "        # name=\"Measuring_rewards\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=100,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"../../ray_results\",\n",
    "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        # resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "print('this_path', this_path)\n",
    "\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"my_ray_soccer_agent\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(\n",
    "        agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"default\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return ExperimentAnalysis(experiment)\n",
    "\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\n",
    "        \"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_player_custom_rewards\")\n",
    "\n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "\n",
    "export()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
