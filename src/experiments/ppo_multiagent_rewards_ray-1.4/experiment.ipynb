{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Sempre) Outras configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiente da competição\n",
    "# !pip install --upgrade ceia-soccer-twos > /dev/null 2>&1\n",
    "# # a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
    "# !pip install 'aioredis==1.3.1' > /dev/null 2>&1\n",
    "# !pip install 'aiohttp==3.7.4' > /dev/null 2>&1\n",
    "# !pip install 'ray==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install 'ray[tune]==1.4.0' > /dev/null 2>&1\n",
    "# !pip install torch > /dev/null 2>&1\n",
    "# !pip install lz4 > /dev/null 2>&1\n",
    "# !pip install GPUtil > /dev/null 2>&1\n",
    "\n",
    "# # Dependências necessárias para gravar os vídeos\n",
    "# !apt-get install - y xvfb x11-utils > /dev/null 2>&1\n",
    "# !pip install 'pyvirtualdisplay==0.2.*' > /dev/null 2>&1\n",
    "# !pip install tensorboard > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ray\n",
      "Version: 1.4.0\n",
      "Summary: Ray provides a simple, universal API for building distributed applications.\n",
      "Home-page: https://github.com/ray-project/ray\n",
      "Author: Ray Team\n",
      "Author-email: ray-dev@googlegroups.com\n",
      "License: Apache 2.0\n",
      "Location: /home/bruno/anaconda3/envs/soccer-twos/lib/python3.8/site-packages\n",
      "Requires: redis, jsonschema, aiohttp, grpcio, filelock, colorama, pyyaml, gpustat, aioredis, numpy, pydantic, prometheus-client, msgpack, py-spy, requests, protobuf, opencensus, click, aiohttp-cors\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
    "\n",
    "> Visualização do ambiente\n",
    "\n",
    "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
    "\n",
    "\n",
    "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Analysis\n",
    "# from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "# from ray.rllib.utils.typing import PolicyID\n",
    "# from ray.tune.registry import get_trainable_cls\n",
    "# from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from collections import deque\n",
    "# import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import soccer_twos\n",
    "from soccer_twos import EnvType\n",
    "from soccer_twos.side_channels import EnvConfigurationChannel\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "MAX_STEPS = 1000\n",
    "MATCH_STEPS = 5000\n",
    "\n",
    "\n",
    "def get_scalar_projection(x, y):\n",
    "    assert np.linalg.norm(y) > 0.000001\n",
    "    return np.dot(x, y) / np.linalg.norm(y)\n",
    "\n",
    "\n",
    "# Os seguintes valores foram obtidos experimentalmente executando pré-experimentos\n",
    "# A partir desses valores vamops derivar vários outros como posições ddos gols etc\n",
    "min_ball_position_x, max_ball_position_x = - \\\n",
    "    15.563264846801758, 15.682827949523926\n",
    "min_ball_position_y, max_ball_position_y = -7.08929967880249, 7.223850250244141\n",
    "min_player_position_x, max_player_position_x = - \\\n",
    "    17.26804542541504, 17.16301727294922\n",
    "min_player_position_y, max_player_position_y = - \\\n",
    "    7.399587631225586, 7.406457424163818\n",
    "min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity = - \\\n",
    "    -23.366606239568615, 23.749571761530724\n",
    "\n",
    "max_ball_abs_velocity = 78.25721740722656\n",
    "max_goals_one_team = -9999999\n",
    "max_goals_one_match = -9999999\n",
    "max_steps = -999999\n",
    "\n",
    "max_diff_reward = -np.inf\n",
    "\n",
    "# Infered\n",
    "max_ball_abs_avg_velocity = max(\n",
    "    abs(min_ball_to_goal_avg_velocity), abs(max_ball_to_goal_avg_velocity))\n",
    "\n",
    "\n",
    "SPEED_IMPORTANCE = 1.0 / (14.0)\n",
    "CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE = True\n",
    "\n",
    "AFTER_BALL_STEP_PENALTY = 1 / MAX_STEPS  # 0.001\n",
    "\n",
    "# OBS.: Este hyperparâmetro não pode ser modificado sem fazer novos testes em\n",
    "# min_ball_to_goal_avg_velocity e\n",
    "# max_ball_to_goal_avg_velocity:\n",
    "AVG_SPEED_TIMESTEPS_WINDOW = 1\n",
    "\n",
    "\n",
    "def is_after_the_ball(player_id: int, player_pos: np.array, ball_pos: np.array):\n",
    "    if player_id in range(2):\n",
    "        return player_pos[0] > ball_pos[0]\n",
    "    elif player_id in [2, 3]:\n",
    "        return player_pos[0] < ball_pos[0]\n",
    "\n",
    "\n",
    "def get_center_of_goal_pos(player_id):\n",
    "    global min_ball_position_x, max_ball_position_x, \\\n",
    "        min_ball_position_y, max_ball_position_y, \\\n",
    "        min_player_position_x, max_player_position_x, \\\n",
    "        min_player_position_y, max_player_position_y\n",
    "    if player_id in [0, 1]:\n",
    "        return np.array([max_ball_position_x, 0.0])\n",
    "    elif player_id in [2, 3]:\n",
    "        return np.array([min_ball_position_x, 0.0])\n",
    "\n",
    "\n",
    "def calculate_ball_to_goal_scalar_velocity(player_id: int, info: Dict, x_axis_only=True):\n",
    "    ball_velocity = info[\"ball_info\"][\"velocity\"]\n",
    "    if x_axis_only and player_id in [0, 1]:\n",
    "        return ball_velocity[0]\n",
    "    elif x_axis_only and player_id in [2, 3]:\n",
    "        return -ball_velocity[0]\n",
    "\n",
    "    goal_pos = get_center_of_goal_pos(player_id)\n",
    "    ball_pos = info[\"ball_info\"][\"position\"]\n",
    "\n",
    "    # print(f\"ball_pos: {ball_pos}\")\n",
    "    direction_to_center_of_goal = goal_pos - ball_pos\n",
    "    # print(f\"direction_to_center_of_goal: {direction_to_center_of_goal}\")\n",
    "\n",
    "    # global max_ball_abs_velocity\n",
    "    # if np.linalg.norm(ball_velocity) > max_ball_abs_velocity:\n",
    "    #     max_ball_abs_velocity = np.linalg.norm(ball_velocity)\n",
    "\n",
    "    # print(f\"ball_velocity: {ball_velocity}\")\n",
    "    ball_velocity_to_center_of_goal = get_scalar_projection(\n",
    "        ball_velocity, direction_to_center_of_goal)\n",
    "    # print(f\"ball_velocity_to_center_of_goal: {ball_velocity_to_center_of_goal}\")\n",
    "    return ball_velocity_to_center_of_goal\n",
    "\n",
    "# print('ball_velocity_to_center_of_goal', calculate_ball_to_goal_scalar_velocity(0, { \"ball_info\": { \"position\": np.array([3.0, 2.0]), \"velocity\": np.array([0.0, 0.0]) }}))\n",
    "\n",
    "\n",
    "def calculate_distance(pt1: np.ndarray, pt2: np.ndarray):\n",
    "    assert pt1.shape == (2,) and pt2.shape == (2,)\n",
    "    return np.linalg.norm(pt1 - pt2)\n",
    "\n",
    "\n",
    "class CustomRewardWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    # def __init__(self, env):\n",
    "    #     gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action: Union[Dict[int, List[Any]], List[Any]]):\n",
    "        obs, rewards, done, info = super().step(action)\n",
    "\n",
    "        # print(info)\n",
    "        # if rewards[0] > 0.0:\n",
    "        #     assert False\n",
    "\n",
    "        ball_pos = info[0][\"ball_info\"][\"position\"]\n",
    "        ball_velocity = info[0][\"ball_info\"][\"velocity\"]\n",
    "        player0_pos = info[0][\"player_info\"][\"position\"]\n",
    "        player1_pos = info[1][\"player_info\"][\"position\"]\n",
    "        player2_pos = info[2][\"player_info\"][\"position\"]\n",
    "        player3_pos = info[3][\"player_info\"][\"position\"]\n",
    "\n",
    "        # print('ball_velocity', ball_velocity)\n",
    "        if self._was_ball_effective_touched(self.prev_ball_velocity, ball_velocity):\n",
    "            ball_toucher = self._get_ball_toucher(\n",
    "                ball_velocity, ball_pos, player0_pos, player1_pos, player2_pos, player3_pos)\n",
    "            # self.ball_touchers.append(ball_toucher)\n",
    "\n",
    "        if type(action) is dict:\n",
    "            new_rewards = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k]) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        if type(action) is dict:\n",
    "            splitted_rets = {k: self._calculate_reward(\n",
    "                rewards[k], k, info[k], splitted_returns=True) for k in info.keys()}\n",
    "        else:\n",
    "            raise NotImplementedError('Necessário implementar!')\n",
    "\n",
    "        info = {\n",
    "            i: {\n",
    "                **info[i],\n",
    "                \"ep_metrics\": {\n",
    "                    # \"total_timesteps\": np.array([0.0008], dtype=np.float32)\n",
    "                    \"total_timesteps\": self.n_step + 1,\n",
    "                    \"total_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"],\n",
    "                    \"goals_opponent\": self.scoreboard[\"team_1\"] if i in range(2) else self.scoreboard[\"team_0\"],\n",
    "                    \"goals_in_favor\": self.scoreboard[\"team_0\"] if i in range(2) else self.scoreboard[\"team_1\"],\n",
    "                    \"team_0_goals\": self.scoreboard[\"team_0\"],\n",
    "                    \"team_1_goals\": self.scoreboard[\"team_1\"],\n",
    "                    \"episode_ended\": done[\"__all__\"],\n",
    "                    \"have_goals\": self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > 0,\n",
    "                    \"env_reward\": splitted_rets[i][0],\n",
    "                    \"ball_to_goal_speed_reward\": splitted_rets[i][1],\n",
    "                    # \"agent_position_to_ball_reward\": splitted_rets[i][2],\n",
    "                }\n",
    "            } for i in info.keys()\n",
    "        }\n",
    "\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y, \\\n",
    "        #     max_goals_one_team, max_goals_one_match\n",
    "        # if done:\n",
    "        #     print(f'min_ball_position_x: {min_ball_position_x}')\n",
    "        #     print(f'max_ball_position_x: {max_ball_position_x}')\n",
    "        #     print(f'min_ball_position_y: {min_ball_position_y}')\n",
    "        #     print(f'max_ball_position_y: {max_ball_position_y}')\n",
    "        #     print(f'min_player_position_x: {min_player_position_x}')\n",
    "        #     print(f'max_player_position_x: {max_player_position_x}')\n",
    "        #     print(f'min_player_position_y: {min_player_position_y}')\n",
    "        #     print(f'max_player_position_y: {max_player_position_y}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_goals_one_team: {max_goals_one_team}')\n",
    "        #     print(f'max_goals_one_match: {max_goals_one_match}')\n",
    "        #     print(self.scoreboard)\n",
    "        #     print(f'Done... last n_step: {self.n_step}')\n",
    "        #     if self.scoreboard[\"team_0\"] > 0 or self.scoreboard[\"team_1\"] > 0:\n",
    "        #         input(\"Press Enter to continue...\")\n",
    "\n",
    "        # global max_steps\n",
    "        # if done:\n",
    "        #     if self.n_step + 1 > max_steps:\n",
    "        #         max_steps = self.n_step + 1\n",
    "        #     print('max_steps', max_steps)\n",
    "\n",
    "        # global max_diff_reward\n",
    "        # if done:\n",
    "        #     print(f'max_diff_reward: {max_diff_reward}')\n",
    "        #     print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        #     print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "\n",
    "        # if done:\n",
    "        #     print(f'max_ball_abs_velocity: {max_ball_abs_velocity}')\n",
    "        # if done:\n",
    "        #     print('self.ball_touched', self.ball_touched)\n",
    "        #     print('self.ball_touchers', self.ball_touchers)\n",
    "\n",
    "        self.n_step += 1\n",
    "        self.prev_ball_velocity = ball_velocity.copy()\n",
    "\n",
    "        return obs, new_rewards, done, info\n",
    "            \n",
    "    def reset(self, **kwargs):\n",
    "        obs = super().reset(**kwargs)\n",
    "        self.n_step = 0\n",
    "        self.last_ball_speed_mean_per_player = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}\n",
    "        self.ball_speed_deque_per_player = {0: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            1: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            2: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW),\n",
    "                                            3: deque(maxlen=AVG_SPEED_TIMESTEPS_WINDOW)}\n",
    "        self.scoreboard = {\"team_0\": 0, \"team_1\": 0}\n",
    "        self.await_press = False\n",
    "        self.prev_ball_velocity = np.array([0.0, 0.0])\n",
    "        self.last_ball_toucher = -1\n",
    "        # self.ball_touched = []\n",
    "        # self.ball_touchers = []\n",
    "        # print(f'min_ball_to_goal_avg_velocity: {min_ball_to_goal_avg_velocity}')\n",
    "        # print(f'max_ball_to_goal_avg_velocity: {max_ball_to_goal_avg_velocity}')\n",
    "        return obs\n",
    "\n",
    "    def _was_ball_effective_touched(self, prev_ball_velocity: np.ndarray, curr_ball_velocity: np.ndarray):\n",
    "        \"\"\"Get if ball was touched (either by player or wall)\n",
    "\n",
    "        Args:\n",
    "            prev_ball_velocity (np.ndarray): Previous ball coordinates\n",
    "            curr_ball_velocity (np.ndarray): Current ball coordinates\n",
    "        \"\"\"\n",
    "        assert prev_ball_velocity.shape == (\n",
    "            2,) and curr_ball_velocity.shape == (2,)\n",
    "        percentual_scalar_thresold = 0.2  # 20%\n",
    "        diff = curr_ball_velocity - prev_ball_velocity\n",
    "\n",
    "\n",
    "        if np.linalg.norm(curr_ball_velocity) < 1.0:\n",
    "            self.last_ball_toucher = -1\n",
    "\n",
    "        if np.linalg.norm(prev_ball_velocity) > 0.0000001:\n",
    "            return np.linalg.norm(diff) / np.linalg.norm(prev_ball_velocity) > percentual_scalar_thresold\n",
    "        return np.linalg.norm(curr_ball_velocity) > np.linalg.norm(prev_ball_velocity)\n",
    "\n",
    "    def _get_ball_toucher(self,\n",
    "                          ball_velocity: np.ndarray,\n",
    "                          ball_position: np.ndarray,\n",
    "                          player_0_pos: np.ndarray,\n",
    "                          player_1_pos: np.ndarray,\n",
    "                          player_2_pos: np.ndarray,\n",
    "                          player_3_pos: np.ndarray):\n",
    "        assert ball_position.shape == (2,) and \\\n",
    "            player_0_pos.shape == (2,) and \\\n",
    "            player_1_pos.shape == (2,) and \\\n",
    "            player_2_pos.shape == (2,) and \\\n",
    "            player_3_pos.shape == (2,)\n",
    "        top_wall_y = max_ball_position_y\n",
    "        bottom_wall_y = min_ball_position_y\n",
    "        left_wall_x = min_ball_position_x\n",
    "        right_wall_x = max_ball_position_x\n",
    "\n",
    "        if np.linalg.norm(ball_velocity) > 0.000001:\n",
    "            distances = np.array([\n",
    "                calculate_distance(ball_position, player_0_pos),\n",
    "                calculate_distance(ball_position, player_1_pos),\n",
    "                calculate_distance(ball_position, player_2_pos),\n",
    "                calculate_distance(ball_position, player_3_pos),\n",
    "                np.abs(ball_position[1] - top_wall_y),\n",
    "                np.abs(ball_position[1] - bottom_wall_y),\n",
    "                np.abs(ball_position[0] - left_wall_x),\n",
    "                np.abs(ball_position[0] - right_wall_x)\n",
    "            ])\n",
    "\n",
    "            # print(distances)\n",
    "            nearest = np.argmin(distances)\n",
    "            # print(nearest)\n",
    "            if nearest < 4:\n",
    "                self.last_ball_toucher = nearest\n",
    "\n",
    "        return self.last_ball_toucher\n",
    "\n",
    "    def _calculate_reward(self, reward: float, player_id: int, info: Dict, splitted_returns=False) -> float:\n",
    "        # print('calculating reward')\n",
    "        if reward != 0.0:\n",
    "            # print('Goal was made!', reward, info)\n",
    "            self._update_scoreboard(player_id, reward)\n",
    "        # global min_ball_position_x, max_ball_position_x, \\\n",
    "        #     min_ball_position_y, max_ball_position_y, \\\n",
    "        #     min_player_position_x, max_player_position_x, \\\n",
    "        #     min_player_position_y, max_player_position_y\n",
    "        # print(f\"info: {info}\")\n",
    "        # if info[\"ball_info\"][\"position\"][0] < min_ball_position_x:\n",
    "        #     min_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][0] > max_ball_position_x:\n",
    "        #     max_ball_position_x = info[\"ball_info\"][\"position\"][0]\n",
    "        # if info[\"ball_info\"][\"position\"][1] < min_ball_position_y:\n",
    "        #     min_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"ball_info\"][\"position\"][1] > max_ball_position_y:\n",
    "        #     max_ball_position_y = info[\"ball_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][0] < min_player_position_x:\n",
    "        #     min_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][0] > max_player_position_x:\n",
    "        #     max_player_position_x = info[\"player_info\"][\"position\"][0]\n",
    "        # if info[\"player_info\"][\"position\"][1] < min_player_position_y:\n",
    "        #     min_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "        # if info[\"player_info\"][\"position\"][1] > max_player_position_y:\n",
    "        #     max_player_position_y = info[\"player_info\"][\"position\"][1]\n",
    "\n",
    "        self._update_avg_ball_speed_to_goal(\n",
    "            player_id, calculate_ball_to_goal_scalar_velocity(player_id, info))\n",
    "        # global max_diff_reward\n",
    "        # if (np.abs(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity) > max_diff_reward):\n",
    "        #     max_diff_reward = SPEED_IMPORTANCE * \\\n",
    "        #         self.last_ball_speed_mean_per_player[player_id] / \\\n",
    "        #         max_ball_abs_avg_velocity\n",
    "\n",
    "        # ball_pos = info[\"ball_info\"][\"position\"]\n",
    "        # player_pos = info[\"player_info\"][\"position\"]\n",
    "\n",
    "        env_reward = reward\n",
    "        \n",
    "        ball_to_goal_speed_reward = np.clip(SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity, -SPEED_IMPORTANCE,\n",
    "                                            SPEED_IMPORTANCE) if CLIP_SPEED_REWARD_BY_SPEED_IMPORTANCE else SPEED_IMPORTANCE * self.last_ball_speed_mean_per_player[player_id] / max_ball_abs_avg_velocity\n",
    "        ball_to_goal_speed_reward = (\n",
    "            player_id == self.last_ball_toucher) * ball_to_goal_speed_reward\n",
    "        # agent_position_to_ball_reward = is_after_the_ball(player_id, player_pos,\n",
    "        #                                                   ball_pos) * (-AFTER_BALL_STEP_PENALTY)\n",
    "\n",
    "        # if splitted_returns:\n",
    "        #     return (env_reward, ball_to_goal_speed_reward, agent_position_to_ball_reward)\n",
    "        # return env_reward + ball_to_goal_speed_reward + agent_position_to_ball_reward\n",
    "        if splitted_returns:\n",
    "            return (env_reward, ball_to_goal_speed_reward)\n",
    "        return env_reward + ball_to_goal_speed_reward\n",
    "\n",
    "    def _update_avg_ball_speed_to_goal(self, player_id: int, ball_speed: float):\n",
    "        assert player_id in [0, 1, 2, 3]\n",
    "        # global min_ball_to_goal_avg_velocity, max_ball_to_goal_avg_velocity\n",
    "\n",
    "        # Getting min/max ball to goal speed forr normalization\n",
    "        # print(f'player_id: {player_id}')\n",
    "        # print(f'self.last_ball_speed_mean_per_player: {self.last_ball_speed_mean_per_player}')\n",
    "        # print(f'self.n_step: {self.n_step}')\n",
    "        # print(f'ball_speed: {ball_speed}')\n",
    "\n",
    "        self.ball_speed_deque_per_player[player_id].append(ball_speed)\n",
    "        avg = np.mean(self.ball_speed_deque_per_player[player_id])\n",
    "        # if avg < min_ball_to_goal_avg_velocity:\n",
    "        #     min_ball_to_goal_avg_velocity = avg\n",
    "        # elif avg > max_ball_to_goal_avg_velocity:\n",
    "        #     max_ball_to_goal_avg_velocity = avg\n",
    "\n",
    "        self.last_ball_speed_mean_per_player[player_id] = avg\n",
    "\n",
    "    def _update_scoreboard(self, player_id, reward):\n",
    "        # global max_goals_one_team, max_goals_one_match\n",
    "\n",
    "        if player_id == 0 and reward == -1.0:\n",
    "            self.scoreboard[\"team_1\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_1\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_1\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n",
    "        elif player_id == 2 and reward == -1.0:\n",
    "            self.scoreboard[\"team_0\"] += 1\n",
    "            # print(self.scoreboard)\n",
    "\n",
    "            # if self.scoreboard[\"team_0\"] > max_goals_one_team:\n",
    "            #     max_goals_one_team = self.scoreboard[\"team_0\"]\n",
    "            # if self.scoreboard[\"team_0\"] + self.scoreboard[\"team_1\"] > max_goals_one_match:\n",
    "            #     max_goals_one_match = self.scoreboard[\"team_0\"] + \\\n",
    "            #         self.scoreboard[\"team_1\"]\n",
    "            # if max_goals_one_match > 0:\n",
    "            #     if not self.await_press:\n",
    "            #         input(\"Press Enter to continue...\")\n",
    "            #         self.await_press = True\n",
    "            #     else:\n",
    "            #         self.await_press = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)\n",
    "\n",
    "\n",
    "def create_custom_env(env_config: dict = {}):\n",
    "    env = create_rllib_env(env_config)\n",
    "    return CustomRewardWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_RATE_THEWSHOLD = .2\n",
    "env_channel = EnvConfigurationChannel()\n",
    "\n",
    "class SelfPlayCallback(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker: \"RolloutWorker\",\n",
    "                        base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index: Optional[int] = None,\n",
    "                        **kwargs) -> None:\n",
    "        total_timesteps = episode.last_info_for(\n",
    "            0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "        total_goals = float(episode.last_info_for(0)[\n",
    "                            \"ep_metrics\"][\"total_goals\"])\n",
    "        estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "            float(total_timesteps) if total_goals > 0 else 0.0\n",
    "        timesteps_to_goal = float(\n",
    "            total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "        if not episode.user_data:\n",
    "            episode.user_data = {\n",
    "                0: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                1: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                2: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                },\n",
    "                3: {\n",
    "                    \"total_env_reward\": 0.0,\n",
    "                    \"total_ball_to_goal_speed_reward\": 0.0,\n",
    "                    # \"total_agent_position_to_ball_reward\": 0.0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        episode.user_data = {\n",
    "            **episode.user_data,\n",
    "            0: {\n",
    "                \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            1: {\n",
    "                \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            2: {\n",
    "                \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            },\n",
    "            3: {\n",
    "                \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "                \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "                # \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        episode.custom_metrics = {\n",
    "            # \"total_timesteps\": total_timesteps,\n",
    "            # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "            # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "            # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "            # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "            # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "            \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "            \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "            # \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "        }\n",
    "\n",
    "    # def on_episode_end(self,\n",
    "    #                    *,\n",
    "    #                    worker: \"RolloutWorker\",\n",
    "    #                    base_env: BaseEnv,\n",
    "    #                    policies: Dict[PolicyID, Policy],\n",
    "    #                    episode: MultiAgentEpisode,\n",
    "    #                    env_index: Optional[int] = None,\n",
    "    #                    **kwargs) -> None:\n",
    "    #     total_timesteps = episode.last_info_for(\n",
    "    #         0)[\"ep_metrics\"][\"total_timesteps\"]\n",
    "    #     total_goals = float(episode.last_info_for(0)[\n",
    "    #                         \"ep_metrics\"][\"total_goals\"])\n",
    "    #     estimated_goals_in_match = total_goals * MATCH_STEPS / \\\n",
    "    #         float(total_timesteps) if total_goals > 0 else 0.0\n",
    "    #     timesteps_to_goal = float(\n",
    "    #         total_timesteps) if total_goals > 0 else 9999.0\n",
    "\n",
    "    #     episode.user_data = {\n",
    "    #         **episode.user_data,\n",
    "    #         0: {\n",
    "    #             \"total_env_reward\": episode.user_data[0][\"total_env_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(0)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         1: {\n",
    "    #             \"total_env_reward\": episode.user_data[1][\"total_env_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[1][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[1][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(1)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         2: {\n",
    "    #             \"total_env_reward\": episode.user_data[2][\"total_env_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[2][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[2][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(2)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         },\n",
    "    #         3: {\n",
    "    #             \"total_env_reward\": episode.user_data[3][\"total_env_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"env_reward\"],\n",
    "    #             \"total_ball_to_goal_speed_reward\": episode.user_data[3][\"total_ball_to_goal_speed_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"ball_to_goal_speed_reward\"],\n",
    "    #             # \"total_agent_position_to_ball_reward\": episode.user_data[3][\"total_agent_position_to_ball_reward\"] + episode.last_info_for(3)[\"ep_metrics\"][\"agent_position_to_ball_reward\"],\n",
    "    #         }\n",
    "    #     }\n",
    "\n",
    "    #     episode.custom_metrics = {\n",
    "    #         # \"total_timesteps\": total_timesteps,\n",
    "    #         # \"timesteps_to_goal\": timesteps_to_goal,\n",
    "    #         # \"estimated_goals_in_match\": estimated_goals_in_match,\n",
    "    #         # \"team_0_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_0_goals\"],\n",
    "    #         # \"team_1_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"team_1_goals\"],\n",
    "    #         # \"have_goals\": episode.last_info_for(0)[\"ep_metrics\"][\"have_goals\"],\n",
    "    #         \"agent_0_total_env_reward\": episode.user_data[0][\"total_env_reward\"],\n",
    "    #         \"agent_0_total_ball_to_goal_speed_reward\": episode.user_data[0][\"total_ball_to_goal_speed_reward\"],\n",
    "    #         # \"agent_0_total_agent_position_to_ball_reward\": episode.user_data[0][\"total_agent_position_to_ball_reward\"],\n",
    "    #     }\n",
    "\n",
    "    # def __init__(self):\n",
    "    #     super().__init__()\n",
    "    #     # 0=RandomPolicy, 1=1st main policy snapshot,\n",
    "    #     # 2=2nd main policy snapshot, etc..\n",
    "    #     self.current_opponent = 0\n",
    "\n",
    "    # def on_train_result(self, *, trainer, result, **kwargs):\n",
    "    #     # Get the win rate for the train batch.\n",
    "    #     # Note that normally, one should set up a proper evaluation config,\n",
    "    #     # such that evaluation always happens on the already updated policy,\n",
    "    #     # instead of on the already used train_batch.\n",
    "    #     # print(\"result\", result)\n",
    "    #     # print(\"result[hist_stats]\", result[\"hist_stats\"])\n",
    "    #     main_rew = result[\"hist_stats\"].pop(\"policy_main_reward\")\n",
    "    #     opponent_rew = result[\"hist_stats\"].pop(\"policy_random_reward\")\n",
    "    #     # opponent_rew = list(result[\"hist_stats\"].values())[0]\n",
    "    #     # print('len(main_rew)', len(main_rew))\n",
    "    #     # print(\"len(opponent_rew)\", len(opponent_rew))\n",
    "    #     assert len(main_rew) == len(opponent_rew)\n",
    "    #     won = 0\n",
    "    #     for r_main, r_opponent in zip(main_rew, opponent_rew):\n",
    "    #         if r_main > r_opponent:\n",
    "    #             won += 1\n",
    "    #     win_rate = won / len(main_rew)\n",
    "    #     result[\"win_rate\"] = win_rate\n",
    "    #     print(f\"Iter={trainer.iteration} win-rate={win_rate} -> \", end=\"\")\n",
    "    #     # If win rate is good -> Snapshot current policy and play against\n",
    "    #     # it next, keeping the snapshot fixed and only improving the \"main\"\n",
    "    #     # policy.\n",
    "    #     if win_rate > WIN_RATE_THEWSHOLD:\n",
    "    #         self.current_opponent += 1\n",
    "    #         new_pol_id = f\"main_v{self.current_opponent}\"\n",
    "    #         print(f\"adding new opponent to the mix ({new_pol_id}).\")\n",
    "\n",
    "    #         # Re-define the mapping function, such that \"main\" is forced\n",
    "    #         # to play against any of the previously played policies\n",
    "    #         # (excluding \"random\").\n",
    "    #         alternator = Alternator()\n",
    "    #         def new_policy_mapping_fn(agent_id, **kwargs):\n",
    "    #             # agent_id = [0|1] -> policy depends on episode ID\n",
    "    #             # This way, we make sure that both policies sometimes play\n",
    "    #             # (start player) and sometimes agent1 (player to move 2nd).\n",
    "    #             selected_pol = \"main\" if alternator.step_value() == agent_id \\\n",
    "    #                 else \"main_v{}\".format(np.random.choice(\n",
    "    #                     list(range(1, self.current_opponent + 1))))\n",
    "    #             print(f'policy_mapping_fn selected_pol: {selected_pol}\\nself.current_opponent: {self.current_opponent}')\n",
    "    #             return selected_pol\n",
    "\n",
    "    #         # new_policy = trainer.add_policy(\n",
    "    #         #     policy_id=new_pol_id,\n",
    "    #         #     policy_cls=type(trainer.get_policy(\"main\")),\n",
    "    #         #     policy_mapping_fn=policy_mapping_fn,\n",
    "    #         # )\n",
    "\n",
    "    #         trainer.workers.local_worker().policy_config[\"multiagent\"][\"policy_mapping_fn\"] = new_policy_mapping_fn\n",
    "    #         trainer.workers.local_worker().policy_mapping_fn = new_policy_mapping_fn\n",
    "\n",
    "    #         trainer.workers.local_worker().policy_map[new_pol_id] = trainer.get_policy(\"main\")\n",
    "\n",
    "    #         # for r in trainer.workers.remote_workers():\n",
    "    #         #     # r.policy_config[\"multiagent\"][\"policy_mapping_fn\"] = policy_mapping_fn\n",
    "    #         #     # r.policy_mapping_fn = policy_mapping_fn\n",
    "    #         #     r.policy_map[new_pol_id] = trainer.get_policy(\"main\")\n",
    "    #         #     # r.policy_map[new_pol_id].set_state(main_state)\n",
    "\n",
    "    #         # Set the weights of the new policy to the main policy.\n",
    "    #         # We'll keep training the main policy, whereas `new_pol_id` will\n",
    "    #         # remain fixed.\n",
    "    #         main_state = trainer.get_policy(\"main\").get_state()\n",
    "    #         # new_policy.set_state(main_state)\n",
    "    #         trainer.workers.local_worker().policy_map[new_pol_id].set_state(main_state)\n",
    "    #         # We need to sync the just copied local weights (from main policy)\n",
    "    #         # to all the remote workers as well.\n",
    "    #         # trainer.workers.sync_weights()\n",
    "    #     else:\n",
    "    #         print(\"not good enough; will keep learning ...\")\n",
    "\n",
    "    #     # +2 = main + random\n",
    "    #     result[\"league_size\"] = self.current_opponent + 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"timesteps_total\": 15000000,  # 15M\n",
    "    # \"time_total_s\": 14400, # 4h\n",
    "    # \"episodes_total\": 10,\n",
    "    # \"training_iteration\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    }
   ],
   "source": [
    "NUM_ENVS_PER_WORKER = 4\n",
    "ENVIRONMENT_ID = \"Soccer\"\n",
    "\n",
    "ENVIRONMENT_CONFIG = {\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"variation\": EnvType.multiagent_player,\n",
    "    # \"env_channel\": env_channel,\n",
    "}\n",
    "\n",
    "temp_env = create_custom_env(ENVIRONMENT_CONFIG)\n",
    "obs_space = temp_env.observation_space\n",
    "act_space = temp_env.action_space\n",
    "temp_env.close()\n",
    "\n",
    "# alternator = Alternator()\n",
    "# def policy_mapping_fn(agent_id, **kwargs):\n",
    "#     print('chamando policy_mapping_fn original')\n",
    "#     # agent_id = [0|1] -> policy depends on episode ID\n",
    "#     # This way, we make sure that both policies sometimes play agent0\n",
    "#     # (start player) and sometimes agent1 (player to move 2nd).\n",
    "#     return \"main\" if alternator.step_value() == agent_id else \"random\"\n",
    "\n",
    "gpu_count = 1\n",
    "num_workers = 3\n",
    "num_gpus_for_driver = 1 / (num_workers + 1) # Driver GPU\n",
    "num_gpus_per_worker = (gpu_count - num_gpus_for_driver) / num_workers if num_workers > 0 else 0\n",
    "\n",
    "config = {\n",
    "    # system settings\n",
    "    \"num_gpus\": num_gpus_for_driver,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
    "    \"num_cpus_for_driver\": 2,\n",
    "    \"num_cpus_per_worker\": 2,\n",
    "    \"num_gpus_per_worker\": num_gpus_per_worker,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"framework\": \"torch\",\n",
    "    # RL setup\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"main\": (None, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda _: \"main\",\n",
    "        \"policies_to_train\": [\"main\"],\n",
    "    },\n",
    "    \"env\": ENVIRONMENT_ID,\n",
    "    \"env_config\": {\n",
    "        **ENVIRONMENT_CONFIG,\n",
    "        # \"render\": True,\n",
    "        # \"time_scale\": 1,\n",
    "    },\n",
    "    \"callbacks\": SelfPlayCallback,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PPO SelfPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:42:54,648\tWARNING ppo.py:135 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=4 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 333.\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m 2021-12-10 14:42:58,523\tINFO torch_policy.py:148 -- TorchPolicy (worker=3) running on 0.25 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:42:58,523\tINFO torch_policy.py:148 -- TorchPolicy (worker=1) running on 0.25 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m 2021-12-10 14:42:58,523\tINFO torch_policy.py:148 -- TorchPolicy (worker=2) running on 0.25 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78459)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78458)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m INFO:mlagents_envs.environment:Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=1\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m [INFO] Connected new brain: SoccerTwos?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:24,293\tINFO torch_policy.py:148 -- TorchPolicy (worker=local) running on 0.25 GPU(s).\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:26,715\tINFO rollout_worker.py:1199 -- Built policy map: {'main': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x7f835027fbe0>}\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:26,715\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'main': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f835021c640>}\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:26,715\tINFO rollout_worker.py:583 -- Built filter map: {'main': <ray.rllib.utils.filter.NoFilter object at 0x7f835041b070>}\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:26,718\tINFO trainable.py:101 -- Trainable.setup took 32.071 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,742\tINFO rollout_worker.py:723 -- Generating sample batch of size 1332\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,763\tINFO sampler.py:590 -- Raw obs from env: { 0: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   1: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   2: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   3: { 0: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        1: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        2: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        3: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192)}}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,763\tINFO sampler.py:592 -- Info return from env: { 0: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   1: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   2: {0: {}, 1: {}, 2: {}, 3: {}},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   3: {0: {}, 1: {}, 2: {}, 3: {}}}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,763\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,764\tINFO sampler.py:817 -- Filtered obs: np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187)\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,771\tINFO sampler.py:1004 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m { 'main': [ { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'data': { 'agent_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'info': {},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'obs': np.ndarray((336,), dtype=float32, min=0.0, max=1.0, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                         'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:26,776\tINFO sampler.py:1022 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m { 'main': ( np.ndarray((16, 3), dtype=int64, min=0.0, max=2.0, mean=1.062),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             [],\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m             { 'action_dist_inputs': np.ndarray((16, 9), dtype=float32, min=-0.008, max=0.006, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'action_logp': np.ndarray((16,), dtype=float32, min=-3.302, max=-3.287, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'action_prob': np.ndarray((16,), dtype=float32, min=0.037, max=0.037, mean=0.037),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m               'vf_preds': np.ndarray((16,), dtype=float32, min=-0.003, max=-0.001, mean=-0.002)})}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:38,460\tINFO simple_list_collector.py:659 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m { 0: { 'action_dist_inputs': np.ndarray((333, 9), dtype=float32, min=-0.01, max=0.012, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'action_logp': np.ndarray((333,), dtype=float32, min=-3.309, max=-3.282, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'actions': np.ndarray((333, 3), dtype=int64, min=0.0, max=2.0, mean=0.988),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'advantages': np.ndarray((333,), dtype=float32, min=-0.006, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'agent_index': np.ndarray((333,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'dones': np.ndarray((333,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'eps_id': np.ndarray((333,), dtype=int64, min=253228657.0, max=253228657.0, mean=253228657.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'infos': np.ndarray((333,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.031, max=1.2, mean=-3.916), 'rotation_y': 98.31837, 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'new_obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.163),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.163),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'rewards': np.ndarray((333,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'unroll_id': np.ndarray((333,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'value_targets': np.ndarray((333,), dtype=float32, min=-0.002, max=-0.0, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'vf_preds': np.ndarray((333,), dtype=float32, min=-0.007, max=0.005, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   1: { 'action_dist_inputs': np.ndarray((333, 9), dtype=float32, min=-0.012, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'action_logp': np.ndarray((333,), dtype=float32, min=-3.309, max=-3.283, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'actions': np.ndarray((333, 3), dtype=int64, min=0.0, max=2.0, mean=0.985),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'advantages': np.ndarray((333,), dtype=float32, min=-0.007, max=0.005, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'agent_index': np.ndarray((333,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'dones': np.ndarray((333,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'eps_id': np.ndarray((333,), dtype=int64, min=253228657.0, max=253228657.0, mean=253228657.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'infos': np.ndarray((333,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-6.24, max=-1.338, mean=-3.789), 'rotation_y': 97.002495, 'velocity': np.ndarray((2,), dtype=float32, min=-2.212, max=-0.058, mean=-1.135)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'new_obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.189),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.189),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'rewards': np.ndarray((333,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'unroll_id': np.ndarray((333,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'value_targets': np.ndarray((333,), dtype=float32, min=0.0, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'vf_preds': np.ndarray((333,), dtype=float32, min=-0.004, max=0.007, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   2: { 'action_dist_inputs': np.ndarray((333, 9), dtype=float32, min=-0.01, max=0.01, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'action_logp': np.ndarray((333,), dtype=float32, min=-3.31, max=-3.281, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'actions': np.ndarray((333, 3), dtype=int64, min=0.0, max=2.0, mean=1.029),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'advantages': np.ndarray((333,), dtype=float32, min=-0.21, max=-0.004, mean=-0.07),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'agent_index': np.ndarray((333,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'dones': np.ndarray((333,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'eps_id': np.ndarray((333,), dtype=int64, min=253228657.0, max=253228657.0, mean=253228657.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'infos': np.ndarray((333,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=1.338, max=6.467, mean=3.902), 'rotation_y': 275.33014, 'velocity': np.ndarray((2,), dtype=float32, min=0.206, max=2.205, mean=1.206)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'new_obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.182),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.182),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'rewards': np.ndarray((333,), dtype=float32, min=-0.024, max=0.0, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'unroll_id': np.ndarray((333,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'value_targets': np.ndarray((333,), dtype=float32, min=-0.212, max=-0.004, mean=-0.069),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'vf_preds': np.ndarray((333,), dtype=float32, min=-0.002, max=0.006, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   3: { 'action_dist_inputs': np.ndarray((333, 9), dtype=float32, min=-0.009, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'action_logp': np.ndarray((333,), dtype=float32, min=-3.31, max=-3.28, mean=-3.295),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'actions': np.ndarray((333, 3), dtype=int64, min=0.0, max=2.0, mean=1.054),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'advantages': np.ndarray((333,), dtype=float32, min=-0.006, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'agent_index': np.ndarray((333,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'dones': np.ndarray((333,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'eps_id': np.ndarray((333,), dtype=int64, min=253228657.0, max=253228657.0, mean=253228657.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'infos': np.ndarray((333,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-1.338, max=6.657, mean=2.66), 'rotation_y': 280.03668, 'velocity': np.ndarray((2,), dtype=float32, min=-2.205, max=-0.175, mean=-1.19)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'new_obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'obs': np.ndarray((333, 336), dtype=float32, min=0.0, max=1.0, mean=0.179),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'rewards': np.ndarray((333,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'unroll_id': np.ndarray((333,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'value_targets': np.ndarray((333,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m        'vf_preds': np.ndarray((333,), dtype=float32, min=-0.007, max=0.006, mean=-0.0)}}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m 2021-12-10 14:43:38,539\tINFO rollout_worker.py:761 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m { 'count': 1332,\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   'policy_batches': { 'main': { 'action_dist_inputs': np.ndarray((5328, 9), dtype=float32, min=-0.012, max=0.015, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'action_logp': np.ndarray((5328,), dtype=float32, min=-3.311, max=-3.28, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'actions': np.ndarray((5328, 3), dtype=int64, min=0.0, max=2.0, mean=1.013),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'advantages': np.ndarray((5328,), dtype=float32, min=-0.21, max=0.052, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'agent_index': np.ndarray((5328,), dtype=int64, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'dones': np.ndarray((5328,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'eps_id': np.ndarray((5328,), dtype=int64, min=253228657.0, max=736187081.0, mean=492933735.25),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'infos': np.ndarray((5328,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=-9.031, max=1.2, mean=-3.916), 'rotation_y': 98.31837, 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=1.091, max=1.825, mean=1.458), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 1, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'new_obs': np.ndarray((5328, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'obs': np.ndarray((5328, 336), dtype=float32, min=0.0, max=1.0, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'rewards': np.ndarray((5328,), dtype=float32, min=-0.031, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'unroll_id': np.ndarray((5328,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'value_targets': np.ndarray((5328,), dtype=float32, min=-0.212, max=0.049, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m                                 'vf_preds': np.ndarray((5328,), dtype=float32, min=-0.007, max=0.007, mean=0.0)}},\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=78454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m 2021-12-10 14:43:51,993\tINFO rollout_worker.py:901 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m   'policy_batches': { 'main': { 'action_dist_inputs': np.ndarray((128, 9), dtype=float32, min=-0.012, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'action_logp': np.ndarray((128,), dtype=float32, min=-3.309, max=-3.281, mean=-3.296),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'actions': np.ndarray((128, 3), dtype=int64, min=0.0, max=2.0, mean=1.008),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'advantages': np.ndarray((128,), dtype=float32, min=-7.11, max=7.643, mean=0.029),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.383),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'eps_id': np.ndarray((128,), dtype=int64, min=253228657.0, max=1992448462.0, mean=1167900953.391),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'infos': np.ndarray((128,), dtype=object, head={'player_info': {'position': np.ndarray((2,), dtype=float32, min=0.755, max=2.933, mean=1.844), 'rotation_y': 277.00217, 'velocity': np.ndarray((2,), dtype=float32, min=-1.317, max=1.175, mean=-0.071)}, 'ball_info': {'position': np.ndarray((2,), dtype=float32, min=-6.025, max=6.849, mean=0.412), 'velocity': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0)}, 'ep_metrics': {'total_timesteps': 194, 'total_goals': 0, 'goals_opponent': 0, 'goals_in_favor': 0, 'team_0_goals': 0, 'team_1_goals': 0, 'episode_ended': False, 'have_goals': False, 'env_reward': 0.0, 'ball_to_goal_speed_reward': 0.0}}),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'new_obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'obs': np.ndarray((128, 336), dtype=float32, min=0.0, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'rewards': np.ndarray((128,), dtype=float32, min=0.0, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'unroll_id': np.ndarray((128,), dtype=int64, min=0.0, max=37.0, mean=19.633),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'value_targets': np.ndarray((128,), dtype=float32, min=-0.951, max=1.015, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m                                 'vf_preds': np.ndarray((128,), dtype=float32, min=-0.005, max=0.005, mean=0.0)}},\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=78457)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 31968\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.09728485505489814\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.032428285018299384\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: 0.0\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-45-20\n",
      "  done: false\n",
      "  episode_len_mean: 309.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.11174192541708239\n",
      "  episode_reward_mean: -0.2056598825681235\n",
      "  episode_reward_min: -0.8217776877294751\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.269151714324951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026881471447646618\n",
      "          policy_loss: -0.03726667866110802\n",
      "          total_loss: -0.018202843282371758\n",
      "          vf_explained_var: 0.187073215842247\n",
      "          vf_loss: 0.013687539633363485\n",
      "    num_agent_steps_sampled: 31968\n",
      "    num_agent_steps_trained: 31968\n",
      "    num_steps_sampled: 7992\n",
      "    num_steps_trained: 7992\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.271523178807946\n",
      "    gpu_util_percent0: 0.20576158940397354\n",
      "    ram_util_percent: 80.5569536423841\n",
      "    vram_util_percent0: 0.5050371658505652\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.3116560897171228\n",
      "  policy_reward_mean:\n",
      "    main: -0.051414970642030856\n",
      "  policy_reward_min:\n",
      "    main: -1.105631540047133\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2972476783840135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.18154081662019\n",
      "    mean_inference_ms: 4.45534454948124\n",
      "    mean_raw_obs_processing_ms: 0.8555528582602009\n",
      "  time_since_restore: 113.36343908309937\n",
      "  time_this_iter_s: 113.36343908309937\n",
      "  time_total_s: 113.36343908309937\n",
      "  timers:\n",
      "    learn_throughput: 90.687\n",
      "    learn_time_ms: 88127.105\n",
      "    sample_throughput: 316.9\n",
      "    sample_time_ms: 25219.324\n",
      "    update_time_ms: 3.416\n",
      "  timestamp: 1639158320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7992\n",
      "  training_iteration: 1\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         113.363</td><td style=\"text-align: right;\">7992</td><td style=\"text-align: right;\">-0.20566</td><td style=\"text-align: right;\">            0.111742</td><td style=\"text-align: right;\">           -0.821778</td><td style=\"text-align: right;\">               309</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 63936\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.24623458681754068\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.02060156918429559\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31071368664981575\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 836.7142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.2290840970655967\n",
      "  episode_reward_mean: -0.4992387167970335\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 14\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.2449067277908323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01993465358018875\n",
      "          policy_loss: -0.035254899866878985\n",
      "          total_loss: -0.01993135965615511\n",
      "          vf_explained_var: 0.2867244780063629\n",
      "          vf_loss: 0.0093431441700086\n",
      "    num_agent_steps_sampled: 63936\n",
      "    num_agent_steps_trained: 63936\n",
      "    num_steps_sampled: 15984\n",
      "    num_steps_trained: 15984\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.84285714285715\n",
      "    gpu_util_percent0: 0.19210884353741495\n",
      "    ram_util_percent: 83.24557823129251\n",
      "    vram_util_percent0: 0.5042195226231329\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.3116560897171228\n",
      "  policy_reward_mean:\n",
      "    main: -0.12480967919925837\n",
      "  policy_reward_min:\n",
      "    main: -1.3141088474364648\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29788115592341535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.209834233622704\n",
      "    mean_inference_ms: 4.432298426428355\n",
      "    mean_raw_obs_processing_ms: 0.8890338125147267\n",
      "  time_since_restore: 225.57915425300598\n",
      "  time_this_iter_s: 112.21571516990662\n",
      "  time_total_s: 225.57915425300598\n",
      "  timers:\n",
      "    learn_throughput: 91.364\n",
      "    learn_time_ms: 87473.906\n",
      "    sample_throughput: 317.301\n",
      "    sample_time_ms: 25187.431\n",
      "    update_time_ms: 3.334\n",
      "  timestamp: 1639158432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15984\n",
      "  training_iteration: 2\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         225.579</td><td style=\"text-align: right;\">15984</td><td style=\"text-align: right;\">-0.499239</td><td style=\"text-align: right;\">            0.229084</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           836.714</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 95904\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.24623458681754068\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: -0.025806652258028372\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31071368664981575\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 775.2380952380952\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4459429308364111\n",
      "  episode_reward_mean: -0.43640088748686007\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 21\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.219842350959778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019415135495364666\n",
      "          policy_loss: -0.035688108269125225\n",
      "          total_loss: -0.014043258761987091\n",
      "          vf_explained_var: 0.24510955810546875\n",
      "          vf_loss: 0.01582030801847577\n",
      "    num_agent_steps_sampled: 95904\n",
      "    num_agent_steps_trained: 95904\n",
      "    num_steps_sampled: 23976\n",
      "    num_steps_trained: 23976\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.46938775510204\n",
      "    gpu_util_percent0: 0.14537414965986392\n",
      "    ram_util_percent: 83.65034013605441\n",
      "    vram_util_percent0: 0.50163408568429\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.3116560897171228\n",
      "  policy_reward_mean:\n",
      "    main: -0.10910022187171502\n",
      "  policy_reward_min:\n",
      "    main: -1.3141088474364648\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2946153802240848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.629242476638346\n",
      "    mean_inference_ms: 4.375779998988665\n",
      "    mean_raw_obs_processing_ms: 0.892060846918685\n",
      "  time_since_restore: 336.87161588668823\n",
      "  time_this_iter_s: 111.29246163368225\n",
      "  time_total_s: 336.87161588668823\n",
      "  timers:\n",
      "    learn_throughput: 92.698\n",
      "    learn_time_ms: 86215.437\n",
      "    sample_throughput: 308.489\n",
      "    sample_time_ms: 25906.927\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1639158543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23976\n",
      "  training_iteration: 3\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         336.872</td><td style=\"text-align: right;\">23976</td><td style=\"text-align: right;\">-0.436401</td><td style=\"text-align: right;\">            0.445943</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           775.238</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 127872\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.3513794674076383\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.010707858583108246\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31071368664981575\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-51-03\n",
      "  done: false\n",
      "  episode_len_mean: 715.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8576535264391476\n",
      "  episode_reward_mean: -0.28574559436406305\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 38\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1988379011154175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01969517620652914\n",
      "          policy_loss: -0.0359469651337713\n",
      "          total_loss: -0.00419734208099544\n",
      "          vf_explained_var: 0.24651336669921875\n",
      "          vf_loss: 0.0258410703483969\n",
      "    num_agent_steps_sampled: 127872\n",
      "    num_agent_steps_trained: 127872\n",
      "    num_steps_sampled: 31968\n",
      "    num_steps_trained: 31968\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.45031847133758\n",
      "    gpu_util_percent0: 0.17605095541401275\n",
      "    ram_util_percent: 83.99617834394904\n",
      "    vram_util_percent0: 0.5091957419784834\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.07143639859101576\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2957726943755666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.086065858373157\n",
      "    mean_inference_ms: 4.37810492008645\n",
      "    mean_raw_obs_processing_ms: 0.9068479683254441\n",
      "  time_since_restore: 456.9230012893677\n",
      "  time_this_iter_s: 120.05138540267944\n",
      "  time_total_s: 456.9230012893677\n",
      "  timers:\n",
      "    learn_throughput: 90.942\n",
      "    learn_time_ms: 87879.798\n",
      "    sample_throughput: 305.477\n",
      "    sample_time_ms: 26162.372\n",
      "    update_time_ms: 3.093\n",
      "  timestamp: 1639158663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31968\n",
      "  training_iteration: 4\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         456.923</td><td style=\"text-align: right;\">31968</td><td style=\"text-align: right;\">-0.285746</td><td style=\"text-align: right;\">            0.857654</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">             715.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 159840\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.3805253480926408\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.026408206733916688\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31071368664981575\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 718.8085106382979\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8576535264391476\n",
      "  episode_reward_mean: -0.31118105012600555\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 47\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1771447057724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0186182245016098\n",
      "          policy_loss: -0.033689037773758174\n",
      "          total_loss: -0.014091512210667134\n",
      "          vf_explained_var: 0.22330619394779205\n",
      "          vf_loss: 0.014012057656422258\n",
      "    num_agent_steps_sampled: 159840\n",
      "    num_agent_steps_trained: 159840\n",
      "    num_steps_sampled: 39960\n",
      "    num_steps_trained: 39960\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.367721518987345\n",
      "    gpu_util_percent0: 0.17386075949367089\n",
      "    ram_util_percent: 85.47151898734177\n",
      "    vram_util_percent0: 0.5076245100611878\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.07779526253150142\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.29708979330593166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.2288204876224\n",
      "    mean_inference_ms: 4.398713083038132\n",
      "    mean_raw_obs_processing_ms: 0.9151612679741385\n",
      "  time_since_restore: 578.0038597583771\n",
      "  time_this_iter_s: 121.0808584690094\n",
      "  time_total_s: 578.0038597583771\n",
      "  timers:\n",
      "    learn_throughput: 89.812\n",
      "    learn_time_ms: 88985.407\n",
      "    sample_throughput: 302.577\n",
      "    sample_time_ms: 26413.078\n",
      "    update_time_ms: 3.152\n",
      "  timestamp: 1639158784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39960\n",
      "  training_iteration: 5\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         578.004</td><td style=\"text-align: right;\">39960</td><td style=\"text-align: right;\">-0.311181</td><td style=\"text-align: right;\">            0.857654</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           718.809</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 191808\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.3805253480926408\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.023656723958151764\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31527870872636465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 695.9032258064516\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8576535264391476\n",
      "  episode_reward_mean: -0.26271791876276573\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 62\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1517552518844605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021433729700744152\n",
      "          policy_loss: -0.03925465269759297\n",
      "          total_loss: -0.00803753286972642\n",
      "          vf_explained_var: 0.31485024094581604\n",
      "          vf_loss: 0.02478700090572238\n",
      "    num_agent_steps_sampled: 191808\n",
      "    num_agent_steps_trained: 191808\n",
      "    num_steps_sampled: 47952\n",
      "    num_steps_trained: 47952\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.3891719745223\n",
      "    gpu_util_percent0: 0.17853503184713376\n",
      "    ram_util_percent: 85.35859872611465\n",
      "    vram_util_percent0: 0.5097532502101135\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.06567947969069145\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2998508374602074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.46431045104821\n",
      "    mean_inference_ms: 4.438645555602727\n",
      "    mean_raw_obs_processing_ms: 0.9320424469984262\n",
      "  time_since_restore: 698.0769155025482\n",
      "  time_this_iter_s: 120.07305574417114\n",
      "  time_total_s: 698.0769155025482\n",
      "  timers:\n",
      "    learn_throughput: 89.46\n",
      "    learn_time_ms: 89336.026\n",
      "    sample_throughput: 298.231\n",
      "    sample_time_ms: 26797.99\n",
      "    update_time_ms: 3.084\n",
      "  timestamp: 1639158905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47952\n",
      "  training_iteration: 6\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         698.077</td><td style=\"text-align: right;\">47952</td><td style=\"text-align: right;\">-0.262718</td><td style=\"text-align: right;\">            0.857654</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           695.903</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 223776\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.4531517158426204\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02331241706010743\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.31527870872636465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 708.5277777777778\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.24505779723393928\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 72\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.143507083892822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016934656333178283\n",
      "          policy_loss: -0.03751481519080699\n",
      "          total_loss: -0.01640899748913944\n",
      "          vf_explained_var: 0.3065681457519531\n",
      "          vf_loss: 0.01348522255755961\n",
      "    num_agent_steps_sampled: 223776\n",
      "    num_agent_steps_trained: 223776\n",
      "    num_steps_sampled: 55944\n",
      "    num_steps_trained: 55944\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.59387755102041\n",
      "    gpu_util_percent0: 0.14761904761904765\n",
      "    ram_util_percent: 85.16666666666666\n",
      "    vram_util_percent0: 0.5107648192544538\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.06126444930848482\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30118815950246536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.556455391975142\n",
      "    mean_inference_ms: 4.454070316950992\n",
      "    mean_raw_obs_processing_ms: 0.9379998305992794\n",
      "  time_since_restore: 810.7414877414703\n",
      "  time_this_iter_s: 112.66457223892212\n",
      "  time_total_s: 810.7414877414703\n",
      "  timers:\n",
      "    learn_throughput: 90.001\n",
      "    learn_time_ms: 88798.948\n",
      "    sample_throughput: 298.191\n",
      "    sample_time_ms: 26801.633\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1639159017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55944\n",
      "  training_iteration: 7\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         810.741</td><td style=\"text-align: right;\">55944</td><td style=\"text-align: right;\">-0.245058</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           708.528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 255744\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.4531517158426204\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.016839161085087036\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_14-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 693.5421686746988\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.23651238361561533\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 83\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1195417165756227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01657498499378562\n",
      "          policy_loss: -0.03763402140513063\n",
      "          total_loss: -0.01679282421991229\n",
      "          vf_explained_var: 0.3539659380912781\n",
      "          vf_loss: 0.013382455070503056\n",
      "    num_agent_steps_sampled: 255744\n",
      "    num_agent_steps_trained: 255744\n",
      "    num_steps_sampled: 63936\n",
      "    num_steps_trained: 63936\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.383561643835616\n",
      "    gpu_util_percent0: 0.1454109589041096\n",
      "    ram_util_percent: 85.20479452054794\n",
      "    vram_util_percent0: 0.5068414267490658\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.05912809590390384\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30201293578631183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.590411568427538\n",
      "    mean_inference_ms: 4.463630051728101\n",
      "    mean_raw_obs_processing_ms: 0.9439096833580823\n",
      "  time_since_restore: 921.9237551689148\n",
      "  time_this_iter_s: 111.18226742744446\n",
      "  time_total_s: 921.9237551689148\n",
      "  timers:\n",
      "    learn_throughput: 90.333\n",
      "    learn_time_ms: 88472.997\n",
      "    sample_throughput: 301.12\n",
      "    sample_time_ms: 26540.937\n",
      "    update_time_ms: 2.989\n",
      "  timestamp: 1639159129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63936\n",
      "  training_iteration: 8\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         921.924</td><td style=\"text-align: right;\">63936</td><td style=\"text-align: right;\">-0.236512</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           693.542</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 287712\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5265592951549064\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.031999395938698154\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 719.3473684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.2100777564140678\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 95\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.1010805931091308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016118733443319797\n",
      "          policy_loss: -0.035631996054202315\n",
      "          total_loss: -0.01985836370661855\n",
      "          vf_explained_var: 0.2940879464149475\n",
      "          vf_loss: 0.008520201961509884\n",
      "    num_agent_steps_sampled: 287712\n",
      "    num_agent_steps_trained: 287712\n",
      "    num_steps_sampled: 71928\n",
      "    num_steps_trained: 71928\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.967567567567574\n",
      "    gpu_util_percent0: 0.14945945945945946\n",
      "    ram_util_percent: 85.2081081081081\n",
      "    vram_util_percent0: 0.5023011659240683\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.05251943910351694\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3024339705391735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.58984190310926\n",
      "    mean_inference_ms: 4.466656605632659\n",
      "    mean_raw_obs_processing_ms: 0.9478947095484774\n",
      "  time_since_restore: 1034.4489135742188\n",
      "  time_this_iter_s: 112.52515840530396\n",
      "  time_total_s: 1034.4489135742188\n",
      "  timers:\n",
      "    learn_throughput: 90.486\n",
      "    learn_time_ms: 88322.827\n",
      "    sample_throughput: 302.949\n",
      "    sample_time_ms: 26380.675\n",
      "    update_time_ms: 2.965\n",
      "  timestamp: 1639159241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71928\n",
      "  training_iteration: 9\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1034.45</td><td style=\"text-align: right;\">71928</td><td style=\"text-align: right;\">-0.210078</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">           719.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 319680\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03790285296298854\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 740.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.21492309447811503\n",
      "  episode_reward_min: -1.9025360697938178\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 102\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.082317168235779\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016452342450618745\n",
      "          policy_loss: -0.035876040011644364\n",
      "          total_loss: -0.021601302056573332\n",
      "          vf_explained_var: 0.32709190249443054\n",
      "          vf_loss: 0.0068711841036565606\n",
      "    num_agent_steps_sampled: 319680\n",
      "    num_agent_steps_trained: 319680\n",
      "    num_steps_sampled: 79920\n",
      "    num_steps_trained: 79920\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.87777777777778\n",
      "    gpu_util_percent0: 0.14215277777777777\n",
      "    ram_util_percent: 85.25624999999998\n",
      "    vram_util_percent0: 0.5025867427150744\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.05373077361952877\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30256377485035857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.602515569426874\n",
      "    mean_inference_ms: 4.466009146438937\n",
      "    mean_raw_obs_processing_ms: 0.9512961759844717\n",
      "  time_since_restore: 1144.9232294559479\n",
      "  time_this_iter_s: 110.47431588172913\n",
      "  time_total_s: 1144.9232294559479\n",
      "  timers:\n",
      "    learn_throughput: 90.771\n",
      "    learn_time_ms: 88046.103\n",
      "    sample_throughput: 304.966\n",
      "    sample_time_ms: 26206.212\n",
      "    update_time_ms: 2.939\n",
      "  timestamp: 1639159352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79920\n",
      "  training_iteration: 10\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1144.92</td><td style=\"text-align: right;\">79920</td><td style=\"text-align: right;\">-0.214923</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">            -1.90254</td><td style=\"text-align: right;\">            740.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 351648\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05085254738605124\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 725.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.1372994479067455\n",
      "  episode_reward_min: -1.5974366302777525\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 112\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.062376546859741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016701875060796737\n",
      "          policy_loss: -0.03798035593703389\n",
      "          total_loss: -0.02268302944302559\n",
      "          vf_explained_var: 0.3740890920162201\n",
      "          vf_loss: 0.00778148251120001\n",
      "    num_agent_steps_sampled: 351648\n",
      "    num_agent_steps_trained: 351648\n",
      "    num_steps_sampled: 87912\n",
      "    num_steps_trained: 87912\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.994482758620688\n",
      "    gpu_util_percent0: 0.1426206896551724\n",
      "    ram_util_percent: 85.28068965517242\n",
      "    vram_util_percent0: 0.500402809454108\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.034324861976686376\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30294683980148407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.704746788811637\n",
      "    mean_inference_ms: 4.467489501992441\n",
      "    mean_raw_obs_processing_ms: 0.9593218875044732\n",
      "  time_since_restore: 1255.4396181106567\n",
      "  time_this_iter_s: 110.51638865470886\n",
      "  time_total_s: 1255.4396181106567\n",
      "  timers:\n",
      "    learn_throughput: 91.077\n",
      "    learn_time_ms: 87749.482\n",
      "    sample_throughput: 305.184\n",
      "    sample_time_ms: 26187.521\n",
      "    update_time_ms: 2.856\n",
      "  timestamp: 1639159462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87912\n",
      "  training_iteration: 11\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         1255.44</td><td style=\"text-align: right;\">87912</td><td style=\"text-align: right;\">-0.137299</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">            -1.59744</td><td style=\"text-align: right;\">            725.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 383616\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06523087917057578\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 705.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.13733928360578104\n",
      "  episode_reward_min: -1.677296420066651\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 128\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.0411830492019654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01722344350814819\n",
      "          policy_loss: -0.03935365657880902\n",
      "          total_loss: -0.004843003684654832\n",
      "          vf_explained_var: 0.41606763005256653\n",
      "          vf_loss: 0.026760104194283485\n",
      "    num_agent_steps_sampled: 383616\n",
      "    num_agent_steps_trained: 383616\n",
      "    num_steps_sampled: 95904\n",
      "    num_steps_trained: 95904\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.46734693877551\n",
      "    gpu_util_percent0: 0.15183673469387754\n",
      "    ram_util_percent: 86.05374149659865\n",
      "    vram_util_percent0: 0.5025675291505217\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.569226428756731\n",
      "  policy_reward_mean:\n",
      "    main: -0.03433482090144527\n",
      "  policy_reward_min:\n",
      "    main: -1.5546648371511231\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3041377766234589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.659521994398624\n",
      "    mean_inference_ms: 4.4791002548744245\n",
      "    mean_raw_obs_processing_ms: 0.9688151008522903\n",
      "  time_since_restore: 1370.6443696022034\n",
      "  time_this_iter_s: 115.20475149154663\n",
      "  time_total_s: 1370.6443696022034\n",
      "  timers:\n",
      "    learn_throughput: 90.879\n",
      "    learn_time_ms: 87941.062\n",
      "    sample_throughput: 305.28\n",
      "    sample_time_ms: 26179.261\n",
      "    update_time_ms: 78.345\n",
      "  timestamp: 1639159578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95904\n",
      "  training_iteration: 12\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 15:06:19,659\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2021-12-10 15:06:19,669\tWARNING util.py:161 -- The `process_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "2021-12-10 15:06:19,670\tWARNING util.py:161 -- Processing trial results took 1.426 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2021-12-10 15:06:19,672\tWARNING util.py:161 -- The `process_trial` operation took 1.430 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         1370.64</td><td style=\"text-align: right;\">95904</td><td style=\"text-align: right;\">-0.137339</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">             -1.6773</td><td style=\"text-align: right;\">            705.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 415584\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07393095056803207\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 712.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.13989980881760383\n",
      "  episode_reward_min: -1.677296420066651\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 138\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 3.024588822364807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016043229557573795\n",
      "          policy_loss: -0.040484351862221955\n",
      "          total_loss: -0.016501946356147528\n",
      "          vf_explained_var: 0.3681725859642029\n",
      "          vf_loss: 0.01676295275054872\n",
      "    num_agent_steps_sampled: 415584\n",
      "    num_agent_steps_trained: 415584\n",
      "    num_steps_sampled: 103896\n",
      "    num_steps_trained: 103896\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.3620253164557\n",
      "    gpu_util_percent0: 0.2008227848101266\n",
      "    ram_util_percent: 91.12088607594939\n",
      "    vram_util_percent0: 0.5159914778762167\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.4858540749926057\n",
      "  policy_reward_mean:\n",
      "    main: -0.034974952204400965\n",
      "  policy_reward_min:\n",
      "    main: -1.4851367887479645\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30503652996519115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.618326831248474\n",
      "    mean_inference_ms: 4.488153862678034\n",
      "    mean_raw_obs_processing_ms: 0.9821709721528507\n",
      "  time_since_restore: 1493.1214480400085\n",
      "  time_this_iter_s: 122.47707843780518\n",
      "  time_total_s: 1493.1214480400085\n",
      "  timers:\n",
      "    learn_throughput: 89.802\n",
      "    learn_time_ms: 88995.6\n",
      "    sample_throughput: 304.643\n",
      "    sample_time_ms: 26233.964\n",
      "    update_time_ms: 78.375\n",
      "  timestamp: 1639159702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103896\n",
      "  training_iteration: 13\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         1493.12</td><td style=\"text-align: right;\">103896</td><td style=\"text-align: right;\"> -0.1399</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">             -1.6773</td><td style=\"text-align: right;\">            712.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 447552\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08458801065545567\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 697.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9217838478646715\n",
      "  episode_reward_mean: -0.061765902312208555\n",
      "  episode_reward_min: -1.677296420066651\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 155\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.997699625968933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01695074559003115\n",
      "          policy_loss: -0.039709417002275586\n",
      "          total_loss: -0.00738648733869195\n",
      "          vf_explained_var: 0.39230406284332275\n",
      "          vf_loss: 0.024695093777030705\n",
      "    num_agent_steps_sampled: 447552\n",
      "    num_agent_steps_trained: 447552\n",
      "    num_steps_sampled: 111888\n",
      "    num_steps_trained: 111888\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.555128205128206\n",
      "    gpu_util_percent0: 0.18102564102564103\n",
      "    ram_util_percent: 91.14358974358976\n",
      "    vram_util_percent0: 0.5162755967296935\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.5757902622806705\n",
      "  policy_reward_mean:\n",
      "    main: -0.015441475578052142\n",
      "  policy_reward_min:\n",
      "    main: -1.4851367887479645\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30462425802953486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.486146660747956\n",
      "    mean_inference_ms: 4.476483045826013\n",
      "    mean_raw_obs_processing_ms: 0.9996673535110437\n",
      "  time_since_restore: 1613.3169212341309\n",
      "  time_this_iter_s: 120.19547319412231\n",
      "  time_total_s: 1613.3169212341309\n",
      "  timers:\n",
      "    learn_throughput: 89.832\n",
      "    learn_time_ms: 88965.809\n",
      "    sample_throughput: 304.225\n",
      "    sample_time_ms: 26270.069\n",
      "    update_time_ms: 78.402\n",
      "  timestamp: 1639159822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111888\n",
      "  training_iteration: 14\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         1613.32</td><td style=\"text-align: right;\">111888</td><td style=\"text-align: right;\">-0.0617659</td><td style=\"text-align: right;\">            0.921784</td><td style=\"text-align: right;\">             -1.6773</td><td style=\"text-align: right;\">             697.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 479520\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7109492516866475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09219621929332224\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47843032964343485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-12-20\n",
      "  done: false\n",
      "  episode_len_mean: 634.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8805780482511579\n",
      "  episode_reward_mean: -0.06512175199083113\n",
      "  episode_reward_min: -1.677296420066651\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 173\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.9718074111938475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016694580923765898\n",
      "          policy_loss: -0.0421007083170116\n",
      "          total_loss: 0.002640216127038002\n",
      "          vf_explained_var: 0.4035874009132385\n",
      "          vf_loss: 0.03722836209833622\n",
      "    num_agent_steps_sampled: 479520\n",
      "    num_agent_steps_trained: 479520\n",
      "    num_steps_sampled: 119880\n",
      "    num_steps_trained: 119880\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.83051948051948\n",
      "    gpu_util_percent0: 0.15954545454545455\n",
      "    ram_util_percent: 88.37012987012989\n",
      "    vram_util_percent0: 0.5084251056610386\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.5757902622806705\n",
      "  policy_reward_mean:\n",
      "    main: -0.01628043799770779\n",
      "  policy_reward_min:\n",
      "    main: -1.4851367887479645\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3042015895532526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.364862853764194\n",
      "    mean_inference_ms: 4.463104699378061\n",
      "    mean_raw_obs_processing_ms: 1.0150409415566033\n",
      "  time_since_restore: 1731.2466599941254\n",
      "  time_this_iter_s: 117.9297387599945\n",
      "  time_total_s: 1731.2466599941254\n",
      "  timers:\n",
      "    learn_throughput: 90.348\n",
      "    learn_time_ms: 88458.145\n",
      "    sample_throughput: 302.083\n",
      "    sample_time_ms: 26456.334\n",
      "    update_time_ms: 78.32\n",
      "  timestamp: 1639159940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119880\n",
      "  training_iteration: 15\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         1731.25</td><td style=\"text-align: right;\">119880</td><td style=\"text-align: right;\">-0.0651218</td><td style=\"text-align: right;\">            0.880578</td><td style=\"text-align: right;\">             -1.6773</td><td style=\"text-align: right;\">            634.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 511488\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5997550873148613\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0902458090489127\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4573456022342602\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 516.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8805780482511579\n",
      "  episode_reward_mean: -0.03697233697707178\n",
      "  episode_reward_min: -1.677296420066651\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 199\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.957978724479675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017067170862108468\n",
      "          policy_loss: -0.04212025077268481\n",
      "          total_loss: 0.02886438350379467\n",
      "          vf_explained_var: 0.37294113636016846\n",
      "          vf_loss: 0.06330440659821034\n",
      "    num_agent_steps_sampled: 511488\n",
      "    num_agent_steps_trained: 511488\n",
      "    num_steps_sampled: 127872\n",
      "    num_steps_trained: 127872\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.288888888888884\n",
      "    gpu_util_percent0: 0.13083333333333333\n",
      "    ram_util_percent: 89.09722222222223\n",
      "    vram_util_percent0: 0.5059984095645498\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.5757902622806705\n",
      "  policy_reward_mean:\n",
      "    main: -0.009243084244267941\n",
      "  policy_reward_min:\n",
      "    main: -1.4808745961830003\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3042287596092272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.298480577811524\n",
      "    mean_inference_ms: 4.459470183197848\n",
      "    mean_raw_obs_processing_ms: 1.0384122355847223\n",
      "  time_since_restore: 1842.0986807346344\n",
      "  time_this_iter_s: 110.85202074050903\n",
      "  time_total_s: 1842.0986807346344\n",
      "  timers:\n",
      "    learn_throughput: 90.946\n",
      "    learn_time_ms: 87876.255\n",
      "    sample_throughput: 306.089\n",
      "    sample_time_ms: 26110.095\n",
      "    update_time_ms: 78.304\n",
      "  timestamp: 1639160051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127872\n",
      "  training_iteration: 16\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">          1842.1</td><td style=\"text-align: right;\">127872</td><td style=\"text-align: right;\">-0.0369723</td><td style=\"text-align: right;\">            0.880578</td><td style=\"text-align: right;\">             -1.6773</td><td style=\"text-align: right;\">            516.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 543456\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5997550873148613\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07957117672390206\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5230697811764543\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 424.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8910615750749931\n",
      "  episode_reward_mean: -0.08892658800960236\n",
      "  episode_reward_min: -3.0122025516785746\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 226\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.941058054924011\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016658195696771145\n",
      "          policy_loss: -0.044236705377697944\n",
      "          total_loss: 0.018160746958106756\n",
      "          vf_explained_var: 0.4169239401817322\n",
      "          vf_loss: 0.05490126598626375\n",
      "    num_agent_steps_sampled: 543456\n",
      "    num_agent_steps_trained: 543456\n",
      "    num_steps_sampled: 135864\n",
      "    num_steps_trained: 135864\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.063636363636366\n",
      "    gpu_util_percent0: 0.1334965034965035\n",
      "    ram_util_percent: 89.22657342657344\n",
      "    vram_util_percent0: 0.5054259659392927\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6601099938853245\n",
      "  policy_reward_mean:\n",
      "    main: -0.022231647002400597\n",
      "  policy_reward_min:\n",
      "    main: -1.5597736991236992\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30490036005351706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.31475000009967\n",
      "    mean_inference_ms: 4.466051078640844\n",
      "    mean_raw_obs_processing_ms: 1.063085408074569\n",
      "  time_since_restore: 1951.5445914268494\n",
      "  time_this_iter_s: 109.44591069221497\n",
      "  time_total_s: 1951.5445914268494\n",
      "  timers:\n",
      "    learn_throughput: 91.057\n",
      "    learn_time_ms: 87769.507\n",
      "    sample_throughput: 308.721\n",
      "    sample_time_ms: 25887.423\n",
      "    update_time_ms: 78.314\n",
      "  timestamp: 1639160160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135864\n",
      "  training_iteration: 17\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         1951.54</td><td style=\"text-align: right;\">135864</td><td style=\"text-align: right;\">-0.0889266</td><td style=\"text-align: right;\">            0.891062</td><td style=\"text-align: right;\">             -3.0122</td><td style=\"text-align: right;\">            424.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 575424\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5997550873148613\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06413194695101297\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5992973890040367\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 303.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2189818340163963\n",
      "  episode_reward_mean: -0.10471627265551225\n",
      "  episode_reward_min: -3.0122025516785746\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 262\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.909072337150574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017509805560111998\n",
      "          policy_loss: -0.043965283591300246\n",
      "          total_loss: 0.05347798370197415\n",
      "          vf_explained_var: 0.41939935088157654\n",
      "          vf_loss: 0.08956385505199432\n",
      "    num_agent_steps_sampled: 575424\n",
      "    num_agent_steps_trained: 575424\n",
      "    num_steps_sampled: 143856\n",
      "    num_steps_trained: 143856\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.33450704225352\n",
      "    gpu_util_percent0: 0.13380281690140847\n",
      "    ram_util_percent: 89.1218309859155\n",
      "    vram_util_percent0: 0.5027286125439706\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6601099938853245\n",
      "  policy_reward_mean:\n",
      "    main: -0.026179068163878064\n",
      "  policy_reward_min:\n",
      "    main: -1.5992973890040367\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3052285113684155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.2814035999037\n",
      "    mean_inference_ms: 4.464110675812883\n",
      "    mean_raw_obs_processing_ms: 1.061202766247938\n",
      "  time_since_restore: 2061.051992416382\n",
      "  time_this_iter_s: 109.50740098953247\n",
      "  time_total_s: 2061.051992416382\n",
      "  timers:\n",
      "    learn_throughput: 91.248\n",
      "    learn_time_ms: 87585.42\n",
      "    sample_throughput: 308.541\n",
      "    sample_time_ms: 25902.516\n",
      "    update_time_ms: 78.312\n",
      "  timestamp: 1639160270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143856\n",
      "  training_iteration: 18\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         2061.05</td><td style=\"text-align: right;\">143856</td><td style=\"text-align: right;\">-0.104716</td><td style=\"text-align: right;\">             1.21898</td><td style=\"text-align: right;\">             -3.0122</td><td style=\"text-align: right;\">            303.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 607392\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7329101350694548\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.052136528826457536\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5992973890040367\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 250.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2189818340163963\n",
      "  episode_reward_mean: -0.10664419155304226\n",
      "  episode_reward_min: -3.0122025516785746\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 297\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8785661487579346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016775329545140265\n",
      "          policy_loss: -0.044903517860919236\n",
      "          total_loss: 0.05023160433769226\n",
      "          vf_explained_var: 0.44596582651138306\n",
      "          vf_loss: 0.08758622388541698\n",
      "    num_agent_steps_sampled: 607392\n",
      "    num_agent_steps_trained: 607392\n",
      "    num_steps_sampled: 151848\n",
      "    num_steps_trained: 151848\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.27132867132867\n",
      "    gpu_util_percent0: 0.1325874125874126\n",
      "    ram_util_percent: 89.07062937062938\n",
      "    vram_util_percent0: 0.4979934511030267\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6601099938853245\n",
      "  policy_reward_mean:\n",
      "    main: -0.026661047888260566\n",
      "  policy_reward_min:\n",
      "    main: -1.5992973890040367\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30430362859184024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.16014011116947\n",
      "    mean_inference_ms: 4.445073170867447\n",
      "    mean_raw_obs_processing_ms: 1.0561337448306\n",
      "  time_since_restore: 2170.0725350379944\n",
      "  time_this_iter_s: 109.02054262161255\n",
      "  time_total_s: 2170.0725350379944\n",
      "  timers:\n",
      "    learn_throughput: 91.568\n",
      "    learn_time_ms: 87279.751\n",
      "    sample_throughput: 309.1\n",
      "    sample_time_ms: 25855.737\n",
      "    update_time_ms: 78.296\n",
      "  timestamp: 1639160379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151848\n",
      "  training_iteration: 19\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         2170.07</td><td style=\"text-align: right;\">151848</td><td style=\"text-align: right;\">-0.106644</td><td style=\"text-align: right;\">             1.21898</td><td style=\"text-align: right;\">             -3.0122</td><td style=\"text-align: right;\">            250.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 639360\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7329101350694548\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06404511748613363\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9029321359531005\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-21-33\n",
      "  done: false\n",
      "  episode_len_mean: 237.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2189818340163963\n",
      "  episode_reward_mean: -0.045687319179986724\n",
      "  episode_reward_min: -1.9944709211565634\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 326\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.8709294233322145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016585361156612636\n",
      "          policy_loss: -0.04516520619019866\n",
      "          total_loss: 0.029614546110853554\n",
      "          vf_explained_var: 0.4233543276786804\n",
      "          vf_loss: 0.0673163390904665\n",
      "    num_agent_steps_sampled: 639360\n",
      "    num_agent_steps_trained: 639360\n",
      "    num_steps_sampled: 159840\n",
      "    num_steps_trained: 159840\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.73513513513514\n",
      "    gpu_util_percent0: 0.15736486486486487\n",
      "    ram_util_percent: 89.45202702702704\n",
      "    vram_util_percent0: 0.5044455857634534\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6305415437163302\n",
      "  policy_reward_mean:\n",
      "    main: -0.011421829794996684\n",
      "  policy_reward_min:\n",
      "    main: -1.5992973890040367\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3040994953641329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.084835267013727\n",
      "    mean_inference_ms: 4.435487233976947\n",
      "    mean_raw_obs_processing_ms: 1.0530273002494268\n",
      "  time_since_restore: 2284.3435587882996\n",
      "  time_this_iter_s: 114.27102375030518\n",
      "  time_total_s: 2284.3435587882996\n",
      "  timers:\n",
      "    learn_throughput: 91.213\n",
      "    learn_time_ms: 87618.664\n",
      "    sample_throughput: 308.639\n",
      "    sample_time_ms: 25894.302\n",
      "    update_time_ms: 78.293\n",
      "  timestamp: 1639160493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159840\n",
      "  training_iteration: 20\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         2284.34</td><td style=\"text-align: right;\">159840</td><td style=\"text-align: right;\">-0.0456873</td><td style=\"text-align: right;\">             1.21898</td><td style=\"text-align: right;\">            -1.99447</td><td style=\"text-align: right;\">            237.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 671328\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7329101350694548\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09923041798108077\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9029321359531005\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 231.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.8103790845533876\n",
      "  episode_reward_mean: -0.004615616593443983\n",
      "  episode_reward_min: -1.4792459759102874\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 369\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.817068691253662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01751513722166419\n",
      "          policy_loss: -0.047348526675254105\n",
      "          total_loss: 0.06095998245850205\n",
      "          vf_explained_var: 0.4341917037963867\n",
      "          vf_loss: 0.10042669822275639\n",
      "    num_agent_steps_sampled: 671328\n",
      "    num_agent_steps_trained: 671328\n",
      "    num_steps_sampled: 167832\n",
      "    num_steps_trained: 167832\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.268\n",
      "    gpu_util_percent0: 0.156\n",
      "    ram_util_percent: 89.57733333333334\n",
      "    vram_util_percent0: 0.5081090270922454\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6358916222152677\n",
      "  policy_reward_mean:\n",
      "    main: -0.0011539041483609913\n",
      "  policy_reward_min:\n",
      "    main: -1.5089841460046782\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3023582961009503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.987653595873816\n",
      "    mean_inference_ms: 4.412211795774328\n",
      "    mean_raw_obs_processing_ms: 1.0529146324095813\n",
      "  time_since_restore: 2398.4958748817444\n",
      "  time_this_iter_s: 114.15231609344482\n",
      "  time_total_s: 2398.4958748817444\n",
      "  timers:\n",
      "    learn_throughput: 90.901\n",
      "    learn_time_ms: 87919.618\n",
      "    sample_throughput: 307.91\n",
      "    sample_time_ms: 25955.653\n",
      "    update_time_ms: 78.318\n",
      "  timestamp: 1639160608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167832\n",
      "  training_iteration: 21\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          2398.5</td><td style=\"text-align: right;\">167832</td><td style=\"text-align: right;\">-0.00461562</td><td style=\"text-align: right;\">            0.810379</td><td style=\"text-align: right;\">            -1.47925</td><td style=\"text-align: right;\">            231.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 703296\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8437270039645854\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10688457741459388\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.839807942963664\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 185.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.932574707075625\n",
      "  episode_reward_mean: 0.06730597864202278\n",
      "  episode_reward_min: -1.205223379807408\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 428\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7610202007293703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016686666157096623\n",
      "          policy_loss: -0.04765400268882513\n",
      "          total_loss: 0.09024506000056863\n",
      "          vf_explained_var: 0.4732985496520996\n",
      "          vf_loss: 0.130390064150095\n",
      "    num_agent_steps_sampled: 703296\n",
      "    num_agent_steps_trained: 703296\n",
      "    num_steps_sampled: 175824\n",
      "    num_steps_trained: 175824\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.867763157894736\n",
      "    gpu_util_percent0: 0.18625000000000003\n",
      "    ram_util_percent: 89.73486842105264\n",
      "    vram_util_percent0: 0.5120733101262535\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7171269768802349\n",
      "  policy_reward_mean:\n",
      "    main: 0.016826494660505717\n",
      "  policy_reward_min:\n",
      "    main: -1.5278809977042336\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30232647379597144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.940263273218125\n",
      "    mean_inference_ms: 4.405782231795765\n",
      "    mean_raw_obs_processing_ms: 1.0544591927008253\n",
      "  time_since_restore: 2515.4803409576416\n",
      "  time_this_iter_s: 116.98446607589722\n",
      "  time_total_s: 2515.4803409576416\n",
      "  timers:\n",
      "    learn_throughput: 90.669\n",
      "    learn_time_ms: 88145.103\n",
      "    sample_throughput: 307.24\n",
      "    sample_time_ms: 26012.232\n",
      "    update_time_ms: 2.818\n",
      "  timestamp: 1639160725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175824\n",
      "  training_iteration: 22\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         2515.48</td><td style=\"text-align: right;\">175824</td><td style=\"text-align: right;\">0.067306</td><td style=\"text-align: right;\">            0.932575</td><td style=\"text-align: right;\">            -1.20522</td><td style=\"text-align: right;\">            185.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 735264\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8437270039645854\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1424227799617964\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.24260932429047677\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 126.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.932574707075625\n",
      "  episode_reward_mean: 0.08937245620887933\n",
      "  episode_reward_min: -2.246940910265278\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 481\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7700326137542723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017300563480705022\n",
      "          policy_loss: -0.04809417654573917\n",
      "          total_loss: 0.09164240579679608\n",
      "          vf_explained_var: 0.4643465578556061\n",
      "          vf_loss: 0.13195132839679719\n",
      "    num_agent_steps_sampled: 735264\n",
      "    num_agent_steps_trained: 735264\n",
      "    num_steps_sampled: 183816\n",
      "    num_steps_trained: 183816\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.40758620689655\n",
      "    gpu_util_percent0: 0.14406896551724138\n",
      "    ram_util_percent: 85.62068965517241\n",
      "    vram_util_percent0: 0.5073265933666926\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7171269768802349\n",
      "  policy_reward_mean:\n",
      "    main: 0.02234311405221984\n",
      "  policy_reward_min:\n",
      "    main: -1.936773606791123\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30178698657727127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.94593822707545\n",
      "    mean_inference_ms: 4.4071854497781\n",
      "    mean_raw_obs_processing_ms: 1.059632391812529\n",
      "  time_since_restore: 2627.2460005283356\n",
      "  time_this_iter_s: 111.76565957069397\n",
      "  time_total_s: 2627.2460005283356\n",
      "  timers:\n",
      "    learn_throughput: 91.717\n",
      "    learn_time_ms: 87137.152\n",
      "    sample_throughput: 307.941\n",
      "    sample_time_ms: 25953.032\n",
      "    update_time_ms: 2.747\n",
      "  timestamp: 1639160837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183816\n",
      "  training_iteration: 23\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         2627.25</td><td style=\"text-align: right;\">183816</td><td style=\"text-align: right;\">0.0893725</td><td style=\"text-align: right;\">            0.932575</td><td style=\"text-align: right;\">            -2.24694</td><td style=\"text-align: right;\">             126.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 767232\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7183933097989852\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10761680694016858\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49826279149159725\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 126.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9985830055862761\n",
      "  episode_reward_mean: 0.0743004111433906\n",
      "  episode_reward_min: -2.246940910265278\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 545\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6890694246292113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01737684315815568\n",
      "          policy_loss: -0.05076754614710808\n",
      "          total_loss: 0.10452828170731664\n",
      "          vf_explained_var: 0.48419052362442017\n",
      "          vf_loss: 0.147476248472929\n",
      "    num_agent_steps_sampled: 767232\n",
      "    num_agent_steps_trained: 767232\n",
      "    num_steps_sampled: 191808\n",
      "    num_steps_trained: 191808\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.66558441558441\n",
      "    gpu_util_percent0: 0.18948051948051947\n",
      "    ram_util_percent: 85.38116883116884\n",
      "    vram_util_percent0: 0.5049133985461725\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.691008440239513\n",
      "  policy_reward_mean:\n",
      "    main: 0.01857510278584764\n",
      "  policy_reward_min:\n",
      "    main: -1.936773606791123\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30164192006263596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.930203762455584\n",
      "    mean_inference_ms: 4.406763705480379\n",
      "    mean_raw_obs_processing_ms: 1.0615879890156206\n",
      "  time_since_restore: 2745.2834594249725\n",
      "  time_this_iter_s: 118.03745889663696\n",
      "  time_total_s: 2745.2834594249725\n",
      "  timers:\n",
      "    learn_throughput: 91.685\n",
      "    learn_time_ms: 87168.216\n",
      "    sample_throughput: 310.913\n",
      "    sample_time_ms: 25704.912\n",
      "    update_time_ms: 2.717\n",
      "  timestamp: 1639160955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191808\n",
      "  training_iteration: 24\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         2745.28</td><td style=\"text-align: right;\">191808</td><td style=\"text-align: right;\">0.0743004</td><td style=\"text-align: right;\">            0.998583</td><td style=\"text-align: right;\">            -2.24694</td><td style=\"text-align: right;\">            126.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 799200\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5636678695025463\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06322482055418936\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6075340994450533\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 130.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1427115969242434\n",
      "  episode_reward_mean: 0.07149776998470184\n",
      "  episode_reward_min: -2.3121816392503156\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 598\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.7203041400909425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017777338296175\n",
      "          policy_loss: -0.05120946476235986\n",
      "          total_loss: 0.06868247020989657\n",
      "          vf_explained_var: 0.5212036967277527\n",
      "          vf_loss: 0.1118921325802803\n",
      "    num_agent_steps_sampled: 799200\n",
      "    num_agent_steps_trained: 799200\n",
      "    num_steps_sampled: 199800\n",
      "    num_steps_trained: 199800\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.72960526315789\n",
      "    gpu_util_percent0: 0.16723684210526316\n",
      "    ram_util_percent: 85.4296052631579\n",
      "    vram_util_percent0: 0.508088922949031\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6315186036418932\n",
      "  policy_reward_mean:\n",
      "    main: 0.017874442496175456\n",
      "  policy_reward_min:\n",
      "    main: -1.6075340994450533\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3018857677273743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.946512348835657\n",
      "    mean_inference_ms: 4.411043775329464\n",
      "    mean_raw_obs_processing_ms: 1.0641498266302767\n",
      "  time_since_restore: 2861.7461726665497\n",
      "  time_this_iter_s: 116.46271324157715\n",
      "  time_total_s: 2861.7461726665497\n",
      "  timers:\n",
      "    learn_throughput: 91.695\n",
      "    learn_time_ms: 87158.545\n",
      "    sample_throughput: 312.603\n",
      "    sample_time_ms: 25565.951\n",
      "    update_time_ms: 2.724\n",
      "  timestamp: 1639161071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199800\n",
      "  training_iteration: 25\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         2861.75</td><td style=\"text-align: right;\">199800</td><td style=\"text-align: right;\">0.0714978</td><td style=\"text-align: right;\">             1.14271</td><td style=\"text-align: right;\">            -2.31218</td><td style=\"text-align: right;\">            130.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 831168\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.771068064432019\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05607933240581566\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6075340994450533\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 141.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.870708301059421\n",
      "  episode_reward_mean: -0.0053111612048622955\n",
      "  episode_reward_min: -2.3121816392503156\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 656\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.6730389347076415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017701614804565908\n",
      "          policy_loss: -0.05119945443421602\n",
      "          total_loss: 0.09441196924075484\n",
      "          vf_explained_var: 0.47167742252349854\n",
      "          vf_loss: 0.13764569640159607\n",
      "    num_agent_steps_sampled: 831168\n",
      "    num_agent_steps_trained: 831168\n",
      "    num_steps_sampled: 207792\n",
      "    num_steps_trained: 207792\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2303448275862\n",
      "    gpu_util_percent0: 0.12648275862068967\n",
      "    ram_util_percent: 85.47655172413795\n",
      "    vram_util_percent0: 0.5064165844027642\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6315186036418932\n",
      "  policy_reward_mean:\n",
      "    main: -0.0013277903012155724\n",
      "  policy_reward_min:\n",
      "    main: -1.6315854085521722\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3026111414683802\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.97179987039184\n",
      "    mean_inference_ms: 4.419308326621739\n",
      "    mean_raw_obs_processing_ms: 1.0676357200781288\n",
      "  time_since_restore: 2973.8768825531006\n",
      "  time_this_iter_s: 112.1307098865509\n",
      "  time_total_s: 2973.8768825531006\n",
      "  timers:\n",
      "    learn_throughput: 91.563\n",
      "    learn_time_ms: 87283.77\n",
      "    sample_throughput: 312.557\n",
      "    sample_time_ms: 25569.771\n",
      "    update_time_ms: 2.762\n",
      "  timestamp: 1639161183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207792\n",
      "  training_iteration: 26\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         2973.88</td><td style=\"text-align: right;\">207792</td><td style=\"text-align: right;\">-0.00531116</td><td style=\"text-align: right;\">            0.870708</td><td style=\"text-align: right;\">            -2.31218</td><td style=\"text-align: right;\">            141.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 863136\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.771068064432019\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09243537420106619\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5736994562224772\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-34-55\n",
      "  done: false\n",
      "  episode_len_mean: 150.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0793398397042826\n",
      "  episode_reward_mean: 0.03803548496732454\n",
      "  episode_reward_min: -1.4597340509679881\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 706\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.68263383102417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01736149314045906\n",
      "          policy_loss: -0.05185189560800791\n",
      "          total_loss: 0.07252437154203653\n",
      "          vf_explained_var: 0.46165093779563904\n",
      "          vf_loss: 0.11656359618902207\n",
      "    num_agent_steps_sampled: 863136\n",
      "    num_agent_steps_trained: 863136\n",
      "    num_steps_sampled: 215784\n",
      "    num_steps_trained: 215784\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.04931506849315\n",
      "    gpu_util_percent0: 0.125\n",
      "    ram_util_percent: 85.49383561643837\n",
      "    vram_util_percent0: 0.5042022204492285\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.674897928230616\n",
      "  policy_reward_mean:\n",
      "    main: 0.00950887124183113\n",
      "  policy_reward_min:\n",
      "    main: -1.6307814209186597\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3024715389551242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.94290682205232\n",
      "    mean_inference_ms: 4.41460976878072\n",
      "    mean_raw_obs_processing_ms: 1.069283588934181\n",
      "  time_since_restore: 3085.8215589523315\n",
      "  time_this_iter_s: 111.94467639923096\n",
      "  time_total_s: 3085.8215589523315\n",
      "  timers:\n",
      "    learn_throughput: 91.368\n",
      "    learn_time_ms: 87470.366\n",
      "    sample_throughput: 311.742\n",
      "    sample_time_ms: 25636.603\n",
      "    update_time_ms: 2.766\n",
      "  timestamp: 1639161295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215784\n",
      "  training_iteration: 27\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         3085.82</td><td style=\"text-align: right;\">215784</td><td style=\"text-align: right;\">0.0380355</td><td style=\"text-align: right;\">             1.07934</td><td style=\"text-align: right;\">            -1.45973</td><td style=\"text-align: right;\">            150.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 895104\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6921799614884523\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08776757250402464\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4663155306085428\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-36-53\n",
      "  done: false\n",
      "  episode_len_mean: 156.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1405092772458008\n",
      "  episode_reward_mean: 0.08461530655729672\n",
      "  episode_reward_min: -1.398619475200428\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 759\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.641651237487793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017943853940814734\n",
      "          policy_loss: -0.05500253295712173\n",
      "          total_loss: 0.07971281450241804\n",
      "          vf_explained_var: 0.4730664789676666\n",
      "          vf_loss: 0.12664061325788498\n",
      "    num_agent_steps_sampled: 895104\n",
      "    num_agent_steps_trained: 895104\n",
      "    num_steps_sampled: 223776\n",
      "    num_steps_trained: 223776\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.00588235294117\n",
      "    gpu_util_percent0: 0.17771241830065362\n",
      "    ram_util_percent: 85.60065359477125\n",
      "    vram_util_percent0: 0.5081683216228248\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8067302527477458\n",
      "  policy_reward_mean:\n",
      "    main: 0.02115382663932417\n",
      "  policy_reward_min:\n",
      "    main: -1.6080654739918443\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3016286589425953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.904093659505552\n",
      "    mean_inference_ms: 4.404428823222397\n",
      "    mean_raw_obs_processing_ms: 1.070877468956989\n",
      "  time_since_restore: 3203.434356212616\n",
      "  time_this_iter_s: 117.61279726028442\n",
      "  time_total_s: 3203.434356212616\n",
      "  timers:\n",
      "    learn_throughput: 90.628\n",
      "    learn_time_ms: 88184.252\n",
      "    sample_throughput: 310.647\n",
      "    sample_time_ms: 25726.951\n",
      "    update_time_ms: 2.786\n",
      "  timestamp: 1639161413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223776\n",
      "  training_iteration: 28\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         3203.43</td><td style=\"text-align: right;\">223776</td><td style=\"text-align: right;\">0.0846153</td><td style=\"text-align: right;\">             1.14051</td><td style=\"text-align: right;\">            -1.39862</td><td style=\"text-align: right;\">            156.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 927072\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6921799614884523\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04065200979685645\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4067520247798571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 137.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1405092772458008\n",
      "  episode_reward_mean: 0.05425221089074629\n",
      "  episode_reward_min: -1.849520038727472\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 819\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5858803968429567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017502691332250832\n",
      "          policy_loss: -0.05381397974491119\n",
      "          total_loss: 0.08580550410971045\n",
      "          vf_explained_var: 0.48265016078948975\n",
      "          vf_loss: 0.13174327284097673\n",
      "    num_agent_steps_sampled: 927072\n",
      "    num_agent_steps_trained: 927072\n",
      "    num_steps_sampled: 231768\n",
      "    num_steps_trained: 231768\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.69807692307692\n",
      "    gpu_util_percent0: 0.22282051282051285\n",
      "    ram_util_percent: 85.60833333333335\n",
      "    vram_util_percent0: 0.5035510584622135\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8067302527477458\n",
      "  policy_reward_mean:\n",
      "    main: 0.013563052722686576\n",
      "  policy_reward_min:\n",
      "    main: -1.6080654739918443\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3018053100731114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.91611868340351\n",
      "    mean_inference_ms: 4.405166197219585\n",
      "    mean_raw_obs_processing_ms: 1.0723223292722541\n",
      "  time_since_restore: 3324.26970744133\n",
      "  time_this_iter_s: 120.83535122871399\n",
      "  time_total_s: 3324.26970744133\n",
      "  timers:\n",
      "    learn_throughput: 89.685\n",
      "    learn_time_ms: 89112.067\n",
      "    sample_throughput: 307.613\n",
      "    sample_time_ms: 25980.68\n",
      "    update_time_ms: 2.787\n",
      "  timestamp: 1639161534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231768\n",
      "  training_iteration: 29\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         3324.27</td><td style=\"text-align: right;\">231768</td><td style=\"text-align: right;\">0.0542522</td><td style=\"text-align: right;\">             1.14051</td><td style=\"text-align: right;\">            -1.84952</td><td style=\"text-align: right;\">            137.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 959040\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7297111617201453\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08672498819057514\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4067520247798571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-41-11\n",
      "  done: false\n",
      "  episode_len_mean: 118.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.083146464167112\n",
      "  episode_reward_mean: 0.16698721565737557\n",
      "  episode_reward_min: -1.2925118274570742\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 892\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.5601398258209227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01817975875362754\n",
      "          policy_loss: -0.05675587859749794\n",
      "          total_loss: 0.09301567885279656\n",
      "          vf_explained_var: 0.5351426005363464\n",
      "          vf_loss: 0.14159066742658616\n",
      "    num_agent_steps_sampled: 959040\n",
      "    num_agent_steps_trained: 959040\n",
      "    num_steps_sampled: 239760\n",
      "    num_steps_trained: 239760\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.67885714285715\n",
      "    gpu_util_percent0: 0.18777142857142856\n",
      "    ram_util_percent: 90.61028571428574\n",
      "    vram_util_percent0: 0.515785267710243\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6715878276667588\n",
      "  policy_reward_mean:\n",
      "    main: 0.041746803914343905\n",
      "  policy_reward_min:\n",
      "    main: -1.6052254549556513\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3042580641885413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.167763346815523\n",
      "    mean_inference_ms: 4.44657590213242\n",
      "    mean_raw_obs_processing_ms: 1.0896894640981747\n",
      "  time_since_restore: 3460.8415632247925\n",
      "  time_this_iter_s: 136.57185578346252\n",
      "  time_total_s: 3460.8415632247925\n",
      "  timers:\n",
      "    learn_throughput: 88.475\n",
      "    learn_time_ms: 90330.171\n",
      "    sample_throughput: 296.1\n",
      "    sample_time_ms: 26990.868\n",
      "    update_time_ms: 2.779\n",
      "  timestamp: 1639161671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239760\n",
      "  training_iteration: 30\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         3460.84</td><td style=\"text-align: right;\">239760</td><td style=\"text-align: right;\">0.166987</td><td style=\"text-align: right;\">             1.08315</td><td style=\"text-align: right;\">            -1.29251</td><td style=\"text-align: right;\">             118.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 991008\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6722871554640288\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07844392079448068\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4076640462382134\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 111.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2448599792498334\n",
      "  episode_reward_mean: 0.08397222226237165\n",
      "  episode_reward_min: -1.1965523428205336\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 954\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.533053985595703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017927091382443906\n",
      "          policy_loss: -0.05828986755758524\n",
      "          total_loss: 0.0819535938706249\n",
      "          vf_explained_var: 0.49905306100845337\n",
      "          vf_loss: 0.13217627063393592\n",
      "    num_agent_steps_sampled: 991008\n",
      "    num_agent_steps_trained: 991008\n",
      "    num_steps_sampled: 247752\n",
      "    num_steps_trained: 247752\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.364999999999995\n",
      "    gpu_util_percent0: 0.20168750000000002\n",
      "    ram_util_percent: 90.639375\n",
      "    vram_util_percent0: 0.5209454179006252\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.5760987882044182\n",
      "  policy_reward_mean:\n",
      "    main: 0.020993055565592927\n",
      "  policy_reward_min:\n",
      "    main: -1.5034218251762153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3059543404702801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.29772078492213\n",
      "    mean_inference_ms: 4.4725711996633315\n",
      "    mean_raw_obs_processing_ms: 1.0982584524122454\n",
      "  time_since_restore: 3583.626063346863\n",
      "  time_this_iter_s: 122.78450012207031\n",
      "  time_total_s: 3583.626063346863\n",
      "  timers:\n",
      "    learn_throughput: 87.855\n",
      "    learn_time_ms: 90967.996\n",
      "    sample_throughput: 293.683\n",
      "    sample_time_ms: 27212.991\n",
      "    update_time_ms: 2.763\n",
      "  timestamp: 1639161794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247752\n",
      "  training_iteration: 31\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         3583.63</td><td style=\"text-align: right;\">247752</td><td style=\"text-align: right;\">0.0839722</td><td style=\"text-align: right;\">             1.24486</td><td style=\"text-align: right;\">            -1.19655</td><td style=\"text-align: right;\">            111.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1022976\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8603634830596627\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11519017689362414\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6226283454767704\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-45-13\n",
      "  done: false\n",
      "  episode_len_mean: 115.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2448599792498334\n",
      "  episode_reward_mean: 0.12563660910577978\n",
      "  episode_reward_min: -1.4534806654779606\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 1042\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.475295141220093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01754534276202321\n",
      "          policy_loss: -0.05766204323992133\n",
      "          total_loss: 0.13693456498906018\n",
      "          vf_explained_var: 0.5087024569511414\n",
      "          vf_loss: 0.1867012048959732\n",
      "    num_agent_steps_sampled: 1022976\n",
      "    num_agent_steps_trained: 1022976\n",
      "    num_steps_sampled: 255744\n",
      "    num_steps_trained: 255744\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7\n",
      "    gpu_util_percent0: 0.1907741935483871\n",
      "    ram_util_percent: 90.7258064516129\n",
      "    vram_util_percent0: 0.5182986763472706\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.712540037161465\n",
      "  policy_reward_mean:\n",
      "    main: 0.03140915227644494\n",
      "  policy_reward_min:\n",
      "    main: -1.6226283454767705\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3070339766505989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.35237812109678\n",
      "    mean_inference_ms: 4.4874631482616305\n",
      "    mean_raw_obs_processing_ms: 1.103477977762988\n",
      "  time_since_restore: 3703.308373451233\n",
      "  time_this_iter_s: 119.68231010437012\n",
      "  time_total_s: 3703.308373451233\n",
      "  timers:\n",
      "    learn_throughput: 87.77\n",
      "    learn_time_ms: 91056.556\n",
      "    sample_throughput: 291.696\n",
      "    sample_time_ms: 27398.415\n",
      "    update_time_ms: 2.716\n",
      "  timestamp: 1639161913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255744\n",
      "  training_iteration: 32\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         3703.31</td><td style=\"text-align: right;\">255744</td><td style=\"text-align: right;\">0.125637</td><td style=\"text-align: right;\">             1.24486</td><td style=\"text-align: right;\">            -1.45348</td><td style=\"text-align: right;\">            115.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1054944\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9075239175113855\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07793838753751972\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6226283454767704\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 95.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0938584137918874\n",
      "  episode_reward_mean: 0.15600843002202866\n",
      "  episode_reward_min: -1.7521982655652095\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.490173532485962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01708492437377572\n",
      "          policy_loss: -0.05755357035622001\n",
      "          total_loss: 0.09125354782491922\n",
      "          vf_explained_var: 0.5238393545150757\n",
      "          vf_loss: 0.14111890229582785\n",
      "    num_agent_steps_sampled: 1054944\n",
      "    num_agent_steps_trained: 1054944\n",
      "    num_steps_sampled: 263736\n",
      "    num_steps_trained: 263736\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.874213836477985\n",
      "    gpu_util_percent0: 0.190377358490566\n",
      "    ram_util_percent: 90.78301886792453\n",
      "    vram_util_percent0: 0.517319914486932\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.795817703508528\n",
      "  policy_reward_mean:\n",
      "    main: 0.039002107505507165\n",
      "  policy_reward_min:\n",
      "    main: -1.760691459899647\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3096737159202586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.453640189138245\n",
      "    mean_inference_ms: 4.517319649904736\n",
      "    mean_raw_obs_processing_ms: 1.1072442879487667\n",
      "  time_since_restore: 3825.3900802135468\n",
      "  time_this_iter_s: 122.08170676231384\n",
      "  time_total_s: 3825.3900802135468\n",
      "  timers:\n",
      "    learn_throughput: 86.98\n",
      "    learn_time_ms: 91883.469\n",
      "    sample_throughput: 289.616\n",
      "    sample_time_ms: 27595.207\n",
      "    update_time_ms: 2.764\n",
      "  timestamp: 1639162035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263736\n",
      "  training_iteration: 33\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         3825.39</td><td style=\"text-align: right;\">263736</td><td style=\"text-align: right;\">0.156008</td><td style=\"text-align: right;\">             1.09386</td><td style=\"text-align: right;\">             -1.7522</td><td style=\"text-align: right;\">             95.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1086912\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.826072581358564\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08095938037158802\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48254579581934476\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 125.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9789646511915058\n",
      "  episode_reward_mean: 0.09574572523461806\n",
      "  episode_reward_min: -1.7521982655652095\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 1177\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.469194839477539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0181138808503747\n",
      "          policy_loss: -0.05925543778762221\n",
      "          total_loss: 0.09408324491977692\n",
      "          vf_explained_var: 0.5178745985031128\n",
      "          vf_loss: 0.14518743589520455\n",
      "    num_agent_steps_sampled: 1086912\n",
      "    num_agent_steps_trained: 1086912\n",
      "    num_steps_sampled: 271728\n",
      "    num_steps_trained: 271728\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.37096774193548\n",
      "    gpu_util_percent0: 0.19051612903225806\n",
      "    ram_util_percent: 90.78516129032256\n",
      "    vram_util_percent0: 0.512254667813054\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7836800177507173\n",
      "  policy_reward_mean:\n",
      "    main: 0.023936431308654527\n",
      "  policy_reward_min:\n",
      "    main: -1.760691459899647\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30831244676451297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.448942775159882\n",
      "    mean_inference_ms: 4.505922013165144\n",
      "    mean_raw_obs_processing_ms: 1.1119899398269297\n",
      "  time_since_restore: 3945.1447722911835\n",
      "  time_this_iter_s: 119.75469207763672\n",
      "  time_total_s: 3945.1447722911835\n",
      "  timers:\n",
      "    learn_throughput: 86.997\n",
      "    learn_time_ms: 91865.458\n",
      "    sample_throughput: 287.624\n",
      "    sample_time_ms: 27786.257\n",
      "    update_time_ms: 2.836\n",
      "  timestamp: 1639162155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271728\n",
      "  training_iteration: 34\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         3945.14</td><td style=\"text-align: right;\">271728</td><td style=\"text-align: right;\">0.0957457</td><td style=\"text-align: right;\">            0.978965</td><td style=\"text-align: right;\">             -1.7522</td><td style=\"text-align: right;\">            125.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1118880\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7127832576066483\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0505140052837348\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6497480008230457\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 112.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9789646511915058\n",
      "  episode_reward_mean: 0.09164105369288848\n",
      "  episode_reward_min: -2.7613674356939875\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 1236\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4817667541503905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018726662166416646\n",
      "          policy_loss: -0.06289542403444648\n",
      "          total_loss: 0.08229216149076819\n",
      "          vf_explained_var: 0.44322556257247925\n",
      "          vf_loss: 0.13676058825850487\n",
      "    num_agent_steps_sampled: 1118880\n",
      "    num_agent_steps_trained: 1118880\n",
      "    num_steps_sampled: 279720\n",
      "    num_steps_trained: 279720\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.077215189873414\n",
      "    gpu_util_percent0: 0.18588607594936712\n",
      "    ram_util_percent: 91.3012658227848\n",
      "    vram_util_percent0: 0.5138015919627125\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7309377165607267\n",
      "  policy_reward_mean:\n",
      "    main: 0.02291026342322214\n",
      "  policy_reward_min:\n",
      "    main: -1.7080289479402984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30892352367332976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.479858740611444\n",
      "    mean_inference_ms: 4.513514392636292\n",
      "    mean_raw_obs_processing_ms: 1.1146589339538238\n",
      "  time_since_restore: 4066.6032819747925\n",
      "  time_this_iter_s: 121.45850968360901\n",
      "  time_total_s: 4066.6032819747925\n",
      "  timers:\n",
      "    learn_throughput: 86.51\n",
      "    learn_time_ms: 92382.392\n",
      "    sample_throughput: 287.826\n",
      "    sample_time_ms: 27766.801\n",
      "    update_time_ms: 2.912\n",
      "  timestamp: 1639162277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279720\n",
      "  training_iteration: 35\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">          4066.6</td><td style=\"text-align: right;\">279720</td><td style=\"text-align: right;\">0.0916411</td><td style=\"text-align: right;\">            0.978965</td><td style=\"text-align: right;\">            -2.76137</td><td style=\"text-align: right;\">            112.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1150848\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8407122768172964\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05141835836763148\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7500155363549612\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-53-19\n",
      "  done: false\n",
      "  episode_len_mean: 135.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9081537808887484\n",
      "  episode_reward_mean: -0.01101272360908278\n",
      "  episode_reward_min: -2.7613674356939875\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1304\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4089397478103636\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01803684179112315\n",
      "          policy_loss: -0.06135407299175859\n",
      "          total_loss: 0.10276051825657487\n",
      "          vf_explained_var: 0.47432562708854675\n",
      "          vf_loss: 0.15599801275134087\n",
      "    num_agent_steps_sampled: 1150848\n",
      "    num_agent_steps_trained: 1150848\n",
      "    num_steps_sampled: 287712\n",
      "    num_steps_trained: 287712\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.66075949367089\n",
      "    gpu_util_percent0: 0.19525316455696204\n",
      "    ram_util_percent: 91.20949367088606\n",
      "    vram_util_percent0: 0.5139963179093723\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7309377165607267\n",
      "  policy_reward_mean:\n",
      "    main: -0.002753180902270701\n",
      "  policy_reward_min:\n",
      "    main: -1.7568882819720852\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3086140937515322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.49135085054425\n",
      "    mean_inference_ms: 4.511966648678006\n",
      "    mean_raw_obs_processing_ms: 1.117598198689378\n",
      "  time_since_restore: 4188.613153457642\n",
      "  time_this_iter_s: 122.00987148284912\n",
      "  time_total_s: 4188.613153457642\n",
      "  timers:\n",
      "    learn_throughput: 85.772\n",
      "    learn_time_ms: 93177.116\n",
      "    sample_throughput: 285.894\n",
      "    sample_time_ms: 27954.414\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1639162399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287712\n",
      "  training_iteration: 36\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         4188.61</td><td style=\"text-align: right;\">287712</td><td style=\"text-align: right;\">-0.0110127</td><td style=\"text-align: right;\">            0.908154</td><td style=\"text-align: right;\">            -2.76137</td><td style=\"text-align: right;\">            135.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1182816\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6898266692408466\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06113971660986652\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.996474616418056\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 103.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0801172352851902\n",
      "  episode_reward_mean: 0.12498581169654858\n",
      "  episode_reward_min: -1.7101148783258373\n",
      "  episodes_this_iter: 84\n",
      "  episodes_total: 1388\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.4030255756378174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018477363865822553\n",
      "          policy_loss: -0.06317944314703346\n",
      "          total_loss: 0.12483502820506692\n",
      "          vf_explained_var: 0.4959712624549866\n",
      "          vf_loss: 0.1796996579170227\n",
      "    num_agent_steps_sampled: 1182816\n",
      "    num_agent_steps_trained: 1182816\n",
      "    num_steps_sampled: 295704\n",
      "    num_steps_trained: 295704\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75064102564103\n",
      "    gpu_util_percent0: 0.19839743589743591\n",
      "    ram_util_percent: 91.29807692307692\n",
      "    vram_util_percent0: 0.5136821744669722\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8768223235936607\n",
      "  policy_reward_mean:\n",
      "    main: 0.031246452924137146\n",
      "  policy_reward_min:\n",
      "    main: -2.003505148503271\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30959752065282564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.5243264925946\n",
      "    mean_inference_ms: 4.524927466173719\n",
      "    mean_raw_obs_processing_ms: 1.1217913188574011\n",
      "  time_since_restore: 4309.344401359558\n",
      "  time_this_iter_s: 120.7312479019165\n",
      "  time_total_s: 4309.344401359558\n",
      "  timers:\n",
      "    learn_throughput: 85.135\n",
      "    learn_time_ms: 93874.502\n",
      "    sample_throughput: 284.075\n",
      "    sample_time_ms: 28133.451\n",
      "    update_time_ms: 2.91\n",
      "  timestamp: 1639162520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295704\n",
      "  training_iteration: 37\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         4309.34</td><td style=\"text-align: right;\">295704</td><td style=\"text-align: right;\">0.124986</td><td style=\"text-align: right;\">             1.08012</td><td style=\"text-align: right;\">            -1.71011</td><td style=\"text-align: right;\">            103.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1214784\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1443359136909788\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.061976191918488646\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.901941717027658\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-57-21\n",
      "  done: false\n",
      "  episode_len_mean: 99.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9076263812960603\n",
      "  episode_reward_mean: 0.10892567334591831\n",
      "  episode_reward_min: -1.7314555403387946\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1463\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3960438346862794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01841314612329006\n",
      "          policy_loss: -0.06334637221693992\n",
      "          total_loss: 0.1085268838442862\n",
      "          vf_explained_var: 0.4913378953933716\n",
      "          vf_loss: 0.16358734107017517\n",
      "    num_agent_steps_sampled: 1214784\n",
      "    num_agent_steps_trained: 1214784\n",
      "    num_steps_sampled: 303696\n",
      "    num_steps_trained: 303696\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.706329113924056\n",
      "    gpu_util_percent0: 0.18829113924050633\n",
      "    ram_util_percent: 91.3006329113924\n",
      "    vram_util_percent0: 0.5134465034717449\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9028262831947103\n",
      "  policy_reward_mean:\n",
      "    main: 0.027231418336479588\n",
      "  policy_reward_min:\n",
      "    main: -1.901941717027658\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3088644108467211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.517471378906002\n",
      "    mean_inference_ms: 4.520788133905885\n",
      "    mean_raw_obs_processing_ms: 1.1258808957951518\n",
      "  time_since_restore: 4430.804900169373\n",
      "  time_this_iter_s: 121.46049880981445\n",
      "  time_total_s: 4430.804900169373\n",
      "  timers:\n",
      "    learn_throughput: 84.949\n",
      "    learn_time_ms: 94079.717\n",
      "    sample_throughput: 282.292\n",
      "    sample_time_ms: 28311.063\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1639162641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303696\n",
      "  training_iteration: 38\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">          4430.8</td><td style=\"text-align: right;\">303696</td><td style=\"text-align: right;\">0.108926</td><td style=\"text-align: right;\">            0.907626</td><td style=\"text-align: right;\">            -1.73146</td><td style=\"text-align: right;\">             99.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1246752\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9352401932772264\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07587433068737347\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.46651236914002364\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_15-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 81.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1843438301918083\n",
      "  episode_reward_mean: 0.14454232642139636\n",
      "  episode_reward_min: -1.4198967138648237\n",
      "  episodes_this_iter: 95\n",
      "  episodes_total: 1558\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.3414823656082153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018166551902890207\n",
      "          policy_loss: -0.06473050904273987\n",
      "          total_loss: 0.13126356235891581\n",
      "          vf_explained_var: 0.5129080414772034\n",
      "          vf_loss: 0.18781912380456925\n",
      "    num_agent_steps_sampled: 1246752\n",
      "    num_agent_steps_trained: 1246752\n",
      "    num_steps_sampled: 311688\n",
      "    num_steps_trained: 311688\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.43821656050956\n",
      "    gpu_util_percent0: 0.16751592356687897\n",
      "    ram_util_percent: 92.52547770700637\n",
      "    vram_util_percent0: 0.5199833166709633\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8454802810578927\n",
      "  policy_reward_mean:\n",
      "    main: 0.03613558160534912\n",
      "  policy_reward_min:\n",
      "    main: -1.6517853266962605\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31192266782122213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.63875289148531\n",
      "    mean_inference_ms: 4.547804346802119\n",
      "    mean_raw_obs_processing_ms: 1.130223272490643\n",
      "  time_since_restore: 4552.498030424118\n",
      "  time_this_iter_s: 121.69313025474548\n",
      "  time_total_s: 4552.498030424118\n",
      "  timers:\n",
      "    learn_throughput: 85.128\n",
      "    learn_time_ms: 93881.969\n",
      "    sample_throughput: 279.547\n",
      "    sample_time_ms: 28589.082\n",
      "    update_time_ms: 2.933\n",
      "  timestamp: 1639162763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311688\n",
      "  training_iteration: 39\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">          4552.5</td><td style=\"text-align: right;\">311688</td><td style=\"text-align: right;\">0.144542</td><td style=\"text-align: right;\">             1.18434</td><td style=\"text-align: right;\">             -1.4199</td><td style=\"text-align: right;\">             81.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1278720\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6449469878352555\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0668259015370779\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4442072219104475\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-01-27\n",
      "  done: false\n",
      "  episode_len_mean: 102.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1410214104210334\n",
      "  episode_reward_mean: 0.08278330938032435\n",
      "  episode_reward_min: -1.5052915843752352\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 1635\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.348832069396973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018312649197876455\n",
      "          policy_loss: -0.06596506226435303\n",
      "          total_loss: 0.1058167369440198\n",
      "          vf_explained_var: 0.527336597442627\n",
      "          vf_loss: 0.16354110813140868\n",
      "    num_agent_steps_sampled: 1278720\n",
      "    num_agent_steps_trained: 1278720\n",
      "    num_steps_sampled: 319680\n",
      "    num_steps_trained: 319680\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.61761006289308\n",
      "    gpu_util_percent0: 0.18509433962264152\n",
      "    ram_util_percent: 92.81635220125789\n",
      "    vram_util_percent0: 0.5235316152077502\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8259985359963191\n",
      "  policy_reward_mean:\n",
      "    main: 0.02069582734508111\n",
      "  policy_reward_min:\n",
      "    main: -1.6517853266962605\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31105306959080503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.64039157696488\n",
      "    mean_inference_ms: 4.548118018624116\n",
      "    mean_raw_obs_processing_ms: 1.1356460561349075\n",
      "  time_since_restore: 4676.870027065277\n",
      "  time_this_iter_s: 124.37199664115906\n",
      "  time_total_s: 4676.870027065277\n",
      "  timers:\n",
      "    learn_throughput: 85.542\n",
      "    learn_time_ms: 93427.648\n",
      "    sample_throughput: 287.305\n",
      "    sample_time_ms: 27817.096\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1639162887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319680\n",
      "  training_iteration: 40\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 16:01:31,106\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2021-12-10 16:01:31,123\tWARNING util.py:161 -- The `process_trial_result` operation took 2.828 s, which may be a performance bottleneck.\n",
      "2021-12-10 16:01:31,125\tWARNING util.py:161 -- Processing trial results took 2.831 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2021-12-10 16:01:31,127\tWARNING util.py:161 -- The `process_trial` operation took 2.835 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         4676.87</td><td style=\"text-align: right;\">319680</td><td style=\"text-align: right;\">0.0827833</td><td style=\"text-align: right;\">             1.14102</td><td style=\"text-align: right;\">            -1.50529</td><td style=\"text-align: right;\">            102.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1310688\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7761991944739829\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07577451324795631\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6801506588093417\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-03-35\n",
      "  done: false\n",
      "  episode_len_mean: 99.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1820370588028553\n",
      "  episode_reward_mean: 0.08344590693747328\n",
      "  episode_reward_min: -1.6902636413953984\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 1711\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.296562340736389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018724830191582442\n",
      "          policy_loss: -0.06819279908388853\n",
      "          total_loss: 0.0988575356118381\n",
      "          vf_explained_var: 0.5281072854995728\n",
      "          vf_loss: 0.15862416192889214\n",
      "    num_agent_steps_sampled: 1310688\n",
      "    num_agent_steps_trained: 1310688\n",
      "    num_steps_sampled: 327672\n",
      "    num_steps_trained: 327672\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.3820987654321\n",
      "    gpu_util_percent0: 0.15617283950617283\n",
      "    ram_util_percent: 92.47098765432098\n",
      "    vram_util_percent0: 0.5247269041554442\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.869391637197018\n",
      "  policy_reward_mean:\n",
      "    main: 0.020861476734368323\n",
      "  policy_reward_min:\n",
      "    main: -1.6801506588093416\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3111547003307755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.66315709437913\n",
      "    mean_inference_ms: 4.548641021005413\n",
      "    mean_raw_obs_processing_ms: 1.1391893327158593\n",
      "  time_since_restore: 4801.057464361191\n",
      "  time_this_iter_s: 124.1874372959137\n",
      "  time_total_s: 4801.057464361191\n",
      "  timers:\n",
      "    learn_throughput: 85.439\n",
      "    learn_time_ms: 93540.832\n",
      "    sample_throughput: 287.005\n",
      "    sample_time_ms: 27846.252\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1639163015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327672\n",
      "  training_iteration: 41\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         4801.06</td><td style=\"text-align: right;\">327672</td><td style=\"text-align: right;\">0.0834459</td><td style=\"text-align: right;\">             1.18204</td><td style=\"text-align: right;\">            -1.69026</td><td style=\"text-align: right;\">             99.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1342656\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8645251886638065\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.030862373906532646\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6801506588093417\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.111144243489612\n",
      "  episode_reward_mean: 0.14122505745607739\n",
      "  episode_reward_min: -1.4655674838262396\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 1799\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2706633129119873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019158680468797682\n",
      "          policy_loss: -0.06820431761443616\n",
      "          total_loss: 0.12075697576254606\n",
      "          vf_explained_var: 0.5209018588066101\n",
      "          vf_loss: 0.1803398875594139\n",
      "    num_agent_steps_sampled: 1342656\n",
      "    num_agent_steps_trained: 1342656\n",
      "    num_steps_sampled: 335664\n",
      "    num_steps_trained: 335664\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.99679487179487\n",
      "    gpu_util_percent0: 0.16519230769230767\n",
      "    ram_util_percent: 91.67115384615386\n",
      "    vram_util_percent0: 0.524012622235722\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8371145763081285\n",
      "  policy_reward_mean:\n",
      "    main: 0.03530626436401933\n",
      "  policy_reward_min:\n",
      "    main: -1.6801506588093416\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.312058608984499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.69654050940239\n",
      "    mean_inference_ms: 4.55353724987334\n",
      "    mean_raw_obs_processing_ms: 1.1421746813947808\n",
      "  time_since_restore: 4923.803386926651\n",
      "  time_this_iter_s: 122.7459225654602\n",
      "  time_total_s: 4923.803386926651\n",
      "  timers:\n",
      "    learn_throughput: 85.163\n",
      "    learn_time_ms: 93844.054\n",
      "    sample_throughput: 287.122\n",
      "    sample_time_ms: 27834.899\n",
      "    update_time_ms: 3.254\n",
      "  timestamp: 1639163138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335664\n",
      "  training_iteration: 42\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">          4923.8</td><td style=\"text-align: right;\">335664</td><td style=\"text-align: right;\">0.141225</td><td style=\"text-align: right;\">             1.11114</td><td style=\"text-align: right;\">            -1.46557</td><td style=\"text-align: right;\">            100.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1374624\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8955261795651522\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09884649885227416\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5297169058166485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 89.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1385759329751544\n",
      "  episode_reward_mean: 0.15074370216796498\n",
      "  episode_reward_min: -1.604489454676513\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 1884\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2314757795333864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019024562492966652\n",
      "          policy_loss: -0.07056594387069344\n",
      "          total_loss: 0.0954263006709516\n",
      "          vf_explained_var: 0.5540452599525452\n",
      "          vf_loss: 0.15743119153380394\n",
      "    num_agent_steps_sampled: 1374624\n",
      "    num_agent_steps_trained: 1374624\n",
      "    num_steps_sampled: 343656\n",
      "    num_steps_trained: 343656\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.880745341614904\n",
      "    gpu_util_percent0: 0.1846583850931677\n",
      "    ram_util_percent: 90.14906832298136\n",
      "    vram_util_percent0: 0.5207417444852528\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8761465020431456\n",
      "  policy_reward_mean:\n",
      "    main: 0.037685925541991246\n",
      "  policy_reward_min:\n",
      "    main: -1.533365224732354\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31153723037037234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.70509022513719\n",
      "    mean_inference_ms: 4.554211298565138\n",
      "    mean_raw_obs_processing_ms: 1.1465583107332689\n",
      "  time_since_restore: 5049.914685487747\n",
      "  time_this_iter_s: 126.11129856109619\n",
      "  time_total_s: 5049.914685487747\n",
      "  timers:\n",
      "    learn_throughput: 84.743\n",
      "    learn_time_ms: 94309.085\n",
      "    sample_throughput: 287.807\n",
      "    sample_time_ms: 27768.61\n",
      "    update_time_ms: 6.126\n",
      "  timestamp: 1639163264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343656\n",
      "  training_iteration: 43\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         5049.91</td><td style=\"text-align: right;\">343656</td><td style=\"text-align: right;\">0.150744</td><td style=\"text-align: right;\">             1.13858</td><td style=\"text-align: right;\">            -1.60449</td><td style=\"text-align: right;\">             89.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1406592\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8251652392629054\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08227691157480557\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5855389140574512\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.446415523769038\n",
      "  episode_reward_mean: 0.15296505020812848\n",
      "  episode_reward_min: -0.8821487414322287\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 1969\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.2143423261642456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01946483553946018\n",
      "          policy_loss: -0.07265168433636426\n",
      "          total_loss: 0.10630249619856477\n",
      "          vf_explained_var: 0.5377718210220337\n",
      "          vf_loss: 0.17019500589370729\n",
      "    num_agent_steps_sampled: 1406592\n",
      "    num_agent_steps_trained: 1406592\n",
      "    num_steps_sampled: 351648\n",
      "    num_steps_trained: 351648\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.027388535031854\n",
      "    gpu_util_percent0: 0.16515923566878982\n",
      "    ram_util_percent: 90.13566878980892\n",
      "    vram_util_percent0: 0.5219880408196629\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7132686142068845\n",
      "  policy_reward_mean:\n",
      "    main: 0.03824126255203214\n",
      "  policy_reward_min:\n",
      "    main: -1.599319151213451\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3114925091323326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.7027050733038\n",
      "    mean_inference_ms: 4.5519273480819376\n",
      "    mean_raw_obs_processing_ms: 1.1492407676068905\n",
      "  time_since_restore: 5173.929029941559\n",
      "  time_this_iter_s: 124.01434445381165\n",
      "  time_total_s: 5173.929029941559\n",
      "  timers:\n",
      "    learn_throughput: 84.373\n",
      "    learn_time_ms: 94722.324\n",
      "    sample_throughput: 287.683\n",
      "    sample_time_ms: 27780.57\n",
      "    update_time_ms: 6.103\n",
      "  timestamp: 1639163388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351648\n",
      "  training_iteration: 44\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         5173.93</td><td style=\"text-align: right;\">351648</td><td style=\"text-align: right;\">0.152965</td><td style=\"text-align: right;\">             1.44642</td><td style=\"text-align: right;\">           -0.882149</td><td style=\"text-align: right;\">             91.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1438560\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8369842400470648\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08452115167932459\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.46950123808874633\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-11-52\n",
      "  done: false\n",
      "  episode_len_mean: 88.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3852194845423527\n",
      "  episode_reward_mean: 0.11745030889760052\n",
      "  episode_reward_min: -1.2496425219134633\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 2062\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.19003227519989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019367699712514876\n",
      "          policy_loss: -0.07167381298914552\n",
      "          total_loss: 0.11114846911281347\n",
      "          vf_explained_var: 0.5738922357559204\n",
      "          vf_loss: 0.17410681718587875\n",
      "    num_agent_steps_sampled: 1438560\n",
      "    num_agent_steps_trained: 1438560\n",
      "    num_steps_sampled: 359640\n",
      "    num_steps_trained: 359640\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.8937106918239\n",
      "    gpu_util_percent0: 0.19044025157232702\n",
      "    ram_util_percent: 90.23459119496854\n",
      "    vram_util_percent0: 0.5239424173377124\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8080255850156528\n",
      "  policy_reward_mean:\n",
      "    main: 0.029362577224400135\n",
      "  policy_reward_min:\n",
      "    main: -1.6974516362959522\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3121550493171023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.739204208887468\n",
      "    mean_inference_ms: 4.560037177734232\n",
      "    mean_raw_obs_processing_ms: 1.1532238164489892\n",
      "  time_since_restore: 5297.777725696564\n",
      "  time_this_iter_s: 123.84869575500488\n",
      "  time_total_s: 5297.777725696564\n",
      "  timers:\n",
      "    learn_throughput: 84.231\n",
      "    learn_time_ms: 94882.1\n",
      "    sample_throughput: 286.857\n",
      "    sample_time_ms: 27860.535\n",
      "    update_time_ms: 6.075\n",
      "  timestamp: 1639163512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359640\n",
      "  training_iteration: 45\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         5297.78</td><td style=\"text-align: right;\">359640</td><td style=\"text-align: right;\"> 0.11745</td><td style=\"text-align: right;\">             1.38522</td><td style=\"text-align: right;\">            -1.24964</td><td style=\"text-align: right;\">             88.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1470528\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8764844755697462\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.028975687606940092\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5305499179853442\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-13-56\n",
      "  done: false\n",
      "  episode_len_mean: 85.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1473576946456197\n",
      "  episode_reward_mean: 0.09852802874356226\n",
      "  episode_reward_min: -1.2496937486618414\n",
      "  episodes_this_iter: 90\n",
      "  episodes_total: 2152\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.1561039190292357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019496548499912024\n",
      "          policy_loss: -0.07392370611429215\n",
      "          total_loss: 0.10565197032690048\n",
      "          vf_explained_var: 0.5476794242858887\n",
      "          vf_loss: 0.17080222991108895\n",
      "    num_agent_steps_sampled: 1470528\n",
      "    num_agent_steps_trained: 1470528\n",
      "    num_steps_sampled: 367632\n",
      "    num_steps_trained: 367632\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.38176100628931\n",
      "    gpu_util_percent0: 0.1716981132075472\n",
      "    ram_util_percent: 90.00377358490566\n",
      "    vram_util_percent0: 0.5217611304612366\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8355049853497314\n",
      "  policy_reward_mean:\n",
      "    main: 0.024632007185890568\n",
      "  policy_reward_min:\n",
      "    main: -1.5305499179853443\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3134018628373612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.814235347620976\n",
      "    mean_inference_ms: 4.575639498862488\n",
      "    mean_raw_obs_processing_ms: 1.1585253284715187\n",
      "  time_since_restore: 5422.085338592529\n",
      "  time_this_iter_s: 124.30761289596558\n",
      "  time_total_s: 5422.085338592529\n",
      "  timers:\n",
      "    learn_throughput: 84.244\n",
      "    learn_time_ms: 94867.844\n",
      "    sample_throughput: 284.333\n",
      "    sample_time_ms: 28107.906\n",
      "    update_time_ms: 6.076\n",
      "  timestamp: 1639163636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367632\n",
      "  training_iteration: 46\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         5422.09</td><td style=\"text-align: right;\">367632</td><td style=\"text-align: right;\">0.098528</td><td style=\"text-align: right;\">             1.14736</td><td style=\"text-align: right;\">            -1.24969</td><td style=\"text-align: right;\">             85.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1502496\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9361416048733182\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09451021870619837\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5028830102570857\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 80.03738317757009\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.22152306165669\n",
      "  episode_reward_mean: 0.06124683481608093\n",
      "  episode_reward_min: -0.8966834528142031\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 2259\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.103758729457855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01967891788482666\n",
      "          policy_loss: -0.07369440248608589\n",
      "          total_loss: 0.12766817297786473\n",
      "          vf_explained_var: 0.5615202784538269\n",
      "          vf_loss: 0.19250706326961517\n",
      "    num_agent_steps_sampled: 1502496\n",
      "    num_agent_steps_trained: 1502496\n",
      "    num_steps_sampled: 375624\n",
      "    num_steps_trained: 375624\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.40133333333333\n",
      "    gpu_util_percent0: 0.13713333333333333\n",
      "    ram_util_percent: 89.01933333333334\n",
      "    vram_util_percent0: 0.5198760557200832\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.859911003299239\n",
      "  policy_reward_mean:\n",
      "    main: 0.015311708704020243\n",
      "  policy_reward_min:\n",
      "    main: -1.5653137489971047\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3128546181771974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.790318100102226\n",
      "    mean_inference_ms: 4.56803235287297\n",
      "    mean_raw_obs_processing_ms: 1.1611993906434726\n",
      "  time_since_restore: 5537.627523422241\n",
      "  time_this_iter_s: 115.54218482971191\n",
      "  time_total_s: 5537.627523422241\n",
      "  timers:\n",
      "    learn_throughput: 84.622\n",
      "    learn_time_ms: 94443.739\n",
      "    sample_throughput: 285.293\n",
      "    sample_time_ms: 28013.304\n",
      "    update_time_ms: 6.068\n",
      "  timestamp: 1639163752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375624\n",
      "  training_iteration: 47\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         5537.63</td><td style=\"text-align: right;\">375624</td><td style=\"text-align: right;\">0.0612468</td><td style=\"text-align: right;\">             1.22152</td><td style=\"text-align: right;\">           -0.896683</td><td style=\"text-align: right;\">           80.0374</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1534464\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5916107636508037\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07078356821607915\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9146442166436733\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 82.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.166731619782749\n",
      "  episode_reward_mean: 0.10730161410267261\n",
      "  episode_reward_min: -1.3835459917602568\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 2358\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.1015295910835268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019547720186412335\n",
      "          policy_loss: -0.07430901150777935\n",
      "          total_loss: 0.1421882408708334\n",
      "          vf_explained_var: 0.5300207734107971\n",
      "          vf_loss: 0.20770077872276307\n",
      "    num_agent_steps_sampled: 1534464\n",
      "    num_agent_steps_trained: 1534464\n",
      "    num_steps_sampled: 383616\n",
      "    num_steps_trained: 383616\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.18187919463087\n",
      "    gpu_util_percent0: 0.13986577181208057\n",
      "    ram_util_percent: 88.89194630872481\n",
      "    vram_util_percent0: 0.5187462318715756\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8189115444374873\n",
      "  policy_reward_mean:\n",
      "    main: 0.026825403525668142\n",
      "  policy_reward_min:\n",
      "    main: -1.9187301559120167\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31294738374386855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.77863891252965\n",
      "    mean_inference_ms: 4.567181922020904\n",
      "    mean_raw_obs_processing_ms: 1.162629595936022\n",
      "  time_since_restore: 5653.3331162929535\n",
      "  time_this_iter_s: 115.70559287071228\n",
      "  time_total_s: 5653.3331162929535\n",
      "  timers:\n",
      "    learn_throughput: 85.047\n",
      "    learn_time_ms: 93971.044\n",
      "    sample_throughput: 286.297\n",
      "    sample_time_ms: 27915.091\n",
      "    update_time_ms: 6.073\n",
      "  timestamp: 1639163868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383616\n",
      "  training_iteration: 48\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         5653.33</td><td style=\"text-align: right;\">383616</td><td style=\"text-align: right;\">0.107302</td><td style=\"text-align: right;\">             1.16673</td><td style=\"text-align: right;\">            -1.38355</td><td style=\"text-align: right;\">             82.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1566432\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8214846991260323\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09509526634553477\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5353433647658811\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 75.86538461538461\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.083836813844317\n",
      "  episode_reward_mean: 0.18570159756475524\n",
      "  episode_reward_min: -0.7240535729755913\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 2462\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.0710823574066164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01974510659649968\n",
      "          policy_loss: -0.07717244028672576\n",
      "          total_loss: 0.13706996135413646\n",
      "          vf_explained_var: 0.533165693283081\n",
      "          vf_loss: 0.2053571028113365\n",
      "    num_agent_steps_sampled: 1566432\n",
      "    num_agent_steps_trained: 1566432\n",
      "    num_steps_sampled: 391608\n",
      "    num_steps_trained: 391608\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.8797385620915\n",
      "    gpu_util_percent0: 0.1820261437908497\n",
      "    ram_util_percent: 86.59869281045752\n",
      "    vram_util_percent0: 0.5095727223652431\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8073560145272634\n",
      "  policy_reward_mean:\n",
      "    main: 0.046425399391188824\n",
      "  policy_reward_min:\n",
      "    main: -1.5451611842409443\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31265074762154027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.75101215629126\n",
      "    mean_inference_ms: 4.561737093528895\n",
      "    mean_raw_obs_processing_ms: 1.1641261249584316\n",
      "  time_since_restore: 5771.683414936066\n",
      "  time_this_iter_s: 118.35029864311218\n",
      "  time_total_s: 5771.683414936066\n",
      "  timers:\n",
      "    learn_throughput: 84.976\n",
      "    learn_time_ms: 94049.788\n",
      "    sample_throughput: 290.583\n",
      "    sample_time_ms: 27503.31\n",
      "    update_time_ms: 6.187\n",
      "  timestamp: 1639163986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391608\n",
      "  training_iteration: 49\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         5771.68</td><td style=\"text-align: right;\">391608</td><td style=\"text-align: right;\">0.185702</td><td style=\"text-align: right;\">             1.08384</td><td style=\"text-align: right;\">           -0.724054</td><td style=\"text-align: right;\">           75.8654</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1598400\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7905157311914756\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09599287327450129\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.44234565829501704\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-21-47\n",
      "  done: false\n",
      "  episode_len_mean: 84.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2871376906518812\n",
      "  episode_reward_mean: 0.08242733873646163\n",
      "  episode_reward_min: -1.2789570367172307\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 2555\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.0506978244781493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01963054320961237\n",
      "          policy_loss: -0.07873871725797653\n",
      "          total_loss: 0.12991080452874304\n",
      "          vf_explained_var: 0.5179961323738098\n",
      "          vf_loss: 0.19981577628850936\n",
      "    num_agent_steps_sampled: 1598400\n",
      "    num_agent_steps_trained: 1598400\n",
      "    num_steps_sampled: 399600\n",
      "    num_steps_trained: 399600\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.17612903225806\n",
      "    gpu_util_percent0: 0.17096774193548386\n",
      "    ram_util_percent: 85.22903225806452\n",
      "    vram_util_percent0: 0.5078166629515226\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.2013190259134747\n",
      "  policy_reward_mean:\n",
      "    main: 0.020606834684115415\n",
      "  policy_reward_min:\n",
      "    main: -1.7630638136081958\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3120332706832966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.730286621211416\n",
      "    mean_inference_ms: 4.558465583951883\n",
      "    mean_raw_obs_processing_ms: 1.167026467971886\n",
      "  time_since_restore: 5891.974574565887\n",
      "  time_this_iter_s: 120.29115962982178\n",
      "  time_total_s: 5891.974574565887\n",
      "  timers:\n",
      "    learn_throughput: 85.257\n",
      "    learn_time_ms: 93739.829\n",
      "    sample_throughput: 291.559\n",
      "    sample_time_ms: 27411.303\n",
      "    update_time_ms: 6.19\n",
      "  timestamp: 1639164107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399600\n",
      "  training_iteration: 50\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         5891.97</td><td style=\"text-align: right;\">399600</td><td style=\"text-align: right;\">0.0824273</td><td style=\"text-align: right;\">             1.28714</td><td style=\"text-align: right;\">            -1.27896</td><td style=\"text-align: right;\">             84.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1630368\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7132817220343212\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09218601984770787\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4791945859691698\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 70.61538461538461\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4215164563728044\n",
      "  episode_reward_mean: 0.1845456734207123\n",
      "  episode_reward_min: -0.770935093753462\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 2659\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 2.008533737182617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019031800404191015\n",
      "          policy_loss: -0.07713326010480523\n",
      "          total_loss: 0.11099068773724138\n",
      "          vf_explained_var: 0.5881332159042358\n",
      "          vf_loss: 0.17955963760614396\n",
      "    num_agent_steps_sampled: 1630368\n",
      "    num_agent_steps_trained: 1630368\n",
      "    num_steps_sampled: 407592\n",
      "    num_steps_trained: 407592\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.825465838509324\n",
      "    gpu_util_percent0: 0.1731055900621118\n",
      "    ram_util_percent: 94.44409937888199\n",
      "    vram_util_percent0: 0.5203207168098366\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7683979318330652\n",
      "  policy_reward_mean:\n",
      "    main: 0.04613641835517806\n",
      "  policy_reward_min:\n",
      "    main: -1.613479836708567\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31361006201781516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.82831282352617\n",
      "    mean_inference_ms: 4.576587357847793\n",
      "    mean_raw_obs_processing_ms: 1.172644401690468\n",
      "  time_since_restore: 6018.419720411301\n",
      "  time_this_iter_s: 126.44514584541321\n",
      "  time_total_s: 6018.419720411301\n",
      "  timers:\n",
      "    learn_throughput: 85.317\n",
      "    learn_time_ms: 93674.529\n",
      "    sample_throughput: 288.755\n",
      "    sample_time_ms: 27677.481\n",
      "    update_time_ms: 17.133\n",
      "  timestamp: 1639164233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407592\n",
      "  training_iteration: 51\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 16:23:54,590\tWARNING util.py:161 -- The `callbacks.on_trial_result` operation took 0.767 s, which may be a performance bottleneck.\n",
      "2021-12-10 16:23:54,600\tWARNING util.py:161 -- The `process_trial_result` operation took 0.810 s, which may be a performance bottleneck.\n",
      "2021-12-10 16:23:54,601\tWARNING util.py:161 -- Processing trial results took 0.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2021-12-10 16:23:54,603\tWARNING util.py:161 -- The `process_trial` operation took 0.814 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         6018.42</td><td style=\"text-align: right;\">407592</td><td style=\"text-align: right;\">0.184546</td><td style=\"text-align: right;\">             1.42152</td><td style=\"text-align: right;\">           -0.770935</td><td style=\"text-align: right;\">           70.6154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1662336\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9505892526127697\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07239659390781451\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4500886229114827\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 78.0925925925926\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.059681941404515\n",
      "  episode_reward_mean: 0.21431818837135155\n",
      "  episode_reward_min: -0.9008795587103222\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 2767\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9811372237205505\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019466248400509357\n",
      "          policy_loss: -0.0787171411588788\n",
      "          total_loss: 0.11500105434656144\n",
      "          vf_explained_var: 0.5819682478904724\n",
      "          vf_loss: 0.1849583856165409\n",
      "    num_agent_steps_sampled: 1662336\n",
      "    num_agent_steps_trained: 1662336\n",
      "    num_steps_sampled: 415584\n",
      "    num_steps_trained: 415584\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.16644736842105\n",
      "    gpu_util_percent0: 0.1613157894736842\n",
      "    ram_util_percent: 95.41842105263157\n",
      "    vram_util_percent0: 0.5220748688107237\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8088295851110285\n",
      "  policy_reward_mean:\n",
      "    main: 0.053579547092837874\n",
      "  policy_reward_min:\n",
      "    main: -1.6126130884022538\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31319102819646955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.814141174197047\n",
      "    mean_inference_ms: 4.572832323714243\n",
      "    mean_raw_obs_processing_ms: 1.1761548842967233\n",
      "  time_since_restore: 6135.427141427994\n",
      "  time_this_iter_s: 117.00742101669312\n",
      "  time_total_s: 6135.427141427994\n",
      "  timers:\n",
      "    learn_throughput: 85.753\n",
      "    learn_time_ms: 93198.309\n",
      "    sample_throughput: 289.629\n",
      "    sample_time_ms: 27593.962\n",
      "    update_time_ms: 17.127\n",
      "  timestamp: 1639164351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415584\n",
      "  training_iteration: 52\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         6135.43</td><td style=\"text-align: right;\">415584</td><td style=\"text-align: right;\">0.214318</td><td style=\"text-align: right;\">             1.05968</td><td style=\"text-align: right;\">            -0.90088</td><td style=\"text-align: right;\">           78.0926</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1694304\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.701456817370238\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04421958418214598\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4589041312411541\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 71.76146788990826\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3391532891456097\n",
      "  episode_reward_mean: 0.09202048484575627\n",
      "  episode_reward_min: -1.4771380528389066\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 2876\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.96769904756546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01982059111073613\n",
      "          policy_loss: -0.07978992479294539\n",
      "          total_loss: 0.1253021084703505\n",
      "          vf_explained_var: 0.5631133913993835\n",
      "          vf_loss: 0.1961727688908577\n",
      "    num_agent_steps_sampled: 1694304\n",
      "    num_agent_steps_trained: 1694304\n",
      "    num_steps_sampled: 423576\n",
      "    num_steps_trained: 423576\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.78933333333334\n",
      "    gpu_util_percent0: 0.15726666666666664\n",
      "    ram_util_percent: 95.21266666666665\n",
      "    vram_util_percent0: 0.5174596906877262\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6880577430192418\n",
      "  policy_reward_mean:\n",
      "    main: 0.023005121211439054\n",
      "  policy_reward_min:\n",
      "    main: -1.6821638196679918\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31279674652831396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.798597066643982\n",
      "    mean_inference_ms: 4.5711944121465695\n",
      "    mean_raw_obs_processing_ms: 1.1789714957738364\n",
      "  time_since_restore: 6251.014890909195\n",
      "  time_this_iter_s: 115.58774948120117\n",
      "  time_total_s: 6251.014890909195\n",
      "  timers:\n",
      "    learn_throughput: 86.554\n",
      "    learn_time_ms: 92335.337\n",
      "    sample_throughput: 291.578\n",
      "    sample_time_ms: 27409.512\n",
      "    update_time_ms: 14.269\n",
      "  timestamp: 1639164467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423576\n",
      "  training_iteration: 53\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         6251.01</td><td style=\"text-align: right;\">423576</td><td style=\"text-align: right;\">0.0920205</td><td style=\"text-align: right;\">             1.33915</td><td style=\"text-align: right;\">            -1.47714</td><td style=\"text-align: right;\">           71.7615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1726272\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7607122204640752\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06637117223714527\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6006938647860443\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 78.45045045045045\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0308981585720627\n",
      "  episode_reward_mean: 0.1309366030712793\n",
      "  episode_reward_min: -1.1021441458781278\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 2987\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9297397966384888\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01956093871220946\n",
      "          policy_loss: -0.08008413419872522\n",
      "          total_loss: 0.13811320232599975\n",
      "          vf_explained_var: 0.5410097241401672\n",
      "          vf_loss: 0.20939491361379622\n",
      "    num_agent_steps_sampled: 1726272\n",
      "    num_agent_steps_trained: 1726272\n",
      "    num_steps_sampled: 431568\n",
      "    num_steps_trained: 431568\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.667785234899334\n",
      "    gpu_util_percent0: 0.16248322147651006\n",
      "    ram_util_percent: 95.24429530201343\n",
      "    vram_util_percent0: 0.5144177151173448\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7241650400558215\n",
      "  policy_reward_mean:\n",
      "    main: 0.03273415076781983\n",
      "  policy_reward_min:\n",
      "    main: -1.5796487886613622\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31232552516180045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.766150983345927\n",
      "    mean_inference_ms: 4.561402291094993\n",
      "    mean_raw_obs_processing_ms: 1.1798259150000172\n",
      "  time_since_restore: 6366.301121473312\n",
      "  time_this_iter_s: 115.28623056411743\n",
      "  time_total_s: 6366.301121473312\n",
      "  timers:\n",
      "    learn_throughput: 87.26\n",
      "    learn_time_ms: 91587.883\n",
      "    sample_throughput: 292.885\n",
      "    sample_time_ms: 27287.205\n",
      "    update_time_ms: 14.241\n",
      "  timestamp: 1639164582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431568\n",
      "  training_iteration: 54\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          6366.3</td><td style=\"text-align: right;\">431568</td><td style=\"text-align: right;\">0.130937</td><td style=\"text-align: right;\">              1.0309</td><td style=\"text-align: right;\">            -1.10214</td><td style=\"text-align: right;\">           78.4505</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1758240\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0682727172607236\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10047655023606371\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.40671965141658817\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 78.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.684876410034489\n",
      "  episode_reward_mean: 0.1342696307345293\n",
      "  episode_reward_min: -1.382387296210911\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 3085\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.926538782596588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0197081004306674\n",
      "          policy_loss: -0.0822843996360898\n",
      "          total_loss: 0.13887897750362754\n",
      "          vf_explained_var: 0.516375720500946\n",
      "          vf_loss: 0.21229472970962524\n",
      "    num_agent_steps_sampled: 1758240\n",
      "    num_agent_steps_trained: 1758240\n",
      "    num_steps_sampled: 439560\n",
      "    num_steps_trained: 439560\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.43066666666667\n",
      "    gpu_util_percent0: 0.1577333333333333\n",
      "    ram_util_percent: 95.43666666666668\n",
      "    vram_util_percent0: 0.5175101458813204\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.027729492592079\n",
      "  policy_reward_mean:\n",
      "    main: 0.03356740768363233\n",
      "  policy_reward_min:\n",
      "    main: -1.830266946209348\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3123883707313563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.759079506833377\n",
      "    mean_inference_ms: 4.563512303728539\n",
      "    mean_raw_obs_processing_ms: 1.1823980796233908\n",
      "  time_since_restore: 6482.266912698746\n",
      "  time_this_iter_s: 115.96579122543335\n",
      "  time_total_s: 6482.266912698746\n",
      "  timers:\n",
      "    learn_throughput: 87.813\n",
      "    learn_time_ms: 91011.85\n",
      "    sample_throughput: 295.159\n",
      "    sample_time_ms: 27076.912\n",
      "    update_time_ms: 14.219\n",
      "  timestamp: 1639164698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439560\n",
      "  training_iteration: 55\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         6482.27</td><td style=\"text-align: right;\">439560</td><td style=\"text-align: right;\"> 0.13427</td><td style=\"text-align: right;\">             1.68488</td><td style=\"text-align: right;\">            -1.38239</td><td style=\"text-align: right;\">             78.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1790208\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7770735575765354\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0897513262537277\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49839428224668014\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-33-35\n",
      "  done: false\n",
      "  episode_len_mean: 75.39423076923077\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9587070330552738\n",
      "  episode_reward_mean: 0.1837474118220133\n",
      "  episode_reward_min: -1.07755418437084\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 3189\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.9104894261360168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0199511312879622\n",
      "          policy_loss: -0.08389111087843776\n",
      "          total_loss: 0.12441339359432459\n",
      "          vf_explained_var: 0.5511708855628967\n",
      "          vf_loss: 0.19932649594545365\n",
      "    num_agent_steps_sampled: 1790208\n",
      "    num_agent_steps_trained: 1790208\n",
      "    num_steps_sampled: 447552\n",
      "    num_steps_trained: 447552\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.649668874172185\n",
      "    gpu_util_percent0: 0.16496688741721852\n",
      "    ram_util_percent: 95.62781456953641\n",
      "    vram_util_percent0: 0.5099021767791341\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.741164290055163\n",
      "  policy_reward_mean:\n",
      "    main: 0.045936852955503316\n",
      "  policy_reward_min:\n",
      "    main: -1.49839428224668\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3124369761189515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.75018087075968\n",
      "    mean_inference_ms: 4.560290007357274\n",
      "    mean_raw_obs_processing_ms: 1.183413344314416\n",
      "  time_since_restore: 6598.848454713821\n",
      "  time_this_iter_s: 116.58154201507568\n",
      "  time_total_s: 6598.848454713821\n",
      "  timers:\n",
      "    learn_throughput: 88.255\n",
      "    learn_time_ms: 90555.488\n",
      "    sample_throughput: 298.703\n",
      "    sample_time_ms: 26755.701\n",
      "    update_time_ms: 14.237\n",
      "  timestamp: 1639164815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447552\n",
      "  training_iteration: 56\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         6598.85</td><td style=\"text-align: right;\">447552</td><td style=\"text-align: right;\">0.183747</td><td style=\"text-align: right;\">            0.958707</td><td style=\"text-align: right;\">            -1.07755</td><td style=\"text-align: right;\">           75.3942</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1822176\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1123519633450805\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.12957073450769876\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5523995905914408\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-35-30\n",
      "  done: false\n",
      "  episode_len_mean: 66.50833333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2397863658210748\n",
      "  episode_reward_mean: 0.20881047798246552\n",
      "  episode_reward_min: -0.7196357383419096\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 3309\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8630052576065064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020022203408181668\n",
      "          policy_loss: -0.08236587158590555\n",
      "          total_loss: 0.13993910279124974\n",
      "          vf_explained_var: 0.5646203756332397\n",
      "          vf_loss: 0.21329498255252838\n",
      "    num_agent_steps_sampled: 1822176\n",
      "    num_agent_steps_trained: 1822176\n",
      "    num_steps_sampled: 455544\n",
      "    num_steps_trained: 455544\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.059060402684565\n",
      "    gpu_util_percent0: 0.15677852348993285\n",
      "    ram_util_percent: 95.80604026845639\n",
      "    vram_util_percent0: 0.5055961538036842\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.980197494539592\n",
      "  policy_reward_mean:\n",
      "    main: 0.05220261949561638\n",
      "  policy_reward_min:\n",
      "    main: -1.5592416058300236\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3116947106693147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.718381512908127\n",
      "    mean_inference_ms: 4.55166843785748\n",
      "    mean_raw_obs_processing_ms: 1.1868343375964259\n",
      "  time_since_restore: 6714.059302330017\n",
      "  time_this_iter_s: 115.21084761619568\n",
      "  time_total_s: 6714.059302330017\n",
      "  timers:\n",
      "    learn_throughput: 88.28\n",
      "    learn_time_ms: 90530.289\n",
      "    sample_throughput: 298.861\n",
      "    sample_time_ms: 26741.569\n",
      "    update_time_ms: 14.267\n",
      "  timestamp: 1639164930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455544\n",
      "  training_iteration: 57\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         6714.06</td><td style=\"text-align: right;\">455544</td><td style=\"text-align: right;\"> 0.20881</td><td style=\"text-align: right;\">             1.23979</td><td style=\"text-align: right;\">           -0.719636</td><td style=\"text-align: right;\">           66.5083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1854144\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9216974411280054\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05379508004018465\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.43415142214591357\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-37-25\n",
      "  done: false\n",
      "  episode_len_mean: 65.89075630252101\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3350613994549523\n",
      "  episode_reward_mean: 0.14245552521759808\n",
      "  episode_reward_min: -0.9808505287796434\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 3428\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8435912094116211\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016654959619045256\n",
      "          policy_loss: -0.08116627798974514\n",
      "          total_loss: 0.14505049081146718\n",
      "          vf_explained_var: 0.5592561960220337\n",
      "          vf_loss: 0.2149746715426445\n",
      "    num_agent_steps_sampled: 1854144\n",
      "    num_agent_steps_trained: 1854144\n",
      "    num_steps_sampled: 463536\n",
      "    num_steps_trained: 463536\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.099999999999994\n",
      "    gpu_util_percent0: 0.15422818791946308\n",
      "    ram_util_percent: 95.68389261744964\n",
      "    vram_util_percent0: 0.5030387954356232\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9334668187981017\n",
      "  policy_reward_mean:\n",
      "    main: 0.03561388130439951\n",
      "  policy_reward_min:\n",
      "    main: -1.6605127337165864\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3125289456528282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.7190446634136\n",
      "    mean_inference_ms: 4.5569022188269335\n",
      "    mean_raw_obs_processing_ms: 1.1876808082411867\n",
      "  time_since_restore: 6829.2452964782715\n",
      "  time_this_iter_s: 115.1859941482544\n",
      "  time_total_s: 6829.2452964782715\n",
      "  timers:\n",
      "    learn_throughput: 88.286\n",
      "    learn_time_ms: 90524.205\n",
      "    sample_throughput: 299.436\n",
      "    sample_time_ms: 26690.203\n",
      "    update_time_ms: 14.275\n",
      "  timestamp: 1639165045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463536\n",
      "  training_iteration: 58\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         6829.25</td><td style=\"text-align: right;\">463536</td><td style=\"text-align: right;\">0.142456</td><td style=\"text-align: right;\">             1.33506</td><td style=\"text-align: right;\">           -0.980851</td><td style=\"text-align: right;\">           65.8908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1886112\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6115712107206795\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07273859829933711\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5518049554450536\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 76.41666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3507456886387041\n",
      "  episode_reward_mean: 0.13293031434767286\n",
      "  episode_reward_min: -2.1643593717615333\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 3536\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.852336555480957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016897303748875856\n",
      "          policy_loss: -0.08254172070324421\n",
      "          total_loss: 0.13377644629403948\n",
      "          vf_explained_var: 0.5434074997901917\n",
      "          vf_loss: 0.2049124871492386\n",
      "    num_agent_steps_sampled: 1886112\n",
      "    num_agent_steps_trained: 1886112\n",
      "    num_steps_sampled: 471528\n",
      "    num_steps_trained: 471528\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.062666666666665\n",
      "    gpu_util_percent0: 0.1575333333333333\n",
      "    ram_util_percent: 95.742\n",
      "    vram_util_percent0: 0.4978172644510255\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7373927845140955\n",
      "  policy_reward_mean:\n",
      "    main: 0.03323257858691821\n",
      "  policy_reward_min:\n",
      "    main: -1.951465571778222\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3108926513170734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.674035717714006\n",
      "    mean_inference_ms: 4.543366901200151\n",
      "    mean_raw_obs_processing_ms: 1.1908519182249557\n",
      "  time_since_restore: 6944.949700593948\n",
      "  time_this_iter_s: 115.70440411567688\n",
      "  time_total_s: 6944.949700593948\n",
      "  timers:\n",
      "    learn_throughput: 88.548\n",
      "    learn_time_ms: 90255.694\n",
      "    sample_throughput: 299.423\n",
      "    sample_time_ms: 26691.342\n",
      "    update_time_ms: 14.155\n",
      "  timestamp: 1639165161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471528\n",
      "  training_iteration: 59\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         6944.95</td><td style=\"text-align: right;\">471528</td><td style=\"text-align: right;\"> 0.13293</td><td style=\"text-align: right;\">             1.35075</td><td style=\"text-align: right;\">            -2.16436</td><td style=\"text-align: right;\">           76.4167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1918080\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9290114218846391\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10783750746387419\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.44824232203519154\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 84.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.892296127735387\n",
      "  episode_reward_mean: 0.1770057621608309\n",
      "  episode_reward_min: -1.2621554646725976\n",
      "  episodes_this_iter: 87\n",
      "  episodes_total: 3623\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8665865740776062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017050778422504662\n",
      "          policy_loss: -0.0837168497890234\n",
      "          total_loss: 0.10645134616270661\n",
      "          vf_explained_var: 0.548751175403595\n",
      "          vf_loss: 0.17865892148017884\n",
      "    num_agent_steps_sampled: 1918080\n",
      "    num_agent_steps_trained: 1918080\n",
      "    num_steps_sampled: 479520\n",
      "    num_steps_trained: 479520\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.031333333333336\n",
      "    gpu_util_percent0: 0.1634\n",
      "    ram_util_percent: 95.76266666666665\n",
      "    vram_util_percent0: 0.49768344850279683\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6549384532747955\n",
      "  policy_reward_mean:\n",
      "    main: 0.044251440540207715\n",
      "  policy_reward_min:\n",
      "    main: -1.5724229081073624\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3114291729529153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.674007508653744\n",
      "    mean_inference_ms: 4.55089706869495\n",
      "    mean_raw_obs_processing_ms: 1.1920536641867583\n",
      "  time_since_restore: 7060.918134689331\n",
      "  time_this_iter_s: 115.96843409538269\n",
      "  time_total_s: 7060.918134689331\n",
      "  timers:\n",
      "    learn_throughput: 88.937\n",
      "    learn_time_ms: 89861.406\n",
      "    sample_throughput: 299.912\n",
      "    sample_time_ms: 26647.852\n",
      "    update_time_ms: 14.158\n",
      "  timestamp: 1639165277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479520\n",
      "  training_iteration: 60\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         7060.92</td><td style=\"text-align: right;\">479520</td><td style=\"text-align: right;\">0.177006</td><td style=\"text-align: right;\">            0.892296</td><td style=\"text-align: right;\">            -1.26216</td><td style=\"text-align: right;\">             84.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1950048\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6554977551978007\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0952274350315007\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.736226201134685\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 62.94615384615385\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4920755336684832\n",
      "  episode_reward_mean: 0.1942600641168125\n",
      "  episode_reward_min: -1.2461782622053577\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 3753\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7828270049095154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016493841998279093\n",
      "          policy_loss: -0.08413568386062979\n",
      "          total_loss: 0.15824976610392333\n",
      "          vf_explained_var: 0.5516213178634644\n",
      "          vf_loss: 0.23125210523605347\n",
      "    num_agent_steps_sampled: 1950048\n",
      "    num_agent_steps_trained: 1950048\n",
      "    num_steps_sampled: 487512\n",
      "    num_steps_trained: 487512\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.10993377483444\n",
      "    gpu_util_percent0: 0.16344370860927152\n",
      "    ram_util_percent: 95.81258278145694\n",
      "    vram_util_percent0: 0.4977249400181742\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0755655001381097\n",
      "  policy_reward_mean:\n",
      "    main: 0.04856501602920312\n",
      "  policy_reward_min:\n",
      "    main: -1.736226201134685\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31162447910521945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.66529650961471\n",
      "    mean_inference_ms: 4.5465488038356785\n",
      "    mean_raw_obs_processing_ms: 1.1926838745264416\n",
      "  time_since_restore: 7177.645297288895\n",
      "  time_this_iter_s: 116.7271625995636\n",
      "  time_total_s: 7177.645297288895\n",
      "  timers:\n",
      "    learn_throughput: 89.444\n",
      "    learn_time_ms: 89351.711\n",
      "    sample_throughput: 304.956\n",
      "    sample_time_ms: 26207.056\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1639165394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487512\n",
      "  training_iteration: 61\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         7177.65</td><td style=\"text-align: right;\">487512</td><td style=\"text-align: right;\"> 0.19426</td><td style=\"text-align: right;\">             1.49208</td><td style=\"text-align: right;\">            -1.24618</td><td style=\"text-align: right;\">           62.9462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 1982016\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6952220429104756\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05157388166700274\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5074896316315114\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 76.70476190476191\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.225722871174944\n",
      "  episode_reward_mean: 0.121832029386902\n",
      "  episode_reward_min: -0.8575380232318381\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 3858\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.8065487594604492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017338338937610387\n",
      "          policy_loss: -0.08588250354304909\n",
      "          total_loss: 0.13407124420255423\n",
      "          vf_explained_var: 0.5217446088790894\n",
      "          vf_loss: 0.20825036889314652\n",
      "    num_agent_steps_sampled: 1982016\n",
      "    num_agent_steps_trained: 1982016\n",
      "    num_steps_sampled: 495504\n",
      "    num_steps_trained: 495504\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.49266666666667\n",
      "    gpu_util_percent0: 0.16473333333333334\n",
      "    ram_util_percent: 95.82999999999998\n",
      "    vram_util_percent0: 0.4979960513326753\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7299288166167912\n",
      "  policy_reward_mean:\n",
      "    main: 0.03045800734672549\n",
      "  policy_reward_min:\n",
      "    main: -2.006761967166735\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3105048985226901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.639074503511925\n",
      "    mean_inference_ms: 4.536120511238157\n",
      "    mean_raw_obs_processing_ms: 1.1949437107733407\n",
      "  time_since_restore: 7294.565434455872\n",
      "  time_this_iter_s: 116.92013716697693\n",
      "  time_total_s: 7294.565434455872\n",
      "  timers:\n",
      "    learn_throughput: 89.446\n",
      "    learn_time_ms: 89350.165\n",
      "    sample_throughput: 305.058\n",
      "    sample_time_ms: 26198.268\n",
      "    update_time_ms: 2.962\n",
      "  timestamp: 1639165511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495504\n",
      "  training_iteration: 62\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         7294.57</td><td style=\"text-align: right;\">495504</td><td style=\"text-align: right;\">0.121832</td><td style=\"text-align: right;\">             1.22572</td><td style=\"text-align: right;\">           -0.857538</td><td style=\"text-align: right;\">           76.7048</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2013984\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5992188318296012\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06704836639009541\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.540416371316864\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-47-07\n",
      "  done: false\n",
      "  episode_len_mean: 73.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8179328220432884\n",
      "  episode_reward_mean: 0.11460813498577803\n",
      "  episode_reward_min: -0.8798360333558142\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 3956\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7912957062721253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01699207693338394\n",
      "          policy_loss: -0.08448824582993984\n",
      "          total_loss: 0.12320781476423144\n",
      "          vf_explained_var: 0.5218665599822998\n",
      "          vf_loss: 0.19622640866041183\n",
      "    num_agent_steps_sampled: 2013984\n",
      "    num_agent_steps_trained: 2013984\n",
      "    num_steps_sampled: 503496\n",
      "    num_steps_trained: 503496\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.82\n",
      "    gpu_util_percent0: 0.16519999999999999\n",
      "    ram_util_percent: 95.782\n",
      "    vram_util_percent0: 0.49802676318964567\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7888932078978086\n",
      "  policy_reward_mean:\n",
      "    main: 0.028652033746444497\n",
      "  policy_reward_min:\n",
      "    main: -1.527069121580523\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31099350885745364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.638397041267485\n",
      "    mean_inference_ms: 4.539440477574841\n",
      "    mean_raw_obs_processing_ms: 1.195246997929181\n",
      "  time_since_restore: 7410.700727701187\n",
      "  time_this_iter_s: 116.13529324531555\n",
      "  time_total_s: 7410.700727701187\n",
      "  timers:\n",
      "    learn_throughput: 89.344\n",
      "    learn_time_ms: 89451.839\n",
      "    sample_throughput: 305.556\n",
      "    sample_time_ms: 26155.618\n",
      "    update_time_ms: 2.936\n",
      "  timestamp: 1639165627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503496\n",
      "  training_iteration: 63\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">          7410.7</td><td style=\"text-align: right;\">503496</td><td style=\"text-align: right;\">0.114608</td><td style=\"text-align: right;\">             1.81793</td><td style=\"text-align: right;\">           -0.879836</td><td style=\"text-align: right;\">             73.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2045952\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7149858811064288\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06649446206271768\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6148296250783024\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-49-03\n",
      "  done: false\n",
      "  episode_len_mean: 76.90598290598291\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9039982635452924\n",
      "  episode_reward_mean: 0.131043666729228\n",
      "  episode_reward_min: -1.141567373170491\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 4073\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7431410474777222\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017272933814674617\n",
      "          policy_loss: -0.08710802176222206\n",
      "          total_loss: 0.15290440386533738\n",
      "          vf_explained_var: 0.5395963191986084\n",
      "          vf_loss: 0.22835319352149963\n",
      "    num_agent_steps_sampled: 2045952\n",
      "    num_agent_steps_trained: 2045952\n",
      "    num_steps_sampled: 511488\n",
      "    num_steps_trained: 511488\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.771333333333324\n",
      "    gpu_util_percent0: 0.16566666666666663\n",
      "    ram_util_percent: 95.84599999999998\n",
      "    vram_util_percent0: 0.4980048261489526\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7123225779698446\n",
      "  policy_reward_mean:\n",
      "    main: 0.03276091668230698\n",
      "  policy_reward_min:\n",
      "    main: -1.6148296250783023\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31148379905022394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.63894396076603\n",
      "    mean_inference_ms: 4.5449564068210515\n",
      "    mean_raw_obs_processing_ms: 1.1967282062288325\n",
      "  time_since_restore: 7526.492632389069\n",
      "  time_this_iter_s: 115.79190468788147\n",
      "  time_total_s: 7526.492632389069\n",
      "  timers:\n",
      "    learn_throughput: 89.353\n",
      "    learn_time_ms: 89442.767\n",
      "    sample_throughput: 304.897\n",
      "    sample_time_ms: 26212.122\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1639165743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511488\n",
      "  training_iteration: 64\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         7526.49</td><td style=\"text-align: right;\">511488</td><td style=\"text-align: right;\">0.131044</td><td style=\"text-align: right;\">            0.903998</td><td style=\"text-align: right;\">            -1.14157</td><td style=\"text-align: right;\">            76.906</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2077920\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0010207064096965\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10488424690084526\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.44333721315322383\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 71.41904761904762\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.53405735614608\n",
      "  episode_reward_mean: 0.1895554291318477\n",
      "  episode_reward_min: -1.3868085524526466\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 4178\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7454988770484925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017153504818677903\n",
      "          policy_loss: -0.087527501758188\n",
      "          total_loss: 0.13527415975928306\n",
      "          vf_explained_var: 0.5094399452209473\n",
      "          vf_loss: 0.2112230453491211\n",
      "    num_agent_steps_sampled: 2077920\n",
      "    num_agent_steps_trained: 2077920\n",
      "    num_steps_sampled: 519480\n",
      "    num_steps_trained: 519480\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.888079470198676\n",
      "    gpu_util_percent0: 0.16417218543046358\n",
      "    ram_util_percent: 95.8682119205298\n",
      "    vram_util_percent0: 0.4980115016921303\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0450453543542118\n",
      "  policy_reward_mean:\n",
      "    main: 0.047388857282961945\n",
      "  policy_reward_min:\n",
      "    main: -1.4847251465665807\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3109654619385368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.6183397489381\n",
      "    mean_inference_ms: 4.53753465683586\n",
      "    mean_raw_obs_processing_ms: 1.1978174492949327\n",
      "  time_since_restore: 7643.626532077789\n",
      "  time_this_iter_s: 117.1338996887207\n",
      "  time_total_s: 7643.626532077789\n",
      "  timers:\n",
      "    learn_throughput: 89.228\n",
      "    learn_time_ms: 89568.44\n",
      "    sample_throughput: 304.984\n",
      "    sample_time_ms: 26204.662\n",
      "    update_time_ms: 2.962\n",
      "  timestamp: 1639165860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519480\n",
      "  training_iteration: 65\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         7643.63</td><td style=\"text-align: right;\">519480</td><td style=\"text-align: right;\">0.189555</td><td style=\"text-align: right;\">             1.53406</td><td style=\"text-align: right;\">            -1.38681</td><td style=\"text-align: right;\">            71.419</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2109888\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6945263536345533\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04663782895544311\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9609292149454739\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 73.86842105263158\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7266261375826877\n",
      "  episode_reward_mean: 0.18946862041547313\n",
      "  episode_reward_min: -0.7546274310293408\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 4292\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.7145503845214844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017095942683517933\n",
      "          policy_loss: -0.08907159437611699\n",
      "          total_loss: 0.15570729342103004\n",
      "          vf_explained_var: 0.5270218253135681\n",
      "          vf_loss: 0.23323912596702576\n",
      "    num_agent_steps_sampled: 2109888\n",
      "    num_agent_steps_trained: 2109888\n",
      "    num_steps_sampled: 527472\n",
      "    num_steps_trained: 527472\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84066666666667\n",
      "    gpu_util_percent0: 0.16746666666666665\n",
      "    ram_util_percent: 95.88799999999998\n",
      "    vram_util_percent0: 0.49803115059778447\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9932509992828253\n",
      "  policy_reward_mean:\n",
      "    main: 0.0473671551038683\n",
      "  policy_reward_min:\n",
      "    main: -1.9753784573757005\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3110051113174015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.60771592721025\n",
      "    mean_inference_ms: 4.535401399085701\n",
      "    mean_raw_obs_processing_ms: 1.1990058025520307\n",
      "  time_since_restore: 7759.931578159332\n",
      "  time_this_iter_s: 116.30504608154297\n",
      "  time_total_s: 7759.931578159332\n",
      "  timers:\n",
      "    learn_throughput: 89.216\n",
      "    learn_time_ms: 89580.19\n",
      "    sample_throughput: 305.398\n",
      "    sample_time_ms: 26169.092\n",
      "    update_time_ms: 3.006\n",
      "  timestamp: 1639165977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527472\n",
      "  training_iteration: 66\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         7759.93</td><td style=\"text-align: right;\">527472</td><td style=\"text-align: right;\">0.189469</td><td style=\"text-align: right;\">             1.72663</td><td style=\"text-align: right;\">           -0.754627</td><td style=\"text-align: right;\">           73.8684</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2141856\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6863749495909451\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11949636645070814\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4110060089114687\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-54-57\n",
      "  done: false\n",
      "  episode_len_mean: 65.81739130434782\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9903700204626835\n",
      "  episode_reward_mean: 0.2322745680603212\n",
      "  episode_reward_min: -0.6513431950957456\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 4407\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6983989930152894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01770222620293498\n",
      "          policy_loss: -0.08901978233084083\n",
      "          total_loss: 0.1417877187654376\n",
      "          vf_explained_var: 0.5305647253990173\n",
      "          vf_loss: 0.2188584975004196\n",
      "    num_agent_steps_sampled: 2141856\n",
      "    num_agent_steps_trained: 2141856\n",
      "    num_steps_sampled: 535464\n",
      "    num_steps_trained: 535464\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.794155844155846\n",
      "    gpu_util_percent0: 0.17662337662337663\n",
      "    ram_util_percent: 96.0318181818182\n",
      "    vram_util_percent0: 0.5087936906791792\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7901915593158875\n",
      "  policy_reward_mean:\n",
      "    main: 0.0580686420150803\n",
      "  policy_reward_min:\n",
      "    main: -1.6081431790740173\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31030534805083154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.584511622054784\n",
      "    mean_inference_ms: 4.528794800429726\n",
      "    mean_raw_obs_processing_ms: 1.2010915767578663\n",
      "  time_since_restore: 7880.266128778458\n",
      "  time_this_iter_s: 120.33455061912537\n",
      "  time_total_s: 7880.266128778458\n",
      "  timers:\n",
      "    learn_throughput: 88.741\n",
      "    learn_time_ms: 90059.994\n",
      "    sample_throughput: 305.071\n",
      "    sample_time_ms: 26197.165\n",
      "    update_time_ms: 3.031\n",
      "  timestamp: 1639166097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535464\n",
      "  training_iteration: 67\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         7880.27</td><td style=\"text-align: right;\">535464</td><td style=\"text-align: right;\">0.232275</td><td style=\"text-align: right;\">             0.99037</td><td style=\"text-align: right;\">           -0.651343</td><td style=\"text-align: right;\">           65.8174</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2173824\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8216064828973567\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05767911267305805\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47911500557504094\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 69.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5931609534429332\n",
      "  episode_reward_mean: 0.1798005750341291\n",
      "  episode_reward_min: -0.8762566280715562\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 4527\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.650914800643921\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01751050766929984\n",
      "          policy_loss: -0.08942270458489657\n",
      "          total_loss: 0.14733181358128786\n",
      "          vf_explained_var: 0.5552818775177002\n",
      "          vf_loss: 0.22493492674827575\n",
      "    num_agent_steps_sampled: 2173824\n",
      "    num_agent_steps_trained: 2173824\n",
      "    num_steps_sampled: 543456\n",
      "    num_steps_trained: 543456\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.615686274509805\n",
      "    gpu_util_percent0: 0.1633333333333333\n",
      "    ram_util_percent: 96.04509803921569\n",
      "    vram_util_percent0: 0.5196218226239712\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8471806488814477\n",
      "  policy_reward_mean:\n",
      "    main: 0.0449501437585323\n",
      "  policy_reward_min:\n",
      "    main: -1.8401913959688407\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3103843910440888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.584385354215996\n",
      "    mean_inference_ms: 4.5283466911598795\n",
      "    mean_raw_obs_processing_ms: 1.2028841093129155\n",
      "  time_since_restore: 7999.252014636993\n",
      "  time_this_iter_s: 118.98588585853577\n",
      "  time_total_s: 7999.252014636993\n",
      "  timers:\n",
      "    learn_throughput: 88.456\n",
      "    learn_time_ms: 90349.65\n",
      "    sample_throughput: 304.045\n",
      "    sample_time_ms: 26285.602\n",
      "    update_time_ms: 3.031\n",
      "  timestamp: 1639166216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543456\n",
      "  training_iteration: 68\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         7999.25</td><td style=\"text-align: right;\">543456</td><td style=\"text-align: right;\">0.179801</td><td style=\"text-align: right;\">             1.59316</td><td style=\"text-align: right;\">           -0.876257</td><td style=\"text-align: right;\">              69.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2205792\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8046757656354032\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07289764140679898\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6477575988261309\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_16-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 70.33035714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9982254310134299\n",
      "  episode_reward_mean: 0.10136896286330775\n",
      "  episode_reward_min: -1.3536710656797046\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 4639\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6631945371627808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017366346541792154\n",
      "          policy_loss: -0.09026781501993537\n",
      "          total_loss: 0.13927454394474625\n",
      "          vf_explained_var: 0.5388131141662598\n",
      "          vf_loss: 0.21782007366418837\n",
      "    num_agent_steps_sampled: 2205792\n",
      "    num_agent_steps_trained: 2205792\n",
      "    num_steps_sampled: 551448\n",
      "    num_steps_trained: 551448\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.73947368421052\n",
      "    gpu_util_percent0: 0.1875657894736842\n",
      "    ram_util_percent: 92.81907894736842\n",
      "    vram_util_percent0: 0.519151253009127\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8945806367612472\n",
      "  policy_reward_mean:\n",
      "    main: 0.025342240715826938\n",
      "  policy_reward_min:\n",
      "    main: -1.6477575988261308\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3104319764611799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.596720913484237\n",
      "    mean_inference_ms: 4.529385413746214\n",
      "    mean_raw_obs_processing_ms: 1.2052749791462465\n",
      "  time_since_restore: 8119.302442073822\n",
      "  time_this_iter_s: 120.05042743682861\n",
      "  time_total_s: 8119.302442073822\n",
      "  timers:\n",
      "    learn_throughput: 88.213\n",
      "    learn_time_ms: 90599.264\n",
      "    sample_throughput: 301.904\n",
      "    sample_time_ms: 26472.022\n",
      "    update_time_ms: 3.044\n",
      "  timestamp: 1639166336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551448\n",
      "  training_iteration: 69\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          8119.3</td><td style=\"text-align: right;\">551448</td><td style=\"text-align: right;\">0.101369</td><td style=\"text-align: right;\">            0.998225</td><td style=\"text-align: right;\">            -1.35367</td><td style=\"text-align: right;\">           70.3304</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2237760\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7656718263356665\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07049847784024894\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.631113928126711\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 58.956204379562045\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.176101146070894\n",
      "  episode_reward_mean: 0.1431458141989732\n",
      "  episode_reward_min: -1.1504010522708725\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 4776\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6274938740730285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017128465503454208\n",
      "          policy_loss: -0.08836504887975752\n",
      "          total_loss: 0.17890361895039678\n",
      "          vf_explained_var: 0.5376762747764587\n",
      "          vf_loss: 0.25570695465803145\n",
      "    num_agent_steps_sampled: 2237760\n",
      "    num_agent_steps_trained: 2237760\n",
      "    num_steps_sampled: 559440\n",
      "    num_steps_trained: 559440\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.031690140845072\n",
      "    gpu_util_percent0: 0.14028169014084507\n",
      "    ram_util_percent: 83.72535211267606\n",
      "    vram_util_percent0: 0.5024076674591808\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7533867359905735\n",
      "  policy_reward_mean:\n",
      "    main: 0.0357864535497433\n",
      "  policy_reward_min:\n",
      "    main: -1.631113928126711\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31094970568910435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.6030708063768\n",
      "    mean_inference_ms: 4.533634861287995\n",
      "    mean_raw_obs_processing_ms: 1.2083612193093434\n",
      "  time_since_restore: 8229.6315908432\n",
      "  time_this_iter_s: 110.32914876937866\n",
      "  time_total_s: 8229.6315908432\n",
      "  timers:\n",
      "    learn_throughput: 88.827\n",
      "    learn_time_ms: 89972.224\n",
      "    sample_throughput: 301.145\n",
      "    sample_time_ms: 26538.71\n",
      "    update_time_ms: 5.997\n",
      "  timestamp: 1639166447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559440\n",
      "  training_iteration: 70\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         8229.63</td><td style=\"text-align: right;\">559440</td><td style=\"text-align: right;\">0.143146</td><td style=\"text-align: right;\">              1.1761</td><td style=\"text-align: right;\">             -1.1504</td><td style=\"text-align: right;\">           58.9562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2269728\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0384697137147507\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07322098948818032\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5554887561666577\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-02-34\n",
      "  done: false\n",
      "  episode_len_mean: 67.44067796610169\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4949260437103362\n",
      "  episode_reward_mean: 0.12717444546221787\n",
      "  episode_reward_min: -0.8200673632733375\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 4894\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6198496165275573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017201962377876044\n",
      "          policy_loss: -0.09134389294311404\n",
      "          total_loss: 0.14965213790535928\n",
      "          vf_explained_var: 0.5353065729141235\n",
      "          vf_loss: 0.2293847069144249\n",
      "    num_agent_steps_sampled: 2269728\n",
      "    num_agent_steps_trained: 2269728\n",
      "    num_steps_sampled: 567432\n",
      "    num_steps_trained: 567432\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.61007194244604\n",
      "    gpu_util_percent0: 0.13064748201438853\n",
      "    ram_util_percent: 83.56906474820146\n",
      "    vram_util_percent0: 0.5005148891745438\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9186576108403728\n",
      "  policy_reward_mean:\n",
      "    main: 0.031793611365554475\n",
      "  policy_reward_min:\n",
      "    main: -1.5554887561666577\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30990482585390805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.548088982593324\n",
      "    mean_inference_ms: 4.520909850931355\n",
      "    mean_raw_obs_processing_ms: 1.2085929221415228\n",
      "  time_since_restore: 8336.797028064728\n",
      "  time_this_iter_s: 107.1654372215271\n",
      "  time_total_s: 8336.797028064728\n",
      "  timers:\n",
      "    learn_throughput: 89.535\n",
      "    learn_time_ms: 89261.34\n",
      "    sample_throughput: 303.856\n",
      "    sample_time_ms: 26301.89\n",
      "    update_time_ms: 5.972\n",
      "  timestamp: 1639166554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567432\n",
      "  training_iteration: 71\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">          8336.8</td><td style=\"text-align: right;\">567432</td><td style=\"text-align: right;\">0.127174</td><td style=\"text-align: right;\">             1.49493</td><td style=\"text-align: right;\">           -0.820067</td><td style=\"text-align: right;\">           67.4407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2301696\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9311667890829887\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.12021737842870994\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5135365240231461\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 67.84482758620689\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3590615017661505\n",
      "  episode_reward_mean: 0.22644989573987903\n",
      "  episode_reward_min: -0.8422497188985663\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 5010\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.6123317399024963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017623489793390037\n",
      "          policy_loss: -0.09074641119316221\n",
      "          total_loss: 0.14556085719913245\n",
      "          vf_explained_var: 0.5315612554550171\n",
      "          vf_loss: 0.2244114124774933\n",
      "    num_agent_steps_sampled: 2301696\n",
      "    num_agent_steps_trained: 2301696\n",
      "    num_steps_sampled: 575424\n",
      "    num_steps_trained: 575424\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.32836879432624\n",
      "    gpu_util_percent0: 0.12914893617021278\n",
      "    ram_util_percent: 83.58936170212766\n",
      "    vram_util_percent0: 0.4993208852296039\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.825966796426281\n",
      "  policy_reward_mean:\n",
      "    main: 0.05661247393496976\n",
      "  policy_reward_min:\n",
      "    main: -1.513536524023146\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3100401054846165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.52432316182361\n",
      "    mean_inference_ms: 4.517399539533724\n",
      "    mean_raw_obs_processing_ms: 1.2087909422987668\n",
      "  time_since_restore: 8444.839562892914\n",
      "  time_this_iter_s: 108.04253482818604\n",
      "  time_total_s: 8444.839562892914\n",
      "  timers:\n",
      "    learn_throughput: 90.249\n",
      "    learn_time_ms: 88555.025\n",
      "    sample_throughput: 305.935\n",
      "    sample_time_ms: 26123.157\n",
      "    update_time_ms: 5.962\n",
      "  timestamp: 1639166662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575424\n",
      "  training_iteration: 72\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         8444.84</td><td style=\"text-align: right;\">575424</td><td style=\"text-align: right;\"> 0.22645</td><td style=\"text-align: right;\">             1.35906</td><td style=\"text-align: right;\">            -0.84225</td><td style=\"text-align: right;\">           67.8448</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2333664\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6272453088020657\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06246518916994534\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4706380212250524\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-06-10\n",
      "  done: false\n",
      "  episode_len_mean: 63.13178294573643\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6263300698037169\n",
      "  episode_reward_mean: 0.1817974002944584\n",
      "  episode_reward_min: -0.7012193846129426\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 5139\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5619364938735962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01779341684281826\n",
      "          policy_loss: -0.09397609786316752\n",
      "          total_loss: 0.1571141315922141\n",
      "          vf_explained_var: 0.5426218509674072\n",
      "          vf_loss: 0.2390796727538109\n",
      "    num_agent_steps_sampled: 2333664\n",
      "    num_agent_steps_trained: 2333664\n",
      "    num_steps_sampled: 583416\n",
      "    num_steps_trained: 583416\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.141428571428573\n",
      "    gpu_util_percent0: 0.12985714285714287\n",
      "    ram_util_percent: 83.62785714285715\n",
      "    vram_util_percent0: 0.4988483053636066\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.020822439535928\n",
      "  policy_reward_mean:\n",
      "    main: 0.04544935007361459\n",
      "  policy_reward_min:\n",
      "    main: -1.5194574583788558\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3095004207963652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.486720279138506\n",
      "    mean_inference_ms: 4.5113927406749275\n",
      "    mean_raw_obs_processing_ms: 1.209428014940957\n",
      "  time_since_restore: 8552.483922958374\n",
      "  time_this_iter_s: 107.6443600654602\n",
      "  time_total_s: 8552.483922958374\n",
      "  timers:\n",
      "    learn_throughput: 90.925\n",
      "    learn_time_ms: 87896.823\n",
      "    sample_throughput: 308.21\n",
      "    sample_time_ms: 25930.345\n",
      "    update_time_ms: 5.964\n",
      "  timestamp: 1639166770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583416\n",
      "  training_iteration: 73\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         8552.48</td><td style=\"text-align: right;\">583416</td><td style=\"text-align: right;\">0.181797</td><td style=\"text-align: right;\">             1.62633</td><td style=\"text-align: right;\">           -0.701219</td><td style=\"text-align: right;\">           63.1318</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2365632\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7806689783967866\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07849367477360512\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7690576487615872\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-07-57\n",
      "  done: false\n",
      "  episode_len_mean: 66.34920634920636\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.573710192906209\n",
      "  episode_reward_mean: 0.19405182346862282\n",
      "  episode_reward_min: -1.1035403630046696\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 5265\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5557377834320067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017829205490648748\n",
      "          policy_loss: -0.0918546171747148\n",
      "          total_loss: 0.1507148667536676\n",
      "          vf_explained_var: 0.5501312613487244\n",
      "          vf_loss: 0.230534769654274\n",
      "    num_agent_steps_sampled: 2365632\n",
      "    num_agent_steps_trained: 2365632\n",
      "    num_steps_sampled: 591408\n",
      "    num_steps_trained: 591408\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.222302158273386\n",
      "    gpu_util_percent0: 0.1315107913669065\n",
      "    ram_util_percent: 83.69712230215828\n",
      "    vram_util_percent0: 0.49805525766948133\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0207132177845546\n",
      "  policy_reward_mean:\n",
      "    main: 0.048512955867155705\n",
      "  policy_reward_min:\n",
      "    main: -1.7690576487615872\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30876437518327327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.441068631041386\n",
      "    mean_inference_ms: 4.500935023229458\n",
      "    mean_raw_obs_processing_ms: 1.2099940146588366\n",
      "  time_since_restore: 8659.956097364426\n",
      "  time_this_iter_s: 107.47217440605164\n",
      "  time_total_s: 8659.956097364426\n",
      "  timers:\n",
      "    learn_throughput: 91.612\n",
      "    learn_time_ms: 87237.618\n",
      "    sample_throughput: 310.233\n",
      "    sample_time_ms: 25761.289\n",
      "    update_time_ms: 5.872\n",
      "  timestamp: 1639166877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591408\n",
      "  training_iteration: 74\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         8659.96</td><td style=\"text-align: right;\">591408</td><td style=\"text-align: right;\">0.194052</td><td style=\"text-align: right;\">             1.57371</td><td style=\"text-align: right;\">            -1.10354</td><td style=\"text-align: right;\">           66.3492</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2397600\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6752005910448636\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0858080175459635\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4991844454228357\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 60.508620689655174\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.337661218389803\n",
      "  episode_reward_mean: 0.15632320927758672\n",
      "  episode_reward_min: -0.724750403690456\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 5381\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5602878770828248\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01761908907070756\n",
      "          policy_loss: -0.09167408392205835\n",
      "          total_loss: 0.14344888251274823\n",
      "          vf_explained_var: 0.5354542136192322\n",
      "          vf_loss: 0.22323008185625076\n",
      "    num_agent_steps_sampled: 2397600\n",
      "    num_agent_steps_trained: 2397600\n",
      "    num_steps_sampled: 599400\n",
      "    num_steps_trained: 599400\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.292142857142853\n",
      "    gpu_util_percent0: 0.13000000000000003\n",
      "    ram_util_percent: 83.68857142857144\n",
      "    vram_util_percent0: 0.496924505241386\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8321234965896336\n",
      "  policy_reward_mean:\n",
      "    main: 0.03908080231939668\n",
      "  policy_reward_min:\n",
      "    main: -1.5995888768107633\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3094151697001737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.43022316722103\n",
      "    mean_inference_ms: 4.503956820676836\n",
      "    mean_raw_obs_processing_ms: 1.2101571184081052\n",
      "  time_since_restore: 8767.513601779938\n",
      "  time_this_iter_s: 107.55750441551208\n",
      "  time_total_s: 8767.513601779938\n",
      "  timers:\n",
      "    learn_throughput: 92.441\n",
      "    learn_time_ms: 86455.474\n",
      "    sample_throughput: 312.342\n",
      "    sample_time_ms: 25587.331\n",
      "    update_time_ms: 5.843\n",
      "  timestamp: 1639166985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599400\n",
      "  training_iteration: 75\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         8767.51</td><td style=\"text-align: right;\">599400</td><td style=\"text-align: right;\">0.156323</td><td style=\"text-align: right;\">             1.33766</td><td style=\"text-align: right;\">            -0.72475</td><td style=\"text-align: right;\">           60.5086</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2429568\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6244994564588717\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.042286169453356164\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.527391821962743\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 66.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3007612971484135\n",
      "  episode_reward_mean: 0.12556531100753254\n",
      "  episode_reward_min: -1.3895068303432039\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 5511\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5414108414649963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0177266904078424\n",
      "          policy_loss: -0.09213293893635273\n",
      "          total_loss: 0.15315650252625346\n",
      "          vf_explained_var: 0.552013635635376\n",
      "          vf_loss: 0.23332392531633378\n",
      "    num_agent_steps_sampled: 2429568\n",
      "    num_agent_steps_trained: 2429568\n",
      "    num_steps_sampled: 607392\n",
      "    num_steps_trained: 607392\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.22714285714286\n",
      "    gpu_util_percent0: 0.12921428571428573\n",
      "    ram_util_percent: 83.73285714285713\n",
      "    vram_util_percent0: 0.4968939500775632\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8838981088667737\n",
      "  policy_reward_mean:\n",
      "    main: 0.03139132775188314\n",
      "  policy_reward_min:\n",
      "    main: -1.659886740583766\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30831415036211746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.380971835498187\n",
      "    mean_inference_ms: 4.492186146419905\n",
      "    mean_raw_obs_processing_ms: 1.2111075310662156\n",
      "  time_since_restore: 8875.040640592575\n",
      "  time_this_iter_s: 107.52703881263733\n",
      "  time_total_s: 8875.040640592575\n",
      "  timers:\n",
      "    learn_throughput: 93.22\n",
      "    learn_time_ms: 85732.233\n",
      "    sample_throughput: 314.187\n",
      "    sample_time_ms: 25437.083\n",
      "    update_time_ms: 5.754\n",
      "  timestamp: 1639167092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607392\n",
      "  training_iteration: 76\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         8875.04</td><td style=\"text-align: right;\">607392</td><td style=\"text-align: right;\">0.125565</td><td style=\"text-align: right;\">             1.30076</td><td style=\"text-align: right;\">            -1.38951</td><td style=\"text-align: right;\">              66.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2461536\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5567291453465654\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0463624661766184\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.623449814311938\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 68.16239316239316\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2688243169265556\n",
      "  episode_reward_mean: 0.1808904050085819\n",
      "  episode_reward_min: -0.7340871143641876\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 5628\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5292282447814942\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01854180118069053\n",
      "          policy_loss: -0.0960446960888803\n",
      "          total_loss: 0.13127278973162174\n",
      "          vf_explained_var: 0.5691952109336853\n",
      "          vf_loss: 0.21480176854133606\n",
      "    num_agent_steps_sampled: 2461536\n",
      "    num_agent_steps_trained: 2461536\n",
      "    num_steps_sampled: 615384\n",
      "    num_steps_trained: 615384\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.332624113475173\n",
      "    gpu_util_percent0: 0.12617021276595744\n",
      "    ram_util_percent: 83.82978723404253\n",
      "    vram_util_percent0: 0.4969008095701507\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9517653424977939\n",
      "  policy_reward_mean:\n",
      "    main: 0.045222601252145465\n",
      "  policy_reward_min:\n",
      "    main: -1.7020372689771315\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3081316498455576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.352662291307954\n",
      "    mean_inference_ms: 4.486914099277068\n",
      "    mean_raw_obs_processing_ms: 1.2111738837364256\n",
      "  time_since_restore: 8983.701591968536\n",
      "  time_this_iter_s: 108.6609513759613\n",
      "  time_total_s: 8983.701591968536\n",
      "  timers:\n",
      "    learn_throughput: 94.321\n",
      "    learn_time_ms: 84731.624\n",
      "    sample_throughput: 316.095\n",
      "    sample_time_ms: 25283.575\n",
      "    update_time_ms: 5.696\n",
      "  timestamp: 1639167201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615384\n",
      "  training_iteration: 77\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">          8983.7</td><td style=\"text-align: right;\">615384</td><td style=\"text-align: right;\"> 0.18089</td><td style=\"text-align: right;\">             1.26882</td><td style=\"text-align: right;\">           -0.734087</td><td style=\"text-align: right;\">           68.1624</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2493504\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8806337085924287\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08448331085455667\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49003916671135866\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-15-08\n",
      "  done: false\n",
      "  episode_len_mean: 68.28828828828829\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4284477045604538\n",
      "  episode_reward_mean: 0.1355000624635489\n",
      "  episode_reward_min: -1.0376270860724321\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 5739\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5273326835632324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017889710236340763\n",
      "          policy_loss: -0.09257579062879086\n",
      "          total_loss: 0.13089617036655546\n",
      "          vf_explained_var: 0.5407978296279907\n",
      "          vf_loss: 0.21139640867710113\n",
      "    num_agent_steps_sampled: 2493504\n",
      "    num_agent_steps_trained: 2493504\n",
      "    num_steps_sampled: 623376\n",
      "    num_steps_trained: 623376\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.16642857142857\n",
      "    gpu_util_percent0: 0.12828571428571428\n",
      "    ram_util_percent: 83.88071428571428\n",
      "    vram_util_percent0: 0.49689277487895467\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8336087493192545\n",
      "  policy_reward_mean:\n",
      "    main: 0.03387501561588723\n",
      "  policy_reward_min:\n",
      "    main: -1.642989155716862\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3084009944987006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.332901665279667\n",
      "    mean_inference_ms: 4.485454018180064\n",
      "    mean_raw_obs_processing_ms: 1.21136478752107\n",
      "  time_since_restore: 9090.812157869339\n",
      "  time_this_iter_s: 107.11056590080261\n",
      "  time_total_s: 9090.812157869339\n",
      "  timers:\n",
      "    learn_throughput: 95.345\n",
      "    learn_time_ms: 83822.061\n",
      "    sample_throughput: 319.476\n",
      "    sample_time_ms: 25015.976\n",
      "    update_time_ms: 5.652\n",
      "  timestamp: 1639167308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623376\n",
      "  training_iteration: 78\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         9090.81</td><td style=\"text-align: right;\">623376</td><td style=\"text-align: right;\">  0.1355</td><td style=\"text-align: right;\">             1.42845</td><td style=\"text-align: right;\">            -1.03763</td><td style=\"text-align: right;\">           68.2883</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2525472\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7742725896258551\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07187341607174028\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4096211599732369\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-16-56\n",
      "  done: false\n",
      "  episode_len_mean: 65.078125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1261909191341108\n",
      "  episode_reward_mean: 0.08369399886513583\n",
      "  episode_reward_min: -2.3028217246121514\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 5867\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.4835426363945008\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018097167827188967\n",
      "          policy_loss: -0.0942551704980433\n",
      "          total_loss: 0.14513768276572228\n",
      "          vf_explained_var: 0.5362275242805481\n",
      "          vf_loss: 0.22717726427316665\n",
      "    num_agent_steps_sampled: 2525472\n",
      "    num_agent_steps_trained: 2525472\n",
      "    num_steps_sampled: 631368\n",
      "    num_steps_trained: 631368\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.23\n",
      "    gpu_util_percent0: 0.13014285714285717\n",
      "    ram_util_percent: 83.82214285714285\n",
      "    vram_util_percent0: 0.4969033516664318\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7204838816844068\n",
      "  policy_reward_mean:\n",
      "    main: 0.020923499716283968\n",
      "  policy_reward_min:\n",
      "    main: -1.530246700965002\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30706572739964455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.278649209484065\n",
      "    mean_inference_ms: 4.471467102126489\n",
      "    mean_raw_obs_processing_ms: 1.2116219952120524\n",
      "  time_since_restore: 9198.684446811676\n",
      "  time_this_iter_s: 107.87228894233704\n",
      "  time_total_s: 9198.684446811676\n",
      "  timers:\n",
      "    learn_throughput: 96.389\n",
      "    learn_time_ms: 82914.092\n",
      "    sample_throughput: 323.379\n",
      "    sample_time_ms: 24714.042\n",
      "    update_time_ms: 5.635\n",
      "  timestamp: 1639167416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631368\n",
      "  training_iteration: 79\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         9198.68</td><td style=\"text-align: right;\">631368</td><td style=\"text-align: right;\">0.083694</td><td style=\"text-align: right;\">             1.12619</td><td style=\"text-align: right;\">            -2.30282</td><td style=\"text-align: right;\">           65.0781</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2557440\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8775574285456815\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05311528668411017\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5621129535882151\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-18-43\n",
      "  done: false\n",
      "  episode_len_mean: 68.25688073394495\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2975994141234986\n",
      "  episode_reward_mean: 0.18256554624155255\n",
      "  episode_reward_min: -0.9835274093186364\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 5976\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.5068264598846435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018019405215978623\n",
      "          policy_loss: -0.09451178255863488\n",
      "          total_loss: 0.11021362078934908\n",
      "          vf_explained_var: 0.5469119548797607\n",
      "          vf_loss: 0.19256230527162552\n",
      "    num_agent_steps_sampled: 2557440\n",
      "    num_agent_steps_trained: 2557440\n",
      "    num_steps_sampled: 639360\n",
      "    num_steps_trained: 639360\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.59285714285714\n",
      "    gpu_util_percent0: 0.12942857142857145\n",
      "    ram_util_percent: 83.88214285714285\n",
      "    vram_util_percent0: 0.49689159968034613\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8920488624853427\n",
      "  policy_reward_mean:\n",
      "    main: 0.045641386560388145\n",
      "  policy_reward_min:\n",
      "    main: -1.6654983428298136\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3073698187462537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.25823531179044\n",
      "    mean_inference_ms: 4.472528579133808\n",
      "    mean_raw_obs_processing_ms: 1.210956493906581\n",
      "  time_since_restore: 9305.8787317276\n",
      "  time_this_iter_s: 107.19428491592407\n",
      "  time_total_s: 9305.8787317276\n",
      "  timers:\n",
      "    learn_throughput: 96.418\n",
      "    learn_time_ms: 82888.893\n",
      "    sample_throughput: 327.194\n",
      "    sample_time_ms: 24425.872\n",
      "    update_time_ms: 2.662\n",
      "  timestamp: 1639167523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639360\n",
      "  training_iteration: 80\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         9305.88</td><td style=\"text-align: right;\">639360</td><td style=\"text-align: right;\">0.182566</td><td style=\"text-align: right;\">              1.2976</td><td style=\"text-align: right;\">           -0.983527</td><td style=\"text-align: right;\">           68.2569</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2589408\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7663033423560734\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11134273225502875\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5086194844333414\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-20-35\n",
      "  done: false\n",
      "  episode_len_mean: 83.11428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2215746296029155\n",
      "  episode_reward_mean: 0.10949127484528667\n",
      "  episode_reward_min: -1.2810714720175767\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 6081\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.476920433998108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018403019949793816\n",
      "          policy_loss: -0.09796310111880302\n",
      "          total_loss: 0.12763183892145752\n",
      "          vf_explained_var: 0.5275274515151978\n",
      "          vf_loss: 0.21317290169000624\n",
      "    num_agent_steps_sampled: 2589408\n",
      "    num_agent_steps_trained: 2589408\n",
      "    num_steps_sampled: 647352\n",
      "    num_steps_trained: 647352\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.428472222222222\n",
      "    gpu_util_percent0: 0.15597222222222223\n",
      "    ram_util_percent: 84.12847222222223\n",
      "    vram_util_percent0: 0.499545263427297\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7614324005141313\n",
      "  policy_reward_mean:\n",
      "    main: 0.02737281871132166\n",
      "  policy_reward_min:\n",
      "    main: -1.5154754680258065\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30637944952244756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.21356903009641\n",
      "    mean_inference_ms: 4.4610113628905514\n",
      "    mean_raw_obs_processing_ms: 1.2105342352779849\n",
      "  time_since_restore: 9417.497292041779\n",
      "  time_this_iter_s: 111.61856031417847\n",
      "  time_total_s: 9417.497292041779\n",
      "  timers:\n",
      "    learn_throughput: 95.962\n",
      "    learn_time_ms: 83282.836\n",
      "    sample_throughput: 326.519\n",
      "    sample_time_ms: 24476.368\n",
      "    update_time_ms: 2.727\n",
      "  timestamp: 1639167635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647352\n",
      "  training_iteration: 81\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">          9417.5</td><td style=\"text-align: right;\">647352</td><td style=\"text-align: right;\">0.109491</td><td style=\"text-align: right;\">             1.22157</td><td style=\"text-align: right;\">            -1.28107</td><td style=\"text-align: right;\">           83.1143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2621376\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8808459887872638\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0847868122959946\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.511096616643566\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-22-32\n",
      "  done: false\n",
      "  episode_len_mean: 63.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9113629322213637\n",
      "  episode_reward_mean: 0.19288822625148377\n",
      "  episode_reward_min: -1.1924591950222774\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 6197\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.4527760000228882\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018494419384747742\n",
      "          policy_loss: -0.09694586000218987\n",
      "          total_loss: 0.13780899839103222\n",
      "          vf_explained_var: 0.5422698855400085\n",
      "          vf_loss: 0.22227112579345704\n",
      "    num_agent_steps_sampled: 2621376\n",
      "    num_agent_steps_trained: 2621376\n",
      "    num_steps_sampled: 655344\n",
      "    num_steps_trained: 655344\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.55165562913907\n",
      "    gpu_util_percent0: 0.16655629139072847\n",
      "    ram_util_percent: 84.72980132450331\n",
      "    vram_util_percent0: 0.5086720317985396\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9174207162510593\n",
      "  policy_reward_mean:\n",
      "    main: 0.048222056562870944\n",
      "  policy_reward_min:\n",
      "    main: -1.5715909342795156\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30684580511262205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.223669559450236\n",
      "    mean_inference_ms: 4.464449982009913\n",
      "    mean_raw_obs_processing_ms: 1.2121602986406796\n",
      "  time_since_restore: 9534.219754934311\n",
      "  time_this_iter_s: 116.72246289253235\n",
      "  time_total_s: 9534.219754934311\n",
      "  timers:\n",
      "    learn_throughput: 95.209\n",
      "    learn_time_ms: 83941.339\n",
      "    sample_throughput: 323.75\n",
      "    sample_time_ms: 24685.678\n",
      "    update_time_ms: 2.726\n",
      "  timestamp: 1639167752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655344\n",
      "  training_iteration: 82\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         9534.22</td><td style=\"text-align: right;\">655344</td><td style=\"text-align: right;\">0.192888</td><td style=\"text-align: right;\">             1.91136</td><td style=\"text-align: right;\">            -1.19246</td><td style=\"text-align: right;\">             63.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2653344\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7902362927755694\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0994308953784384\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.597218598331109\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-24-31\n",
      "  done: false\n",
      "  episode_len_mean: 71.30578512396694\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8038305608144714\n",
      "  episode_reward_mean: 0.17164334448386048\n",
      "  episode_reward_min: -1.2796727569437438\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 6318\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.410855402469635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018288762494921684\n",
      "          policy_loss: -0.09761337907984853\n",
      "          total_loss: 0.13785105220228433\n",
      "          vf_explained_var: 0.5569013953208923\n",
      "          vf_loss: 0.22311951601505278\n",
      "    num_agent_steps_sampled: 2653344\n",
      "    num_agent_steps_trained: 2653344\n",
      "    num_steps_sampled: 663336\n",
      "    num_steps_trained: 663336\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.90519480519481\n",
      "    gpu_util_percent0: 0.19564935064935066\n",
      "    ram_util_percent: 84.862987012987\n",
      "    vram_util_percent0: 0.5068727751353616\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.026194360593153\n",
      "  policy_reward_mean:\n",
      "    main: 0.04291083612096514\n",
      "  policy_reward_min:\n",
      "    main: -1.6469244640175469\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30671926318177395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.23230528155797\n",
      "    mean_inference_ms: 4.4664797435909245\n",
      "    mean_raw_obs_processing_ms: 1.2137257345683876\n",
      "  time_since_restore: 9653.20374417305\n",
      "  time_this_iter_s: 118.98398923873901\n",
      "  time_total_s: 9653.20374417305\n",
      "  timers:\n",
      "    learn_throughput: 94.31\n",
      "    learn_time_ms: 84742.082\n",
      "    sample_throughput: 319.393\n",
      "    sample_time_ms: 25022.435\n",
      "    update_time_ms: 2.714\n",
      "  timestamp: 1639167871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663336\n",
      "  training_iteration: 83\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          9653.2</td><td style=\"text-align: right;\">663336</td><td style=\"text-align: right;\">0.171643</td><td style=\"text-align: right;\">             1.80383</td><td style=\"text-align: right;\">            -1.27967</td><td style=\"text-align: right;\">           71.3058</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2685312\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7532074041715551\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.054903416876578716\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6148743922539741\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-26-31\n",
      "  done: false\n",
      "  episode_len_mean: 63.66393442622951\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.12436315283147\n",
      "  episode_reward_mean: 0.1775599086094911\n",
      "  episode_reward_min: -2.0047568696219304\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 6440\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.4118429350852966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01818602829053998\n",
      "          policy_loss: -0.09826173852011562\n",
      "          total_loss: 0.16298210851848124\n",
      "          vf_explained_var: 0.5313013195991516\n",
      "          vf_loss: 0.24896827840805053\n",
      "    num_agent_steps_sampled: 2685312\n",
      "    num_agent_steps_trained: 2685312\n",
      "    num_steps_sampled: 671328\n",
      "    num_steps_trained: 671328\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.76774193548387\n",
      "    gpu_util_percent0: 0.1538064516129032\n",
      "    ram_util_percent: 85.6283870967742\n",
      "    vram_util_percent0: 0.5043647634514749\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.773865564735483\n",
      "  policy_reward_mean:\n",
      "    main: 0.04438997715237278\n",
      "  policy_reward_min:\n",
      "    main: -2.11575480902071\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3069788840157388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.25730207807557\n",
      "    mean_inference_ms: 4.470393350596777\n",
      "    mean_raw_obs_processing_ms: 1.2168813982306934\n",
      "  time_since_restore: 9773.074768066406\n",
      "  time_this_iter_s: 119.87102389335632\n",
      "  time_total_s: 9773.074768066406\n",
      "  timers:\n",
      "    learn_throughput: 93.383\n",
      "    learn_time_ms: 85582.703\n",
      "    sample_throughput: 314.435\n",
      "    sample_time_ms: 25417.051\n",
      "    update_time_ms: 2.721\n",
      "  timestamp: 1639167991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 671328\n",
      "  training_iteration: 84\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         9773.07</td><td style=\"text-align: right;\">671328</td><td style=\"text-align: right;\"> 0.17756</td><td style=\"text-align: right;\">             1.12436</td><td style=\"text-align: right;\">            -2.00476</td><td style=\"text-align: right;\">           63.6639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2717280\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1294547003918656\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04013662286261264\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4725515470184552\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 66.09243697478992\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3877897907865235\n",
      "  episode_reward_mean: 0.06635200140375368\n",
      "  episode_reward_min: -2.116088920107957\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 6559\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.415392084121704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018579752344638108\n",
      "          policy_loss: -0.09705694394558669\n",
      "          total_loss: 0.13335034120548517\n",
      "          vf_explained_var: 0.5521549582481384\n",
      "          vf_loss: 0.21786595022678376\n",
      "    num_agent_steps_sampled: 2717280\n",
      "    num_agent_steps_trained: 2717280\n",
      "    num_steps_sampled: 679320\n",
      "    num_steps_trained: 679320\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.96644736842105\n",
      "    gpu_util_percent0: 0.15598684210526317\n",
      "    ram_util_percent: 85.81973684210527\n",
      "    vram_util_percent0: 0.5064912713669663\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0053685208102148\n",
      "  policy_reward_mean:\n",
      "    main: 0.01658800035093842\n",
      "  policy_reward_min:\n",
      "    main: -1.6623020662515258\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30769041166767025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.28675639147262\n",
      "    mean_inference_ms: 4.476442127944941\n",
      "    mean_raw_obs_processing_ms: 1.2196482593726812\n",
      "  time_since_restore: 9891.285860300064\n",
      "  time_this_iter_s: 118.21109223365784\n",
      "  time_total_s: 9891.285860300064\n",
      "  timers:\n",
      "    learn_throughput: 92.603\n",
      "    learn_time_ms: 86303.923\n",
      "    sample_throughput: 310.233\n",
      "    sample_time_ms: 25761.25\n",
      "    update_time_ms: 2.703\n",
      "  timestamp: 1639168109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679320\n",
      "  training_iteration: 85\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         9891.29</td><td style=\"text-align: right;\">679320</td><td style=\"text-align: right;\">0.066352</td><td style=\"text-align: right;\">             1.38779</td><td style=\"text-align: right;\">            -2.11609</td><td style=\"text-align: right;\">           66.0924</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2749248\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8107777028247214\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05398287730303317\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5438919686015948\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 66.98360655737704\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0883168022061986\n",
      "  episode_reward_mean: 0.16795984961867036\n",
      "  episode_reward_min: -0.6797232512618043\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 6681\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3917043418884278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018259979523718356\n",
      "          policy_loss: -0.09652146803587675\n",
      "          total_loss: 0.1411328239440918\n",
      "          vf_explained_var: 0.5506038665771484\n",
      "          vf_loss: 0.225328806579113\n",
      "    num_agent_steps_sampled: 2749248\n",
      "    num_agent_steps_trained: 2749248\n",
      "    num_steps_sampled: 687312\n",
      "    num_steps_trained: 687312\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.17467532467532\n",
      "    gpu_util_percent0: 0.15818181818181817\n",
      "    ram_util_percent: 85.90584415584415\n",
      "    vram_util_percent0: 0.5060885971547374\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7599776773616171\n",
      "  policy_reward_mean:\n",
      "    main: 0.04198996240466759\n",
      "  policy_reward_min:\n",
      "    main: -1.6320727304935456\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30762190877194756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.290774909236163\n",
      "    mean_inference_ms: 4.476814854098126\n",
      "    mean_raw_obs_processing_ms: 1.2211596319737597\n",
      "  time_since_restore: 10010.235865354538\n",
      "  time_this_iter_s: 118.95000505447388\n",
      "  time_total_s: 10010.235865354538\n",
      "  timers:\n",
      "    learn_throughput: 91.663\n",
      "    learn_time_ms: 87188.995\n",
      "    sample_throughput: 307.178\n",
      "    sample_time_ms: 26017.509\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1639168228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 687312\n",
      "  training_iteration: 86\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         10010.2</td><td style=\"text-align: right;\">687312</td><td style=\"text-align: right;\"> 0.16796</td><td style=\"text-align: right;\">             2.08832</td><td style=\"text-align: right;\">           -0.679723</td><td style=\"text-align: right;\">           66.9836</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2781216\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7423491677030012\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06741390819308613\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5101432588508793\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 62.44186046511628\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2745703180398573\n",
      "  episode_reward_mean: 0.12182343446949909\n",
      "  episode_reward_min: -0.8768871924073549\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 6810\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3984122443199158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018858270671218633\n",
      "          policy_loss: -0.09869482224062086\n",
      "          total_loss: 0.1479159134440124\n",
      "          vf_explained_var: 0.5435028076171875\n",
      "          vf_loss: 0.23388140094280244\n",
      "    num_agent_steps_sampled: 2781216\n",
      "    num_agent_steps_trained: 2781216\n",
      "    num_steps_sampled: 695304\n",
      "    num_steps_trained: 695304\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.433098591549296\n",
      "    gpu_util_percent0: 0.13274647887323945\n",
      "    ram_util_percent: 85.71971830985916\n",
      "    vram_util_percent0: 0.5015549036237827\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.876232817806981\n",
      "  policy_reward_mean:\n",
      "    main: 0.03045585861737478\n",
      "  policy_reward_min:\n",
      "    main: -1.5808730389241892\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3072657731040139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.26930887685428\n",
      "    mean_inference_ms: 4.472848284391705\n",
      "    mean_raw_obs_processing_ms: 1.2220123315063898\n",
      "  time_since_restore: 10119.476660966873\n",
      "  time_this_iter_s: 109.2407956123352\n",
      "  time_total_s: 10119.476660966873\n",
      "  timers:\n",
      "    learn_throughput: 91.658\n",
      "    learn_time_ms: 87193.891\n",
      "    sample_throughput: 306.614\n",
      "    sample_time_ms: 26065.372\n",
      "    update_time_ms: 2.702\n",
      "  timestamp: 1639168337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695304\n",
      "  training_iteration: 87\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         10119.5</td><td style=\"text-align: right;\">695304</td><td style=\"text-align: right;\">0.121823</td><td style=\"text-align: right;\">             1.27457</td><td style=\"text-align: right;\">           -0.876887</td><td style=\"text-align: right;\">           62.4419</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2813184\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5698416611172333\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.020598959095855764\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.44062930732549843\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-34-07\n",
      "  done: false\n",
      "  episode_len_mean: 64.38842975206612\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3577301407061215\n",
      "  episode_reward_mean: 0.10406015053994946\n",
      "  episode_reward_min: -0.9732196254988419\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 6931\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3786593909263611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01884940066188574\n",
      "          policy_loss: -0.0976991151869297\n",
      "          total_loss: 0.13672226964868606\n",
      "          vf_explained_var: 0.5571312308311462\n",
      "          vf_loss: 0.22169804048538208\n",
      "    num_agent_steps_sampled: 2813184\n",
      "    num_agent_steps_trained: 2813184\n",
      "    num_steps_sampled: 703296\n",
      "    num_steps_trained: 703296\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.580851063829783\n",
      "    gpu_util_percent0: 0.1324113475177305\n",
      "    ram_util_percent: 85.77517730496453\n",
      "    vram_util_percent0: 0.4996721112534684\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9575692767480142\n",
      "  policy_reward_mean:\n",
      "    main: 0.02601503763498737\n",
      "  policy_reward_min:\n",
      "    main: -1.602562516946938\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3065046452698758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.236483001046597\n",
      "    mean_inference_ms: 4.465720619460302\n",
      "    mean_raw_obs_processing_ms: 1.2220474344271963\n",
      "  time_since_restore: 10228.63598036766\n",
      "  time_this_iter_s: 109.15931940078735\n",
      "  time_total_s: 10228.63598036766\n",
      "  timers:\n",
      "    learn_throughput: 91.516\n",
      "    learn_time_ms: 87329.203\n",
      "    sample_throughput: 305.811\n",
      "    sample_time_ms: 26133.751\n",
      "    update_time_ms: 2.922\n",
      "  timestamp: 1639168447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 703296\n",
      "  training_iteration: 88\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         10228.6</td><td style=\"text-align: right;\">703296</td><td style=\"text-align: right;\"> 0.10406</td><td style=\"text-align: right;\">             1.35773</td><td style=\"text-align: right;\">            -0.97322</td><td style=\"text-align: right;\">           64.3884</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2845152\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6266504387552385\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0012589107989407326\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5649126601116935\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 65.95121951219512\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1384236718063487\n",
      "  episode_reward_mean: 0.05620513201594232\n",
      "  episode_reward_min: -1.2527234368266673\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 7054\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3709570870399475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018991756737232208\n",
      "          policy_loss: -0.09847609405964614\n",
      "          total_loss: 0.1518036920428276\n",
      "          vf_explained_var: 0.5127862095832825\n",
      "          vf_loss: 0.23746035021543502\n",
      "    num_agent_steps_sampled: 2845152\n",
      "    num_agent_steps_trained: 2845152\n",
      "    num_steps_sampled: 711288\n",
      "    num_steps_trained: 711288\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.23191489361702\n",
      "    gpu_util_percent0: 0.13340425531914896\n",
      "    ram_util_percent: 85.79432624113473\n",
      "    vram_util_percent0: 0.49919369706813776\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7252675402160658\n",
      "  policy_reward_mean:\n",
      "    main: 0.014051283003985584\n",
      "  policy_reward_min:\n",
      "    main: -1.7735751100900314\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3059488171923835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.205610814955733\n",
      "    mean_inference_ms: 4.457177623757374\n",
      "    mean_raw_obs_processing_ms: 1.2225587533160607\n",
      "  time_since_restore: 10337.507272481918\n",
      "  time_this_iter_s: 108.87129211425781\n",
      "  time_total_s: 10337.507272481918\n",
      "  timers:\n",
      "    learn_throughput: 91.423\n",
      "    learn_time_ms: 87418.101\n",
      "    sample_throughput: 305.742\n",
      "    sample_time_ms: 26139.71\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1639168556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 711288\n",
      "  training_iteration: 89\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         10337.5</td><td style=\"text-align: right;\">711288</td><td style=\"text-align: right;\">0.0562051</td><td style=\"text-align: right;\">             1.13842</td><td style=\"text-align: right;\">            -1.25272</td><td style=\"text-align: right;\">           65.9512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2877120\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6883251992792014\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.045665982000129036\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5019694452614394\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 67.19834710743801\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0632652593836327\n",
      "  episode_reward_mean: 0.14723810639784804\n",
      "  episode_reward_min: -1.0688858976215763\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 7175\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3696611924171447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018920526038855315\n",
      "          policy_loss: -0.09796650522202253\n",
      "          total_loss: 0.13790989287570118\n",
      "          vf_explained_var: 0.5366919040679932\n",
      "          vf_loss: 0.22310504442453385\n",
      "    num_agent_steps_sampled: 2877120\n",
      "    num_agent_steps_trained: 2877120\n",
      "    num_steps_sampled: 719280\n",
      "    num_steps_trained: 719280\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.429787234042557\n",
      "    gpu_util_percent0: 0.13276595744680852\n",
      "    ram_util_percent: 85.79432624113473\n",
      "    vram_util_percent0: 0.49921003316227097\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8075794585181661\n",
      "  policy_reward_mean:\n",
      "    main: 0.03680952659946203\n",
      "  policy_reward_min:\n",
      "    main: -1.5592427854033715\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3066133674458479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.20719729699957\n",
      "    mean_inference_ms: 4.461716684481299\n",
      "    mean_raw_obs_processing_ms: 1.2231311797956324\n",
      "  time_since_restore: 10446.325366020203\n",
      "  time_this_iter_s: 108.8180935382843\n",
      "  time_total_s: 10446.325366020203\n",
      "  timers:\n",
      "    learn_throughput: 91.38\n",
      "    learn_time_ms: 87459.296\n",
      "    sample_throughput: 304.343\n",
      "    sample_time_ms: 26259.81\n",
      "    update_time_ms: 2.845\n",
      "  timestamp: 1639168665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 719280\n",
      "  training_iteration: 90\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         10446.3</td><td style=\"text-align: right;\">719280</td><td style=\"text-align: right;\">0.147238</td><td style=\"text-align: right;\">             1.06327</td><td style=\"text-align: right;\">            -1.06889</td><td style=\"text-align: right;\">           67.1983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2909088\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0410841195185276\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.045095656799332355\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6995114162039189\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 60.484848484848484\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2514483935660645\n",
      "  episode_reward_mean: 0.12739258921371907\n",
      "  episode_reward_min: -0.8715094499557463\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 7307\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3344507007598876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018934591069817545\n",
      "          policy_loss: -0.09902010644972324\n",
      "          total_loss: 0.1652066254094243\n",
      "          vf_explained_var: 0.5359165072441101\n",
      "          vf_loss: 0.2514458831548691\n",
      "    num_agent_steps_sampled: 2909088\n",
      "    num_agent_steps_trained: 2909088\n",
      "    num_steps_sampled: 727272\n",
      "    num_steps_trained: 727272\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.279432624113472\n",
      "    gpu_util_percent0: 0.13276595744680852\n",
      "    ram_util_percent: 85.8581560283688\n",
      "    vram_util_percent0: 0.4985764260826748\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9982872971279104\n",
      "  policy_reward_mean:\n",
      "    main: 0.03184814730342977\n",
      "  policy_reward_min:\n",
      "    main: -1.8886741643691185\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3058706970685322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.17088155444565\n",
      "    mean_inference_ms: 4.451669658531962\n",
      "    mean_raw_obs_processing_ms: 1.223708628054166\n",
      "  time_since_restore: 10554.966626167297\n",
      "  time_this_iter_s: 108.64126014709473\n",
      "  time_total_s: 10554.966626167297\n",
      "  timers:\n",
      "    learn_throughput: 91.71\n",
      "    learn_time_ms: 87143.986\n",
      "    sample_throughput: 304.187\n",
      "    sample_time_ms: 26273.306\n",
      "    update_time_ms: 2.774\n",
      "  timestamp: 1639168773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 727272\n",
      "  training_iteration: 91\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">           10555</td><td style=\"text-align: right;\">727272</td><td style=\"text-align: right;\">0.127393</td><td style=\"text-align: right;\">             1.25145</td><td style=\"text-align: right;\">           -0.871509</td><td style=\"text-align: right;\">           60.4848</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2941056\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9196459838099259\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.046852840003101075\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5174433624187584\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 62.627906976744185\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1426231307013919\n",
      "  episode_reward_mean: 0.1279207468632079\n",
      "  episode_reward_min: -1.0577098907669193\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 7436\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3298298058509828\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019081804394721985\n",
      "          policy_loss: -0.09927478773891926\n",
      "          total_loss: 0.15493749763071538\n",
      "          vf_explained_var: 0.5437818169593811\n",
      "          vf_loss: 0.24133206701278687\n",
      "    num_agent_steps_sampled: 2941056\n",
      "    num_agent_steps_trained: 2941056\n",
      "    num_steps_sampled: 735264\n",
      "    num_steps_trained: 735264\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.476056338028165\n",
      "    gpu_util_percent0: 0.13169014084507044\n",
      "    ram_util_percent: 85.91830985915493\n",
      "    vram_util_percent0: 0.49838484675741207\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7210260224342904\n",
      "  policy_reward_mean:\n",
      "    main: 0.03198018671580196\n",
      "  policy_reward_min:\n",
      "    main: -1.670311600472548\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.305522921960274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.148790779774778\n",
      "    mean_inference_ms: 4.447767780619475\n",
      "    mean_raw_obs_processing_ms: 1.2241269423883447\n",
      "  time_since_restore: 10664.065603256226\n",
      "  time_this_iter_s: 109.09897708892822\n",
      "  time_total_s: 10664.065603256226\n",
      "  timers:\n",
      "    learn_throughput: 92.335\n",
      "    learn_time_ms: 86554.179\n",
      "    sample_throughput: 306.247\n",
      "    sample_time_ms: 26096.556\n",
      "    update_time_ms: 2.794\n",
      "  timestamp: 1639168882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735264\n",
      "  training_iteration: 92\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         10664.1</td><td style=\"text-align: right;\">735264</td><td style=\"text-align: right;\">0.127921</td><td style=\"text-align: right;\">             1.14262</td><td style=\"text-align: right;\">            -1.05771</td><td style=\"text-align: right;\">           62.6279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 2973024\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9071327693001323\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06031973983900761\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5278717652351532\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 57.94074074074074\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.317339995194423\n",
      "  episode_reward_mean: 0.14643903639442044\n",
      "  episode_reward_min: -0.9605656377342437\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 7571\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.306732982635498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019239426776766776\n",
      "          policy_loss: -0.09946712719649077\n",
      "          total_loss: 0.1530240349434316\n",
      "          vf_explained_var: 0.5348238348960876\n",
      "          vf_loss: 0.23950454950332642\n",
      "    num_agent_steps_sampled: 2973024\n",
      "    num_agent_steps_trained: 2973024\n",
      "    num_steps_sampled: 743256\n",
      "    num_steps_trained: 743256\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.2255033557047\n",
      "    gpu_util_percent0: 0.16939597315436242\n",
      "    ram_util_percent: 86.19999999999999\n",
      "    vram_util_percent0: 0.5029504583590063\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8711430155389195\n",
      "  policy_reward_mean:\n",
      "    main: 0.036609759098605095\n",
      "  policy_reward_min:\n",
      "    main: -1.7525302148708235\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3057611242360805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.14019401374069\n",
      "    mean_inference_ms: 4.447528837933174\n",
      "    mean_raw_obs_processing_ms: 1.225118439952134\n",
      "  time_since_restore: 10779.54529762268\n",
      "  time_this_iter_s: 115.47969436645508\n",
      "  time_total_s: 10779.54529762268\n",
      "  timers:\n",
      "    learn_throughput: 92.431\n",
      "    learn_time_ms: 86464.588\n",
      "    sample_throughput: 309.364\n",
      "    sample_time_ms: 25833.683\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1639168998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 743256\n",
      "  training_iteration: 93\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         10779.5</td><td style=\"text-align: right;\">743256</td><td style=\"text-align: right;\">0.146439</td><td style=\"text-align: right;\">             1.31734</td><td style=\"text-align: right;\">           -0.960566</td><td style=\"text-align: right;\">           57.9407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3004992\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7783183671436192\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07139557501304454\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6103194207599072\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 70.91891891891892\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.000093805861642\n",
      "  episode_reward_mean: 0.15731187730992136\n",
      "  episode_reward_min: -1.792686280007794\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 7682\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3195203881263733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01885138361901045\n",
      "          policy_loss: -0.09928797943145037\n",
      "          total_loss: 0.1394972232580185\n",
      "          vf_explained_var: 0.5278275609016418\n",
      "          vf_loss: 0.22606051886081696\n",
      "    num_agent_steps_sampled: 3004992\n",
      "    num_agent_steps_trained: 3004992\n",
      "    num_steps_sampled: 751248\n",
      "    num_steps_trained: 751248\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.622916666666672\n",
      "    gpu_util_percent0: 0.13\n",
      "    ram_util_percent: 89.35833333333333\n",
      "    vram_util_percent0: 0.5027158513399876\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9418015508965032\n",
      "  policy_reward_mean:\n",
      "    main: 0.03932796932748035\n",
      "  policy_reward_min:\n",
      "    main: -1.855907495323824\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30576959845313545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.14685993206395\n",
      "    mean_inference_ms: 4.446897512951245\n",
      "    mean_raw_obs_processing_ms: 1.2265705547836245\n",
      "  time_since_restore: 10890.40598487854\n",
      "  time_this_iter_s: 110.86068725585938\n",
      "  time_total_s: 10890.40598487854\n",
      "  timers:\n",
      "    learn_throughput: 93.296\n",
      "    learn_time_ms: 85662.799\n",
      "    sample_throughput: 310.553\n",
      "    sample_time_ms: 25734.736\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1639169109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 751248\n",
      "  training_iteration: 94\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         10890.4</td><td style=\"text-align: right;\">751248</td><td style=\"text-align: right;\">0.157312</td><td style=\"text-align: right;\">             1.00009</td><td style=\"text-align: right;\">            -1.79269</td><td style=\"text-align: right;\">           70.9189</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3036960\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.922195938388625\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03485793043435664\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4764053206018642\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 59.82945736434109\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.391740781022865\n",
      "  episode_reward_mean: 0.12310494669129561\n",
      "  episode_reward_min: -0.7696138810858542\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 7811\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3066504168510438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019163916654884816\n",
      "          policy_loss: -0.1002584488466382\n",
      "          total_loss: 0.14016710868477822\n",
      "          vf_explained_var: 0.5605194568634033\n",
      "          vf_loss: 0.22748991417884826\n",
      "    num_agent_steps_sampled: 3036960\n",
      "    num_agent_steps_trained: 3036960\n",
      "    num_steps_sampled: 759240\n",
      "    num_steps_trained: 759240\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.310071942446047\n",
      "    gpu_util_percent0: 0.13489208633093525\n",
      "    ram_util_percent: 89.56187050359713\n",
      "    vram_util_percent0: 0.5029615004935835\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8871761576472963\n",
      "  policy_reward_mean:\n",
      "    main: 0.0307762366728239\n",
      "  policy_reward_min:\n",
      "    main: -1.613320393838242\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3051986717101784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11438874688777\n",
      "    mean_inference_ms: 4.439708878614745\n",
      "    mean_raw_obs_processing_ms: 1.2268101595980827\n",
      "  time_since_restore: 10997.84070944786\n",
      "  time_this_iter_s: 107.43472456932068\n",
      "  time_total_s: 10997.84070944786\n",
      "  timers:\n",
      "    learn_throughput: 94.114\n",
      "    learn_time_ms: 84918.576\n",
      "    sample_throughput: 314.626\n",
      "    sample_time_ms: 25401.555\n",
      "    update_time_ms: 2.989\n",
      "  timestamp: 1639169216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 759240\n",
      "  training_iteration: 95\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         10997.8</td><td style=\"text-align: right;\">759240</td><td style=\"text-align: right;\">0.123105</td><td style=\"text-align: right;\">             1.39174</td><td style=\"text-align: right;\">           -0.769614</td><td style=\"text-align: right;\">           59.8295</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3068928\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7606224820633488\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06570379627924154\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5587545394407071\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-48-43\n",
      "  done: false\n",
      "  episode_len_mean: 68.70689655172414\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1337868817356718\n",
      "  episode_reward_mean: 0.12497730298418325\n",
      "  episode_reward_min: -0.8602995237648909\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 7927\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.3056403789520263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019193279828876256\n",
      "          policy_loss: -0.10040453460812569\n",
      "          total_loss: 0.14642676950991154\n",
      "          vf_explained_var: 0.5119283199310303\n",
      "          vf_loss: 0.23387583857774735\n",
      "    num_agent_steps_sampled: 3068928\n",
      "    num_agent_steps_trained: 3068928\n",
      "    num_steps_sampled: 767232\n",
      "    num_steps_trained: 767232\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.362589928057552\n",
      "    gpu_util_percent0: 0.13510791366906474\n",
      "    ram_util_percent: 89.62014388489209\n",
      "    vram_util_percent0: 0.5014381387288984\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7784433498423782\n",
      "  policy_reward_mean:\n",
      "    main: 0.031244325746045826\n",
      "  policy_reward_min:\n",
      "    main: -1.8186576152748026\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30594756990046085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11554789469228\n",
      "    mean_inference_ms: 4.44412342262961\n",
      "    mean_raw_obs_processing_ms: 1.2265800527147102\n",
      "  time_since_restore: 11104.302620649338\n",
      "  time_this_iter_s: 106.46191120147705\n",
      "  time_total_s: 11104.302620649338\n",
      "  timers:\n",
      "    learn_throughput: 95.181\n",
      "    learn_time_ms: 83966.509\n",
      "    sample_throughput: 318.328\n",
      "    sample_time_ms: 25106.18\n",
      "    update_time_ms: 2.985\n",
      "  timestamp: 1639169323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767232\n",
      "  training_iteration: 96\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         11104.3</td><td style=\"text-align: right;\">767232</td><td style=\"text-align: right;\">0.124977</td><td style=\"text-align: right;\">             1.13379</td><td style=\"text-align: right;\">             -0.8603</td><td style=\"text-align: right;\">           68.7069</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3100896\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6505934053232142\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05290895234677252\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8627242925831732\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 60.35251798561151\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4058856975473653\n",
      "  episode_reward_mean: 0.0703918008246611\n",
      "  episode_reward_min: -1.2991710471456521\n",
      "  episodes_this_iter: 139\n",
      "  episodes_total: 8066\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2674468545913695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019092539981007576\n",
      "          policy_loss: -0.10061268797516823\n",
      "          total_loss: 0.15717939015477897\n",
      "          vf_explained_var: 0.5393033623695374\n",
      "          vf_loss: 0.24490461355447768\n",
      "    num_agent_steps_sampled: 3100896\n",
      "    num_agent_steps_trained: 3100896\n",
      "    num_steps_sampled: 775224\n",
      "    num_steps_trained: 775224\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71523178807947\n",
      "    gpu_util_percent0: 0.1703973509933775\n",
      "    ram_util_percent: 91.1794701986755\n",
      "    vram_util_percent0: 0.5068622259413498\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.746460711310987\n",
      "  policy_reward_mean:\n",
      "    main: 0.017597950206165273\n",
      "  policy_reward_min:\n",
      "    main: -1.8627242925831733\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.305002622882905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.073930295743562\n",
      "    mean_inference_ms: 4.434542043957526\n",
      "    mean_raw_obs_processing_ms: 1.2269154766709855\n",
      "  time_since_restore: 11221.006680727005\n",
      "  time_this_iter_s: 116.70406007766724\n",
      "  time_total_s: 11221.006680727005\n",
      "  timers:\n",
      "    learn_throughput: 94.255\n",
      "    learn_time_ms: 84790.922\n",
      "    sample_throughput: 319.281\n",
      "    sample_time_ms: 25031.233\n",
      "    update_time_ms: 2.996\n",
      "  timestamp: 1639169440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 775224\n",
      "  training_iteration: 97\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">           11221</td><td style=\"text-align: right;\">775224</td><td style=\"text-align: right;\">0.0703918</td><td style=\"text-align: right;\">             1.40589</td><td style=\"text-align: right;\">            -1.29917</td><td style=\"text-align: right;\">           60.3525</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3132864\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7046516554968187\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07677119472500854\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.555630515239379\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 64.73553719008264\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4615731218219985\n",
      "  episode_reward_mean: 0.09689006941667531\n",
      "  episode_reward_min: -0.9633652087758652\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 8187\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2854437818527222\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018981605745851993\n",
      "          policy_loss: -0.09936021174490452\n",
      "          total_loss: 0.13893410931527614\n",
      "          vf_explained_var: 0.5341510772705078\n",
      "          vf_loss: 0.2254817351102829\n",
      "    num_agent_steps_sampled: 3132864\n",
      "    num_agent_steps_trained: 3132864\n",
      "    num_steps_sampled: 783216\n",
      "    num_steps_trained: 783216\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.405161290322575\n",
      "    gpu_util_percent0: 0.18903225806451612\n",
      "    ram_util_percent: 92.56516129032258\n",
      "    vram_util_percent0: 0.5227398656179345\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7185803704750364\n",
      "  policy_reward_mean:\n",
      "    main: 0.02422251735416882\n",
      "  policy_reward_min:\n",
      "    main: -1.6084829675190722\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30485509957892215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.07697197939693\n",
      "    mean_inference_ms: 4.4316185419236165\n",
      "    mean_raw_obs_processing_ms: 1.2288398354967764\n",
      "  time_since_restore: 11340.52346611023\n",
      "  time_this_iter_s: 119.51678538322449\n",
      "  time_total_s: 11340.52346611023\n",
      "  timers:\n",
      "    learn_throughput: 93.432\n",
      "    learn_time_ms: 85538.512\n",
      "    sample_throughput: 315.735\n",
      "    sample_time_ms: 25312.399\n",
      "    update_time_ms: 2.786\n",
      "  timestamp: 1639169559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 783216\n",
      "  training_iteration: 98\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         11340.5</td><td style=\"text-align: right;\">783216</td><td style=\"text-align: right;\">0.0968901</td><td style=\"text-align: right;\">             1.46157</td><td style=\"text-align: right;\">           -0.963365</td><td style=\"text-align: right;\">           64.7355</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3164832\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6428444714156551\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06302961187988568\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5949702998467258\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 68.21551724137932\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3106478078208543\n",
      "  episode_reward_mean: 0.15914707741764944\n",
      "  episode_reward_min: -1.618421218164855\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 8303\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2715581774711608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01917740024998784\n",
      "          policy_loss: -0.10058897299319505\n",
      "          total_loss: 0.13718648134171962\n",
      "          vf_explained_var: 0.5320066213607788\n",
      "          vf_loss: 0.224830708861351\n",
      "    num_agent_steps_sampled: 3164832\n",
      "    num_agent_steps_trained: 3164832\n",
      "    num_steps_sampled: 791208\n",
      "    num_steps_trained: 791208\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.08145695364238\n",
      "    gpu_util_percent0: 0.18271523178807944\n",
      "    ram_util_percent: 93.78874172185431\n",
      "    vram_util_percent0: 0.5285199688813634\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9363947287037853\n",
      "  policy_reward_mean:\n",
      "    main: 0.03978676935441236\n",
      "  policy_reward_min:\n",
      "    main: -1.6258573908819836\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30587641854928743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.111568942102807\n",
      "    mean_inference_ms: 4.441044625815439\n",
      "    mean_raw_obs_processing_ms: 1.229940192787581\n",
      "  time_since_restore: 11459.53015255928\n",
      "  time_this_iter_s: 119.0066864490509\n",
      "  time_total_s: 11459.53015255928\n",
      "  timers:\n",
      "    learn_throughput: 92.605\n",
      "    learn_time_ms: 86302.363\n",
      "    sample_throughput: 312.693\n",
      "    sample_time_ms: 25558.576\n",
      "    update_time_ms: 6.804\n",
      "  timestamp: 1639169678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 791208\n",
      "  training_iteration: 99\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         11459.5</td><td style=\"text-align: right;\">791208</td><td style=\"text-align: right;\">0.159147</td><td style=\"text-align: right;\">             1.31065</td><td style=\"text-align: right;\">            -1.61842</td><td style=\"text-align: right;\">           68.2155</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3196800\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8141692441187846\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07052173102877636\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5228192492843641\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 68.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.316043358709459\n",
      "  episode_reward_mean: 0.14054376401239344\n",
      "  episode_reward_min: -0.7919931649536087\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 8423\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2665585927963257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019636399302631617\n",
      "          policy_loss: -0.10037420523911715\n",
      "          total_loss: 0.12733927618339658\n",
      "          vf_explained_var: 0.5724285244941711\n",
      "          vf_loss: 0.21445891350507737\n",
      "    num_agent_steps_sampled: 3196800\n",
      "    num_agent_steps_trained: 3196800\n",
      "    num_steps_sampled: 799200\n",
      "    num_steps_trained: 799200\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.56\n",
      "    gpu_util_percent0: 0.20237499999999997\n",
      "    ram_util_percent: 95.11937500000002\n",
      "    vram_util_percent0: 0.5447680157946693\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8472713925699487\n",
      "  policy_reward_mean:\n",
      "    main: 0.03513594100309835\n",
      "  policy_reward_min:\n",
      "    main: -1.684183156396247\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30620723592423166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.136588375651318\n",
      "    mean_inference_ms: 4.446050251110112\n",
      "    mean_raw_obs_processing_ms: 1.2367057889668762\n",
      "  time_since_restore: 11584.667429447174\n",
      "  time_this_iter_s: 125.13727688789368\n",
      "  time_total_s: 11584.667429447174\n",
      "  timers:\n",
      "    learn_throughput: 91.283\n",
      "    learn_time_ms: 87551.42\n",
      "    sample_throughput: 308.044\n",
      "    sample_time_ms: 25944.35\n",
      "    update_time_ms: 6.879\n",
      "  timestamp: 1639169804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 799200\n",
      "  training_iteration: 100\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         11584.7</td><td style=\"text-align: right;\">799200</td><td style=\"text-align: right;\">0.140544</td><td style=\"text-align: right;\">             1.31604</td><td style=\"text-align: right;\">           -0.791993</td><td style=\"text-align: right;\">             68.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3228768\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9967084909857761\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11008542231118633\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6365688841485116\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_17-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 64.21428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.204123072214534\n",
      "  episode_reward_mean: 0.17038046181145577\n",
      "  episode_reward_min: -1.0464100157587461\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 8549\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2337596192359925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01893481919914484\n",
      "          policy_loss: -0.09898985939845442\n",
      "          total_loss: 0.13844625974446534\n",
      "          vf_explained_var: 0.5618411898612976\n",
      "          vf_loss: 0.22465511757135392\n",
      "    num_agent_steps_sampled: 3228768\n",
      "    num_agent_steps_trained: 3228768\n",
      "    num_steps_sampled: 807192\n",
      "    num_steps_trained: 807192\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.49556962025316\n",
      "    gpu_util_percent0: 0.18569620253164554\n",
      "    ram_util_percent: 95.14683544303799\n",
      "    vram_util_percent0: 0.5424585868935902\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8927085186423311\n",
      "  policy_reward_mean:\n",
      "    main: 0.04259511545286396\n",
      "  policy_reward_min:\n",
      "    main: -1.6365688841485118\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3064833048641778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.170253790859014\n",
      "    mean_inference_ms: 4.454573564205016\n",
      "    mean_raw_obs_processing_ms: 1.2398453449687548\n",
      "  time_since_restore: 11706.729946613312\n",
      "  time_this_iter_s: 122.0625171661377\n",
      "  time_total_s: 11706.729946613312\n",
      "  timers:\n",
      "    learn_throughput: 90.437\n",
      "    learn_time_ms: 88370.443\n",
      "    sample_throughput: 302.002\n",
      "    sample_time_ms: 26463.388\n",
      "    update_time_ms: 6.895\n",
      "  timestamp: 1639169926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 807192\n",
      "  training_iteration: 101\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         11706.7</td><td style=\"text-align: right;\">807192</td><td style=\"text-align: right;\"> 0.17038</td><td style=\"text-align: right;\">             1.20412</td><td style=\"text-align: right;\">            -1.04641</td><td style=\"text-align: right;\">           64.2143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3260736\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.803026656586438\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09577625782313234\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4893499654059666\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 67.71186440677967\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1532751880913539\n",
      "  episode_reward_mean: 0.20182069651534498\n",
      "  episode_reward_min: -0.770997883525347\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 8667\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2359133787155152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019636260133236647\n",
      "          policy_loss: -0.10168171249330044\n",
      "          total_loss: 0.13596680317819118\n",
      "          vf_explained_var: 0.5446547269821167\n",
      "          vf_loss: 0.22439403837919236\n",
      "    num_agent_steps_sampled: 3260736\n",
      "    num_agent_steps_trained: 3260736\n",
      "    num_steps_sampled: 815184\n",
      "    num_steps_trained: 815184\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.72929936305732\n",
      "    gpu_util_percent0: 0.18528662420382166\n",
      "    ram_util_percent: 95.53439490445861\n",
      "    vram_util_percent0: 0.5419168642048278\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8969995203327508\n",
      "  policy_reward_mean:\n",
      "    main: 0.05045517412883623\n",
      "  policy_reward_min:\n",
      "    main: -1.5980562063516264\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30645129667872256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.1795671644607\n",
      "    mean_inference_ms: 4.454397755550698\n",
      "    mean_raw_obs_processing_ms: 1.2418564960739782\n",
      "  time_since_restore: 11829.659308433533\n",
      "  time_this_iter_s: 122.92936182022095\n",
      "  time_total_s: 11829.659308433533\n",
      "  timers:\n",
      "    learn_throughput: 89.363\n",
      "    learn_time_ms: 89433.284\n",
      "    sample_throughput: 298.378\n",
      "    sample_time_ms: 26784.789\n",
      "    update_time_ms: 6.907\n",
      "  timestamp: 1639170049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 815184\n",
      "  training_iteration: 102\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         11829.7</td><td style=\"text-align: right;\">815184</td><td style=\"text-align: right;\">0.201821</td><td style=\"text-align: right;\">             1.15328</td><td style=\"text-align: right;\">           -0.770998</td><td style=\"text-align: right;\">           67.7119</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3292704\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6511178315842253\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07218932364322465\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6376830160893316\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-02-54\n",
      "  done: false\n",
      "  episode_len_mean: 61.333333333333336\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1843121413153455\n",
      "  episode_reward_mean: 0.09042617025057789\n",
      "  episode_reward_min: -1.0654398942712353\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 8799\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2232996201515198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01940695694088936\n",
      "          policy_loss: -0.10185551145300269\n",
      "          total_loss: 0.1619917140081525\n",
      "          vf_explained_var: 0.5326682329177856\n",
      "          vf_loss: 0.2507475305199623\n",
      "    num_agent_steps_sampled: 3292704\n",
      "    num_agent_steps_trained: 3292704\n",
      "    num_steps_sampled: 823176\n",
      "    num_steps_trained: 823176\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.46708074534162\n",
      "    gpu_util_percent0: 0.2781987577639751\n",
      "    ram_util_percent: 95.32111801242236\n",
      "    vram_util_percent0: 0.5470038566952599\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.793732548074119\n",
      "  policy_reward_mean:\n",
      "    main: 0.022606542562644473\n",
      "  policy_reward_min:\n",
      "    main: -1.6437573333233686\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3058509386800007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.1805964418157\n",
      "    mean_inference_ms: 4.455745567606693\n",
      "    mean_raw_obs_processing_ms: 1.2435984115972931\n",
      "  time_since_restore: 11954.555426597595\n",
      "  time_this_iter_s: 124.8961181640625\n",
      "  time_total_s: 11954.555426597595\n",
      "  timers:\n",
      "    learn_throughput: 88.729\n",
      "    learn_time_ms: 90072.313\n",
      "    sample_throughput: 295.084\n",
      "    sample_time_ms: 27083.791\n",
      "    update_time_ms: 6.758\n",
      "  timestamp: 1639170174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 823176\n",
      "  training_iteration: 103\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         11954.6</td><td style=\"text-align: right;\">823176</td><td style=\"text-align: right;\">0.0904262</td><td style=\"text-align: right;\">             1.18431</td><td style=\"text-align: right;\">            -1.06544</td><td style=\"text-align: right;\">           61.3333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3324672\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7033286257927854\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.12753006062842046\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.541084951221246\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-05-01\n",
      "  done: false\n",
      "  episode_len_mean: 58.45522388059702\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3122695644020999\n",
      "  episode_reward_mean: 0.19877120810321128\n",
      "  episode_reward_min: -1.1060699723292244\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 8933\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.675\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.2081178503036498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020245289012789727\n",
      "          policy_loss: -0.10246729852631688\n",
      "          total_loss: 0.17764231679216028\n",
      "          vf_explained_var: 0.5145453810691833\n",
      "          vf_loss: 0.26644404447078707\n",
      "    num_agent_steps_sampled: 3324672\n",
      "    num_agent_steps_trained: 3324672\n",
      "    num_steps_sampled: 831168\n",
      "    num_steps_trained: 831168\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.40736196319018\n",
      "    gpu_util_percent0: 0.27865030674846625\n",
      "    ram_util_percent: 95.40368098159509\n",
      "    vram_util_percent0: 0.5405172431196087\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.813990386312637\n",
      "  policy_reward_mean:\n",
      "    main: 0.049692802025802806\n",
      "  policy_reward_min:\n",
      "    main: -1.9614582667259826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30653400310254875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19762853685675\n",
      "    mean_inference_ms: 4.4630563904484095\n",
      "    mean_raw_obs_processing_ms: 1.2452641372102302\n",
      "  time_since_restore: 12081.07816028595\n",
      "  time_this_iter_s: 126.52273368835449\n",
      "  time_total_s: 12081.07816028595\n",
      "  timers:\n",
      "    learn_throughput: 87.17\n",
      "    learn_time_ms: 91682.979\n",
      "    sample_throughput: 295.613\n",
      "    sample_time_ms: 27035.329\n",
      "    update_time_ms: 6.788\n",
      "  timestamp: 1639170301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 831168\n",
      "  training_iteration: 104\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         12081.1</td><td style=\"text-align: right;\">831168</td><td style=\"text-align: right;\">0.198771</td><td style=\"text-align: right;\">             1.31227</td><td style=\"text-align: right;\">            -1.10607</td><td style=\"text-align: right;\">           58.4552</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3356640\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7167311323976864\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0584655467453551\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5893076745308586\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 64.0078125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3673666012260415\n",
      "  episode_reward_mean: 0.08899709561145021\n",
      "  episode_reward_min: -0.8206778223613416\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 9061\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1990696949958801\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015758787009865046\n",
      "          policy_loss: -0.1002748940885067\n",
      "          total_loss: 0.16839287374913692\n",
      "          vf_explained_var: 0.5330814719200134\n",
      "          vf_loss: 0.25271199584007265\n",
      "    num_agent_steps_sampled: 3356640\n",
      "    num_agent_steps_trained: 3356640\n",
      "    num_steps_sampled: 839160\n",
      "    num_steps_trained: 839160\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.435333333333325\n",
      "    gpu_util_percent0: 0.16186666666666663\n",
      "    ram_util_percent: 95.67400000000002\n",
      "    vram_util_percent0: 0.5364154875507295\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8652241056178127\n",
      "  policy_reward_mean:\n",
      "    main: 0.022249273902862556\n",
      "  policy_reward_min:\n",
      "    main: -1.592493468121285\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3061315001318251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.190247010753247\n",
      "    mean_inference_ms: 4.459652998700485\n",
      "    mean_raw_obs_processing_ms: 1.246569902378133\n",
      "  time_since_restore: 12197.9947078228\n",
      "  time_this_iter_s: 116.91654753684998\n",
      "  time_total_s: 12197.9947078228\n",
      "  timers:\n",
      "    learn_throughput: 86.489\n",
      "    learn_time_ms: 92405.346\n",
      "    sample_throughput: 293.179\n",
      "    sample_time_ms: 27259.801\n",
      "    update_time_ms: 6.832\n",
      "  timestamp: 1639170418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 839160\n",
      "  training_iteration: 105\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">           12198</td><td style=\"text-align: right;\">839160</td><td style=\"text-align: right;\">0.0889971</td><td style=\"text-align: right;\">             1.36737</td><td style=\"text-align: right;\">           -0.820678</td><td style=\"text-align: right;\">           64.0078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3388608\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8161510755477155\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06193870822182444\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48316861877068457\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 60.485074626865675\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2844458503992553\n",
      "  episode_reward_mean: 0.14018725832273268\n",
      "  episode_reward_min: -1.0542327922287766\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 9195\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1888385882377626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01565992744266987\n",
      "          policy_loss: -0.09782476553693414\n",
      "          total_loss: 0.15418011333048343\n",
      "          vf_explained_var: 0.5563085675239563\n",
      "          vf_loss: 0.23614920145273208\n",
      "    num_agent_steps_sampled: 3388608\n",
      "    num_agent_steps_trained: 3388608\n",
      "    num_steps_sampled: 847152\n",
      "    num_steps_trained: 847152\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.01\n",
      "    gpu_util_percent0: 0.16579999999999995\n",
      "    ram_util_percent: 95.70733333333334\n",
      "    vram_util_percent0: 0.5307798617966436\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.778942170857783\n",
      "  policy_reward_mean:\n",
      "    main: 0.03504681458068317\n",
      "  policy_reward_min:\n",
      "    main: -1.6914204150994927\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3066670089017343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.200240531002482\n",
      "    mean_inference_ms: 4.464291441521001\n",
      "    mean_raw_obs_processing_ms: 1.247599769914513\n",
      "  time_since_restore: 12313.310110092163\n",
      "  time_this_iter_s: 115.3154022693634\n",
      "  time_total_s: 12313.310110092163\n",
      "  timers:\n",
      "    learn_throughput: 85.847\n",
      "    learn_time_ms: 93096.153\n",
      "    sample_throughput: 291.21\n",
      "    sample_time_ms: 27444.142\n",
      "    update_time_ms: 6.867\n",
      "  timestamp: 1639170533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 847152\n",
      "  training_iteration: 106\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         12313.3</td><td style=\"text-align: right;\">847152</td><td style=\"text-align: right;\">0.140187</td><td style=\"text-align: right;\">             1.28445</td><td style=\"text-align: right;\">            -1.05423</td><td style=\"text-align: right;\">           60.4851</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3420576\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6872892125293583\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.056433897699001104\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7177490435605799\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-10-53\n",
      "  done: false\n",
      "  episode_len_mean: 58.07971014492754\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3131459635491725\n",
      "  episode_reward_mean: 0.2336179452695623\n",
      "  episode_reward_min: -0.7886277076419894\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 9333\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1713146090507507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01604538081213832\n",
      "          policy_loss: -0.0997767311707139\n",
      "          total_loss: 0.17523589422553779\n",
      "          vf_explained_var: 0.5516926646232605\n",
      "          vf_loss: 0.2587666773200035\n",
      "    num_agent_steps_sampled: 3420576\n",
      "    num_agent_steps_trained: 3420576\n",
      "    num_steps_sampled: 855144\n",
      "    num_steps_trained: 855144\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.56298701298701\n",
      "    gpu_util_percent0: 0.1951948051948052\n",
      "    ram_util_percent: 96.01038961038962\n",
      "    vram_util_percent0: 0.5243725507792635\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.052490276521985\n",
      "  policy_reward_mean:\n",
      "    main: 0.05840448631739059\n",
      "  policy_reward_min:\n",
      "    main: -1.7226042454611985\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30631695274205223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18248242875647\n",
      "    mean_inference_ms: 4.459967137894596\n",
      "    mean_raw_obs_processing_ms: 1.2488037731672623\n",
      "  time_since_restore: 12432.662241458893\n",
      "  time_this_iter_s: 119.35213136672974\n",
      "  time_total_s: 12432.662241458893\n",
      "  timers:\n",
      "    learn_throughput: 85.758\n",
      "    learn_time_ms: 93192.332\n",
      "    sample_throughput: 289.509\n",
      "    sample_time_ms: 27605.332\n",
      "    update_time_ms: 6.907\n",
      "  timestamp: 1639170653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 855144\n",
      "  training_iteration: 107\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         12432.7</td><td style=\"text-align: right;\">855144</td><td style=\"text-align: right;\">0.233618</td><td style=\"text-align: right;\">             1.31315</td><td style=\"text-align: right;\">           -0.788628</td><td style=\"text-align: right;\">           58.0797</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3452544\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7993871542977937\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05133040880666167\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6614967345133934\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 63.81818181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1724851535985934\n",
      "  episode_reward_mean: 0.16528693902199154\n",
      "  episode_reward_min: -0.860466361592777\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 9454\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1736805033683777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016102071266621353\n",
      "          policy_loss: -0.09904573493264615\n",
      "          total_loss: 0.1564465545862913\n",
      "          vf_explained_var: 0.523012638092041\n",
      "          vf_loss: 0.23918894040584565\n",
      "    num_agent_steps_sampled: 3452544\n",
      "    num_agent_steps_trained: 3452544\n",
      "    num_steps_sampled: 863136\n",
      "    num_steps_trained: 863136\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.246249999999996\n",
      "    gpu_util_percent0: 0.2615625\n",
      "    ram_util_percent: 96.48375000000001\n",
      "    vram_util_percent0: 0.5322988647581441\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8261038656190942\n",
      "  policy_reward_mean:\n",
      "    main: 0.04132173475549788\n",
      "  policy_reward_min:\n",
      "    main: -1.6614967345133933\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3064289806430623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.20089141592614\n",
      "    mean_inference_ms: 4.464772877538437\n",
      "    mean_raw_obs_processing_ms: 1.2502755886551438\n",
      "  time_since_restore: 12562.362461566925\n",
      "  time_this_iter_s: 129.70022010803223\n",
      "  time_total_s: 12562.362461566925\n",
      "  timers:\n",
      "    learn_throughput: 84.909\n",
      "    learn_time_ms: 94123.854\n",
      "    sample_throughput: 288.561\n",
      "    sample_time_ms: 27696.033\n",
      "    update_time_ms: 6.949\n",
      "  timestamp: 1639170782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 863136\n",
      "  training_iteration: 108\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         12562.4</td><td style=\"text-align: right;\">863136</td><td style=\"text-align: right;\">0.165287</td><td style=\"text-align: right;\">             1.17249</td><td style=\"text-align: right;\">           -0.860466</td><td style=\"text-align: right;\">           63.8182</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3484512\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5725499070648025\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06809327345696904\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49656319920356506\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-15-04\n",
      "  done: false\n",
      "  episode_len_mean: 58.03007518796993\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6149382322658696\n",
      "  episode_reward_mean: 0.1816706251554928\n",
      "  episode_reward_min: -1.0498719951824622\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 9587\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1669876804351806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016005202922970058\n",
      "          policy_loss: -0.1008343038186431\n",
      "          total_loss: 0.15023793114349246\n",
      "          vf_explained_var: 0.5635552406311035\n",
      "          vf_loss: 0.23486696714162827\n",
      "    num_agent_steps_sampled: 3484512\n",
      "    num_agent_steps_trained: 3484512\n",
      "    num_steps_sampled: 871128\n",
      "    num_steps_trained: 871128\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.757051282051286\n",
      "    gpu_util_percent0: 0.21737179487179484\n",
      "    ram_util_percent: 94.71666666666667\n",
      "    vram_util_percent0: 0.5393674960555513\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.922700800936021\n",
      "  policy_reward_mean:\n",
      "    main: 0.04541765628887321\n",
      "  policy_reward_min:\n",
      "    main: -1.5343802275170733\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30669210374228495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.225039065105094\n",
      "    mean_inference_ms: 4.468706289535115\n",
      "    mean_raw_obs_processing_ms: 1.2531797020810427\n",
      "  time_since_restore: 12683.317381620407\n",
      "  time_this_iter_s: 120.95492005348206\n",
      "  time_total_s: 12683.317381620407\n",
      "  timers:\n",
      "    learn_throughput: 84.933\n",
      "    learn_time_ms: 94097.465\n",
      "    sample_throughput: 286.307\n",
      "    sample_time_ms: 27914.11\n",
      "    update_time_ms: 6.858\n",
      "  timestamp: 1639170904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 871128\n",
      "  training_iteration: 109\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         12683.3</td><td style=\"text-align: right;\">871128</td><td style=\"text-align: right;\">0.181671</td><td style=\"text-align: right;\">             1.61494</td><td style=\"text-align: right;\">            -1.04987</td><td style=\"text-align: right;\">           58.0301</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3516480\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8867804583109175\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.032410075832713395\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.561167294111984\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 67.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2171458520392133\n",
      "  episode_reward_mean: 0.11109611792890671\n",
      "  episode_reward_min: -1.112198573270864\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 9707\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1523093552589416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016036059357225897\n",
      "          policy_loss: -0.10082602670416235\n",
      "          total_loss: 0.14785755624249577\n",
      "          vf_explained_var: 0.5474857687950134\n",
      "          vf_loss: 0.23244707274436952\n",
      "    num_agent_steps_sampled: 3516480\n",
      "    num_agent_steps_trained: 3516480\n",
      "    num_steps_sampled: 879120\n",
      "    num_steps_trained: 879120\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.8132530120482\n",
      "    gpu_util_percent0: 0.251144578313253\n",
      "    ram_util_percent: 95.2524096385542\n",
      "    vram_util_percent0: 0.5442203166070005\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8633332930628723\n",
      "  policy_reward_mean:\n",
      "    main: 0.02777402948222667\n",
      "  policy_reward_min:\n",
      "    main: -1.563421428973557\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3061099399084951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.222486982480824\n",
      "    mean_inference_ms: 4.466504299126953\n",
      "    mean_raw_obs_processing_ms: 1.2545387026110324\n",
      "  time_since_restore: 12812.651772260666\n",
      "  time_this_iter_s: 129.3343906402588\n",
      "  time_total_s: 12812.651772260666\n",
      "  timers:\n",
      "    learn_throughput: 84.475\n",
      "    learn_time_ms: 94607.344\n",
      "    sample_throughput: 287.347\n",
      "    sample_time_ms: 27813.103\n",
      "    update_time_ms: 6.841\n",
      "  timestamp: 1639171033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 879120\n",
      "  training_iteration: 110\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         12812.7</td><td style=\"text-align: right;\">879120</td><td style=\"text-align: right;\">0.111096</td><td style=\"text-align: right;\">             1.21715</td><td style=\"text-align: right;\">             -1.1122</td><td style=\"text-align: right;\">              67.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3548448\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6803698602589896\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.01984940943161353\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7682692076709826\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 65.70866141732283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.039981297210029\n",
      "  episode_reward_mean: 0.11074360786136138\n",
      "  episode_reward_min: -1.0575086457169518\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 9834\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1515671877861022\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615986682474613\n",
      "          policy_loss: -0.10034777621179819\n",
      "          total_loss: 0.15188223646581173\n",
      "          vf_explained_var: 0.5599560141563416\n",
      "          vf_loss: 0.23586814612150192\n",
      "    num_agent_steps_sampled: 3548448\n",
      "    num_agent_steps_trained: 3548448\n",
      "    num_steps_sampled: 887112\n",
      "    num_steps_trained: 887112\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.71461988304093\n",
      "    gpu_util_percent0: 0.303859649122807\n",
      "    ram_util_percent: 95.21052631578947\n",
      "    vram_util_percent0: 0.5459956241376723\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.803388051298941\n",
      "  policy_reward_mean:\n",
      "    main: 0.027685901965340345\n",
      "  policy_reward_min:\n",
      "    main: -1.7682692076709827\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30701120137284643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.292523825294143\n",
      "    mean_inference_ms: 4.483215135184307\n",
      "    mean_raw_obs_processing_ms: 1.2580995632800038\n",
      "  time_since_restore: 12947.537215709686\n",
      "  time_this_iter_s: 134.88544344902039\n",
      "  time_total_s: 12947.537215709686\n",
      "  timers:\n",
      "    learn_throughput: 83.564\n",
      "    learn_time_ms: 95639.733\n",
      "    sample_throughput: 284.73\n",
      "    sample_time_ms: 28068.744\n",
      "    update_time_ms: 6.888\n",
      "  timestamp: 1639171168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 887112\n",
      "  training_iteration: 111\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         12947.5</td><td style=\"text-align: right;\">887112</td><td style=\"text-align: right;\">0.110744</td><td style=\"text-align: right;\">             1.03998</td><td style=\"text-align: right;\">            -1.05751</td><td style=\"text-align: right;\">           65.7087</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3580416\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9067326480795882\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03779216159519647\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6867222644998803\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 61.38461538461539\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3025802129435835\n",
      "  episode_reward_mean: 0.101450647034486\n",
      "  episode_reward_min: -1.1544452876169478\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 9964\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1269352831840516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015814843498170375\n",
      "          policy_loss: -0.0985085593573749\n",
      "          total_loss: 0.14899151195585728\n",
      "          vf_explained_var: 0.568011462688446\n",
      "          vf_loss: 0.23148754292726517\n",
      "    num_agent_steps_sampled: 3580416\n",
      "    num_agent_steps_trained: 3580416\n",
      "    num_steps_sampled: 895104\n",
      "    num_steps_trained: 895104\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.47692307692308\n",
      "    gpu_util_percent0: 0.24497041420118346\n",
      "    ram_util_percent: 95.15088757396448\n",
      "    vram_util_percent0: 0.5509637045820507\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8345326477934858\n",
      "  policy_reward_mean:\n",
      "    main: 0.025362661758621508\n",
      "  policy_reward_min:\n",
      "    main: -1.84803223348178\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30763887983411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.333354904291866\n",
      "    mean_inference_ms: 4.492802821145989\n",
      "    mean_raw_obs_processing_ms: 1.2605616512307969\n",
      "  time_since_restore: 13078.525167942047\n",
      "  time_this_iter_s: 130.98795223236084\n",
      "  time_total_s: 13078.525167942047\n",
      "  timers:\n",
      "    learn_throughput: 83.028\n",
      "    learn_time_ms: 96256.267\n",
      "    sample_throughput: 282.833\n",
      "    sample_time_ms: 28256.914\n",
      "    update_time_ms: 6.876\n",
      "  timestamp: 1639171299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 895104\n",
      "  training_iteration: 112\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         13078.5</td><td style=\"text-align: right;\">895104</td><td style=\"text-align: right;\">0.101451</td><td style=\"text-align: right;\">             1.30258</td><td style=\"text-align: right;\">            -1.15445</td><td style=\"text-align: right;\">           61.3846</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3612384\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8375869479858243\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.051512837370230345\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6165458448523694\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 58.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0677034873711602\n",
      "  episode_reward_mean: 0.13673786615841096\n",
      "  episode_reward_min: -0.7080986364452908\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 10089\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1452933974266053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015968508906662463\n",
      "          policy_loss: -0.10065999526903033\n",
      "          total_loss: 0.15296310605481267\n",
      "          vf_explained_var: 0.543207049369812\n",
      "          vf_loss: 0.237454984664917\n",
      "    num_agent_steps_sampled: 3612384\n",
      "    num_agent_steps_trained: 3612384\n",
      "    num_steps_sampled: 903096\n",
      "    num_steps_trained: 903096\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.1\n",
      "    gpu_util_percent0: 0.13986301369863016\n",
      "    ram_util_percent: 93.92671232876712\n",
      "    vram_util_percent0: 0.5573616050701611\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.773477529046271\n",
      "  policy_reward_mean:\n",
      "    main: 0.034184466539602734\n",
      "  policy_reward_min:\n",
      "    main: -1.6882948578572363\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3082482867598951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.359815448183205\n",
      "    mean_inference_ms: 4.498609469172271\n",
      "    mean_raw_obs_processing_ms: 1.2618260648258535\n",
      "  time_since_restore: 13191.695626735687\n",
      "  time_this_iter_s: 113.17045879364014\n",
      "  time_total_s: 13191.695626735687\n",
      "  timers:\n",
      "    learn_throughput: 84.034\n",
      "    learn_time_ms: 95104.657\n",
      "    sample_throughput: 282.977\n",
      "    sample_time_ms: 28242.6\n",
      "    update_time_ms: 6.827\n",
      "  timestamp: 1639171412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 903096\n",
      "  training_iteration: 113\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         13191.7</td><td style=\"text-align: right;\">903096</td><td style=\"text-align: right;\">0.136738</td><td style=\"text-align: right;\">              1.0677</td><td style=\"text-align: right;\">           -0.708099</td><td style=\"text-align: right;\">              58.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3644352\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.584647825222127\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07619437644928319\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.534172400097925\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 71.76271186440678\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1226496302206659\n",
      "  episode_reward_mean: 0.15357805288490545\n",
      "  episode_reward_min: -1.0014191728217505\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 10207\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1360234410762786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016266381442546843\n",
      "          policy_loss: -0.10102097947522998\n",
      "          total_loss: 0.14025627743452787\n",
      "          vf_explained_var: 0.5532577037811279\n",
      "          vf_loss: 0.22480754393339158\n",
      "    num_agent_steps_sampled: 3644352\n",
      "    num_agent_steps_trained: 3644352\n",
      "    num_steps_sampled: 911088\n",
      "    num_steps_trained: 911088\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.23194444444444\n",
      "    gpu_util_percent0: 0.13722222222222222\n",
      "    ram_util_percent: 94.04791666666667\n",
      "    vram_util_percent0: 0.5515828945925194\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8651083650822926\n",
      "  policy_reward_mean:\n",
      "    main: 0.03839451322122636\n",
      "  policy_reward_min:\n",
      "    main: -1.6364182120039854\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3076153540042705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.33724629060618\n",
      "    mean_inference_ms: 4.491245277487471\n",
      "    mean_raw_obs_processing_ms: 1.2618962179843196\n",
      "  time_since_restore: 13302.147102832794\n",
      "  time_this_iter_s: 110.45147609710693\n",
      "  time_total_s: 13302.147102832794\n",
      "  timers:\n",
      "    learn_throughput: 85.355\n",
      "    learn_time_ms: 93633.012\n",
      "    sample_throughput: 284.275\n",
      "    sample_time_ms: 28113.606\n",
      "    update_time_ms: 6.778\n",
      "  timestamp: 1639171523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 911088\n",
      "  training_iteration: 114\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         13302.1</td><td style=\"text-align: right;\">911088</td><td style=\"text-align: right;\">0.153578</td><td style=\"text-align: right;\">             1.12265</td><td style=\"text-align: right;\">            -1.00142</td><td style=\"text-align: right;\">           71.7627</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3676320\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.733176324798962\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04870433648078965\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5167185948641608\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 62.46456692913386\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4388210720849541\n",
      "  episode_reward_mean: 0.15657375695803294\n",
      "  episode_reward_min: -0.7703815110544041\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 10334\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1170676257610321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016062697432935237\n",
      "          policy_loss: -0.1002904359921813\n",
      "          total_loss: 0.15885761238262058\n",
      "          vf_explained_var: 0.5467604994773865\n",
      "          vf_loss: 0.24288456749916076\n",
      "    num_agent_steps_sampled: 3676320\n",
      "    num_agent_steps_trained: 3676320\n",
      "    num_steps_sampled: 919080\n",
      "    num_steps_trained: 919080\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.48028169014085\n",
      "    gpu_util_percent0: 0.13753521126760565\n",
      "    ram_util_percent: 93.97464788732397\n",
      "    vram_util_percent0: 0.5509491632254866\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7446835635869475\n",
      "  policy_reward_mean:\n",
      "    main: 0.03914343923950823\n",
      "  policy_reward_min:\n",
      "    main: -1.6462030386731716\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3072007868135069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.312018945554367\n",
      "    mean_inference_ms: 4.486128991151428\n",
      "    mean_raw_obs_processing_ms: 1.2621129822391497\n",
      "  time_since_restore: 13411.487751960754\n",
      "  time_this_iter_s: 109.3406491279602\n",
      "  time_total_s: 13411.487751960754\n",
      "  timers:\n",
      "    learn_throughput: 85.842\n",
      "    learn_time_ms: 93101.776\n",
      "    sample_throughput: 286.577\n",
      "    sample_time_ms: 27887.822\n",
      "    update_time_ms: 6.745\n",
      "  timestamp: 1639171632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 919080\n",
      "  training_iteration: 115\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         13411.5</td><td style=\"text-align: right;\">919080</td><td style=\"text-align: right;\">0.156574</td><td style=\"text-align: right;\">             1.43882</td><td style=\"text-align: right;\">           -0.770382</td><td style=\"text-align: right;\">           62.4646</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3708288\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6740717871605364\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05863994648488178\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47005424435847837\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 66.91803278688525\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1613332031847743\n",
      "  episode_reward_mean: 0.13565806840829234\n",
      "  episode_reward_min: -1.0183128734799207\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 10456\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1241020534038544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015997948735952377\n",
      "          policy_loss: -0.09989481828734278\n",
      "          total_loss: 0.15139534652233125\n",
      "          vf_explained_var: 0.5403341054916382\n",
      "          vf_loss: 0.23509224033355713\n",
      "    num_agent_steps_sampled: 3708288\n",
      "    num_agent_steps_trained: 3708288\n",
      "    num_steps_sampled: 927072\n",
      "    num_steps_trained: 927072\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.993706293706296\n",
      "    gpu_util_percent0: 0.13937062937062936\n",
      "    ram_util_percent: 94.17412587412588\n",
      "    vram_util_percent0: 0.5414736628951832\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.01553559370601\n",
      "  policy_reward_mean:\n",
      "    main: 0.033914517102073084\n",
      "  policy_reward_min:\n",
      "    main: -1.583090394045405\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30785526317498124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.31974714871597\n",
      "    mean_inference_ms: 4.490508323609066\n",
      "    mean_raw_obs_processing_ms: 1.2628215547770478\n",
      "  time_since_restore: 13521.644869565964\n",
      "  time_this_iter_s: 110.15711760520935\n",
      "  time_total_s: 13521.644869565964\n",
      "  timers:\n",
      "    learn_throughput: 86.261\n",
      "    learn_time_ms: 92649.051\n",
      "    sample_throughput: 287.136\n",
      "    sample_time_ms: 27833.455\n",
      "    update_time_ms: 6.741\n",
      "  timestamp: 1639171743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 927072\n",
      "  training_iteration: 116\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         13521.6</td><td style=\"text-align: right;\">927072</td><td style=\"text-align: right;\">0.135658</td><td style=\"text-align: right;\">             1.16133</td><td style=\"text-align: right;\">            -1.01831</td><td style=\"text-align: right;\">            66.918</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3740256\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7823726274683942\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.036968979256190346\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4774146646121558\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-30-53\n",
      "  done: false\n",
      "  episode_len_mean: 65.272\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5700825495914028\n",
      "  episode_reward_mean: 0.10786721095489729\n",
      "  episode_reward_min: -1.3995170396800278\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 10581\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1100629892349243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016122587271034716\n",
      "          policy_loss: -0.10117588033154606\n",
      "          total_loss: 0.15543935399502515\n",
      "          vf_explained_var: 0.540423572063446\n",
      "          vf_loss: 0.2402911126613617\n",
      "    num_agent_steps_sampled: 3740256\n",
      "    num_agent_steps_trained: 3740256\n",
      "    num_steps_sampled: 935064\n",
      "    num_steps_trained: 935064\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.987412587412592\n",
      "    gpu_util_percent0: 0.13916083916083916\n",
      "    ram_util_percent: 94.22727272727273\n",
      "    vram_util_percent0: 0.5185513729442652\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7573510151861402\n",
      "  policy_reward_mean:\n",
      "    main: 0.026966802738724316\n",
      "  policy_reward_min:\n",
      "    main: -1.8400268381839548\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30756987456709467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.304853173849718\n",
      "    mean_inference_ms: 4.487857033149798\n",
      "    mean_raw_obs_processing_ms: 1.2629201288192544\n",
      "  time_since_restore: 13631.84875702858\n",
      "  time_this_iter_s: 110.20388746261597\n",
      "  time_total_s: 13631.84875702858\n",
      "  timers:\n",
      "    learn_throughput: 87.06\n",
      "    learn_time_ms: 91798.77\n",
      "    sample_throughput: 287.716\n",
      "    sample_time_ms: 27777.394\n",
      "    update_time_ms: 6.736\n",
      "  timestamp: 1639171853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 935064\n",
      "  training_iteration: 117\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         13631.8</td><td style=\"text-align: right;\">935064</td><td style=\"text-align: right;\">0.107867</td><td style=\"text-align: right;\">             1.57008</td><td style=\"text-align: right;\">            -1.39952</td><td style=\"text-align: right;\">            65.272</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3772224\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7836398943729417\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06140064028467777\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49821152429851384\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 66.13513513513513\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0609339845536092\n",
      "  episode_reward_mean: 0.12551532863780443\n",
      "  episode_reward_min: -1.1691293568791785\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 10692\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1120358693599701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01612337051704526\n",
      "          policy_loss: -0.10143604323640466\n",
      "          total_loss: 0.13192306531593204\n",
      "          vf_explained_var: 0.5461092591285706\n",
      "          vf_loss: 0.21703419595956802\n",
      "    num_agent_steps_sampled: 3772224\n",
      "    num_agent_steps_trained: 3772224\n",
      "    num_steps_sampled: 943056\n",
      "    num_steps_trained: 943056\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.820833333333333\n",
      "    gpu_util_percent0: 0.13819444444444443\n",
      "    ram_util_percent: 94.24722222222223\n",
      "    vram_util_percent0: 0.5158529395634529\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8679554410921235\n",
      "  policy_reward_mean:\n",
      "    main: 0.031378832159451114\n",
      "  policy_reward_min:\n",
      "    main: -1.6724145057908406\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30750558217108953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.29501952432197\n",
      "    mean_inference_ms: 4.4858112924618645\n",
      "    mean_raw_obs_processing_ms: 1.2629216758029742\n",
      "  time_since_restore: 13742.46082663536\n",
      "  time_this_iter_s: 110.612069606781\n",
      "  time_total_s: 13742.46082663536\n",
      "  timers:\n",
      "    learn_throughput: 88.601\n",
      "    learn_time_ms: 90201.638\n",
      "    sample_throughput: 290.952\n",
      "    sample_time_ms: 27468.442\n",
      "    update_time_ms: 6.707\n",
      "  timestamp: 1639171964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 943056\n",
      "  training_iteration: 118\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         13742.5</td><td style=\"text-align: right;\">943056</td><td style=\"text-align: right;\">0.125515</td><td style=\"text-align: right;\">             1.06093</td><td style=\"text-align: right;\">            -1.16913</td><td style=\"text-align: right;\">           66.1351</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3804192\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6803772323660875\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07334324943128477\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6184585883854359\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 70.89830508474576\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3309943190420523\n",
      "  episode_reward_mean: 0.16063193233739267\n",
      "  episode_reward_min: -1.5116432765484578\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 10810\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.1059112265110016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01641176026687026\n",
      "          policy_loss: -0.10132062186673284\n",
      "          total_loss: 0.14981634479016065\n",
      "          vf_explained_var: 0.5290407538414001\n",
      "          vf_loss: 0.23452005779743196\n",
      "    num_agent_steps_sampled: 3804192\n",
      "    num_agent_steps_trained: 3804192\n",
      "    num_steps_sampled: 951048\n",
      "    num_steps_trained: 951048\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.864189189189187\n",
      "    gpu_util_percent0: 0.16114864864864867\n",
      "    ram_util_percent: 94.41621621621621\n",
      "    vram_util_percent0: 0.5219822487838283\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8642002760741656\n",
      "  policy_reward_mean:\n",
      "    main: 0.04015798308434818\n",
      "  policy_reward_min:\n",
      "    main: -1.9152287442105467\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30686678171230986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.272909451447042\n",
      "    mean_inference_ms: 4.479905983571552\n",
      "    mean_raw_obs_processing_ms: 1.2624917601987995\n",
      "  time_since_restore: 13856.467052221298\n",
      "  time_this_iter_s: 114.0062255859375\n",
      "  time_total_s: 13856.467052221298\n",
      "  timers:\n",
      "    learn_throughput: 88.875\n",
      "    learn_time_ms: 89923.868\n",
      "    sample_throughput: 295.324\n",
      "    sample_time_ms: 27061.808\n",
      "    update_time_ms: 2.883\n",
      "  timestamp: 1639172078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 951048\n",
      "  training_iteration: 119\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         13856.5</td><td style=\"text-align: right;\">951048</td><td style=\"text-align: right;\">0.160632</td><td style=\"text-align: right;\">             1.33099</td><td style=\"text-align: right;\">            -1.51164</td><td style=\"text-align: right;\">           70.8983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3836160\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8266688179355995\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09413815270688708\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.550347076280207\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 61.2406015037594\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3408445845665384\n",
      "  episode_reward_mean: 0.17983873619548577\n",
      "  episode_reward_min: -0.8637865883515234\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 10943\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0869268724918366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016073571335524322\n",
      "          policy_loss: -0.10217331375181675\n",
      "          total_loss: 0.14179676270112396\n",
      "          vf_explained_var: 0.5730448365211487\n",
      "          vf_loss: 0.22769558662176131\n",
      "    num_agent_steps_sampled: 3836160\n",
      "    num_agent_steps_trained: 3836160\n",
      "    num_steps_sampled: 959040\n",
      "    num_steps_trained: 959040\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.453521126760567\n",
      "    gpu_util_percent0: 0.1347887323943662\n",
      "    ram_util_percent: 94.45563380281688\n",
      "    vram_util_percent0: 0.5289117065009337\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9042192362172732\n",
      "  policy_reward_mean:\n",
      "    main: 0.04495968404887144\n",
      "  policy_reward_min:\n",
      "    main: -1.6212611576198763\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.306694232127947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.256972756985203\n",
      "    mean_inference_ms: 4.475780474981771\n",
      "    mean_raw_obs_processing_ms: 1.2627103828855284\n",
      "  time_since_restore: 13965.678848743439\n",
      "  time_this_iter_s: 109.2117965221405\n",
      "  time_total_s: 13965.678848743439\n",
      "  timers:\n",
      "    learn_throughput: 90.625\n",
      "    learn_time_ms: 88187.829\n",
      "    sample_throughput: 298.28\n",
      "    sample_time_ms: 26793.575\n",
      "    update_time_ms: 2.851\n",
      "  timestamp: 1639172187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 959040\n",
      "  training_iteration: 120\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         13965.7</td><td style=\"text-align: right;\">959040</td><td style=\"text-align: right;\">0.179839</td><td style=\"text-align: right;\">             1.34084</td><td style=\"text-align: right;\">           -0.863787</td><td style=\"text-align: right;\">           61.2406</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3868128\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1222425344149207\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04141913201832468\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5591888909530688\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 58.6015037593985\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2391533786762459\n",
      "  episode_reward_mean: 0.15242612331182806\n",
      "  episode_reward_min: -0.6915213750534259\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 11076\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0704778552055358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016140475265681745\n",
      "          policy_loss: -0.10156974992528557\n",
      "          total_loss: 0.17650789776444434\n",
      "          vf_explained_var: 0.5290439128875732\n",
      "          vf_loss: 0.2617354150414467\n",
      "    num_agent_steps_sampled: 3868128\n",
      "    num_agent_steps_trained: 3868128\n",
      "    num_steps_sampled: 967032\n",
      "    num_steps_trained: 967032\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.570629370629373\n",
      "    gpu_util_percent0: 0.13356643356643358\n",
      "    ram_util_percent: 94.44685314685316\n",
      "    vram_util_percent0: 0.5288556458349153\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.026584998046968\n",
      "  policy_reward_mean:\n",
      "    main: 0.038106530827957036\n",
      "  policy_reward_min:\n",
      "    main: -1.5940375733239822\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30664140677944124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.247944664983027\n",
      "    mean_inference_ms: 4.474281989066232\n",
      "    mean_raw_obs_processing_ms: 1.2631417423907612\n",
      "  time_since_restore: 14075.43528676033\n",
      "  time_this_iter_s: 109.75643801689148\n",
      "  time_total_s: 14075.43528676033\n",
      "  timers:\n",
      "    learn_throughput: 92.527\n",
      "    learn_time_ms: 86374.605\n",
      "    sample_throughput: 306.243\n",
      "    sample_time_ms: 26096.93\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1639172297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 967032\n",
      "  training_iteration: 121\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         14075.4</td><td style=\"text-align: right;\">967032</td><td style=\"text-align: right;\">0.152426</td><td style=\"text-align: right;\">             1.23915</td><td style=\"text-align: right;\">           -0.691521</td><td style=\"text-align: right;\">           58.6015</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3900096\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0390415726013256\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04612495318091399\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.577238863567454\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-40-06\n",
      "  done: false\n",
      "  episode_len_mean: 63.52755905511811\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.666204390852804\n",
      "  episode_reward_mean: 0.1572957012187473\n",
      "  episode_reward_min: -1.112571293122341\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 11203\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0895512702465058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016334957417100667\n",
      "          policy_loss: -0.10055332421511412\n",
      "          total_loss: 0.14879900707304478\n",
      "          vf_explained_var: 0.5454318523406982\n",
      "          vf_loss: 0.23281318688392638\n",
      "    num_agent_steps_sampled: 3900096\n",
      "    num_agent_steps_trained: 3900096\n",
      "    num_steps_sampled: 975024\n",
      "    num_steps_trained: 975024\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.554225352112674\n",
      "    gpu_util_percent0: 0.13401408450704227\n",
      "    ram_util_percent: 94.51549295774649\n",
      "    vram_util_percent0: 0.528521242625215\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9673844281453796\n",
      "  policy_reward_mean:\n",
      "    main: 0.03932392530468682\n",
      "  policy_reward_min:\n",
      "    main: -1.577238863567454\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30695416067752207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.24487234229911\n",
      "    mean_inference_ms: 4.474873575053878\n",
      "    mean_raw_obs_processing_ms: 1.263471832883738\n",
      "  time_since_restore: 14184.731250047684\n",
      "  time_this_iter_s: 109.29596328735352\n",
      "  time_total_s: 14184.731250047684\n",
      "  timers:\n",
      "    learn_throughput: 94.359\n",
      "    learn_time_ms: 84698.061\n",
      "    sample_throughput: 312.131\n",
      "    sample_time_ms: 25604.618\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639172406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 975024\n",
      "  training_iteration: 122\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         14184.7</td><td style=\"text-align: right;\">975024</td><td style=\"text-align: right;\">0.157296</td><td style=\"text-align: right;\">              1.6662</td><td style=\"text-align: right;\">            -1.11257</td><td style=\"text-align: right;\">           63.5276</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3932064\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1392695051302617\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04103615483026074\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.41971556306644764\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-41-56\n",
      "  done: false\n",
      "  episode_len_mean: 61.388429752066116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1450497442682583\n",
      "  episode_reward_mean: 0.15204903578051004\n",
      "  episode_reward_min: -0.8458802594030316\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 11324\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0763094055652618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016385595623403786\n",
      "          policy_loss: -0.09962715998664498\n",
      "          total_loss: 0.1472845642529428\n",
      "          vf_explained_var: 0.5248649716377258\n",
      "          vf_loss: 0.23032130748033525\n",
      "    num_agent_steps_sampled: 3932064\n",
      "    num_agent_steps_trained: 3932064\n",
      "    num_steps_sampled: 983016\n",
      "    num_steps_trained: 983016\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.07832167832168\n",
      "    gpu_util_percent0: 0.13139860139860143\n",
      "    ram_util_percent: 94.64825174825175\n",
      "    vram_util_percent0: 0.528548450562271\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9648135463067984\n",
      "  policy_reward_mean:\n",
      "    main: 0.03801225894512751\n",
      "  policy_reward_min:\n",
      "    main: -1.589818834773255\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30686198961692374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.236183819807348\n",
      "    mean_inference_ms: 4.47295803206386\n",
      "    mean_raw_obs_processing_ms: 1.2637710495675905\n",
      "  time_since_restore: 14294.712243556976\n",
      "  time_this_iter_s: 109.9809935092926\n",
      "  time_total_s: 14294.712243556976\n",
      "  timers:\n",
      "    learn_throughput: 94.457\n",
      "    learn_time_ms: 84610.22\n",
      "    sample_throughput: 315.027\n",
      "    sample_time_ms: 25369.293\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1639172516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 983016\n",
      "  training_iteration: 123\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         14294.7</td><td style=\"text-align: right;\">983016</td><td style=\"text-align: right;\">0.152049</td><td style=\"text-align: right;\">             1.14505</td><td style=\"text-align: right;\">            -0.84588</td><td style=\"text-align: right;\">           61.3884</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3964032\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7216800295863763\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0045644552999777235\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6721854539445473\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-43-46\n",
      "  done: false\n",
      "  episode_len_mean: 61.1865671641791\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7576271726478483\n",
      "  episode_reward_mean: 0.10378554791933432\n",
      "  episode_reward_min: -1.1937567336852526\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 11458\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0548890993595124\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016118567779660226\n",
      "          policy_loss: -0.10063773915730417\n",
      "          total_loss: 0.16092667043581604\n",
      "          vf_explained_var: 0.5391793251037598\n",
      "          vf_loss: 0.24524435871839523\n",
      "    num_agent_steps_sampled: 3964032\n",
      "    num_agent_steps_trained: 3964032\n",
      "    num_steps_sampled: 991008\n",
      "    num_steps_trained: 991008\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.697183098591548\n",
      "    gpu_util_percent0: 0.13492957746478876\n",
      "    ram_util_percent: 94.68802816901412\n",
      "    vram_util_percent0: 0.5225843378798624\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.097974700427436\n",
      "  policy_reward_mean:\n",
      "    main: 0.02594638697983359\n",
      "  policy_reward_min:\n",
      "    main: -1.6721854539445473\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.306579461173782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.222172694761007\n",
      "    mean_inference_ms: 4.470413693444197\n",
      "    mean_raw_obs_processing_ms: 1.264083381880389\n",
      "  time_since_restore: 14404.330520868301\n",
      "  time_this_iter_s: 109.61827731132507\n",
      "  time_total_s: 14404.330520868301\n",
      "  timers:\n",
      "    learn_throughput: 94.486\n",
      "    learn_time_ms: 84583.823\n",
      "    sample_throughput: 315.717\n",
      "    sample_time_ms: 25313.808\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1639172626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 991008\n",
      "  training_iteration: 124\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         14404.3</td><td style=\"text-align: right;\">991008</td><td style=\"text-align: right;\">0.103786</td><td style=\"text-align: right;\">             1.75763</td><td style=\"text-align: right;\">            -1.19376</td><td style=\"text-align: right;\">           61.1866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 3996000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9030095621426198\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.036676209002041174\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5701945513121283\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 69.52100840336135\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5449748091431825\n",
      "  episode_reward_mean: 0.20527433975271783\n",
      "  episode_reward_min: -1.101592478963973\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 11577\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0588390712738036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016399735301733016\n",
      "          policy_loss: -0.10128596099466085\n",
      "          total_loss: 0.15001743944361806\n",
      "          vf_explained_var: 0.5366340279579163\n",
      "          vf_loss: 0.23469866728782654\n",
      "    num_agent_steps_sampled: 3996000\n",
      "    num_agent_steps_trained: 3996000\n",
      "    num_steps_sampled: 999000\n",
      "    num_steps_trained: 999000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.124475524475525\n",
      "    gpu_util_percent0: 0.13496503496503498\n",
      "    ram_util_percent: 94.83216783216783\n",
      "    vram_util_percent0: 0.5163457799193238\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.746755923556541\n",
      "  policy_reward_mean:\n",
      "    main: 0.05131858493817945\n",
      "  policy_reward_min:\n",
      "    main: -1.621548479250721\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3064225528858797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.209237237286107\n",
      "    mean_inference_ms: 4.467405389725669\n",
      "    mean_raw_obs_processing_ms: 1.2637672399707491\n",
      "  time_since_restore: 14514.013174533844\n",
      "  time_this_iter_s: 109.6826536655426\n",
      "  time_total_s: 14514.013174533844\n",
      "  timers:\n",
      "    learn_throughput: 94.52\n",
      "    learn_time_ms: 84553.506\n",
      "    sample_throughput: 314.92\n",
      "    sample_time_ms: 25377.884\n",
      "    update_time_ms: 2.902\n",
      "  timestamp: 1639172736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 999000\n",
      "  training_iteration: 125\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">           14514</td><td style=\"text-align: right;\">999000</td><td style=\"text-align: right;\">0.205274</td><td style=\"text-align: right;\">             1.54497</td><td style=\"text-align: right;\">            -1.10159</td><td style=\"text-align: right;\">            69.521</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4027968\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9224967740429831\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04596292608012641\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8699287095302569\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 59.525925925925925\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3047436479374945\n",
      "  episode_reward_mean: 0.10767035378829415\n",
      "  episode_reward_min: -1.5743264941572654\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 11712\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0208970391750336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016556772757321595\n",
      "          policy_loss: -0.10205288488045335\n",
      "          total_loss: 0.16190006583184005\n",
      "          vf_explained_var: 0.557743489742279\n",
      "          vf_loss: 0.24718921971321106\n",
      "    num_agent_steps_sampled: 4027968\n",
      "    num_agent_steps_trained: 4027968\n",
      "    num_steps_sampled: 1006992\n",
      "    num_steps_trained: 1006992\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.5027972027972\n",
      "    gpu_util_percent0: 0.13272727272727272\n",
      "    ram_util_percent: 95.03636363636365\n",
      "    vram_util_percent0: 0.5153448065590219\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.835930345287119\n",
      "  policy_reward_mean:\n",
      "    main: 0.02691758844707354\n",
      "  policy_reward_min:\n",
      "    main: -1.8652107445553494\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3062588106455514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.195497165183372\n",
      "    mean_inference_ms: 4.464489489567131\n",
      "    mean_raw_obs_processing_ms: 1.263959418227007\n",
      "  time_since_restore: 14624.211473703384\n",
      "  time_this_iter_s: 110.1982991695404\n",
      "  time_total_s: 14624.211473703384\n",
      "  timers:\n",
      "    learn_throughput: 94.536\n",
      "    learn_time_ms: 84538.858\n",
      "    sample_throughput: 314.761\n",
      "    sample_time_ms: 25390.727\n",
      "    update_time_ms: 2.901\n",
      "  timestamp: 1639172846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1006992\n",
      "  training_iteration: 126\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         14624.2</td><td style=\"text-align: right;\">1006992</td><td style=\"text-align: right;\"> 0.10767</td><td style=\"text-align: right;\">             1.30474</td><td style=\"text-align: right;\">            -1.57433</td><td style=\"text-align: right;\">           59.5259</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4059936\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7959358918012277\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06711519287014195\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4997564263984353\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 59.05263157894737\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.142848980955466\n",
      "  episode_reward_mean: 0.20942797896230542\n",
      "  episode_reward_min: -0.7362951578483556\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 11845\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0261011469364165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016341949090361595\n",
      "          policy_loss: -0.09978719333559274\n",
      "          total_loss: 0.16118357339128853\n",
      "          vf_explained_var: 0.543962299823761\n",
      "          vf_loss: 0.24442454189062118\n",
      "    num_agent_steps_sampled: 4059936\n",
      "    num_agent_steps_trained: 4059936\n",
      "    num_steps_sampled: 1014984\n",
      "    num_steps_trained: 1014984\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.658450704225352\n",
      "    gpu_util_percent0: 0.13330985915492957\n",
      "    ram_util_percent: 95.06760563380283\n",
      "    vram_util_percent0: 0.5153277347533707\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0923309762836784\n",
      "  policy_reward_mean:\n",
      "    main: 0.052356994740576354\n",
      "  policy_reward_min:\n",
      "    main: -1.611666579934799\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3059917120332223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.184261985133073\n",
      "    mean_inference_ms: 4.462493922230738\n",
      "    mean_raw_obs_processing_ms: 1.2641309273163628\n",
      "  time_since_restore: 14733.744775295258\n",
      "  time_this_iter_s: 109.53330159187317\n",
      "  time_total_s: 14733.744775295258\n",
      "  timers:\n",
      "    learn_throughput: 94.566\n",
      "    learn_time_ms: 84512.558\n",
      "    sample_throughput: 315.277\n",
      "    sample_time_ms: 25349.106\n",
      "    update_time_ms: 2.892\n",
      "  timestamp: 1639172955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1014984\n",
      "  training_iteration: 127\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         14733.7</td><td style=\"text-align: right;\">1014984</td><td style=\"text-align: right;\">0.209428</td><td style=\"text-align: right;\">             1.14285</td><td style=\"text-align: right;\">           -0.736295</td><td style=\"text-align: right;\">           59.0526</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4091904\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.75850147112681\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03698992021794353\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.43519860452115666\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-51-06\n",
      "  done: false\n",
      "  episode_len_mean: 59.833333333333336\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7628729855978857\n",
      "  episode_reward_mean: 0.22451611366399826\n",
      "  episode_reward_min: -0.8006872775054112\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 11977\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0262014424800874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01618693159520626\n",
      "          policy_loss: -0.09996461293846369\n",
      "          total_loss: 0.156314472168684\n",
      "          vf_explained_var: 0.5444649457931519\n",
      "          vf_loss: 0.23988981729745865\n",
      "    num_agent_steps_sampled: 4091904\n",
      "    num_agent_steps_trained: 4091904\n",
      "    num_steps_sampled: 1022976\n",
      "    num_steps_trained: 1022976\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.920979020979026\n",
      "    gpu_util_percent0: 0.13370629370629375\n",
      "    ram_util_percent: 95.20979020979021\n",
      "    vram_util_percent0: 0.5153609141763141\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.03553200398172\n",
      "  policy_reward_mean:\n",
      "    main: 0.05612902841599957\n",
      "  policy_reward_min:\n",
      "    main: -1.6418006605903774\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30602006178806357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.171202019830663\n",
      "    mean_inference_ms: 4.45840669899769\n",
      "    mean_raw_obs_processing_ms: 1.264657632777481\n",
      "  time_since_restore: 14843.834485530853\n",
      "  time_this_iter_s: 110.0897102355957\n",
      "  time_total_s: 14843.834485530853\n",
      "  timers:\n",
      "    learn_throughput: 94.604\n",
      "    learn_time_ms: 84478.71\n",
      "    sample_throughput: 315.526\n",
      "    sample_time_ms: 25329.116\n",
      "    update_time_ms: 2.918\n",
      "  timestamp: 1639173066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1022976\n",
      "  training_iteration: 128\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         14843.8</td><td style=\"text-align: right;\">1022976</td><td style=\"text-align: right;\">0.224516</td><td style=\"text-align: right;\">             1.76287</td><td style=\"text-align: right;\">           -0.800687</td><td style=\"text-align: right;\">           59.8333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4123872\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8034148597908781\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04805855828980595\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7467031798967539\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 56.39007092198582\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.392047091403895\n",
      "  episode_reward_mean: 0.17638112945991927\n",
      "  episode_reward_min: -0.7839617042449896\n",
      "  episodes_this_iter: 141\n",
      "  episodes_total: 12118\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0161363394260408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016437840078026057\n",
      "          policy_loss: -0.09971852909773588\n",
      "          total_loss: 0.16655265067517758\n",
      "          vf_explained_var: 0.5468668341636658\n",
      "          vf_loss: 0.24962786531448364\n",
      "    num_agent_steps_sampled: 4123872\n",
      "    num_agent_steps_trained: 4123872\n",
      "    num_steps_sampled: 1030968\n",
      "    num_steps_trained: 1030968\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.855944055944054\n",
      "    gpu_util_percent0: 0.1316783216783217\n",
      "    ram_util_percent: 95.22377622377626\n",
      "    vram_util_percent0: 0.515410387572283\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.049575794020238\n",
      "  policy_reward_mean:\n",
      "    main: 0.044095282364979824\n",
      "  policy_reward_min:\n",
      "    main: -1.746703179896754\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3057827947321597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.15757297579096\n",
      "    mean_inference_ms: 4.455210609000387\n",
      "    mean_raw_obs_processing_ms: 1.2652070538260962\n",
      "  time_since_restore: 14954.24858212471\n",
      "  time_this_iter_s: 110.41409659385681\n",
      "  time_total_s: 14954.24858212471\n",
      "  timers:\n",
      "    learn_throughput: 95.008\n",
      "    learn_time_ms: 84118.854\n",
      "    sample_throughput: 315.517\n",
      "    sample_time_ms: 25329.864\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1639173176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1030968\n",
      "  training_iteration: 129\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         14954.2</td><td style=\"text-align: right;\">1030968</td><td style=\"text-align: right;\">0.176381</td><td style=\"text-align: right;\">             1.39205</td><td style=\"text-align: right;\">           -0.783962</td><td style=\"text-align: right;\">           56.3901</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4155840\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7564786712346702\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05452266938046401\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.43297553783686005\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 61.03076923076923\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.952203098186156\n",
      "  episode_reward_mean: 0.12300093676183524\n",
      "  episode_reward_min: -0.9734857828017026\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 12248\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.0181136496067047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016394233267754316\n",
      "          policy_loss: -0.099879694160074\n",
      "          total_loss: 0.14702425887435674\n",
      "          vf_explained_var: 0.554016649723053\n",
      "          vf_loss: 0.23030479121208192\n",
      "    num_agent_steps_sampled: 4155840\n",
      "    num_agent_steps_trained: 4155840\n",
      "    num_steps_sampled: 1038960\n",
      "    num_steps_trained: 1038960\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.704195804195802\n",
      "    gpu_util_percent0: 0.13223776223776224\n",
      "    ram_util_percent: 95.23636363636363\n",
      "    vram_util_percent0: 0.5152504619434531\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8237811041783176\n",
      "  policy_reward_mean:\n",
      "    main: 0.030750234190458803\n",
      "  policy_reward_min:\n",
      "    main: -1.6583249808572407\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30604965084988456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.156568416046536\n",
      "    mean_inference_ms: 4.456478490928033\n",
      "    mean_raw_obs_processing_ms: 1.2656553253627\n",
      "  time_since_restore: 15064.341790676117\n",
      "  time_this_iter_s: 110.09320855140686\n",
      "  time_total_s: 15064.341790676117\n",
      "  timers:\n",
      "    learn_throughput: 94.9\n",
      "    learn_time_ms: 84214.522\n",
      "    sample_throughput: 315.644\n",
      "    sample_time_ms: 25319.647\n",
      "    update_time_ms: 2.885\n",
      "  timestamp: 1639173286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1038960\n",
      "  training_iteration: 130\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         15064.3</td><td style=\"text-align: right;\">1038960</td><td style=\"text-align: right;\">0.123001</td><td style=\"text-align: right;\">            0.952203</td><td style=\"text-align: right;\">           -0.973486</td><td style=\"text-align: right;\">           61.0308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4187808\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1689948869972528\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06980782790103843\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5572142298320111\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-56-37\n",
      "  done: false\n",
      "  episode_len_mean: 68.056\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5150448998195154\n",
      "  episode_reward_mean: 0.18005529904292072\n",
      "  episode_reward_min: -0.9410025116202256\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 12373\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9987958297729492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016562820624560118\n",
      "          policy_loss: -0.10240350533276796\n",
      "          total_loss: 0.14808124514855445\n",
      "          vf_explained_var: 0.5503057837486267\n",
      "          vf_loss: 0.23371489375829696\n",
      "    num_agent_steps_sampled: 4187808\n",
      "    num_agent_steps_trained: 4187808\n",
      "    num_steps_sampled: 1046952\n",
      "    num_steps_trained: 1046952\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.16923076923077\n",
      "    gpu_util_percent0: 0.13181818181818183\n",
      "    ram_util_percent: 95.57692307692308\n",
      "    vram_util_percent0: 0.5153333011180988\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0124876853853197\n",
      "  policy_reward_mean:\n",
      "    main: 0.0450138247607302\n",
      "  policy_reward_min:\n",
      "    main: -1.6553210463251284\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3054778379007253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.137386996295138\n",
      "    mean_inference_ms: 4.451776952064838\n",
      "    mean_raw_obs_processing_ms: 1.265306053365807\n",
      "  time_since_restore: 15175.030444383621\n",
      "  time_this_iter_s: 110.68865370750427\n",
      "  time_total_s: 15175.030444383621\n",
      "  timers:\n",
      "    learn_throughput: 94.795\n",
      "    learn_time_ms: 84308.558\n",
      "    sample_throughput: 315.753\n",
      "    sample_time_ms: 25310.917\n",
      "    update_time_ms: 2.863\n",
      "  timestamp: 1639173397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1046952\n",
      "  training_iteration: 131\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">           15175</td><td style=\"text-align: right;\">1046952</td><td style=\"text-align: right;\">0.180055</td><td style=\"text-align: right;\">             1.51504</td><td style=\"text-align: right;\">           -0.941003</td><td style=\"text-align: right;\">            68.056</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4219776\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8404579931441528\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07761028781547419\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.42328877638222534\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_18-58-27\n",
      "  done: false\n",
      "  episode_len_mean: 60.37301587301587\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1059370925522374\n",
      "  episode_reward_mean: 0.16275324573263472\n",
      "  episode_reward_min: -0.9948707058647914\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 12499\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.013537451505661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016521984543651343\n",
      "          policy_loss: -0.10004104224592447\n",
      "          total_loss: 0.137541085395962\n",
      "          vf_explained_var: 0.5708156824111938\n",
      "          vf_loss: 0.2208536185026169\n",
      "    num_agent_steps_sampled: 4219776\n",
      "    num_agent_steps_trained: 4219776\n",
      "    num_steps_sampled: 1054944\n",
      "    num_steps_trained: 1054944\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.48461538461538\n",
      "    gpu_util_percent0: 0.13237762237762238\n",
      "    ram_util_percent: 95.58041958041959\n",
      "    vram_util_percent0: 0.5153010858835143\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8393059170089636\n",
      "  policy_reward_mean:\n",
      "    main: 0.04068831143315867\n",
      "  policy_reward_min:\n",
      "    main: -1.8275045496488613\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30533154094875614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.124925506545836\n",
      "    mean_inference_ms: 4.44880441403986\n",
      "    mean_raw_obs_processing_ms: 1.26528501657799\n",
      "  time_since_restore: 15285.431700468063\n",
      "  time_this_iter_s: 110.40125608444214\n",
      "  time_total_s: 15285.431700468063\n",
      "  timers:\n",
      "    learn_throughput: 94.673\n",
      "    learn_time_ms: 84417.2\n",
      "    sample_throughput: 315.704\n",
      "    sample_time_ms: 25314.84\n",
      "    update_time_ms: 2.89\n",
      "  timestamp: 1639173507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1054944\n",
      "  training_iteration: 132\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         15285.4</td><td style=\"text-align: right;\">1054944</td><td style=\"text-align: right;\">0.162753</td><td style=\"text-align: right;\">             1.10594</td><td style=\"text-align: right;\">           -0.994871</td><td style=\"text-align: right;\">            60.373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4251744\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.866921963570761\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.027538853696061894\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.0606584677714939\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 65.31451612903226\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1405347766102634\n",
      "  episode_reward_mean: 0.08595914459842675\n",
      "  episode_reward_min: -1.3329653215135302\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 12623\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9993862042427063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016260127767920494\n",
      "          policy_loss: -0.09920459580793976\n",
      "          total_loss: 0.16468650805205107\n",
      "          vf_explained_var: 0.5152458548545837\n",
      "          vf_loss: 0.24742772442102431\n",
      "    num_agent_steps_sampled: 4251744\n",
      "    num_agent_steps_trained: 4251744\n",
      "    num_steps_sampled: 1062936\n",
      "    num_steps_trained: 1062936\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.546853146853145\n",
      "    gpu_util_percent0: 0.1334265734265734\n",
      "    ram_util_percent: 95.54615384615386\n",
      "    vram_util_percent0: 0.5153356022062835\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8662022636324667\n",
      "  policy_reward_mean:\n",
      "    main: 0.021489786149606706\n",
      "  policy_reward_min:\n",
      "    main: -2.0606584677714936\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3052271696347752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11511044436169\n",
      "    mean_inference_ms: 4.4467843771580355\n",
      "    mean_raw_obs_processing_ms: 1.2655015681631054\n",
      "  time_since_restore: 15395.614387750626\n",
      "  time_this_iter_s: 110.18268728256226\n",
      "  time_total_s: 15395.614387750626\n",
      "  timers:\n",
      "    learn_throughput: 94.624\n",
      "    learn_time_ms: 84461.031\n",
      "    sample_throughput: 315.986\n",
      "    sample_time_ms: 25292.222\n",
      "    update_time_ms: 2.961\n",
      "  timestamp: 1639173618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1062936\n",
      "  training_iteration: 133\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         15395.6</td><td style=\"text-align: right;\">1062936</td><td style=\"text-align: right;\">0.0859591</td><td style=\"text-align: right;\">             1.14053</td><td style=\"text-align: right;\">            -1.33297</td><td style=\"text-align: right;\">           65.3145</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4283712\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8668420817513338\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05733066811069163\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.45961076442284465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 68.36752136752136\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3106033271267563\n",
      "  episode_reward_mean: 0.04186896897139866\n",
      "  episode_reward_min: -1.8305440429600623\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 12740\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9821332173347473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016734884064644576\n",
      "          policy_loss: -0.10261286290362477\n",
      "          total_loss: 0.15795455561578273\n",
      "          vf_explained_var: 0.5177697539329529\n",
      "          vf_loss: 0.24362334698438645\n",
      "    num_agent_steps_sampled: 4283712\n",
      "    num_agent_steps_trained: 4283712\n",
      "    num_steps_sampled: 1070928\n",
      "    num_steps_trained: 1070928\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.02684563758389\n",
      "    gpu_util_percent0: 0.15859060402684566\n",
      "    ram_util_percent: 95.55503355704695\n",
      "    vram_util_percent0: 0.5231597730620502\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9782899924652193\n",
      "  policy_reward_mean:\n",
      "    main: 0.010467242242849685\n",
      "  policy_reward_min:\n",
      "    main: -1.742608217550567\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30572272008049606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.12698522945012\n",
      "    mean_inference_ms: 4.451800524848028\n",
      "    mean_raw_obs_processing_ms: 1.2657654816689623\n",
      "  time_since_restore: 15510.402292251587\n",
      "  time_this_iter_s: 114.7879045009613\n",
      "  time_total_s: 15510.402292251587\n",
      "  timers:\n",
      "    learn_throughput: 94.11\n",
      "    learn_time_ms: 84921.646\n",
      "    sample_throughput: 315.371\n",
      "    sample_time_ms: 25341.575\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1639173733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1070928\n",
      "  training_iteration: 134\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         15510.4</td><td style=\"text-align: right;\">1070928</td><td style=\"text-align: right;\">0.041869</td><td style=\"text-align: right;\">              1.3106</td><td style=\"text-align: right;\">            -1.83054</td><td style=\"text-align: right;\">           68.3675</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4315680\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9662746169818411\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07450950903160372\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6550979728609243\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-04-05\n",
      "  done: false\n",
      "  episode_len_mean: 58.49264705882353\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2285505319371772\n",
      "  episode_reward_mean: 0.11946767546650999\n",
      "  episode_reward_min: -2.0687279095397946\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 12876\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9636428265571594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016285256266593932\n",
      "          policy_loss: -0.09910205770283938\n",
      "          total_loss: 0.16174651074782015\n",
      "          vf_explained_var: 0.5514810085296631\n",
      "          vf_loss: 0.24435974633693694\n",
      "    num_agent_steps_sampled: 4315680\n",
      "    num_agent_steps_trained: 4315680\n",
      "    num_steps_sampled: 1078920\n",
      "    num_steps_trained: 1078920\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.29448275862069\n",
      "    gpu_util_percent0: 0.13648275862068965\n",
      "    ram_util_percent: 95.30827586206897\n",
      "    vram_util_percent0: 0.5286800331324959\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8958383309348674\n",
      "  policy_reward_mean:\n",
      "    main: 0.029866918866627498\n",
      "  policy_reward_min:\n",
      "    main: -1.7211417429982547\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3052608067413145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.10478839425382\n",
      "    mean_inference_ms: 4.4449444315816\n",
      "    mean_raw_obs_processing_ms: 1.2659608076307765\n",
      "  time_since_restore: 15622.408059835434\n",
      "  time_this_iter_s: 112.00576758384705\n",
      "  time_total_s: 15622.408059835434\n",
      "  timers:\n",
      "    learn_throughput: 93.906\n",
      "    learn_time_ms: 85106.385\n",
      "    sample_throughput: 314.86\n",
      "    sample_time_ms: 25382.709\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1639173845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1078920\n",
      "  training_iteration: 135\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         15622.4</td><td style=\"text-align: right;\">1078920</td><td style=\"text-align: right;\">0.119468</td><td style=\"text-align: right;\">             1.22855</td><td style=\"text-align: right;\">            -2.06873</td><td style=\"text-align: right;\">           58.4926</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4347648\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0285449032199026\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.045028508237843205\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47231361891261686\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 59.007751937984494\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4923692740501684\n",
      "  episode_reward_mean: 0.07561694459968706\n",
      "  episode_reward_min: -0.9633533027969003\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 13005\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9682573008537293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016358219895511866\n",
      "          policy_loss: -0.10004073322936892\n",
      "          total_loss: 0.16174602053314446\n",
      "          vf_explained_var: 0.5248830318450928\n",
      "          vf_loss: 0.2452240549325943\n",
      "    num_agent_steps_sampled: 4347648\n",
      "    num_agent_steps_trained: 4347648\n",
      "    num_steps_sampled: 1086912\n",
      "    num_steps_trained: 1086912\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.488194444444446\n",
      "    gpu_util_percent0: 0.13805555555555557\n",
      "    ram_util_percent: 95.35902777777777\n",
      "    vram_util_percent0: 0.5281845270739645\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7702115467851776\n",
      "  policy_reward_mean:\n",
      "    main: 0.018904236149921757\n",
      "  policy_reward_min:\n",
      "    main: -1.8582254579524498\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3049910214523648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.093954629823727\n",
      "    mean_inference_ms: 4.441879948322444\n",
      "    mean_raw_obs_processing_ms: 1.2662372749478406\n",
      "  time_since_restore: 15734.168739318848\n",
      "  time_this_iter_s: 111.7606794834137\n",
      "  time_total_s: 15734.168739318848\n",
      "  timers:\n",
      "    learn_throughput: 93.736\n",
      "    learn_time_ms: 85260.473\n",
      "    sample_throughput: 314.783\n",
      "    sample_time_ms: 25388.921\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1639173956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1086912\n",
      "  training_iteration: 136\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         15734.2</td><td style=\"text-align: right;\">1086912</td><td style=\"text-align: right;\">0.0756169</td><td style=\"text-align: right;\">             1.49237</td><td style=\"text-align: right;\">           -0.963353</td><td style=\"text-align: right;\">           59.0078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4379616\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9735350489908231\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08067939900202258\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.570932711518739\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-07-48\n",
      "  done: false\n",
      "  episode_len_mean: 58.51048951048951\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3118063983775783\n",
      "  episode_reward_mean: 0.07296076954166834\n",
      "  episode_reward_min: -1.0121472246231311\n",
      "  episodes_this_iter: 143\n",
      "  episodes_total: 13148\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9439400599002838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01654860620200634\n",
      "          policy_loss: -0.10050610790029169\n",
      "          total_loss: 0.16421702240034938\n",
      "          vf_explained_var: 0.565787672996521\n",
      "          vf_loss: 0.24796766608953477\n",
      "    num_agent_steps_sampled: 4379616\n",
      "    num_agent_steps_trained: 4379616\n",
      "    num_steps_sampled: 1094904\n",
      "    num_steps_trained: 1094904\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.011724137931033\n",
      "    gpu_util_percent0: 0.13606896551724135\n",
      "    ram_util_percent: 95.41724137931033\n",
      "    vram_util_percent0: 0.5268475337849339\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9280174923727054\n",
      "  policy_reward_mean:\n",
      "    main: 0.018240192385417086\n",
      "  policy_reward_min:\n",
      "    main: -1.6748314510940192\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3046591956902336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.08232596568598\n",
      "    mean_inference_ms: 4.439409160419073\n",
      "    mean_raw_obs_processing_ms: 1.2668056521908986\n",
      "  time_since_restore: 15845.788534164429\n",
      "  time_this_iter_s: 111.61979484558105\n",
      "  time_total_s: 15845.788534164429\n",
      "  timers:\n",
      "    learn_throughput: 93.584\n",
      "    learn_time_ms: 85399.641\n",
      "    sample_throughput: 313.987\n",
      "    sample_time_ms: 25453.303\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1639174068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1094904\n",
      "  training_iteration: 137\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         15845.8</td><td style=\"text-align: right;\">1094904</td><td style=\"text-align: right;\">0.0729608</td><td style=\"text-align: right;\">             1.31181</td><td style=\"text-align: right;\">            -1.01215</td><td style=\"text-align: right;\">           58.5105</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4411584\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.829136076722297\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07868967414454872\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.45867718188142487\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-09-40\n",
      "  done: false\n",
      "  episode_len_mean: 59.66428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.203909551529718\n",
      "  episode_reward_mean: 0.16411462159451976\n",
      "  episode_reward_min: -0.8825767694094508\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 13288\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9479724192619323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01666795840486884\n",
      "          policy_loss: -0.10121987620182335\n",
      "          total_loss: 0.1628596777394414\n",
      "          vf_explained_var: 0.5552166700363159\n",
      "          vf_loss: 0.2472032436132431\n",
      "    num_agent_steps_sampled: 4411584\n",
      "    num_agent_steps_trained: 4411584\n",
      "    num_steps_sampled: 1102896\n",
      "    num_steps_trained: 1102896\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.235416666666666\n",
      "    gpu_util_percent0: 0.14006944444444447\n",
      "    ram_util_percent: 95.50347222222223\n",
      "    vram_util_percent0: 0.5200015538737158\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9352902687401312\n",
      "  policy_reward_mean:\n",
      "    main: 0.041028655398629925\n",
      "  policy_reward_min:\n",
      "    main: -1.6048101984897158\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30486557617056587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.08492947016832\n",
      "    mean_inference_ms: 4.442244078869931\n",
      "    mean_raw_obs_processing_ms: 1.267340275077094\n",
      "  time_since_restore: 15957.238688707352\n",
      "  time_this_iter_s: 111.45015454292297\n",
      "  time_total_s: 15957.238688707352\n",
      "  timers:\n",
      "    learn_throughput: 93.456\n",
      "    learn_time_ms: 85516.131\n",
      "    sample_throughput: 313.735\n",
      "    sample_time_ms: 25473.691\n",
      "    update_time_ms: 2.909\n",
      "  timestamp: 1639174180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1102896\n",
      "  training_iteration: 138\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         15957.2</td><td style=\"text-align: right;\">1102896</td><td style=\"text-align: right;\">0.164115</td><td style=\"text-align: right;\">             1.20391</td><td style=\"text-align: right;\">           -0.882577</td><td style=\"text-align: right;\">           59.6643</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4443552\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9607376947642031\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.028810684750692057\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8239876938720386\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 63.36666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6882390697033944\n",
      "  episode_reward_mean: 0.15967540159355362\n",
      "  episode_reward_min: -1.2479147090565639\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 13408\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9503139634132385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016540933657437563\n",
      "          policy_loss: -0.10004013798572123\n",
      "          total_loss: 0.16550932970643042\n",
      "          vf_explained_var: 0.4982271194458008\n",
      "          vf_loss: 0.24880177307128906\n",
      "    num_agent_steps_sampled: 4443552\n",
      "    num_agent_steps_trained: 4443552\n",
      "    num_steps_sampled: 1110888\n",
      "    num_steps_trained: 1110888\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.255172413793105\n",
      "    gpu_util_percent0: 0.13889655172413792\n",
      "    ram_util_percent: 95.6703448275862\n",
      "    vram_util_percent0: 0.5157186461063644\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.905541548771754\n",
      "  policy_reward_mean:\n",
      "    main: 0.0399188503983884\n",
      "  policy_reward_min:\n",
      "    main: -1.8239876938720387\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30488483782816417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.0751867341742\n",
      "    mean_inference_ms: 4.439325710690325\n",
      "    mean_raw_obs_processing_ms: 1.267529779682817\n",
      "  time_since_restore: 16069.081783294678\n",
      "  time_this_iter_s: 111.84309458732605\n",
      "  time_total_s: 16069.081783294678\n",
      "  timers:\n",
      "    learn_throughput: 93.348\n",
      "    learn_time_ms: 85615.39\n",
      "    sample_throughput: 313.216\n",
      "    sample_time_ms: 25515.905\n",
      "    update_time_ms: 2.966\n",
      "  timestamp: 1639174292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1110888\n",
      "  training_iteration: 139\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         16069.1</td><td style=\"text-align: right;\">1110888</td><td style=\"text-align: right;\">0.159675</td><td style=\"text-align: right;\">             1.68824</td><td style=\"text-align: right;\">            -1.24791</td><td style=\"text-align: right;\">           63.3667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4475520\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6529619464724171\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05486780275119442\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5793196537029137\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 59.60769230769231\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0686203422158411\n",
      "  episode_reward_mean: 0.10760208799380087\n",
      "  episode_reward_min: -0.6321934925444701\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 13538\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9506787567138671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016601762976497413\n",
      "          policy_loss: -0.10143569264560938\n",
      "          total_loss: 0.14875739066675306\n",
      "          vf_explained_var: 0.563069760799408\n",
      "          vf_loss: 0.2333837975859642\n",
      "    num_agent_steps_sampled: 4475520\n",
      "    num_agent_steps_trained: 4475520\n",
      "    num_steps_sampled: 1118880\n",
      "    num_steps_trained: 1118880\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.871034482758617\n",
      "    gpu_util_percent0: 0.13634482758620686\n",
      "    ram_util_percent: 95.65241379310346\n",
      "    vram_util_percent0: 0.5157118380592527\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.795338387682651\n",
      "  policy_reward_mean:\n",
      "    main: 0.026900521998450222\n",
      "  policy_reward_min:\n",
      "    main: -1.5793196537029137\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30517032414855794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.07929631406547\n",
      "    mean_inference_ms: 4.442235310961039\n",
      "    mean_raw_obs_processing_ms: 1.2680357170914884\n",
      "  time_since_restore: 16181.258980035782\n",
      "  time_this_iter_s: 112.17719674110413\n",
      "  time_total_s: 16181.258980035782\n",
      "  timers:\n",
      "    learn_throughput: 93.159\n",
      "    learn_time_ms: 85788.944\n",
      "    sample_throughput: 312.725\n",
      "    sample_time_ms: 25556.001\n",
      "    update_time_ms: 2.976\n",
      "  timestamp: 1639174404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1118880\n",
      "  training_iteration: 140\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         16181.3</td><td style=\"text-align: right;\">1118880</td><td style=\"text-align: right;\">0.107602</td><td style=\"text-align: right;\">             1.06862</td><td style=\"text-align: right;\">           -0.632193</td><td style=\"text-align: right;\">           59.6077</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4507488\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.044148613489706\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07756006233235341\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5164316436085384\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 68.51639344262296\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2055304828267586\n",
      "  episode_reward_mean: 0.14931619309569416\n",
      "  episode_reward_min: -0.7597401033908597\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 13660\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9395087110996246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016589269310235976\n",
      "          policy_loss: -0.09958222455531358\n",
      "          total_loss: 0.16459998686611652\n",
      "          vf_explained_var: 0.5297757387161255\n",
      "          vf_loss: 0.24738557463884353\n",
      "    num_agent_steps_sampled: 4507488\n",
      "    num_agent_steps_trained: 4507488\n",
      "    num_steps_sampled: 1126872\n",
      "    num_steps_trained: 1126872\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.552083333333332\n",
      "    gpu_util_percent0: 0.13701388888888888\n",
      "    ram_util_percent: 95.65416666666668\n",
      "    vram_util_percent0: 0.5156929819750649\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.91374861148699\n",
      "  policy_reward_mean:\n",
      "    main: 0.037329048273923554\n",
      "  policy_reward_min:\n",
      "    main: -1.560564333588386\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3050115753949385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.07244237045499\n",
      "    mean_inference_ms: 4.440719159124569\n",
      "    mean_raw_obs_processing_ms: 1.2682410551412195\n",
      "  time_since_restore: 16292.790241479874\n",
      "  time_this_iter_s: 111.5312614440918\n",
      "  time_total_s: 16292.790241479874\n",
      "  timers:\n",
      "    learn_throughput: 93.094\n",
      "    learn_time_ms: 85848.594\n",
      "    sample_throughput: 312.347\n",
      "    sample_time_ms: 25586.923\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1639174515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1126872\n",
      "  training_iteration: 141\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         16292.8</td><td style=\"text-align: right;\">1126872</td><td style=\"text-align: right;\">0.149316</td><td style=\"text-align: right;\">             1.20553</td><td style=\"text-align: right;\">            -0.75974</td><td style=\"text-align: right;\">           68.5164</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4539456\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8949823264570191\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05522872895252496\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6383136761526738\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 63.976\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1556551471271583\n",
      "  episode_reward_mean: 0.06381386922675716\n",
      "  episode_reward_min: -1.7221811148956507\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 13785\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9330976765155792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016570089790970088\n",
      "          policy_loss: -0.10153799112513662\n",
      "          total_loss: 0.16265827203541994\n",
      "          vf_explained_var: 0.5350006222724915\n",
      "          vf_loss: 0.2474190467596054\n",
      "    num_agent_steps_sampled: 4539456\n",
      "    num_agent_steps_trained: 4539456\n",
      "    num_steps_sampled: 1134864\n",
      "    num_steps_trained: 1134864\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.61780821917808\n",
      "    gpu_util_percent0: 0.138013698630137\n",
      "    ram_util_percent: 95.59726027397261\n",
      "    vram_util_percent0: 0.515692121146556\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8259996339028937\n",
      "  policy_reward_mean:\n",
      "    main: 0.01595346730668928\n",
      "  policy_reward_min:\n",
      "    main: -1.8581544687057119\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30498490480733126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.06689432642374\n",
      "    mean_inference_ms: 4.440211408588754\n",
      "    mean_raw_obs_processing_ms: 1.2686008044241015\n",
      "  time_since_restore: 16404.975095272064\n",
      "  time_this_iter_s: 112.18485379219055\n",
      "  time_total_s: 16404.975095272064\n",
      "  timers:\n",
      "    learn_throughput: 92.984\n",
      "    learn_time_ms: 85950.513\n",
      "    sample_throughput: 311.474\n",
      "    sample_time_ms: 25658.674\n",
      "    update_time_ms: 2.966\n",
      "  timestamp: 1639174628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1134864\n",
      "  training_iteration: 142\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">           16405</td><td style=\"text-align: right;\">1134864</td><td style=\"text-align: right;\">0.0638139</td><td style=\"text-align: right;\">             1.15566</td><td style=\"text-align: right;\">            -1.72218</td><td style=\"text-align: right;\">            63.976</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4571424\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8153006977505984\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06095355486163123\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49151726880650953\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 55.27210884353742\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2122657452015795\n",
      "  episode_reward_mean: 0.13949328441378575\n",
      "  episode_reward_min: -0.8824010870546403\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 13932\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9203081512451172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016657905276864768\n",
      "          policy_loss: -0.10157244906201959\n",
      "          total_loss: 0.1880632012821734\n",
      "          vf_explained_var: 0.5242016315460205\n",
      "          vf_loss: 0.27276951825618745\n",
      "    num_agent_steps_sampled: 4571424\n",
      "    num_agent_steps_trained: 4571424\n",
      "    num_steps_sampled: 1142856\n",
      "    num_steps_trained: 1142856\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.15138888888889\n",
      "    gpu_util_percent0: 0.13826388888888888\n",
      "    ram_util_percent: 95.48263888888889\n",
      "    vram_util_percent0: 0.5156929819750649\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8702699630685005\n",
      "  policy_reward_mean:\n",
      "    main: 0.03487332110344644\n",
      "  policy_reward_min:\n",
      "    main: -1.7275416230603033\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3046000660684851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.0523604467794\n",
      "    mean_inference_ms: 4.4365262623242\n",
      "    mean_raw_obs_processing_ms: 1.269070809491247\n",
      "  time_since_restore: 16516.784016609192\n",
      "  time_this_iter_s: 111.80892133712769\n",
      "  time_total_s: 16516.784016609192\n",
      "  timers:\n",
      "    learn_throughput: 92.815\n",
      "    learn_time_ms: 86106.33\n",
      "    sample_throughput: 311.381\n",
      "    sample_time_ms: 25666.312\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1639174740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1142856\n",
      "  training_iteration: 143\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         16516.8</td><td style=\"text-align: right;\">1142856</td><td style=\"text-align: right;\">0.139493</td><td style=\"text-align: right;\">             1.21227</td><td style=\"text-align: right;\">           -0.882401</td><td style=\"text-align: right;\">           55.2721</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4603392\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7367681837430042\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03581416778599368\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6987140327316289\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 55.53284671532847\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1649729092449368\n",
      "  episode_reward_mean: 0.05217783143910553\n",
      "  episode_reward_min: -1.0693106348235533\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 14069\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9192282288074494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016925161115825177\n",
      "          policy_loss: -0.10165363227203489\n",
      "          total_loss: 0.18623146661370993\n",
      "          vf_explained_var: 0.5164356827735901\n",
      "          vf_loss: 0.27074837058782575\n",
      "    num_agent_steps_sampled: 4603392\n",
      "    num_agent_steps_trained: 4603392\n",
      "    num_steps_sampled: 1150848\n",
      "    num_steps_trained: 1150848\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.305517241379313\n",
      "    gpu_util_percent0: 0.13848275862068965\n",
      "    ram_util_percent: 95.58275862068966\n",
      "    vram_util_percent0: 0.5157311275260691\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8431914551506843\n",
      "  policy_reward_mean:\n",
      "    main: 0.013044457859776391\n",
      "  policy_reward_min:\n",
      "    main: -1.836160612104885\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3048864490047691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.054165946440655\n",
      "    mean_inference_ms: 4.438114303206799\n",
      "    mean_raw_obs_processing_ms: 1.2696121043668076\n",
      "  time_since_restore: 16628.514045715332\n",
      "  time_this_iter_s: 111.73002910614014\n",
      "  time_total_s: 16628.514045715332\n",
      "  timers:\n",
      "    learn_throughput: 93.158\n",
      "    learn_time_ms: 85790.112\n",
      "    sample_throughput: 311.189\n",
      "    sample_time_ms: 25682.163\n",
      "    update_time_ms: 2.953\n",
      "  timestamp: 1639174851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1150848\n",
      "  training_iteration: 144\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         16628.5</td><td style=\"text-align: right;\">1150848</td><td style=\"text-align: right;\">0.0521778</td><td style=\"text-align: right;\">             1.16497</td><td style=\"text-align: right;\">            -1.06931</td><td style=\"text-align: right;\">           55.5328</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4635360\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7631750074953703\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06805216667776977\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49101416641708806\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 61.65413533834587\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5711789972027936\n",
      "  episode_reward_mean: 0.15250011430842922\n",
      "  episode_reward_min: -0.7908150133956742\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 14202\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9132832000255585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01684998371079564\n",
      "          policy_loss: -0.101590506054461\n",
      "          total_loss: 0.15884035632386803\n",
      "          vf_explained_var: 0.5630859136581421\n",
      "          vf_loss: 0.24337025171518326\n",
      "    num_agent_steps_sampled: 4635360\n",
      "    num_agent_steps_trained: 4635360\n",
      "    num_steps_sampled: 1158840\n",
      "    num_steps_trained: 1158840\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.0625\n",
      "    gpu_util_percent0: 0.13972222222222222\n",
      "    ram_util_percent: 95.61527777777778\n",
      "    vram_util_percent0: 0.5157386841431758\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8669302548146673\n",
      "  policy_reward_mean:\n",
      "    main: 0.03812502857710732\n",
      "  policy_reward_min:\n",
      "    main: -1.6647751548541763\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3044257974761513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.036376453521736\n",
      "    mean_inference_ms: 4.432951734270753\n",
      "    mean_raw_obs_processing_ms: 1.2697712051383465\n",
      "  time_since_restore: 16739.908772945404\n",
      "  time_this_iter_s: 111.39472723007202\n",
      "  time_total_s: 16739.908772945404\n",
      "  timers:\n",
      "    learn_throughput: 93.188\n",
      "    learn_time_ms: 85761.879\n",
      "    sample_throughput: 311.522\n",
      "    sample_time_ms: 25654.722\n",
      "    update_time_ms: 2.955\n",
      "  timestamp: 1639174963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1158840\n",
      "  training_iteration: 145\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         16739.9</td><td style=\"text-align: right;\">1158840</td><td style=\"text-align: right;\">  0.1525</td><td style=\"text-align: right;\">             1.57118</td><td style=\"text-align: right;\">           -0.790815</td><td style=\"text-align: right;\">           61.6541</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4667328\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6291385904222239\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0708796893721373\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.42994344662909234\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-24-38\n",
      "  done: false\n",
      "  episode_len_mean: 65.65322580645162\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1886859506689245\n",
      "  episode_reward_mean: 0.09505860654832851\n",
      "  episode_reward_min: -1.067952036299885\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 14326\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9181484847068787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016604552827775478\n",
      "          policy_loss: -0.10035833464190363\n",
      "          total_loss: 0.14537211087718607\n",
      "          vf_explained_var: 0.5451111793518066\n",
      "          vf_loss: 0.22891833472251893\n",
      "    num_agent_steps_sampled: 4667328\n",
      "    num_agent_steps_trained: 4667328\n",
      "    num_steps_sampled: 1166832\n",
      "    num_steps_trained: 1166832\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.66241610738255\n",
      "    gpu_util_percent0: 0.14\n",
      "    ram_util_percent: 95.59865771812082\n",
      "    vram_util_percent0: 0.5160387004732658\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9409480642566825\n",
      "  policy_reward_mean:\n",
      "    main: 0.023764651637082117\n",
      "  policy_reward_min:\n",
      "    main: -1.6829850371457846\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3048289527606972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.0498067845311\n",
      "    mean_inference_ms: 4.435365153539844\n",
      "    mean_raw_obs_processing_ms: 1.270578648110786\n",
      "  time_since_restore: 16854.745159864426\n",
      "  time_this_iter_s: 114.8363869190216\n",
      "  time_total_s: 16854.745159864426\n",
      "  timers:\n",
      "    learn_throughput: 93.004\n",
      "    learn_time_ms: 85931.925\n",
      "    sample_throughput: 309.891\n",
      "    sample_time_ms: 25789.694\n",
      "    update_time_ms: 2.932\n",
      "  timestamp: 1639175078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1166832\n",
      "  training_iteration: 146\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         16854.7</td><td style=\"text-align: right;\">1166832</td><td style=\"text-align: right;\">0.0950586</td><td style=\"text-align: right;\">             1.18869</td><td style=\"text-align: right;\">            -1.06795</td><td style=\"text-align: right;\">           65.6532</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4699296\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7910746160423129\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.049159315307973066\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5470298105689505\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-26-29\n",
      "  done: false\n",
      "  episode_len_mean: 51.711409395973156\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3011939657861276\n",
      "  episode_reward_mean: 0.12158592745771152\n",
      "  episode_reward_min: -0.8113106011751858\n",
      "  episodes_this_iter: 149\n",
      "  episodes_total: 14475\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8944336082935334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016832647770643236\n",
      "          policy_loss: -0.10126982954889537\n",
      "          total_loss: 0.1732991325855255\n",
      "          vf_explained_var: 0.560344398021698\n",
      "          vf_loss: 0.2575259053707123\n",
      "    num_agent_steps_sampled: 4699296\n",
      "    num_agent_steps_trained: 4699296\n",
      "    num_steps_sampled: 1174824\n",
      "    num_steps_trained: 1174824\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.316666666666663\n",
      "    gpu_util_percent0: 0.13743055555555556\n",
      "    ram_util_percent: 95.62847222222223\n",
      "    vram_util_percent0: 0.5171348853789622\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9686042983908192\n",
      "  policy_reward_mean:\n",
      "    main: 0.030396481864427886\n",
      "  policy_reward_min:\n",
      "    main: -1.552548331806316\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3045796291956181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.03991002730547\n",
      "    mean_inference_ms: 4.432996032740167\n",
      "    mean_raw_obs_processing_ms: 1.2711738914872577\n",
      "  time_since_restore: 16966.105984926224\n",
      "  time_this_iter_s: 111.3608250617981\n",
      "  time_total_s: 16966.105984926224\n",
      "  timers:\n",
      "    learn_throughput: 93.007\n",
      "    learn_time_ms: 85928.99\n",
      "    sample_throughput: 310.152\n",
      "    sample_time_ms: 25768.046\n",
      "    update_time_ms: 2.933\n",
      "  timestamp: 1639175189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1174824\n",
      "  training_iteration: 147\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         16966.1</td><td style=\"text-align: right;\">1174824</td><td style=\"text-align: right;\">0.121586</td><td style=\"text-align: right;\">             1.30119</td><td style=\"text-align: right;\">           -0.811311</td><td style=\"text-align: right;\">           51.7114</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4731264\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7986176237534952\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09241401864029643\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5080974797077141\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-28-21\n",
      "  done: false\n",
      "  episode_len_mean: 58.44525547445255\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0761322594365996\n",
      "  episode_reward_mean: 0.17740949435667555\n",
      "  episode_reward_min: -0.7988017703203489\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 14612\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8987103486061097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01679836395010352\n",
      "          policy_loss: -0.10068037654086948\n",
      "          total_loss: 0.15349202424287797\n",
      "          vf_explained_var: 0.5640510320663452\n",
      "          vf_loss: 0.23716405600309373\n",
      "    num_agent_steps_sampled: 4731264\n",
      "    num_agent_steps_trained: 4731264\n",
      "    num_steps_sampled: 1182816\n",
      "    num_steps_trained: 1182816\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.290277777777778\n",
      "    gpu_util_percent0: 0.13861111111111113\n",
      "    ram_util_percent: 95.65833333333336\n",
      "    vram_util_percent0: 0.517086898102446\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7384722359208786\n",
      "  policy_reward_mean:\n",
      "    main: 0.0443523735891689\n",
      "  policy_reward_min:\n",
      "    main: -1.7442318506400771\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30469577399996267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.036064611706184\n",
      "    mean_inference_ms: 4.4316759228567655\n",
      "    mean_raw_obs_processing_ms: 1.2716635782527757\n",
      "  time_since_restore: 17077.683796167374\n",
      "  time_this_iter_s: 111.5778112411499\n",
      "  time_total_s: 17077.683796167374\n",
      "  timers:\n",
      "    learn_throughput: 93.013\n",
      "    learn_time_ms: 85923.164\n",
      "    sample_throughput: 309.923\n",
      "    sample_time_ms: 25787.019\n",
      "    update_time_ms: 2.924\n",
      "  timestamp: 1639175301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1182816\n",
      "  training_iteration: 148\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         17077.7</td><td style=\"text-align: right;\">1182816</td><td style=\"text-align: right;\">0.177409</td><td style=\"text-align: right;\">             1.07613</td><td style=\"text-align: right;\">           -0.798802</td><td style=\"text-align: right;\">           58.4453</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4763232\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5854327460889464\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03342265610337313\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5860868765274263\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 59.95652173913044\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3056552301236075\n",
      "  episode_reward_mean: 0.10684833443579746\n",
      "  episode_reward_min: -1.0271065717749606\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 14750\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8885698552131653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016560887958854437\n",
      "          policy_loss: -0.09962856792844832\n",
      "          total_loss: 0.17194703252613544\n",
      "          vf_explained_var: 0.5413175225257874\n",
      "          vf_loss: 0.25480770111083983\n",
      "    num_agent_steps_sampled: 4763232\n",
      "    num_agent_steps_trained: 4763232\n",
      "    num_steps_sampled: 1190808\n",
      "    num_steps_trained: 1190808\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.211034482758624\n",
      "    gpu_util_percent0: 0.13758620689655174\n",
      "    ram_util_percent: 95.6689655172414\n",
      "    vram_util_percent0: 0.5161781892864032\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9362461854949449\n",
      "  policy_reward_mean:\n",
      "    main: 0.026712083608949368\n",
      "  policy_reward_min:\n",
      "    main: -1.6054062144004806\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3038899707366672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.013278603584503\n",
      "    mean_inference_ms: 4.426242934822984\n",
      "    mean_raw_obs_processing_ms: 1.2719966276862718\n",
      "  time_since_restore: 17189.225651025772\n",
      "  time_this_iter_s: 111.54185485839844\n",
      "  time_total_s: 17189.225651025772\n",
      "  timers:\n",
      "    learn_throughput: 93.019\n",
      "    learn_time_ms: 85917.928\n",
      "    sample_throughput: 310.263\n",
      "    sample_time_ms: 25758.833\n",
      "    update_time_ms: 2.868\n",
      "  timestamp: 1639175412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1190808\n",
      "  training_iteration: 149\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         17189.2</td><td style=\"text-align: right;\">1190808</td><td style=\"text-align: right;\">0.106848</td><td style=\"text-align: right;\">             1.30566</td><td style=\"text-align: right;\">            -1.02711</td><td style=\"text-align: right;\">           59.9565</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4795200\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6255330482551342\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04813423636770907\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6211360083257547\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-32-04\n",
      "  done: false\n",
      "  episode_len_mean: 55.275862068965516\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.192274961452091\n",
      "  episode_reward_mean: 0.07485541376572592\n",
      "  episode_reward_min: -1.4404680635107328\n",
      "  episodes_this_iter: 145\n",
      "  episodes_total: 14895\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8959212956428528\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016967343248426914\n",
      "          policy_loss: -0.09942097090184689\n",
      "          total_loss: 0.16692921220511198\n",
      "          vf_explained_var: 0.5519554018974304\n",
      "          vf_loss: 0.24917074602842332\n",
      "    num_agent_steps_sampled: 4795200\n",
      "    num_agent_steps_trained: 4795200\n",
      "    num_steps_sampled: 1198800\n",
      "    num_steps_trained: 1198800\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.76923076923077\n",
      "    gpu_util_percent0: 0.1388111888111888\n",
      "    ram_util_percent: 95.61608391608394\n",
      "    vram_util_percent0: 0.5161329292622483\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7836370020239856\n",
      "  policy_reward_mean:\n",
      "    main: 0.018713853441431484\n",
      "  policy_reward_min:\n",
      "    main: -1.8493409057191261\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3042614841604671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.021363973816463\n",
      "    mean_inference_ms: 4.430250107486424\n",
      "    mean_raw_obs_processing_ms: 1.272712334923683\n",
      "  time_since_restore: 17300.620796203613\n",
      "  time_this_iter_s: 111.39514517784119\n",
      "  time_total_s: 17300.620796203613\n",
      "  timers:\n",
      "    learn_throughput: 93.126\n",
      "    learn_time_ms: 85818.97\n",
      "    sample_throughput: 310.012\n",
      "    sample_time_ms: 25779.686\n",
      "    update_time_ms: 3.078\n",
      "  timestamp: 1639175524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1198800\n",
      "  training_iteration: 150\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         17300.6</td><td style=\"text-align: right;\">1198800</td><td style=\"text-align: right;\">0.0748554</td><td style=\"text-align: right;\">             1.19227</td><td style=\"text-align: right;\">            -1.44047</td><td style=\"text-align: right;\">           55.2759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4827168\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5569816250634344\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02058345167132713\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.609115507313763\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-33-59\n",
      "  done: false\n",
      "  episode_len_mean: 54.6875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1161450694401938\n",
      "  episode_reward_mean: 0.06603840531817474\n",
      "  episode_reward_min: -1.1724318423381845\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 15039\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8876944477558136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017011272918432952\n",
      "          policy_loss: -0.10298080768063664\n",
      "          total_loss: 0.1787940701879561\n",
      "          vf_explained_var: 0.5374143123626709\n",
      "          vf_loss: 0.2645509639978409\n",
      "    num_agent_steps_sampled: 4827168\n",
      "    num_agent_steps_trained: 4827168\n",
      "    num_steps_sampled: 1206792\n",
      "    num_steps_trained: 1206792\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.684459459459454\n",
      "    gpu_util_percent0: 0.15499999999999997\n",
      "    ram_util_percent: 95.64054054054056\n",
      "    vram_util_percent0: 0.5180369164821288\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8136886373545114\n",
      "  policy_reward_mean:\n",
      "    main: 0.01650960132954368\n",
      "  policy_reward_min:\n",
      "    main: -1.7400204926194252\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30440193706085433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.019324655297492\n",
      "    mean_inference_ms: 4.430609837834948\n",
      "    mean_raw_obs_processing_ms: 1.2731271929450532\n",
      "  time_since_restore: 17415.283112049103\n",
      "  time_this_iter_s: 114.6623158454895\n",
      "  time_total_s: 17415.283112049103\n",
      "  timers:\n",
      "    learn_throughput: 92.772\n",
      "    learn_time_ms: 86147.073\n",
      "    sample_throughput: 310.185\n",
      "    sample_time_ms: 25765.308\n",
      "    update_time_ms: 3.069\n",
      "  timestamp: 1639175639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1206792\n",
      "  training_iteration: 151\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         17415.3</td><td style=\"text-align: right;\">1206792</td><td style=\"text-align: right;\">0.0660384</td><td style=\"text-align: right;\">             1.11615</td><td style=\"text-align: right;\">            -1.17243</td><td style=\"text-align: right;\">           54.6875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4859136\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9461499882070441\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09293449548010427\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48484474749566064\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-35-50\n",
      "  done: false\n",
      "  episode_len_mean: 50.58641975308642\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2823193964951223\n",
      "  episode_reward_mean: 0.15438255580360516\n",
      "  episode_reward_min: -1.5439711000097391\n",
      "  episodes_this_iter: 162\n",
      "  episodes_total: 15201\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8686763782501221\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016753870572894812\n",
      "          policy_loss: -0.10005880846083164\n",
      "          total_loss: 0.18087588535994292\n",
      "          vf_explained_var: 0.5692751407623291\n",
      "          vf_loss: 0.26397139763832095\n",
      "    num_agent_steps_sampled: 4859136\n",
      "    num_agent_steps_trained: 4859136\n",
      "    num_steps_sampled: 1214784\n",
      "    num_steps_trained: 1214784\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.511034482758625\n",
      "    gpu_util_percent0: 0.13951724137931035\n",
      "    ram_util_percent: 95.71034482758621\n",
      "    vram_util_percent0: 0.5206170360032225\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9118389458178628\n",
      "  policy_reward_mean:\n",
      "    main: 0.03859563895090129\n",
      "  policy_reward_min:\n",
      "    main: -1.622932620180564\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30459932710881243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.021879941549916\n",
      "    mean_inference_ms: 4.43183686292251\n",
      "    mean_raw_obs_processing_ms: 1.274121823293145\n",
      "  time_since_restore: 17526.51305770874\n",
      "  time_this_iter_s: 111.22994565963745\n",
      "  time_total_s: 17526.51305770874\n",
      "  timers:\n",
      "    learn_throughput: 92.844\n",
      "    learn_time_ms: 86079.598\n",
      "    sample_throughput: 310.515\n",
      "    sample_time_ms: 25737.864\n",
      "    update_time_ms: 3.045\n",
      "  timestamp: 1639175750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1214784\n",
      "  training_iteration: 152\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         17526.5</td><td style=\"text-align: right;\">1214784</td><td style=\"text-align: right;\">0.154383</td><td style=\"text-align: right;\">             1.28232</td><td style=\"text-align: right;\">            -1.54397</td><td style=\"text-align: right;\">           50.5864</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4891104\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8141472829516269\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06563040176456326\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5217631385904233\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 60.62096774193548\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.613783589477755\n",
      "  episode_reward_mean: 0.1928970891683129\n",
      "  episode_reward_min: -0.8267691955112408\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 15325\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8829178793430328\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016860177103430032\n",
      "          policy_loss: -0.10144595001265407\n",
      "          total_loss: 0.1726751955449581\n",
      "          vf_explained_var: 0.5125104784965515\n",
      "          vf_loss: 0.257050215780735\n",
      "    num_agent_steps_sampled: 4891104\n",
      "    num_agent_steps_trained: 4891104\n",
      "    num_steps_sampled: 1222776\n",
      "    num_steps_trained: 1222776\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.239583333333332\n",
      "    gpu_util_percent0: 0.1371527777777778\n",
      "    ram_util_percent: 95.76250000000002\n",
      "    vram_util_percent0: 0.5198278856348943\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8069752601215359\n",
      "  policy_reward_mean:\n",
      "    main: 0.04822427229207823\n",
      "  policy_reward_min:\n",
      "    main: -1.7058886714318244\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30440181301229485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.012538852872844\n",
      "    mean_inference_ms: 4.429131883648874\n",
      "    mean_raw_obs_processing_ms: 1.2739767879515842\n",
      "  time_since_restore: 17638.017768859863\n",
      "  time_this_iter_s: 111.50471115112305\n",
      "  time_total_s: 17638.017768859863\n",
      "  timers:\n",
      "    learn_throughput: 92.927\n",
      "    learn_time_ms: 86003.433\n",
      "    sample_throughput: 310.009\n",
      "    sample_time_ms: 25779.86\n",
      "    update_time_ms: 3.033\n",
      "  timestamp: 1639175862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1222776\n",
      "  training_iteration: 153\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">           17638</td><td style=\"text-align: right;\">1222776</td><td style=\"text-align: right;\">0.192897</td><td style=\"text-align: right;\">             1.61378</td><td style=\"text-align: right;\">           -0.826769</td><td style=\"text-align: right;\">            60.621</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4923072\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6692432110632522\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06765593429819773\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47294239907405616\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 66.87903225806451\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3468401497883304\n",
      "  episode_reward_mean: 0.16085068108623937\n",
      "  episode_reward_min: -0.6865683634208499\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 15449\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8988233268260956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01710334959998727\n",
      "          policy_loss: -0.09913307933881879\n",
      "          total_loss: 0.1641133917719126\n",
      "          vf_explained_var: 0.5224302411079407\n",
      "          vf_loss: 0.24592932891845704\n",
      "    num_agent_steps_sampled: 4923072\n",
      "    num_agent_steps_trained: 4923072\n",
      "    num_steps_sampled: 1230768\n",
      "    num_steps_trained: 1230768\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.581818181818182\n",
      "    gpu_util_percent0: 0.14013986013986016\n",
      "    ram_util_percent: 95.73006993006992\n",
      "    vram_util_percent0: 0.5191289460786005\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7661664343764065\n",
      "  policy_reward_mean:\n",
      "    main: 0.04021267027155984\n",
      "  policy_reward_min:\n",
      "    main: -1.7791893612768723\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30478520724981417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.017939809229507\n",
      "    mean_inference_ms: 4.431707981851103\n",
      "    mean_raw_obs_processing_ms: 1.2742699812196683\n",
      "  time_since_restore: 17748.957279920578\n",
      "  time_this_iter_s: 110.93951106071472\n",
      "  time_total_s: 17748.957279920578\n",
      "  timers:\n",
      "    learn_throughput: 92.984\n",
      "    learn_time_ms: 85950.129\n",
      "    sample_throughput: 310.305\n",
      "    sample_time_ms: 25755.267\n",
      "    update_time_ms: 3.067\n",
      "  timestamp: 1639175973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1230768\n",
      "  training_iteration: 154\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">           17749</td><td style=\"text-align: right;\">1230768</td><td style=\"text-align: right;\">0.160851</td><td style=\"text-align: right;\">             1.34684</td><td style=\"text-align: right;\">           -0.686568</td><td style=\"text-align: right;\">            66.879</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4955040\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8488770595716203\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06363230057435865\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7984378656793937\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 55.395833333333336\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2860388683200061\n",
      "  episode_reward_mean: 0.1692121134158433\n",
      "  episode_reward_min: -1.0360632220814345\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 15593\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8641545753479004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017166357189416885\n",
      "          policy_loss: -0.10305243705771863\n",
      "          total_loss: 0.17267845879495144\n",
      "          vf_explained_var: 0.5604128837585449\n",
      "          vf_loss: 0.25834995704889296\n",
      "    num_agent_steps_sampled: 4955040\n",
      "    num_agent_steps_trained: 4955040\n",
      "    num_steps_sampled: 1238760\n",
      "    num_steps_trained: 1238760\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.47586206896552\n",
      "    gpu_util_percent0: 0.13793103448275862\n",
      "    ram_util_percent: 95.85172413793103\n",
      "    vram_util_percent0: 0.5188231155892933\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.897554925701161\n",
      "  policy_reward_mean:\n",
      "    main: 0.04230302835396082\n",
      "  policy_reward_min:\n",
      "    main: -1.8294650301511952\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043199635360965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.005324599775943\n",
      "    mean_inference_ms: 4.428879286046358\n",
      "    mean_raw_obs_processing_ms: 1.2749244898268677\n",
      "  time_since_restore: 17860.210592746735\n",
      "  time_this_iter_s: 111.25331282615662\n",
      "  time_total_s: 17860.210592746735\n",
      "  timers:\n",
      "    learn_throughput: 93.035\n",
      "    learn_time_ms: 85903.204\n",
      "    sample_throughput: 309.944\n",
      "    sample_time_ms: 25785.324\n",
      "    update_time_ms: 3.043\n",
      "  timestamp: 1639176084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1238760\n",
      "  training_iteration: 155\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         17860.2</td><td style=\"text-align: right;\">1238760</td><td style=\"text-align: right;\">0.169212</td><td style=\"text-align: right;\">             1.28604</td><td style=\"text-align: right;\">            -1.03606</td><td style=\"text-align: right;\">           55.3958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 4987008\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8036225830431781\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02538735183343171\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4724510459759658\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 59.18840579710145\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4213061832660698\n",
      "  episode_reward_mean: 0.15480895456993088\n",
      "  episode_reward_min: -1.466087649057405\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 15731\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8688217341899872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016591178696602583\n",
      "          policy_loss: -0.09926604193076491\n",
      "          total_loss: 0.17157972579449415\n",
      "          vf_explained_var: 0.541397750377655\n",
      "          vf_loss: 0.25404720014333726\n",
      "    num_agent_steps_sampled: 4987008\n",
      "    num_agent_steps_trained: 4987008\n",
      "    num_steps_sampled: 1246752\n",
      "    num_steps_trained: 1246752\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.47152777777778\n",
      "    gpu_util_percent0: 0.13659722222222223\n",
      "    ram_util_percent: 95.79722222222219\n",
      "    vram_util_percent0: 0.5188407188037001\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.12909048315045\n",
      "  policy_reward_mean:\n",
      "    main: 0.038702238642482734\n",
      "  policy_reward_min:\n",
      "    main: -1.5454030789773245\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.304223039171136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.995444345474425\n",
      "    mean_inference_ms: 4.425327959121574\n",
      "    mean_raw_obs_processing_ms: 1.2752926621437475\n",
      "  time_since_restore: 17971.399927854538\n",
      "  time_this_iter_s: 111.18933510780334\n",
      "  time_total_s: 17971.399927854538\n",
      "  timers:\n",
      "    learn_throughput: 93.276\n",
      "    learn_time_ms: 85681.109\n",
      "    sample_throughput: 311.68\n",
      "    sample_time_ms: 25641.686\n",
      "    update_time_ms: 3.047\n",
      "  timestamp: 1639176195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1246752\n",
      "  training_iteration: 156\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         17971.4</td><td style=\"text-align: right;\">1246752</td><td style=\"text-align: right;\">0.154809</td><td style=\"text-align: right;\">             1.42131</td><td style=\"text-align: right;\">            -1.46609</td><td style=\"text-align: right;\">           59.1884</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5018976\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9031874942054521\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0674535737204779\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5119345852014928\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 59.11450381679389\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4003808601398444\n",
      "  episode_reward_mean: 0.16546999750519478\n",
      "  episode_reward_min: -0.7691723643536852\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 15862\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8594609267711639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016797366254031658\n",
      "          policy_loss: -0.10130309817939996\n",
      "          total_loss: 0.1879411269426346\n",
      "          vf_explained_var: 0.49881041049957275\n",
      "          vf_loss: 0.2722368904352188\n",
      "    num_agent_steps_sampled: 5018976\n",
      "    num_agent_steps_trained: 5018976\n",
      "    num_steps_sampled: 1254744\n",
      "    num_steps_trained: 1254744\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.22430555555555\n",
      "    gpu_util_percent0: 0.1375\n",
      "    ram_util_percent: 95.90069444444443\n",
      "    vram_util_percent0: 0.5140042868633689\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.872131935797912\n",
      "  policy_reward_mean:\n",
      "    main: 0.041367499376298696\n",
      "  policy_reward_min:\n",
      "    main: -1.6005886489491923\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3044910988693113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.00095033035007\n",
      "    mean_inference_ms: 4.428274010133635\n",
      "    mean_raw_obs_processing_ms: 1.2759061094288775\n",
      "  time_since_restore: 18082.647538661957\n",
      "  time_this_iter_s: 111.24761080741882\n",
      "  time_total_s: 18082.647538661957\n",
      "  timers:\n",
      "    learn_throughput: 93.31\n",
      "    learn_time_ms: 85649.642\n",
      "    sample_throughput: 311.434\n",
      "    sample_time_ms: 25661.945\n",
      "    update_time_ms: 3.056\n",
      "  timestamp: 1639176306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1254744\n",
      "  training_iteration: 157\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         18082.6</td><td style=\"text-align: right;\">1254744</td><td style=\"text-align: right;\"> 0.16547</td><td style=\"text-align: right;\">             1.40038</td><td style=\"text-align: right;\">           -0.769172</td><td style=\"text-align: right;\">           59.1145</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5050944\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7426507556515939\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03436435545595616\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6037049105406828\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 54.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2427330722646253\n",
      "  episode_reward_mean: 0.03524889203605812\n",
      "  episode_reward_min: -1.2857796985244194\n",
      "  episodes_this_iter: 145\n",
      "  episodes_total: 16007\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8446917841434479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016978310141712426\n",
      "          policy_loss: -0.10123194888234138\n",
      "          total_loss: 0.17014335802942515\n",
      "          vf_explained_var: 0.5438921451568604\n",
      "          vf_loss: 0.25418476831912995\n",
      "    num_agent_steps_sampled: 5050944\n",
      "    num_agent_steps_trained: 5050944\n",
      "    num_steps_sampled: 1262736\n",
      "    num_steps_trained: 1262736\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.47905405405405\n",
      "    gpu_util_percent0: 0.15074324324324326\n",
      "    ram_util_percent: 95.7858108108108\n",
      "    vram_util_percent0: 0.5189662762466316\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7561856156362974\n",
      "  policy_reward_mean:\n",
      "    main: 0.008812223009014526\n",
      "  policy_reward_min:\n",
      "    main: -1.757081687110792\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3044727085259009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.995094981193756\n",
      "    mean_inference_ms: 4.425940728358123\n",
      "    mean_raw_obs_processing_ms: 1.2766364770301275\n",
      "  time_since_restore: 18198.519238948822\n",
      "  time_this_iter_s: 115.87170028686523\n",
      "  time_total_s: 18198.519238948822\n",
      "  timers:\n",
      "    learn_throughput: 92.906\n",
      "    learn_time_ms: 86022.828\n",
      "    sample_throughput: 310.912\n",
      "    sample_time_ms: 25705.025\n",
      "    update_time_ms: 7.231\n",
      "  timestamp: 1639176422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1262736\n",
      "  training_iteration: 158\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         18198.5</td><td style=\"text-align: right;\">1262736</td><td style=\"text-align: right;\">0.0352489</td><td style=\"text-align: right;\">             1.24273</td><td style=\"text-align: right;\">            -1.28578</td><td style=\"text-align: right;\">              54.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5082912\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9806477671183849\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.059576617487571884\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5860791685306435\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-48-55\n",
      "  done: false\n",
      "  episode_len_mean: 57.824817518248175\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0123852942679519\n",
      "  episode_reward_mean: 0.1427345213171106\n",
      "  episode_reward_min: -0.985519171862179\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 16144\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8630993456840516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016557932648807765\n",
      "          policy_loss: -0.09783473094552755\n",
      "          total_loss: 0.15267223476618527\n",
      "          vf_explained_var: 0.5585895776748657\n",
      "          vf_loss: 0.23374205857515334\n",
      "    num_agent_steps_sampled: 5082912\n",
      "    num_agent_steps_trained: 5082912\n",
      "    num_steps_sampled: 1270728\n",
      "    num_steps_trained: 1270728\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.341379310344827\n",
      "    gpu_util_percent0: 0.1376551724137931\n",
      "    ram_util_percent: 95.27793103448276\n",
      "    vram_util_percent0: 0.5225595987790902\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.967317382777337\n",
      "  policy_reward_mean:\n",
      "    main: 0.03568363032927765\n",
      "  policy_reward_min:\n",
      "    main: -1.8984548155400085\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3046669055724685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.996183662720078\n",
      "    mean_inference_ms: 4.426936417778515\n",
      "    mean_raw_obs_processing_ms: 1.277462448605029\n",
      "  time_since_restore: 18311.002639770508\n",
      "  time_this_iter_s: 112.48340082168579\n",
      "  time_total_s: 18311.002639770508\n",
      "  timers:\n",
      "    learn_throughput: 92.833\n",
      "    learn_time_ms: 86089.676\n",
      "    sample_throughput: 310.601\n",
      "    sample_time_ms: 25730.76\n",
      "    update_time_ms: 7.299\n",
      "  timestamp: 1639176535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1270728\n",
      "  training_iteration: 159\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">           18311</td><td style=\"text-align: right;\">1270728</td><td style=\"text-align: right;\">0.142735</td><td style=\"text-align: right;\">             1.01239</td><td style=\"text-align: right;\">           -0.985519</td><td style=\"text-align: right;\">           57.8248</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5114880\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8903637492240831\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08298975182181163\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6469455526250748\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 54.49333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3797029306082775\n",
      "  episode_reward_mean: 0.1367204162835511\n",
      "  episode_reward_min: -1.3875203429595253\n",
      "  episodes_this_iter: 150\n",
      "  episodes_total: 16294\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8517120635509491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01713658683001995\n",
      "          policy_loss: -0.10227576614171266\n",
      "          total_loss: 0.19693316782265902\n",
      "          vf_explained_var: 0.5290289521217346\n",
      "          vf_loss: 0.2818581389188766\n",
      "    num_agent_steps_sampled: 5114880\n",
      "    num_agent_steps_trained: 5114880\n",
      "    num_steps_sampled: 1278720\n",
      "    num_steps_trained: 1278720\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.25931034482759\n",
      "    gpu_util_percent0: 0.13689655172413795\n",
      "    ram_util_percent: 95.23793103448276\n",
      "    vram_util_percent0: 0.5219241810486662\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8581776322781232\n",
      "  policy_reward_mean:\n",
      "    main: 0.03418010407088778\n",
      "  policy_reward_min:\n",
      "    main: -1.797386704972566\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30438930413459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.987081615484293\n",
      "    mean_inference_ms: 4.424497112991411\n",
      "    mean_raw_obs_processing_ms: 1.277894048414881\n",
      "  time_since_restore: 18423.72056531906\n",
      "  time_this_iter_s: 112.71792554855347\n",
      "  time_total_s: 18423.72056531906\n",
      "  timers:\n",
      "    learn_throughput: 92.726\n",
      "    learn_time_ms: 86189.597\n",
      "    sample_throughput: 310.209\n",
      "    sample_time_ms: 25763.309\n",
      "    update_time_ms: 7.071\n",
      "  timestamp: 1639176648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1278720\n",
      "  training_iteration: 160\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         18423.7</td><td style=\"text-align: right;\">1278720</td><td style=\"text-align: right;\"> 0.13672</td><td style=\"text-align: right;\">              1.3797</td><td style=\"text-align: right;\">            -1.38752</td><td style=\"text-align: right;\">           54.4933</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5146848\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7171116265926205\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06111368749769933\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47244989007317023\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 54.76027397260274\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.573248154252473\n",
      "  episode_reward_mean: 0.11444439455407233\n",
      "  episode_reward_min: -1.0998068947397466\n",
      "  episodes_this_iter: 146\n",
      "  episodes_total: 16440\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8444637024402618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017171085134148598\n",
      "          policy_loss: -0.09895912655442952\n",
      "          total_loss: 0.17643577630259097\n",
      "          vf_explained_var: 0.5450409054756165\n",
      "          vf_loss: 0.2580091791152954\n",
      "    num_agent_steps_sampled: 5146848\n",
      "    num_agent_steps_trained: 5146848\n",
      "    num_steps_sampled: 1286712\n",
      "    num_steps_trained: 1286712\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.245517241379307\n",
      "    gpu_util_percent0: 0.136551724137931\n",
      "    ram_util_percent: 95.14068965517242\n",
      "    vram_util_percent0: 0.5219116996289614\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8788763437881348\n",
      "  policy_reward_mean:\n",
      "    main: 0.028611098638518077\n",
      "  policy_reward_min:\n",
      "    main: -1.6095679692129545\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30376664218139265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.97120957036912\n",
      "    mean_inference_ms: 4.420441120393662\n",
      "    mean_raw_obs_processing_ms: 1.2781123892570183\n",
      "  time_since_restore: 18535.214262485504\n",
      "  time_this_iter_s: 111.49369716644287\n",
      "  time_total_s: 18535.214262485504\n",
      "  timers:\n",
      "    learn_throughput: 93.077\n",
      "    learn_time_ms: 85864.254\n",
      "    sample_throughput: 310.211\n",
      "    sample_time_ms: 25763.124\n",
      "    update_time_ms: 7.075\n",
      "  timestamp: 1639176759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1286712\n",
      "  training_iteration: 161\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         18535.2</td><td style=\"text-align: right;\">1286712</td><td style=\"text-align: right;\">0.114444</td><td style=\"text-align: right;\">             1.57325</td><td style=\"text-align: right;\">            -1.09981</td><td style=\"text-align: right;\">           54.7603</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5178816\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8293257232635123\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07149004845258028\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6422122498349627\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 60.68181818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7156038099487785\n",
      "  episode_reward_mean: 0.1262924359712171\n",
      "  episode_reward_min: -0.8883336026360189\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 16572\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8562452390193939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01717487769201398\n",
      "          policy_loss: -0.10057557336241006\n",
      "          total_loss: 0.15777792293578385\n",
      "          vf_explained_var: 0.5434330105781555\n",
      "          vf_loss: 0.2409639301300049\n",
      "    num_agent_steps_sampled: 5178816\n",
      "    num_agent_steps_trained: 5178816\n",
      "    num_steps_sampled: 1294704\n",
      "    num_steps_trained: 1294704\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.549655172413793\n",
      "    gpu_util_percent0: 0.13717241379310346\n",
      "    ram_util_percent: 95.14137931034486\n",
      "    vram_util_percent0: 0.5179494162099602\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.045773290723192\n",
      "  policy_reward_mean:\n",
      "    main: 0.03157310899280427\n",
      "  policy_reward_min:\n",
      "    main: -1.6512087531547563\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3038551372675759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.966247830704138\n",
      "    mean_inference_ms: 4.417878028273937\n",
      "    mean_raw_obs_processing_ms: 1.2785667055709409\n",
      "  time_since_restore: 18647.5178565979\n",
      "  time_this_iter_s: 112.30359411239624\n",
      "  time_total_s: 18647.5178565979\n",
      "  timers:\n",
      "    learn_throughput: 92.988\n",
      "    learn_time_ms: 85946.618\n",
      "    sample_throughput: 309.908\n",
      "    sample_time_ms: 25788.285\n",
      "    update_time_ms: 7.097\n",
      "  timestamp: 1639176872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1294704\n",
      "  training_iteration: 162\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         18647.5</td><td style=\"text-align: right;\">1294704</td><td style=\"text-align: right;\">0.126292</td><td style=\"text-align: right;\">              1.7156</td><td style=\"text-align: right;\">           -0.888334</td><td style=\"text-align: right;\">           60.6818</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5210784\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8541814520161459\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0017585422360785587\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.0156056773014783\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 65.83471074380165\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2147113680095223\n",
      "  episode_reward_mean: 0.03978979454130755\n",
      "  episode_reward_min: -2.3089509724564277\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 16693\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8480091083049774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01715977280214429\n",
      "          policy_loss: -0.10201707467436791\n",
      "          total_loss: 0.1628446117192507\n",
      "          vf_explained_var: 0.511269748210907\n",
      "          vf_loss: 0.24748741489648818\n",
      "    num_agent_steps_sampled: 5210784\n",
      "    num_agent_steps_trained: 5210784\n",
      "    num_steps_sampled: 1302696\n",
      "    num_steps_trained: 1302696\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.695833333333333\n",
      "    gpu_util_percent0: 0.13791666666666666\n",
      "    ram_util_percent: 95.35138888888889\n",
      "    vram_util_percent0: 0.516287110160506\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8739185381278263\n",
      "  policy_reward_mean:\n",
      "    main: 0.00994744863532689\n",
      "  policy_reward_min:\n",
      "    main: -2.0280798905789412\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303821008687471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.961325426021396\n",
      "    mean_inference_ms: 4.417062194772257\n",
      "    mean_raw_obs_processing_ms: 1.2786309266427702\n",
      "  time_since_restore: 18759.197911024094\n",
      "  time_this_iter_s: 111.68005442619324\n",
      "  time_total_s: 18759.197911024094\n",
      "  timers:\n",
      "    learn_throughput: 92.929\n",
      "    learn_time_ms: 86001.322\n",
      "    sample_throughput: 310.361\n",
      "    sample_time_ms: 25750.694\n",
      "    update_time_ms: 7.105\n",
      "  timestamp: 1639176984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1302696\n",
      "  training_iteration: 163\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         18759.2</td><td style=\"text-align: right;\">1302696</td><td style=\"text-align: right;\">0.0397898</td><td style=\"text-align: right;\">             1.21471</td><td style=\"text-align: right;\">            -2.30895</td><td style=\"text-align: right;\">           65.8347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5242752\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.857871173449933\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06685022291256414\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5242668143747277\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_19-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 57.542857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9948157146671623\n",
      "  episode_reward_mean: 0.14547994148077506\n",
      "  episode_reward_min: -0.8971631653932726\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 16833\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8367165517807007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017432905454188585\n",
      "          policy_loss: -0.10135215226188302\n",
      "          total_loss: 0.15237403625994922\n",
      "          vf_explained_var: 0.5643172860145569\n",
      "          vf_loss: 0.23607537245750426\n",
      "    num_agent_steps_sampled: 5242752\n",
      "    num_agent_steps_trained: 5242752\n",
      "    num_steps_sampled: 1310688\n",
      "    num_steps_trained: 1310688\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.457342657342654\n",
      "    gpu_util_percent0: 0.14013986013986013\n",
      "    ram_util_percent: 95.18671328671331\n",
      "    vram_util_percent0: 0.5165045550040613\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.07494515202622\n",
      "  policy_reward_mean:\n",
      "    main: 0.03636998537019377\n",
      "  policy_reward_min:\n",
      "    main: -1.5849517822073937\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303898646334687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.962570172000447\n",
      "    mean_inference_ms: 4.419011622426619\n",
      "    mean_raw_obs_processing_ms: 1.2789968718642972\n",
      "  time_since_restore: 18870.287612199783\n",
      "  time_this_iter_s: 111.0897011756897\n",
      "  time_total_s: 18870.287612199783\n",
      "  timers:\n",
      "    learn_throughput: 92.959\n",
      "    learn_time_ms: 85973.385\n",
      "    sample_throughput: 309.857\n",
      "    sample_time_ms: 25792.52\n",
      "    update_time_ms: 7.067\n",
      "  timestamp: 1639177095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1310688\n",
      "  training_iteration: 164\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         18870.3</td><td style=\"text-align: right;\">1310688</td><td style=\"text-align: right;\"> 0.14548</td><td style=\"text-align: right;\">             1.99482</td><td style=\"text-align: right;\">           -0.897163</td><td style=\"text-align: right;\">           57.5429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5274720\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8744007701587533\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04033572250668777\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.510013008665352\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-00-06\n",
      "  done: false\n",
      "  episode_len_mean: 55.30821917808219\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4643850587082232\n",
      "  episode_reward_mean: 0.13203669778482593\n",
      "  episode_reward_min: -0.9142489080212146\n",
      "  episodes_this_iter: 146\n",
      "  episodes_total: 16979\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8240803439617157\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017470778971910475\n",
      "          policy_loss: -0.10189924224093556\n",
      "          total_loss: 0.18667796640843154\n",
      "          vf_explained_var: 0.5405367612838745\n",
      "          vf_loss: 0.27088804310560227\n",
      "    num_agent_steps_sampled: 5274720\n",
      "    num_agent_steps_trained: 5274720\n",
      "    num_steps_sampled: 1318680\n",
      "    num_steps_trained: 1318680\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.698620689655172\n",
      "    gpu_util_percent0: 0.1386896551724138\n",
      "    ram_util_percent: 95.24000000000002\n",
      "    vram_util_percent0: 0.5164414337747218\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8853486900961076\n",
      "  policy_reward_mean:\n",
      "    main: 0.0330091744462065\n",
      "  policy_reward_min:\n",
      "    main: -1.6383591715495285\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30410250842527264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.962031469983405\n",
      "    mean_inference_ms: 4.4191541102943095\n",
      "    mean_raw_obs_processing_ms: 1.2796467629348016\n",
      "  time_since_restore: 18981.65108346939\n",
      "  time_this_iter_s: 111.36347126960754\n",
      "  time_total_s: 18981.65108346939\n",
      "  timers:\n",
      "    learn_throughput: 92.942\n",
      "    learn_time_ms: 85989.172\n",
      "    sample_throughput: 309.942\n",
      "    sample_time_ms: 25785.491\n",
      "    update_time_ms: 7.068\n",
      "  timestamp: 1639177206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1318680\n",
      "  training_iteration: 165\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         18981.7</td><td style=\"text-align: right;\">1318680</td><td style=\"text-align: right;\">0.132037</td><td style=\"text-align: right;\">             1.46439</td><td style=\"text-align: right;\">           -0.914249</td><td style=\"text-align: right;\">           55.3082</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5306688\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8717087202323531\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08069654295227477\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47923731364327465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-01-58\n",
      "  done: false\n",
      "  episode_len_mean: 50.32911392405063\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.360030804633472\n",
      "  episode_reward_mean: 0.14409828596039254\n",
      "  episode_reward_min: -0.8774282409992502\n",
      "  episodes_this_iter: 158\n",
      "  episodes_total: 17137\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8229398393630981\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017094357185065745\n",
      "          policy_loss: -0.10025865134969354\n",
      "          total_loss: 0.19542839789390565\n",
      "          vf_explained_var: 0.5482182502746582\n",
      "          vf_loss: 0.2783790096640587\n",
      "    num_agent_steps_sampled: 5306688\n",
      "    num_agent_steps_trained: 5306688\n",
      "    num_steps_sampled: 1326672\n",
      "    num_steps_trained: 1326672\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.561379310344826\n",
      "    gpu_util_percent0: 0.13806896551724135\n",
      "    ram_util_percent: 95.25379310344827\n",
      "    vram_util_percent0: 0.5163291009973789\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.934488943749617\n",
      "  policy_reward_mean:\n",
      "    main: 0.03602457149009814\n",
      "  policy_reward_min:\n",
      "    main: -1.8075939297872663\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3038351308644128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.956150942719788\n",
      "    mean_inference_ms: 4.418404185600209\n",
      "    mean_raw_obs_processing_ms: 1.2803028436790806\n",
      "  time_since_restore: 19093.419810533524\n",
      "  time_this_iter_s: 111.76872706413269\n",
      "  time_total_s: 19093.419810533524\n",
      "  timers:\n",
      "    learn_throughput: 92.932\n",
      "    learn_time_ms: 85997.971\n",
      "    sample_throughput: 309.348\n",
      "    sample_time_ms: 25834.955\n",
      "    update_time_ms: 7.076\n",
      "  timestamp: 1639177318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1326672\n",
      "  training_iteration: 166\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         19093.4</td><td style=\"text-align: right;\">1326672</td><td style=\"text-align: right;\">0.144098</td><td style=\"text-align: right;\">             1.36003</td><td style=\"text-align: right;\">           -0.877428</td><td style=\"text-align: right;\">           50.3291</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5338656\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8685607464442053\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0332756011055099\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5700172631522802\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 56.345588235294116\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0773754310064811\n",
      "  episode_reward_mean: 0.11063890185094011\n",
      "  episode_reward_min: -1.0661185289082953\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 17273\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8289266309738159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01693256187066436\n",
      "          policy_loss: -0.09836765882372857\n",
      "          total_loss: 0.15175857027620077\n",
      "          vf_explained_var: 0.5722470879554749\n",
      "          vf_loss: 0.2329820092320442\n",
      "    num_agent_steps_sampled: 5338656\n",
      "    num_agent_steps_trained: 5338656\n",
      "    num_steps_sampled: 1334664\n",
      "    num_steps_trained: 1334664\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.452777777777776\n",
      "    gpu_util_percent0: 0.139375\n",
      "    ram_util_percent: 95.28819444444443\n",
      "    vram_util_percent0: 0.5160357482358964\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7576008848226143\n",
      "  policy_reward_mean:\n",
      "    main: 0.027659725462735013\n",
      "  policy_reward_min:\n",
      "    main: -1.6147682373718384\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303347318233211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.934111815463304\n",
      "    mean_inference_ms: 4.410161229952559\n",
      "    mean_raw_obs_processing_ms: 1.280409814970231\n",
      "  time_since_restore: 19204.751850366592\n",
      "  time_this_iter_s: 111.33203983306885\n",
      "  time_total_s: 19204.751850366592\n",
      "  timers:\n",
      "    learn_throughput: 92.9\n",
      "    learn_time_ms: 86028.033\n",
      "    sample_throughput: 309.598\n",
      "    sample_time_ms: 25814.097\n",
      "    update_time_ms: 7.052\n",
      "  timestamp: 1639177429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1334664\n",
      "  training_iteration: 167\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         19204.8</td><td style=\"text-align: right;\">1334664</td><td style=\"text-align: right;\">0.110639</td><td style=\"text-align: right;\">             1.07738</td><td style=\"text-align: right;\">            -1.06612</td><td style=\"text-align: right;\">           56.3456</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5370624\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8017600449327462\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07452051726840832\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.56841471070543\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 65.5725806451613\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5978400123936691\n",
      "  episode_reward_mean: 0.1924972492111322\n",
      "  episode_reward_min: -1.4962325773256069\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 17397\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8237422738075256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017090829905122517\n",
      "          policy_loss: -0.10033555422350764\n",
      "          total_loss: 0.15624078736454247\n",
      "          vf_explained_var: 0.5347756147384644\n",
      "          vf_loss: 0.23927187544107437\n",
      "    num_agent_steps_sampled: 5370624\n",
      "    num_agent_steps_trained: 5370624\n",
      "    num_steps_sampled: 1342656\n",
      "    num_steps_trained: 1342656\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.671724137931033\n",
      "    gpu_util_percent0: 0.13882758620689656\n",
      "    ram_util_percent: 95.42965517241379\n",
      "    vram_util_percent0: 0.5160477017167625\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0483286117804473\n",
      "  policy_reward_mean:\n",
      "    main: 0.04812431230278305\n",
      "  policy_reward_min:\n",
      "    main: -1.6353101473608536\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30420625226466774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.95881821202409\n",
      "    mean_inference_ms: 4.42044848692\n",
      "    mean_raw_obs_processing_ms: 1.280708345939458\n",
      "  time_since_restore: 19316.900728225708\n",
      "  time_this_iter_s: 112.1488778591156\n",
      "  time_total_s: 19316.900728225708\n",
      "  timers:\n",
      "    learn_throughput: 93.236\n",
      "    learn_time_ms: 85717.795\n",
      "    sample_throughput: 310.195\n",
      "    sample_time_ms: 25764.42\n",
      "    update_time_ms: 2.899\n",
      "  timestamp: 1639177542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1342656\n",
      "  training_iteration: 168\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         19316.9</td><td style=\"text-align: right;\">1342656</td><td style=\"text-align: right;\">0.192497</td><td style=\"text-align: right;\">             1.59784</td><td style=\"text-align: right;\">            -1.49623</td><td style=\"text-align: right;\">           65.5726</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5402592\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9026335332943529\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07939377907636347\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.46658095078127904\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 57.26712328767123\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2954168914299595\n",
      "  episode_reward_mean: 0.19706694421315007\n",
      "  episode_reward_min: -0.9785034448621985\n",
      "  episodes_this_iter: 146\n",
      "  episodes_total: 17543\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8099859652519226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017239676371216775\n",
      "          policy_loss: -0.10108994440361857\n",
      "          total_loss: 0.17845903271809221\n",
      "          vf_explained_var: 0.5505052804946899\n",
      "          vf_loss: 0.2620938048362732\n",
      "    num_agent_steps_sampled: 5402592\n",
      "    num_agent_steps_trained: 5402592\n",
      "    num_steps_sampled: 1350648\n",
      "    num_steps_trained: 1350648\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.162068965517243\n",
      "    gpu_util_percent0: 0.13882758620689656\n",
      "    ram_util_percent: 95.56344827586207\n",
      "    vram_util_percent0: 0.5160374896460949\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7816335414005846\n",
      "  policy_reward_mean:\n",
      "    main: 0.049266736053287505\n",
      "  policy_reward_min:\n",
      "    main: -1.6089246426204977\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3036669353636569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.941292096155866\n",
      "    mean_inference_ms: 4.415182066545914\n",
      "    mean_raw_obs_processing_ms: 1.2809789324324456\n",
      "  time_since_restore: 19427.887496471405\n",
      "  time_this_iter_s: 110.98676824569702\n",
      "  time_total_s: 19427.887496471405\n",
      "  timers:\n",
      "    learn_throughput: 93.401\n",
      "    learn_time_ms: 85566.311\n",
      "    sample_throughput: 310.153\n",
      "    sample_time_ms: 25767.903\n",
      "    update_time_ms: 2.845\n",
      "  timestamp: 1639177653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1350648\n",
      "  training_iteration: 169\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         19427.9</td><td style=\"text-align: right;\">1350648</td><td style=\"text-align: right;\">0.197067</td><td style=\"text-align: right;\">             1.29542</td><td style=\"text-align: right;\">           -0.978503</td><td style=\"text-align: right;\">           57.2671</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5434560\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8927262098895217\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03405320177563298\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7849125811665506\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 56.75757575757576\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2029617330835136\n",
      "  episode_reward_mean: 0.13808772242005704\n",
      "  episode_reward_min: -0.8754280519473268\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 17675\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8138292002677917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01719358717650175\n",
      "          policy_loss: -0.10031001750007272\n",
      "          total_loss: 0.1657891147583723\n",
      "          vf_explained_var: 0.5380625128746033\n",
      "          vf_loss: 0.24869062495231628\n",
      "    num_agent_steps_sampled: 5434560\n",
      "    num_agent_steps_trained: 5434560\n",
      "    num_steps_sampled: 1358640\n",
      "    num_steps_trained: 1358640\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.556944444444444\n",
      "    gpu_util_percent0: 0.13895833333333335\n",
      "    ram_util_percent: 95.5736111111111\n",
      "    vram_util_percent0: 0.5160700248619795\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8393657689968728\n",
      "  policy_reward_mean:\n",
      "    main: 0.03452193060501425\n",
      "  policy_reward_min:\n",
      "    main: -1.7882462492868898\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039636817642606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.946183916514588\n",
      "    mean_inference_ms: 4.417296356607501\n",
      "    mean_raw_obs_processing_ms: 1.2814099080332884\n",
      "  time_since_restore: 19538.802397727966\n",
      "  time_this_iter_s: 110.91490125656128\n",
      "  time_total_s: 19538.802397727966\n",
      "  timers:\n",
      "    learn_throughput: 93.561\n",
      "    learn_time_ms: 85420.157\n",
      "    sample_throughput: 310.61\n",
      "    sample_time_ms: 25730.004\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1639177764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1358640\n",
      "  training_iteration: 170\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         19538.8</td><td style=\"text-align: right;\">1358640</td><td style=\"text-align: right;\">0.138088</td><td style=\"text-align: right;\">             1.20296</td><td style=\"text-align: right;\">           -0.875428</td><td style=\"text-align: right;\">           56.7576</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5466528\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8529094265858378\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07395070461838243\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5254388754200519\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 58.00714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1011927430805133\n",
      "  episode_reward_mean: 0.1492358405272794\n",
      "  episode_reward_min: -1.1518453465374932\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 17815\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8144886302947998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017439380832016467\n",
      "          policy_loss: -0.09966263268515467\n",
      "          total_loss: 0.1873746253401041\n",
      "          vf_explained_var: 0.5101271867752075\n",
      "          vf_loss: 0.2693798844218254\n",
      "    num_agent_steps_sampled: 5466528\n",
      "    num_agent_steps_trained: 5466528\n",
      "    num_steps_sampled: 1366632\n",
      "    num_steps_trained: 1366632\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.814583333333335\n",
      "    gpu_util_percent0: 0.1388888888888889\n",
      "    ram_util_percent: 95.62916666666668\n",
      "    vram_util_percent0: 0.5160825929582099\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.807351395880391\n",
      "  policy_reward_mean:\n",
      "    main: 0.03730896013181984\n",
      "  policy_reward_min:\n",
      "    main: -1.6415058317529778\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30382018488016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.937574817284883\n",
      "    mean_inference_ms: 4.414666782807505\n",
      "    mean_raw_obs_processing_ms: 1.281725288032731\n",
      "  time_since_restore: 19650.145570278168\n",
      "  time_this_iter_s: 111.34317255020142\n",
      "  time_total_s: 19650.145570278168\n",
      "  timers:\n",
      "    learn_throughput: 93.601\n",
      "    learn_time_ms: 85383.37\n",
      "    sample_throughput: 310.294\n",
      "    sample_time_ms: 25756.256\n",
      "    update_time_ms: 2.828\n",
      "  timestamp: 1639177875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1366632\n",
      "  training_iteration: 171\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         19650.1</td><td style=\"text-align: right;\">1366632</td><td style=\"text-align: right;\">0.149236</td><td style=\"text-align: right;\">             1.10119</td><td style=\"text-align: right;\">            -1.15185</td><td style=\"text-align: right;\">           58.0071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5498496\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9172637963620932\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07850888240763862\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6043353789065272\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-13-08\n",
      "  done: false\n",
      "  episode_len_mean: 61.57251908396947\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3774904435232536\n",
      "  episode_reward_mean: 0.1006359334657125\n",
      "  episode_reward_min: -1.2089025427913223\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 17946\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8069061629772186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017226243767887353\n",
      "          policy_loss: -0.09986038257926702\n",
      "          total_loss: 0.16941217812523246\n",
      "          vf_explained_var: 0.5260759592056274\n",
      "          vf_loss: 0.2518309876322746\n",
      "    num_agent_steps_sampled: 5498496\n",
      "    num_agent_steps_trained: 5498496\n",
      "    num_steps_sampled: 1374624\n",
      "    num_steps_trained: 1374624\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.72361111111111\n",
      "    gpu_util_percent0: 0.13534722222222223\n",
      "    ram_util_percent: 95.48263888888889\n",
      "    vram_util_percent0: 0.516116869584293\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9661595253974409\n",
      "  policy_reward_mean:\n",
      "    main: 0.025158983366428123\n",
      "  policy_reward_min:\n",
      "    main: -1.6104199809506121\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034185481277045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.929439405764423\n",
      "    mean_inference_ms: 4.410821558766588\n",
      "    mean_raw_obs_processing_ms: 1.2820143504068777\n",
      "  time_since_restore: 19763.216658830643\n",
      "  time_this_iter_s: 113.07108855247498\n",
      "  time_total_s: 19763.216658830643\n",
      "  timers:\n",
      "    learn_throughput: 93.593\n",
      "    learn_time_ms: 85391.353\n",
      "    sample_throughput: 309.486\n",
      "    sample_time_ms: 25823.427\n",
      "    update_time_ms: 7.252\n",
      "  timestamp: 1639177988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1374624\n",
      "  training_iteration: 172\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         19763.2</td><td style=\"text-align: right;\">1374624</td><td style=\"text-align: right;\">0.100636</td><td style=\"text-align: right;\">             1.37749</td><td style=\"text-align: right;\">             -1.2089</td><td style=\"text-align: right;\">           61.5725</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5530464\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8951935214157002\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06522271508306891\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7708162455848417\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 67.10924369747899\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.589766328365112\n",
      "  episode_reward_mean: 0.13401652559994673\n",
      "  episode_reward_min: -1.297737859190816\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 18065\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.8182702136039733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017301859498023986\n",
      "          policy_loss: -0.10061266807839274\n",
      "          total_loss: 0.16074176115542652\n",
      "          vf_explained_var: 0.5228478908538818\n",
      "          vf_loss: 0.24383629524707795\n",
      "    num_agent_steps_sampled: 5530464\n",
      "    num_agent_steps_trained: 5530464\n",
      "    num_steps_sampled: 1382616\n",
      "    num_steps_trained: 1382616\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.560689655172414\n",
      "    gpu_util_percent0: 0.13641379310344828\n",
      "    ram_util_percent: 95.0048275862069\n",
      "    vram_util_percent0: 0.5163915080959027\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7753764729698318\n",
      "  policy_reward_mean:\n",
      "    main: 0.03350413139998668\n",
      "  policy_reward_min:\n",
      "    main: -1.7708162455848417\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30402523264619585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.94374618166781\n",
      "    mean_inference_ms: 4.4161236900080185\n",
      "    mean_raw_obs_processing_ms: 1.2824315174749386\n",
      "  time_since_restore: 19874.926504850388\n",
      "  time_this_iter_s: 111.70984601974487\n",
      "  time_total_s: 19874.926504850388\n",
      "  timers:\n",
      "    learn_throughput: 93.597\n",
      "    learn_time_ms: 85387.255\n",
      "    sample_throughput: 309.398\n",
      "    sample_time_ms: 25830.844\n",
      "    update_time_ms: 7.22\n",
      "  timestamp: 1639178100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1382616\n",
      "  training_iteration: 173\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         19874.9</td><td style=\"text-align: right;\">1382616</td><td style=\"text-align: right;\">0.134017</td><td style=\"text-align: right;\">             1.58977</td><td style=\"text-align: right;\">            -1.29774</td><td style=\"text-align: right;\">           67.1092</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5562432\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.78444337671104\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07698065877666459\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5315344301003776\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 55.95035460992908\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.262085927868037\n",
      "  episode_reward_mean: 0.1678171716420447\n",
      "  episode_reward_min: -0.9914696678822414\n",
      "  episodes_this_iter: 141\n",
      "  episodes_total: 18206\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.790758819103241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01753383397683501\n",
      "          policy_loss: -0.10123103719949722\n",
      "          total_loss: 0.18376468355208636\n",
      "          vf_explained_var: 0.5160617828369141\n",
      "          vf_loss: 0.26724271523952486\n",
      "    num_agent_steps_sampled: 5562432\n",
      "    num_agent_steps_trained: 5562432\n",
      "    num_steps_sampled: 1390608\n",
      "    num_steps_trained: 1390608\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.775172413793108\n",
      "    gpu_util_percent0: 0.13662068965517243\n",
      "    ram_util_percent: 95.07379310344827\n",
      "    vram_util_percent0: 0.5163869693978281\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.699788736215977\n",
      "  policy_reward_mean:\n",
      "    main: 0.04195429291051119\n",
      "  policy_reward_min:\n",
      "    main: -1.685046244283952\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031748356199984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.918753856836805\n",
      "    mean_inference_ms: 4.408025804086245\n",
      "    mean_raw_obs_processing_ms: 1.282437012567641\n",
      "  time_since_restore: 19987.060367822647\n",
      "  time_this_iter_s: 112.13386297225952\n",
      "  time_total_s: 19987.060367822647\n",
      "  timers:\n",
      "    learn_throughput: 93.489\n",
      "    learn_time_ms: 85485.624\n",
      "    sample_throughput: 309.404\n",
      "    sample_time_ms: 25830.286\n",
      "    update_time_ms: 7.248\n",
      "  timestamp: 1639178212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1390608\n",
      "  training_iteration: 174\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         19987.1</td><td style=\"text-align: right;\">1390608</td><td style=\"text-align: right;\">0.167817</td><td style=\"text-align: right;\">             1.26209</td><td style=\"text-align: right;\">            -0.99147</td><td style=\"text-align: right;\">           55.9504</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5594400\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7671681597233952\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06486198454504179\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6528891397803178\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-18-44\n",
      "  done: false\n",
      "  episode_len_mean: 62.98412698412698\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2743416883349705\n",
      "  episode_reward_mean: 0.15368975307571822\n",
      "  episode_reward_min: -2.0424461330587302\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 18332\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.80481755900383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01763871802017093\n",
      "          policy_loss: -0.10188342179358005\n",
      "          total_loss: 0.18311842362582684\n",
      "          vf_explained_var: 0.5041447281837463\n",
      "          vf_loss: 0.26714264249801634\n",
      "    num_agent_steps_sampled: 5594400\n",
      "    num_agent_steps_trained: 5594400\n",
      "    num_steps_sampled: 1398600\n",
      "    num_steps_trained: 1398600\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.439999999999994\n",
      "    gpu_util_percent0: 0.1362758620689655\n",
      "    ram_util_percent: 95.08000000000001\n",
      "    vram_util_percent0: 0.5163722186290863\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8020381789632816\n",
      "  policy_reward_mean:\n",
      "    main: 0.038422438268929555\n",
      "  policy_reward_min:\n",
      "    main: -1.8293127791152826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034801637502836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.923217612277522\n",
      "    mean_inference_ms: 4.409780203497711\n",
      "    mean_raw_obs_processing_ms: 1.2827677329885079\n",
      "  time_since_restore: 20098.632833719254\n",
      "  time_this_iter_s: 111.57246589660645\n",
      "  time_total_s: 20098.632833719254\n",
      "  timers:\n",
      "    learn_throughput: 93.462\n",
      "    learn_time_ms: 85510.417\n",
      "    sample_throughput: 309.427\n",
      "    sample_time_ms: 25828.365\n",
      "    update_time_ms: 7.239\n",
      "  timestamp: 1639178324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1398600\n",
      "  training_iteration: 175\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         20098.6</td><td style=\"text-align: right;\">1398600</td><td style=\"text-align: right;\"> 0.15369</td><td style=\"text-align: right;\">             1.27434</td><td style=\"text-align: right;\">            -2.04245</td><td style=\"text-align: right;\">           62.9841</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5626368\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.802097989051981\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.052899328116032596\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5825109603561813\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-20-36\n",
      "  done: false\n",
      "  episode_len_mean: 56.64335664335665\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.276798606287025\n",
      "  episode_reward_mean: 0.07514818153957963\n",
      "  episode_reward_min: -1.1687027364044478\n",
      "  episodes_this_iter: 143\n",
      "  episodes_total: 18475\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.792798355102539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017592971228063106\n",
      "          policy_loss: -0.10186482200026512\n",
      "          total_loss: 0.16196238860487938\n",
      "          vf_explained_var: 0.558351457118988\n",
      "          vf_loss: 0.24601432627439498\n",
      "    num_agent_steps_sampled: 5626368\n",
      "    num_agent_steps_trained: 5626368\n",
      "    num_steps_sampled: 1406592\n",
      "    num_steps_trained: 1406592\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.539583333333333\n",
      "    gpu_util_percent0: 0.13770833333333332\n",
      "    ram_util_percent: 95.07986111111111\n",
      "    vram_util_percent0: 0.5164390698694746\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.739271131875636\n",
      "  policy_reward_mean:\n",
      "    main: 0.0187870453848949\n",
      "  policy_reward_min:\n",
      "    main: -1.631101316685578\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034533315159317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.921593820573936\n",
      "    mean_inference_ms: 4.4103026170084405\n",
      "    mean_raw_obs_processing_ms: 1.283028310405285\n",
      "  time_since_restore: 20210.256551027298\n",
      "  time_this_iter_s: 111.62371730804443\n",
      "  time_total_s: 20210.256551027298\n",
      "  timers:\n",
      "    learn_throughput: 93.428\n",
      "    learn_time_ms: 85542.004\n",
      "    sample_throughput: 309.929\n",
      "    sample_time_ms: 25786.565\n",
      "    update_time_ms: 7.239\n",
      "  timestamp: 1639178436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1406592\n",
      "  training_iteration: 176\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         20210.3</td><td style=\"text-align: right;\">1406592</td><td style=\"text-align: right;\">0.0751482</td><td style=\"text-align: right;\">              1.2768</td><td style=\"text-align: right;\">             -1.1687</td><td style=\"text-align: right;\">           56.6434</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5658336\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6769325839532054\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08740855408279082\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5416101264333565\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-22-30\n",
      "  done: false\n",
      "  episode_len_mean: 54.72222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2450059774277\n",
      "  episode_reward_mean: 0.17462448728419616\n",
      "  episode_reward_min: -0.7704300175389982\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 18619\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7819313855171204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01778581215068698\n",
      "          policy_loss: -0.10233878352120519\n",
      "          total_loss: 0.17880229302123188\n",
      "          vf_explained_var: 0.5400975346565247\n",
      "          vf_loss: 0.26313294053077696\n",
      "    num_agent_steps_sampled: 5658336\n",
      "    num_agent_steps_trained: 5658336\n",
      "    num_steps_sampled: 1414584\n",
      "    num_steps_trained: 1414584\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.250684931506854\n",
      "    gpu_util_percent0: 0.1363013698630137\n",
      "    ram_util_percent: 95.04520547945205\n",
      "    vram_util_percent0: 0.5163885470617138\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7753010040772501\n",
      "  policy_reward_mean:\n",
      "    main: 0.04365612182104904\n",
      "  policy_reward_min:\n",
      "    main: -1.5448063066565338\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30361370486787753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.922701870309577\n",
      "    mean_inference_ms: 4.4111808369306305\n",
      "    mean_raw_obs_processing_ms: 1.283409609429034\n",
      "  time_since_restore: 20324.428565740585\n",
      "  time_this_iter_s: 114.17201471328735\n",
      "  time_total_s: 20324.428565740585\n",
      "  timers:\n",
      "    learn_throughput: 93.147\n",
      "    learn_time_ms: 85800.114\n",
      "    sample_throughput: 309.646\n",
      "    sample_time_ms: 25810.085\n",
      "    update_time_ms: 7.279\n",
      "  timestamp: 1639178550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1414584\n",
      "  training_iteration: 177\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         20324.4</td><td style=\"text-align: right;\">1414584</td><td style=\"text-align: right;\">0.174624</td><td style=\"text-align: right;\">             1.24501</td><td style=\"text-align: right;\">            -0.77043</td><td style=\"text-align: right;\">           54.7222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5690304\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.998993404743924\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05183723209063536\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5179988302341534\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-24-24\n",
      "  done: false\n",
      "  episode_len_mean: 50.9875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7447900890021542\n",
      "  episode_reward_mean: 0.10872469811177685\n",
      "  episode_reward_min: -0.9766649589514227\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 18779\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7766924085617065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017401099231094123\n",
      "          policy_loss: -0.10133665897324681\n",
      "          total_loss: 0.23444076039269565\n",
      "          vf_explained_var: 0.49759724736213684\n",
      "          vf_loss: 0.31815880686044695\n",
      "    num_agent_steps_sampled: 5690304\n",
      "    num_agent_steps_trained: 5690304\n",
      "    num_steps_sampled: 1422576\n",
      "    num_steps_trained: 1422576\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.47465753424657\n",
      "    gpu_util_percent0: 0.13643835616438357\n",
      "    ram_util_percent: 94.61986301369865\n",
      "    vram_util_percent0: 0.5164584150337845\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9586219611998086\n",
      "  policy_reward_mean:\n",
      "    main: 0.02718117452794421\n",
      "  policy_reward_min:\n",
      "    main: -1.5815108939898315\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034881518221347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.921384792313642\n",
      "    mean_inference_ms: 4.409537008725182\n",
      "    mean_raw_obs_processing_ms: 1.2845468081281948\n",
      "  time_since_restore: 20437.998529195786\n",
      "  time_this_iter_s: 113.5699634552002\n",
      "  time_total_s: 20437.998529195786\n",
      "  timers:\n",
      "    learn_throughput: 93.124\n",
      "    learn_time_ms: 85821.208\n",
      "    sample_throughput: 308.21\n",
      "    sample_time_ms: 25930.352\n",
      "    update_time_ms: 7.313\n",
      "  timestamp: 1639178664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1422576\n",
      "  training_iteration: 178\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">           20438</td><td style=\"text-align: right;\">1422576</td><td style=\"text-align: right;\">0.108725</td><td style=\"text-align: right;\">             1.74479</td><td style=\"text-align: right;\">           -0.976665</td><td style=\"text-align: right;\">           50.9875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5722272\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9578919456682237\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06271361594321646\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5597180815826523\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 62.80916030534351\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.381788048193195\n",
      "  episode_reward_mean: 0.14202748640147284\n",
      "  episode_reward_min: -1.1210059472011369\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 18910\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7863701648712158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017472800046205522\n",
      "          policy_loss: -0.10008784087747336\n",
      "          total_loss: 0.18841735815256835\n",
      "          vf_explained_var: 0.5221047401428223\n",
      "          vf_loss: 0.2708139865398407\n",
      "    num_agent_steps_sampled: 5722272\n",
      "    num_agent_steps_trained: 5722272\n",
      "    num_steps_sampled: 1430568\n",
      "    num_steps_trained: 1430568\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.619178082191784\n",
      "    gpu_util_percent0: 0.13623287671232878\n",
      "    ram_util_percent: 94.69931506849316\n",
      "    vram_util_percent0: 0.5164370038810532\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.00164059268073\n",
      "  policy_reward_mean:\n",
      "    main: 0.035506871600368216\n",
      "  policy_reward_min:\n",
      "    main: -1.7676366174131033\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3038337722992378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.926226395180763\n",
      "    mean_inference_ms: 4.411159551932391\n",
      "    mean_raw_obs_processing_ms: 1.2848869138875811\n",
      "  time_since_restore: 20549.908303022385\n",
      "  time_this_iter_s: 111.90977382659912\n",
      "  time_total_s: 20549.908303022385\n",
      "  timers:\n",
      "    learn_throughput: 93.019\n",
      "    learn_time_ms: 85917.834\n",
      "    sample_throughput: 308.292\n",
      "    sample_time_ms: 25923.477\n",
      "    update_time_ms: 7.312\n",
      "  timestamp: 1639178776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1430568\n",
      "  training_iteration: 179\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         20549.9</td><td style=\"text-align: right;\">1430568</td><td style=\"text-align: right;\">0.142027</td><td style=\"text-align: right;\">             1.38179</td><td style=\"text-align: right;\">            -1.12101</td><td style=\"text-align: right;\">           62.8092</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5754240\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9041205273708695\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.010870257250657225\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5756150827004501\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 57.28787878787879\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5511126881267971\n",
      "  episode_reward_mean: 0.09300023645517097\n",
      "  episode_reward_min: -0.9297696668385487\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 19042\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7750306708812713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017693710193037986\n",
      "          policy_loss: -0.10161336655169725\n",
      "          total_loss: 0.15403016285225749\n",
      "          vf_explained_var: 0.5712679624557495\n",
      "          vf_loss: 0.23772864723205567\n",
      "    num_agent_steps_sampled: 5754240\n",
      "    num_agent_steps_trained: 5754240\n",
      "    num_steps_sampled: 1438560\n",
      "    num_steps_trained: 1438560\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.14625850340136\n",
      "    gpu_util_percent0: 0.15034013605442179\n",
      "    ram_util_percent: 94.64897959183675\n",
      "    vram_util_percent0: 0.5178574226663355\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.827018698080907\n",
      "  policy_reward_mean:\n",
      "    main: 0.023250059113792746\n",
      "  policy_reward_min:\n",
      "    main: -1.654912554498984\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030598576354765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.903904522613768\n",
      "    mean_inference_ms: 4.404049834693999\n",
      "    mean_raw_obs_processing_ms: 1.2848162748133225\n",
      "  time_since_restore: 20664.000993728638\n",
      "  time_this_iter_s: 114.09269070625305\n",
      "  time_total_s: 20664.000993728638\n",
      "  timers:\n",
      "    learn_throughput: 92.681\n",
      "    learn_time_ms: 86230.861\n",
      "    sample_throughput: 308.197\n",
      "    sample_time_ms: 25931.501\n",
      "    update_time_ms: 7.327\n",
      "  timestamp: 1639178890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1438560\n",
      "  training_iteration: 180\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">           20664</td><td style=\"text-align: right;\">1438560</td><td style=\"text-align: right;\">0.0930002</td><td style=\"text-align: right;\">             1.55111</td><td style=\"text-align: right;\">            -0.92977</td><td style=\"text-align: right;\">           57.2879</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5786208\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.71213447082528\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05250207958148847\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5134116537785349\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 62.32539682539682\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.809930193815199\n",
      "  episode_reward_mean: 0.16661837542677352\n",
      "  episode_reward_min: -0.8419936979482783\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 19168\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7834176707267761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017161347150802614\n",
      "          policy_loss: -0.099426389567554\n",
      "          total_loss: 0.1544751239977777\n",
      "          vf_explained_var: 0.5407161712646484\n",
      "          vf_loss: 0.23652564936876297\n",
      "    num_agent_steps_sampled: 5786208\n",
      "    num_agent_steps_trained: 5786208\n",
      "    num_steps_sampled: 1446552\n",
      "    num_steps_trained: 1446552\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.091275167785234\n",
      "    gpu_util_percent0: 0.17986577181208052\n",
      "    ram_util_percent: 94.44563758389263\n",
      "    vram_util_percent0: 0.5196450616261531\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7006856788211158\n",
      "  policy_reward_mean:\n",
      "    main: 0.0416545938566934\n",
      "  policy_reward_min:\n",
      "    main: -1.5739070455909743\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30358614964803393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.91378000959976\n",
      "    mean_inference_ms: 4.407279616601788\n",
      "    mean_raw_obs_processing_ms: 1.2852531651183254\n",
      "  time_since_restore: 20779.84952878952\n",
      "  time_this_iter_s: 115.84853506088257\n",
      "  time_total_s: 20779.84952878952\n",
      "  timers:\n",
      "    learn_throughput: 92.232\n",
      "    learn_time_ms: 86651.335\n",
      "    sample_throughput: 307.803\n",
      "    sample_time_ms: 25964.668\n",
      "    update_time_ms: 7.372\n",
      "  timestamp: 1639179006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1446552\n",
      "  training_iteration: 181\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         20779.8</td><td style=\"text-align: right;\">1446552</td><td style=\"text-align: right;\">0.166618</td><td style=\"text-align: right;\">             1.80993</td><td style=\"text-align: right;\">           -0.841994</td><td style=\"text-align: right;\">           62.3254</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5818176\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1028013220000499\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.050503641662947095\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.2485564040974926\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-32-00\n",
      "  done: false\n",
      "  episode_len_mean: 59.5514705882353\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5836905892478859\n",
      "  episode_reward_mean: 0.1315355492100619\n",
      "  episode_reward_min: -1.2742259506885727\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 19304\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7733894670009613\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0173940168581903\n",
      "          policy_loss: -0.10005453752353788\n",
      "          total_loss: 0.18663102922588587\n",
      "          vf_explained_var: 0.5066539645195007\n",
      "          vf_loss: 0.26907412415742876\n",
      "    num_agent_steps_sampled: 5818176\n",
      "    num_agent_steps_trained: 5818176\n",
      "    num_steps_sampled: 1454544\n",
      "    num_steps_trained: 1454544\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.797986577181206\n",
      "    gpu_util_percent0: 0.18100671140939598\n",
      "    ram_util_percent: 94.32751677852349\n",
      "    vram_util_percent0: 0.5194474074172226\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0056674016667904\n",
      "  policy_reward_mean:\n",
      "    main: 0.032883887302515465\n",
      "  policy_reward_min:\n",
      "    main: -2.2485564040974926\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3037973809031705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.921657227484232\n",
      "    mean_inference_ms: 4.411788955012343\n",
      "    mean_raw_obs_processing_ms: 1.2858388115481656\n",
      "  time_since_restore: 20894.037158489227\n",
      "  time_this_iter_s: 114.18762969970703\n",
      "  time_total_s: 20894.037158489227\n",
      "  timers:\n",
      "    learn_throughput: 92.067\n",
      "    learn_time_ms: 86806.488\n",
      "    sample_throughput: 308.322\n",
      "    sample_time_ms: 25920.932\n",
      "    update_time_ms: 2.946\n",
      "  timestamp: 1639179120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1454544\n",
      "  training_iteration: 182\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">           20894</td><td style=\"text-align: right;\">1454544</td><td style=\"text-align: right;\">0.131536</td><td style=\"text-align: right;\">             1.58369</td><td style=\"text-align: right;\">            -1.27423</td><td style=\"text-align: right;\">           59.5515</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5850144\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7966045586853954\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03657673164876356\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5837111436582192\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-33-54\n",
      "  done: false\n",
      "  episode_len_mean: 62.01538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2535031838830815\n",
      "  episode_reward_mean: 0.09069500322470489\n",
      "  episode_reward_min: -0.9008683044542145\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 19434\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7784501445293427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01763515243679285\n",
      "          policy_loss: -0.10140116424672306\n",
      "          total_loss: 0.1755263622943312\n",
      "          vf_explained_var: 0.5029710531234741\n",
      "          vf_loss: 0.25907193517684934\n",
      "    num_agent_steps_sampled: 5850144\n",
      "    num_agent_steps_trained: 5850144\n",
      "    num_steps_sampled: 1462536\n",
      "    num_steps_trained: 1462536\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.93918918918919\n",
      "    gpu_util_percent0: 0.18317567567567566\n",
      "    ram_util_percent: 94.4385135135135\n",
      "    vram_util_percent0: 0.5190018498261342\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7752240440704323\n",
      "  policy_reward_mean:\n",
      "    main: 0.022673750806176233\n",
      "  policy_reward_min:\n",
      "    main: -1.6430450962809515\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033711089494846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.904207958239354\n",
      "    mean_inference_ms: 4.405270610185529\n",
      "    mean_raw_obs_processing_ms: 1.2859297783031174\n",
      "  time_since_restore: 21008.21388745308\n",
      "  time_this_iter_s: 114.17672896385193\n",
      "  time_total_s: 21008.21388745308\n",
      "  timers:\n",
      "    learn_throughput: 91.837\n",
      "    learn_time_ms: 87023.858\n",
      "    sample_throughput: 307.977\n",
      "    sample_time_ms: 25949.97\n",
      "    update_time_ms: 2.959\n",
      "  timestamp: 1639179234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1462536\n",
      "  training_iteration: 183\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         21008.2</td><td style=\"text-align: right;\">1462536</td><td style=\"text-align: right;\">0.090695</td><td style=\"text-align: right;\">              1.2535</td><td style=\"text-align: right;\">           -0.900868</td><td style=\"text-align: right;\">           62.0154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5882112\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7610095177488527\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05045541321629701\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6187134034825432\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 56.156462585034014\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1460564862334546\n",
      "  episode_reward_mean: 0.12343462166836602\n",
      "  episode_reward_min: -1.15492860925539\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 19581\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7613628363609314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01764470301941037\n",
      "          policy_loss: -0.10183164931833744\n",
      "          total_loss: 0.16525436127185822\n",
      "          vf_explained_var: 0.5569560527801514\n",
      "          vf_loss: 0.24922074967622757\n",
      "    num_agent_steps_sampled: 5882112\n",
      "    num_agent_steps_trained: 5882112\n",
      "    num_steps_sampled: 1470528\n",
      "    num_steps_trained: 1470528\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.251020408163264\n",
      "    gpu_util_percent0: 0.18047619047619048\n",
      "    ram_util_percent: 94.59387755102041\n",
      "    vram_util_percent0: 0.5165322463305823\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8481410345465332\n",
      "  policy_reward_mean:\n",
      "    main: 0.030858655417091506\n",
      "  policy_reward_min:\n",
      "    main: -1.7211701743886758\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034523747913686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.908214405008867\n",
      "    mean_inference_ms: 4.407812800376586\n",
      "    mean_raw_obs_processing_ms: 1.2863524654877425\n",
      "  time_since_restore: 21122.034375667572\n",
      "  time_this_iter_s: 113.8204882144928\n",
      "  time_total_s: 21122.034375667572\n",
      "  timers:\n",
      "    learn_throughput: 91.658\n",
      "    learn_time_ms: 87193.561\n",
      "    sample_throughput: 307.907\n",
      "    sample_time_ms: 25955.915\n",
      "    update_time_ms: 2.939\n",
      "  timestamp: 1639179348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1470528\n",
      "  training_iteration: 184\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">           21122</td><td style=\"text-align: right;\">1470528</td><td style=\"text-align: right;\">0.123435</td><td style=\"text-align: right;\">             1.14606</td><td style=\"text-align: right;\">            -1.15493</td><td style=\"text-align: right;\">           56.1565</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5914080\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.37745088423514\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06553841307862725\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6131955710267321\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 56.54814814814815\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7869721730143804\n",
      "  episode_reward_mean: 0.19558471061856794\n",
      "  episode_reward_min: -1.0279280644315185\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 19716\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.767013388633728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017616443525999784\n",
      "          policy_loss: -0.10015288696065545\n",
      "          total_loss: 0.16573929719254374\n",
      "          vf_explained_var: 0.5472802519798279\n",
      "          vf_loss: 0.24805553513765335\n",
      "    num_agent_steps_sampled: 5914080\n",
      "    num_agent_steps_trained: 5914080\n",
      "    num_steps_sampled: 1478520\n",
      "    num_steps_trained: 1478520\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.092567567567567\n",
      "    gpu_util_percent0: 0.18168918918918917\n",
      "    ram_util_percent: 94.5054054054054\n",
      "    vram_util_percent0: 0.516153740117215\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1444509037854633\n",
      "  policy_reward_mean:\n",
      "    main: 0.04889617765464199\n",
      "  policy_reward_min:\n",
      "    main: -1.8276301332437161\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30276333700883684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.886457125823178\n",
      "    mean_inference_ms: 4.400812504567705\n",
      "    mean_raw_obs_processing_ms: 1.286471549274888\n",
      "  time_since_restore: 21236.279314994812\n",
      "  time_this_iter_s: 114.24493932723999\n",
      "  time_total_s: 21236.279314994812\n",
      "  timers:\n",
      "    learn_throughput: 91.41\n",
      "    learn_time_ms: 87430.678\n",
      "    sample_throughput: 307.506\n",
      "    sample_time_ms: 25989.767\n",
      "    update_time_ms: 2.945\n",
      "  timestamp: 1639179462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1478520\n",
      "  training_iteration: 185\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         21236.3</td><td style=\"text-align: right;\">1478520</td><td style=\"text-align: right;\">0.195585</td><td style=\"text-align: right;\">             1.78697</td><td style=\"text-align: right;\">            -1.02793</td><td style=\"text-align: right;\">           56.5481</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5946048\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.604241149793091\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06979996997622025\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4723669350454611\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 66.7063492063492\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.03369093764125\n",
      "  episode_reward_mean: 0.07810983021213741\n",
      "  episode_reward_min: -1.3243236727050256\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 19842\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7732017936706543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017912620719522238\n",
      "          policy_loss: -0.10216283508390188\n",
      "          total_loss: 0.16470085402019322\n",
      "          vf_explained_var: 0.5243101716041565\n",
      "          vf_loss: 0.24872715908288956\n",
      "    num_agent_steps_sampled: 5946048\n",
      "    num_agent_steps_trained: 5946048\n",
      "    num_steps_sampled: 1486512\n",
      "    num_steps_trained: 1486512\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.990540540540543\n",
      "    gpu_util_percent0: 0.18324324324324323\n",
      "    ram_util_percent: 94.5337837837838\n",
      "    vram_util_percent0: 0.5161437350479797\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9152208080547037\n",
      "  policy_reward_mean:\n",
      "    main: 0.019527457553034356\n",
      "  policy_reward_min:\n",
      "    main: -1.7295882099101403\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30351059064882285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.908076441232193\n",
      "    mean_inference_ms: 4.410122332915438\n",
      "    mean_raw_obs_processing_ms: 1.2868607897557818\n",
      "  time_since_restore: 21350.445415973663\n",
      "  time_this_iter_s: 114.16610097885132\n",
      "  time_total_s: 21350.445415973663\n",
      "  timers:\n",
      "    learn_throughput: 91.172\n",
      "    learn_time_ms: 87658.926\n",
      "    sample_throughput: 307.206\n",
      "    sample_time_ms: 26015.104\n",
      "    update_time_ms: 2.975\n",
      "  timestamp: 1639179576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1486512\n",
      "  training_iteration: 186\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         21350.4</td><td style=\"text-align: right;\">1486512</td><td style=\"text-align: right;\">0.0781098</td><td style=\"text-align: right;\">             1.03369</td><td style=\"text-align: right;\">            -1.32432</td><td style=\"text-align: right;\">           66.7063</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 5978016\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.007643729391921\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10179877636155234\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5080993498035522\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 60.83846153846154\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5590185456228076\n",
      "  episode_reward_mean: 0.16820253638124116\n",
      "  episode_reward_min: -0.927355331469573\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 19972\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7575282912254333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018026897810399532\n",
      "          policy_loss: -0.10215113596245647\n",
      "          total_loss: 0.16913301227986813\n",
      "          vf_explained_var: 0.5212662220001221\n",
      "          vf_loss: 0.2530319132804871\n",
      "    num_agent_steps_sampled: 5978016\n",
      "    num_agent_steps_trained: 5978016\n",
      "    num_steps_sampled: 1494504\n",
      "    num_steps_trained: 1494504\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.491946308724827\n",
      "    gpu_util_percent0: 0.17926174496644295\n",
      "    ram_util_percent: 94.60335570469799\n",
      "    vram_util_percent0: 0.5160243456983156\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9540279263759017\n",
      "  policy_reward_mean:\n",
      "    main: 0.04205063409531029\n",
      "  policy_reward_min:\n",
      "    main: -1.8929789612562222\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3035742289238675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.90868340134767\n",
      "    mean_inference_ms: 4.410215229599423\n",
      "    mean_raw_obs_processing_ms: 1.2873909187988506\n",
      "  time_since_restore: 21465.470903396606\n",
      "  time_this_iter_s: 115.02548742294312\n",
      "  time_total_s: 21465.470903396606\n",
      "  timers:\n",
      "    learn_throughput: 91.185\n",
      "    learn_time_ms: 87646.293\n",
      "    sample_throughput: 306.066\n",
      "    sample_time_ms: 26112.028\n",
      "    update_time_ms: 3.056\n",
      "  timestamp: 1639179692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1494504\n",
      "  training_iteration: 187\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         21465.5</td><td style=\"text-align: right;\">1494504</td><td style=\"text-align: right;\">0.168203</td><td style=\"text-align: right;\">             1.55902</td><td style=\"text-align: right;\">           -0.927355</td><td style=\"text-align: right;\">           60.8385</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6009984\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8395253889071226\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02533145142479455\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6301950323717154\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-43-26\n",
      "  done: false\n",
      "  episode_len_mean: 60.20610687022901\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.206384224521177\n",
      "  episode_reward_mean: 0.14341063922273065\n",
      "  episode_reward_min: -0.9029824064452946\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 20103\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7459264376163482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017763865776360035\n",
      "          policy_loss: -0.10063701938465237\n",
      "          total_loss: 0.1807352362126112\n",
      "          vf_explained_var: 0.5223225355148315\n",
      "          vf_loss: 0.2633863388299942\n",
      "    num_agent_steps_sampled: 6009984\n",
      "    num_agent_steps_trained: 6009984\n",
      "    num_steps_sampled: 1502496\n",
      "    num_steps_trained: 1502496\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.764189189189192\n",
      "    gpu_util_percent0: 0.17966216216216216\n",
      "    ram_util_percent: 94.56148648648647\n",
      "    vram_util_percent0: 0.5159747605453429\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9517786965387482\n",
      "  policy_reward_mean:\n",
      "    main: 0.03585265980568266\n",
      "  policy_reward_min:\n",
      "    main: -1.6301950323717154\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30308199057295815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89759248012849\n",
      "    mean_inference_ms: 4.407423025262844\n",
      "    mean_raw_obs_processing_ms: 1.2871960357655154\n",
      "  time_since_restore: 21580.0970621109\n",
      "  time_this_iter_s: 114.62615871429443\n",
      "  time_total_s: 21580.0970621109\n",
      "  timers:\n",
      "    learn_throughput: 90.973\n",
      "    learn_time_ms: 87850.464\n",
      "    sample_throughput: 307.281\n",
      "    sample_time_ms: 26008.787\n",
      "    update_time_ms: 3.022\n",
      "  timestamp: 1639179806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1502496\n",
      "  training_iteration: 188\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         21580.1</td><td style=\"text-align: right;\">1502496</td><td style=\"text-align: right;\">0.143411</td><td style=\"text-align: right;\">             1.20638</td><td style=\"text-align: right;\">           -0.902982</td><td style=\"text-align: right;\">           60.2061</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6041952\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7742367466003336\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05988195329984899\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5681993568902545\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 67.89166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2775719951019413\n",
      "  episode_reward_mean: 0.0721530524761049\n",
      "  episode_reward_min: -1.380075030619175\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 20223\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7686263837814331\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017709760379046202\n",
      "          policy_loss: -0.10086774304136634\n",
      "          total_loss: 0.1699936737883836\n",
      "          vf_explained_var: 0.4916588068008423\n",
      "          vf_loss: 0.2529302839040756\n",
      "    num_agent_steps_sampled: 6041952\n",
      "    num_agent_steps_trained: 6041952\n",
      "    num_steps_sampled: 1510488\n",
      "    num_steps_trained: 1510488\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.87207792207792\n",
      "    gpu_util_percent0: 0.19636363636363635\n",
      "    ram_util_percent: 94.62597402597402\n",
      "    vram_util_percent0: 0.5301075199890599\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8887846725528408\n",
      "  policy_reward_mean:\n",
      "    main: 0.01803826311902621\n",
      "  policy_reward_min:\n",
      "    main: -1.6637481494357655\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032579290199973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.908765274213277\n",
      "    mean_inference_ms: 4.408822493362175\n",
      "    mean_raw_obs_processing_ms: 1.288095060384499\n",
      "  time_since_restore: 21700.351430416107\n",
      "  time_this_iter_s: 120.2543683052063\n",
      "  time_total_s: 21700.351430416107\n",
      "  timers:\n",
      "    learn_throughput: 90.369\n",
      "    learn_time_ms: 88437.572\n",
      "    sample_throughput: 304.32\n",
      "    sample_time_ms: 26261.842\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1639179927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1510488\n",
      "  training_iteration: 189\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         21700.4</td><td style=\"text-align: right;\">1510488</td><td style=\"text-align: right;\">0.0721531</td><td style=\"text-align: right;\">             1.27757</td><td style=\"text-align: right;\">            -1.38008</td><td style=\"text-align: right;\">           67.8917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6073920\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7853172038021964\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04647330082431971\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5644067337556561\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 58.474074074074075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.303595676925299\n",
      "  episode_reward_mean: 0.1424178063821983\n",
      "  episode_reward_min: -0.6540117259600344\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 20358\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7599847991466522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017763957709074022\n",
      "          policy_loss: -0.10105787979438902\n",
      "          total_loss: 0.18277001052349806\n",
      "          vf_explained_var: 0.5030494332313538\n",
      "          vf_loss: 0.26584188121557234\n",
      "    num_agent_steps_sampled: 6073920\n",
      "    num_agent_steps_trained: 6073920\n",
      "    num_steps_sampled: 1518480\n",
      "    num_steps_trained: 1518480\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.00469798657718\n",
      "    gpu_util_percent0: 0.17671140939597313\n",
      "    ram_util_percent: 94.49664429530202\n",
      "    vram_util_percent0: 0.5307733248529738\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7958399181955023\n",
      "  policy_reward_mean:\n",
      "    main: 0.03560445159554959\n",
      "  policy_reward_min:\n",
      "    main: -1.6823283885137998\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039118477068518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.925299207645235\n",
      "    mean_inference_ms: 4.4155231994616\n",
      "    mean_raw_obs_processing_ms: 1.2888874826403456\n",
      "  time_since_restore: 21814.855988025665\n",
      "  time_this_iter_s: 114.5045576095581\n",
      "  time_total_s: 21814.855988025665\n",
      "  timers:\n",
      "    learn_throughput: 90.389\n",
      "    learn_time_ms: 88417.681\n",
      "    sample_throughput: 303.687\n",
      "    sample_time_ms: 26316.538\n",
      "    update_time_ms: 3.059\n",
      "  timestamp: 1639180041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1518480\n",
      "  training_iteration: 190\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         21814.9</td><td style=\"text-align: right;\">1518480</td><td style=\"text-align: right;\">0.142418</td><td style=\"text-align: right;\">              1.3036</td><td style=\"text-align: right;\">           -0.654012</td><td style=\"text-align: right;\">           58.4741</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6105888\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.4466750896247103\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.033140417976984374\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6994074249017165\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 55.208333333333336\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.1372976465250524\n",
      "  episode_reward_mean: 0.06804222733092792\n",
      "  episode_reward_min: -1.088535058260252\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 20502\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7510665717124939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017780515279620886\n",
      "          policy_loss: -0.10025422065332532\n",
      "          total_loss: 0.17764803497493267\n",
      "          vf_explained_var: 0.5466920137405396\n",
      "          vf_loss: 0.2598994826674461\n",
      "    num_agent_steps_sampled: 6105888\n",
      "    num_agent_steps_trained: 6105888\n",
      "    num_steps_sampled: 1526472\n",
      "    num_steps_trained: 1526472\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.904081632653064\n",
      "    gpu_util_percent0: 0.18088435374149658\n",
      "    ram_util_percent: 94.52517006802722\n",
      "    vram_util_percent0: 0.530320124100973\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.3364412880587553\n",
      "  policy_reward_mean:\n",
      "    main: 0.017010556832731987\n",
      "  policy_reward_min:\n",
      "    main: -1.9429286225145246\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032927904059198\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.90854675026301\n",
      "    mean_inference_ms: 4.40947195647554\n",
      "    mean_raw_obs_processing_ms: 1.2890487540478905\n",
      "  time_since_restore: 21928.792680740356\n",
      "  time_this_iter_s: 113.93669271469116\n",
      "  time_total_s: 21928.792680740356\n",
      "  timers:\n",
      "    learn_throughput: 90.581\n",
      "    learn_time_ms: 88230.371\n",
      "    sample_throughput: 303.789\n",
      "    sample_time_ms: 26307.729\n",
      "    update_time_ms: 3.097\n",
      "  timestamp: 1639180155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1526472\n",
      "  training_iteration: 191\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         21928.8</td><td style=\"text-align: right;\">1526472</td><td style=\"text-align: right;\">0.0680422</td><td style=\"text-align: right;\">              2.1373</td><td style=\"text-align: right;\">            -1.08854</td><td style=\"text-align: right;\">           55.2083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6137856\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6232312337564438\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07306947007620129\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5080992286201946\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 66.46280991735537\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.328472698741907\n",
      "  episode_reward_mean: 0.23612729932876472\n",
      "  episode_reward_min: -0.8051881921581476\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 20623\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.744781709909439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018041988406330346\n",
      "          policy_loss: -0.1016288119032979\n",
      "          total_loss: 0.2005796625800431\n",
      "          vf_explained_var: 0.4643480181694031\n",
      "          vf_loss: 0.28394096076488495\n",
      "    num_agent_steps_sampled: 6137856\n",
      "    num_agent_steps_trained: 6137856\n",
      "    num_steps_sampled: 1534464\n",
      "    num_steps_trained: 1534464\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.274496644295304\n",
      "    gpu_util_percent0: 0.17651006711409395\n",
      "    ram_util_percent: 94.68993288590603\n",
      "    vram_util_percent0: 0.5292329470794659\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.935420054459178\n",
      "  policy_reward_mean:\n",
      "    main: 0.05903182483219119\n",
      "  policy_reward_min:\n",
      "    main: -1.541381815320714\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30387319480127367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.92071354538874\n",
      "    mean_inference_ms: 4.414021769267386\n",
      "    mean_raw_obs_processing_ms: 1.2895453455358128\n",
      "  time_since_restore: 22044.386226415634\n",
      "  time_this_iter_s: 115.59354567527771\n",
      "  time_total_s: 22044.386226415634\n",
      "  timers:\n",
      "    learn_throughput: 90.452\n",
      "    learn_time_ms: 88356.64\n",
      "    sample_throughput: 303.57\n",
      "    sample_time_ms: 26326.708\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1639180271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1534464\n",
      "  training_iteration: 192\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         22044.4</td><td style=\"text-align: right;\">1534464</td><td style=\"text-align: right;\">0.236127</td><td style=\"text-align: right;\">             1.32847</td><td style=\"text-align: right;\">           -0.805188</td><td style=\"text-align: right;\">           66.4628</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6169824\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.619928608724948\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.01564764972113936\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7463623115574005\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 63.496\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0900927700585759\n",
      "  episode_reward_mean: 0.09782057252629241\n",
      "  episode_reward_min: -1.5495472684744271\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 20748\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7577931790351867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01823883270844817\n",
      "          policy_loss: -0.10280175554007291\n",
      "          total_loss: 0.18011300286650658\n",
      "          vf_explained_var: 0.4838566780090332\n",
      "          vf_loss: 0.26444794023036955\n",
      "    num_agent_steps_sampled: 6169824\n",
      "    num_agent_steps_trained: 6169824\n",
      "    num_steps_sampled: 1542456\n",
      "    num_steps_trained: 1542456\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.555172413793102\n",
      "    gpu_util_percent0: 0.13882758620689656\n",
      "    ram_util_percent: 94.67793103448278\n",
      "    vram_util_percent0: 0.5288490996357695\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8981687704945638\n",
      "  policy_reward_mean:\n",
      "    main: 0.0244551431315731\n",
      "  policy_reward_min:\n",
      "    main: -1.7914142007349056\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30382268648915706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.919354423178877\n",
      "    mean_inference_ms: 4.414310712313305\n",
      "    mean_raw_obs_processing_ms: 1.2896249861322475\n",
      "  time_since_restore: 22156.720507860184\n",
      "  time_this_iter_s: 112.33428144454956\n",
      "  time_total_s: 22156.720507860184\n",
      "  timers:\n",
      "    learn_throughput: 90.628\n",
      "    learn_time_ms: 88184.335\n",
      "    sample_throughput: 303.647\n",
      "    sample_time_ms: 26320.063\n",
      "    update_time_ms: 3.172\n",
      "  timestamp: 1639180383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1542456\n",
      "  training_iteration: 193\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         22156.7</td><td style=\"text-align: right;\">1542456</td><td style=\"text-align: right;\">0.0978206</td><td style=\"text-align: right;\">             1.09009</td><td style=\"text-align: right;\">            -1.54955</td><td style=\"text-align: right;\">            63.496</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6201792\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7889383646129581\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05192182388185859\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5040533816825635\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 55.798507462686565\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1116692064302764\n",
      "  episode_reward_mean: 0.13163129865558273\n",
      "  episode_reward_min: -1.7381384452459443\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 20882\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7280210688114166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017832975212484597\n",
      "          policy_loss: -0.09941904709115625\n",
      "          total_loss: 0.17295055913552643\n",
      "          vf_explained_var: 0.513254702091217\n",
      "          vf_loss: 0.25431371796131136\n",
      "    num_agent_steps_sampled: 6201792\n",
      "    num_agent_steps_trained: 6201792\n",
      "    num_steps_sampled: 1550448\n",
      "    num_steps_trained: 1550448\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.02551724137931\n",
      "    gpu_util_percent0: 0.1366896551724138\n",
      "    ram_util_percent: 94.63793103448279\n",
      "    vram_util_percent0: 0.5266750632581044\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8791734445675399\n",
      "  policy_reward_mean:\n",
      "    main: 0.03290782466389568\n",
      "  policy_reward_min:\n",
      "    main: -1.5485627054147426\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30272590413094974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89371549356295\n",
      "    mean_inference_ms: 4.406240914458737\n",
      "    mean_raw_obs_processing_ms: 1.2891139867779475\n",
      "  time_since_restore: 22268.642212867737\n",
      "  time_this_iter_s: 111.9217050075531\n",
      "  time_total_s: 22268.642212867737\n",
      "  timers:\n",
      "    learn_throughput: 90.785\n",
      "    learn_time_ms: 88031.947\n",
      "    sample_throughput: 304.086\n",
      "    sample_time_ms: 26282.063\n",
      "    update_time_ms: 3.227\n",
      "  timestamp: 1639180495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1550448\n",
      "  training_iteration: 194\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         22268.6</td><td style=\"text-align: right;\">1550448</td><td style=\"text-align: right;\">0.131631</td><td style=\"text-align: right;\">             1.11167</td><td style=\"text-align: right;\">            -1.73814</td><td style=\"text-align: right;\">           55.7985</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6233760\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9594823725016441\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06828272151939695\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6742923619350515\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 57.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9225701478063573\n",
      "  episode_reward_mean: 0.23150977773438924\n",
      "  episode_reward_min: -0.8932734008436221\n",
      "  episodes_this_iter: 150\n",
      "  episodes_total: 21032\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.731271889925003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01776506356522441\n",
      "          policy_loss: -0.10051357989385724\n",
      "          total_loss: 0.19223458002507687\n",
      "          vf_explained_var: 0.5388915538787842\n",
      "          vf_loss: 0.27476103270053864\n",
      "    num_agent_steps_sampled: 6233760\n",
      "    num_agent_steps_trained: 6233760\n",
      "    num_steps_sampled: 1558440\n",
      "    num_steps_trained: 1558440\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.829655172413794\n",
      "    gpu_util_percent0: 0.13717241379310346\n",
      "    ram_util_percent: 94.74206896551726\n",
      "    vram_util_percent0: 0.5265252862216474\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9849282085419873\n",
      "  policy_reward_mean:\n",
      "    main: 0.057877444433597325\n",
      "  policy_reward_min:\n",
      "    main: -1.7166942393790694\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033071621048363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.902243435327613\n",
      "    mean_inference_ms: 4.409229670678945\n",
      "    mean_raw_obs_processing_ms: 1.290089676287284\n",
      "  time_since_restore: 22380.360358953476\n",
      "  time_this_iter_s: 111.71814608573914\n",
      "  time_total_s: 22380.360358953476\n",
      "  timers:\n",
      "    learn_throughput: 91.027\n",
      "    learn_time_ms: 87798.207\n",
      "    sample_throughput: 304.361\n",
      "    sample_time_ms: 26258.277\n",
      "    update_time_ms: 3.243\n",
      "  timestamp: 1639180607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1558440\n",
      "  training_iteration: 195\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         22380.4</td><td style=\"text-align: right;\">1558440</td><td style=\"text-align: right;\"> 0.23151</td><td style=\"text-align: right;\">             1.92257</td><td style=\"text-align: right;\">           -0.893273</td><td style=\"text-align: right;\">             57.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6265728\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6746371340014998\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04750749835785692\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5081046381305483\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_20-58-39\n",
      "  done: false\n",
      "  episode_len_mean: 52.136986301369866\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3330858867902655\n",
      "  episode_reward_mean: 0.1405483719875892\n",
      "  episode_reward_min: -0.9579431439611552\n",
      "  episodes_this_iter: 146\n",
      "  episodes_total: 21178\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7317963330745697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017718587249517442\n",
      "          policy_loss: -0.1003134457655251\n",
      "          total_loss: 0.17649952943623065\n",
      "          vf_explained_var: 0.5414206385612488\n",
      "          vf_loss: 0.2588729031085968\n",
      "    num_agent_steps_sampled: 6265728\n",
      "    num_agent_steps_trained: 6265728\n",
      "    num_steps_sampled: 1566432\n",
      "    num_steps_trained: 1566432\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.071034482758616\n",
      "    gpu_util_percent0: 0.1393103448275862\n",
      "    ram_util_percent: 94.78068965517242\n",
      "    vram_util_percent0: 0.521238837639423\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.913404985188892\n",
      "  policy_reward_mean:\n",
      "    main: 0.0351370929968973\n",
      "  policy_reward_min:\n",
      "    main: -1.5195637743134602\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303358376699755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.900822727731644\n",
      "    mean_inference_ms: 4.408684450979602\n",
      "    mean_raw_obs_processing_ms: 1.290718910984965\n",
      "  time_since_restore: 22492.251690387726\n",
      "  time_this_iter_s: 111.89133143424988\n",
      "  time_total_s: 22492.251690387726\n",
      "  timers:\n",
      "    learn_throughput: 91.283\n",
      "    learn_time_ms: 87552.138\n",
      "    sample_throughput: 304.215\n",
      "    sample_time_ms: 26270.855\n",
      "    update_time_ms: 3.232\n",
      "  timestamp: 1639180719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1566432\n",
      "  training_iteration: 196\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         22492.3</td><td style=\"text-align: right;\">1566432</td><td style=\"text-align: right;\">0.140548</td><td style=\"text-align: right;\">             1.33309</td><td style=\"text-align: right;\">           -0.957943</td><td style=\"text-align: right;\">            52.137</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6297696\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.953893207389607\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05592520813301104\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.658833406719002\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-00-31\n",
      "  done: false\n",
      "  episode_len_mean: 69.36134453781513\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3197079135776981\n",
      "  episode_reward_mean: 0.07228183292250806\n",
      "  episode_reward_min: -1.2182097045544031\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 21297\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7449686162471771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018307963866740464\n",
      "          policy_loss: -0.10212168891727924\n",
      "          total_loss: 0.16839876768365503\n",
      "          vf_explained_var: 0.49693071842193604\n",
      "          vf_loss: 0.25198364222049713\n",
      "    num_agent_steps_sampled: 6297696\n",
      "    num_agent_steps_trained: 6297696\n",
      "    num_steps_sampled: 1574424\n",
      "    num_steps_trained: 1574424\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.880000000000003\n",
      "    gpu_util_percent0: 0.13834482758620692\n",
      "    ram_util_percent: 94.79655172413793\n",
      "    vram_util_percent0: 0.5159262915432709\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9072296652807488\n",
      "  policy_reward_mean:\n",
      "    main: 0.01807045823062701\n",
      "  policy_reward_min:\n",
      "    main: -1.8177795816158118\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031588076260715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.894301903680176\n",
      "    mean_inference_ms: 4.407045977536645\n",
      "    mean_raw_obs_processing_ms: 1.290625236892883\n",
      "  time_since_restore: 22604.764235019684\n",
      "  time_this_iter_s: 112.51254463195801\n",
      "  time_total_s: 22604.764235019684\n",
      "  timers:\n",
      "    learn_throughput: 91.455\n",
      "    learn_time_ms: 87386.84\n",
      "    sample_throughput: 305.141\n",
      "    sample_time_ms: 26191.209\n",
      "    update_time_ms: 3.14\n",
      "  timestamp: 1639180831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1574424\n",
      "  training_iteration: 197\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         22604.8</td><td style=\"text-align: right;\">1574424</td><td style=\"text-align: right;\">0.0722818</td><td style=\"text-align: right;\">             1.31971</td><td style=\"text-align: right;\">            -1.21821</td><td style=\"text-align: right;\">           69.3613</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6329664\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.159648035998269\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04000146547144838\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5869907380343592\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 57.23357664233577\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0918449085917055\n",
      "  episode_reward_mean: 0.11806428665790528\n",
      "  episode_reward_min: -1.232749852652459\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 21434\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7321428277492523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018289460189640522\n",
      "          policy_loss: -0.10275848590955138\n",
      "          total_loss: 0.17819235900044442\n",
      "          vf_explained_var: 0.5155130624771118\n",
      "          vf_loss: 0.2624327648282051\n",
      "    num_agent_steps_sampled: 6329664\n",
      "    num_agent_steps_trained: 6329664\n",
      "    num_steps_sampled: 1582416\n",
      "    num_steps_trained: 1582416\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.819310344827592\n",
      "    gpu_util_percent0: 0.138\n",
      "    ram_util_percent: 94.89310344827584\n",
      "    vram_util_percent0: 0.5159591971043106\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0020417274571263\n",
      "  policy_reward_mean:\n",
      "    main: 0.029516071664476312\n",
      "  policy_reward_min:\n",
      "    main: -1.717474668770672\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3037814196851387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.906569031141473\n",
      "    mean_inference_ms: 4.411825713763365\n",
      "    mean_raw_obs_processing_ms: 1.2914148958540805\n",
      "  time_since_restore: 22716.59505701065\n",
      "  time_this_iter_s: 111.8308219909668\n",
      "  time_total_s: 22716.59505701065\n",
      "  timers:\n",
      "    learn_throughput: 91.726\n",
      "    learn_time_ms: 87129.469\n",
      "    sample_throughput: 305.341\n",
      "    sample_time_ms: 26174.052\n",
      "    update_time_ms: 3.156\n",
      "  timestamp: 1639180943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1582416\n",
      "  training_iteration: 198\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         22716.6</td><td style=\"text-align: right;\">1582416</td><td style=\"text-align: right;\">0.118064</td><td style=\"text-align: right;\">             1.09184</td><td style=\"text-align: right;\">            -1.23275</td><td style=\"text-align: right;\">           57.2336</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6361632\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9804539401546796\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.055795541688200105\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5138998423594258\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-04-16\n",
      "  done: false\n",
      "  episode_len_mean: 61.946969696969695\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.611874771165052\n",
      "  episode_reward_mean: 0.22871400910693493\n",
      "  episode_reward_min: -0.809922613971259\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 21566\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7350200238227844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01785040605068207\n",
      "          policy_loss: -0.10065357533842326\n",
      "          total_loss: 0.17339230185002089\n",
      "          vf_explained_var: 0.5296878814697266\n",
      "          vf_loss: 0.255972340464592\n",
      "    num_agent_steps_sampled: 6361632\n",
      "    num_agent_steps_trained: 6361632\n",
      "    num_steps_sampled: 1590408\n",
      "    num_steps_trained: 1590408\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.023287671232875\n",
      "    gpu_util_percent0: 0.13664383561643836\n",
      "    ram_util_percent: 94.8335616438356\n",
      "    vram_util_percent0: 0.5162747298814047\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.264951013454797\n",
      "  policy_reward_mean:\n",
      "    main: 0.05717850227673375\n",
      "  policy_reward_min:\n",
      "    main: -1.772287730880565\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030431324144204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.888153458740135\n",
      "    mean_inference_ms: 4.405978122666921\n",
      "    mean_raw_obs_processing_ms: 1.290937517016623\n",
      "  time_since_restore: 22829.40092897415\n",
      "  time_this_iter_s: 112.80587196350098\n",
      "  time_total_s: 22829.40092897415\n",
      "  timers:\n",
      "    learn_throughput: 92.271\n",
      "    learn_time_ms: 86614.762\n",
      "    sample_throughput: 308.112\n",
      "    sample_time_ms: 25938.637\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1639181056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1590408\n",
      "  training_iteration: 199\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         22829.4</td><td style=\"text-align: right;\">1590408</td><td style=\"text-align: right;\">0.228714</td><td style=\"text-align: right;\">             1.61187</td><td style=\"text-align: right;\">           -0.809923</td><td style=\"text-align: right;\">            61.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6393600\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9777486663004101\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06452676389478618\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4813400680295066\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-06-08\n",
      "  done: false\n",
      "  episode_len_mean: 58.41044776119403\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6603411321003305\n",
      "  episode_reward_mean: 0.19181159461978264\n",
      "  episode_reward_min: -0.6952923181759316\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 21700\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.72106418633461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01797175968810916\n",
      "          policy_loss: -0.10041155039146543\n",
      "          total_loss: 0.185720007263124\n",
      "          vf_explained_var: 0.5029791593551636\n",
      "          vf_loss: 0.2679351504445076\n",
      "    num_agent_steps_sampled: 6393600\n",
      "    num_agent_steps_trained: 6393600\n",
      "    num_steps_sampled: 1598400\n",
      "    num_steps_trained: 1598400\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.5848275862069\n",
      "    gpu_util_percent0: 0.13820689655172413\n",
      "    ram_util_percent: 94.93517241379311\n",
      "    vram_util_percent0: 0.5161804586354404\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.96507899642973\n",
      "  policy_reward_mean:\n",
      "    main: 0.04795289865494565\n",
      "  policy_reward_min:\n",
      "    main: -1.5122337609808145\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031630040365817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.885561323512395\n",
      "    mean_inference_ms: 4.404944260753567\n",
      "    mean_raw_obs_processing_ms: 1.2913824524642317\n",
      "  time_since_restore: 22941.569503307343\n",
      "  time_this_iter_s: 112.16857433319092\n",
      "  time_total_s: 22941.569503307343\n",
      "  timers:\n",
      "    learn_throughput: 92.461\n",
      "    learn_time_ms: 86436.66\n",
      "    sample_throughput: 308.751\n",
      "    sample_time_ms: 25884.976\n",
      "    update_time_ms: 3.09\n",
      "  timestamp: 1639181168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1598400\n",
      "  training_iteration: 200\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         22941.6</td><td style=\"text-align: right;\">1598400</td><td style=\"text-align: right;\">0.191812</td><td style=\"text-align: right;\">             1.66034</td><td style=\"text-align: right;\">           -0.695292</td><td style=\"text-align: right;\">           58.4104</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6425568\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.750746468655373\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09130493857025228\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5080992465467268\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 55.70422535211268\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5931987409020745\n",
      "  episode_reward_mean: 0.1483806544391259\n",
      "  episode_reward_min: -0.9736436274159568\n",
      "  episodes_this_iter: 142\n",
      "  episodes_total: 21842\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7293548619747162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017878561314195393\n",
      "          policy_loss: -0.1011920229718089\n",
      "          total_loss: 0.17150771056115627\n",
      "          vf_explained_var: 0.5353049039840698\n",
      "          vf_loss: 0.254597687125206\n",
      "    num_agent_steps_sampled: 6425568\n",
      "    num_agent_steps_trained: 6425568\n",
      "    num_steps_sampled: 1606392\n",
      "    num_steps_trained: 1606392\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.930344827586204\n",
      "    gpu_util_percent0: 0.13620689655172413\n",
      "    ram_util_percent: 94.9\n",
      "    vram_util_percent0: 0.51613166763114\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7982743234395253\n",
      "  policy_reward_mean:\n",
      "    main: 0.03709516360978146\n",
      "  policy_reward_min:\n",
      "    main: -1.5793832136127282\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30359536841370394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.898771507318845\n",
      "    mean_inference_ms: 4.410842803078065\n",
      "    mean_raw_obs_processing_ms: 1.2919075392538235\n",
      "  time_since_restore: 23053.806921482086\n",
      "  time_this_iter_s: 112.23741817474365\n",
      "  time_total_s: 23053.806921482086\n",
      "  timers:\n",
      "    learn_throughput: 92.661\n",
      "    learn_time_ms: 86249.602\n",
      "    sample_throughput: 308.536\n",
      "    sample_time_ms: 25902.977\n",
      "    update_time_ms: 3.021\n",
      "  timestamp: 1639181281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1606392\n",
      "  training_iteration: 201\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         23053.8</td><td style=\"text-align: right;\">1606392</td><td style=\"text-align: right;\">0.148381</td><td style=\"text-align: right;\">              1.5932</td><td style=\"text-align: right;\">           -0.973644</td><td style=\"text-align: right;\">           55.7042</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6457536\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7831294677757393\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07112837327183115\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49509452216197836\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 62.723076923076924\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5308827132228444\n",
      "  episode_reward_mean: 0.2051485956864877\n",
      "  episode_reward_min: -0.6732985746692952\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 21972\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7292510061264038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01804344931617379\n",
      "          policy_loss: -0.10060492438077927\n",
      "          total_loss: 0.18021198899298907\n",
      "          vf_explained_var: 0.5192634463310242\n",
      "          vf_loss: 0.26254791820049284\n",
      "    num_agent_steps_sampled: 6457536\n",
      "    num_agent_steps_trained: 6457536\n",
      "    num_steps_sampled: 1614384\n",
      "    num_steps_trained: 1614384\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.970138888888886\n",
      "    gpu_util_percent0: 0.13791666666666666\n",
      "    ram_util_percent: 94.93819444444443\n",
      "    vram_util_percent0: 0.5161454334393624\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0154065793670966\n",
      "  policy_reward_mean:\n",
      "    main: 0.05128714892162192\n",
      "  policy_reward_min:\n",
      "    main: -1.5079346623376295\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030239365576816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.88196945936714\n",
      "    mean_inference_ms: 4.405547720934255\n",
      "    mean_raw_obs_processing_ms: 1.2916454138791715\n",
      "  time_since_restore: 23165.31796693802\n",
      "  time_this_iter_s: 111.51104545593262\n",
      "  time_total_s: 23165.31796693802\n",
      "  timers:\n",
      "    learn_throughput: 93.001\n",
      "    learn_time_ms: 85934.41\n",
      "    sample_throughput: 309.624\n",
      "    sample_time_ms: 25811.946\n",
      "    update_time_ms: 2.999\n",
      "  timestamp: 1639181392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1614384\n",
      "  training_iteration: 202\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         23165.3</td><td style=\"text-align: right;\">1614384</td><td style=\"text-align: right;\">0.205149</td><td style=\"text-align: right;\">             1.53088</td><td style=\"text-align: right;\">           -0.673299</td><td style=\"text-align: right;\">           62.7231</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6489504\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.88992471989146\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06896144604843071\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5171920073309693\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 60.04347826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2910599182758564\n",
      "  episode_reward_mean: 0.12233439851787066\n",
      "  episode_reward_min: -1.1080360364380066\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 22110\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7280112125873566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01831630038097501\n",
      "          policy_loss: -0.10132835087552666\n",
      "          total_loss: 0.1714463716186583\n",
      "          vf_explained_var: 0.5360590815544128\n",
      "          vf_loss: 0.25422946882247927\n",
      "    num_agent_steps_sampled: 6489504\n",
      "    num_agent_steps_trained: 6489504\n",
      "    num_steps_sampled: 1622376\n",
      "    num_steps_trained: 1622376\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.19795918367347\n",
      "    gpu_util_percent0: 0.1338095238095238\n",
      "    ram_util_percent: 95.0231292517007\n",
      "    vram_util_percent0: 0.5160453783356055\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8286963491950865\n",
      "  policy_reward_mean:\n",
      "    main: 0.030583599629467665\n",
      "  policy_reward_min:\n",
      "    main: -1.695047641809925\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033268867230536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.891847529646252\n",
      "    mean_inference_ms: 4.40816298059749\n",
      "    mean_raw_obs_processing_ms: 1.2924659278147173\n",
      "  time_since_restore: 23278.62558245659\n",
      "  time_this_iter_s: 113.30761551856995\n",
      "  time_total_s: 23278.62558245659\n",
      "  timers:\n",
      "    learn_throughput: 93.046\n",
      "    learn_time_ms: 85892.859\n",
      "    sample_throughput: 308.047\n",
      "    sample_time_ms: 25944.076\n",
      "    update_time_ms: 2.978\n",
      "  timestamp: 1639181506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1622376\n",
      "  training_iteration: 203\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         23278.6</td><td style=\"text-align: right;\">1622376</td><td style=\"text-align: right;\">0.122334</td><td style=\"text-align: right;\">             1.29106</td><td style=\"text-align: right;\">            -1.10804</td><td style=\"text-align: right;\">           60.0435</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6521472\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.897328489681304\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.040551951716648396\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6898015870582153\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-13-40\n",
      "  done: false\n",
      "  episode_len_mean: 60.193548387096776\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0214331634179796\n",
      "  episode_reward_mean: 0.08114010866697552\n",
      "  episode_reward_min: -0.8083381011151283\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 22234\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7238280169963837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01794824180006981\n",
      "          policy_loss: -0.09980247861146926\n",
      "          total_loss: 0.18996508204191923\n",
      "          vf_explained_var: 0.4732663631439209\n",
      "          vf_loss: 0.2715949632525444\n",
      "    num_agent_steps_sampled: 6521472\n",
      "    num_agent_steps_trained: 6521472\n",
      "    num_steps_sampled: 1630368\n",
      "    num_steps_trained: 1630368\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.05850340136055\n",
      "    gpu_util_percent0: 0.1345578231292517\n",
      "    ram_util_percent: 94.778231292517\n",
      "    vram_util_percent0: 0.5158562273214651\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9563525782664821\n",
      "  policy_reward_mean:\n",
      "    main: 0.020285027166743876\n",
      "  policy_reward_min:\n",
      "    main: -1.7612301584867867\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034315634023271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89071366477494\n",
      "    mean_inference_ms: 4.407146423089772\n",
      "    mean_raw_obs_processing_ms: 1.2929469191082976\n",
      "  time_since_restore: 23392.63919878006\n",
      "  time_this_iter_s: 114.01361632347107\n",
      "  time_total_s: 23392.63919878006\n",
      "  timers:\n",
      "    learn_throughput: 92.897\n",
      "    learn_time_ms: 86030.885\n",
      "    sample_throughput: 307.252\n",
      "    sample_time_ms: 26011.186\n",
      "    update_time_ms: 2.888\n",
      "  timestamp: 1639181620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1630368\n",
      "  training_iteration: 204\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         23392.6</td><td style=\"text-align: right;\">1630368</td><td style=\"text-align: right;\">0.0811401</td><td style=\"text-align: right;\">             1.02143</td><td style=\"text-align: right;\">           -0.808338</td><td style=\"text-align: right;\">           60.1935</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6553440\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8371615746907879\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05351521964510209\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6423216558007679\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 65.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0109735124710004\n",
      "  episode_reward_mean: 0.11112749392103921\n",
      "  episode_reward_min: -1.4054166444770788\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 22358\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.731073177576065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017842822756618262\n",
      "          policy_loss: -0.1001732608564198\n",
      "          total_loss: 0.1563413169309497\n",
      "          vf_explained_var: 0.5270593762397766\n",
      "          vf_loss: 0.2384487183690071\n",
      "    num_agent_steps_sampled: 6553440\n",
      "    num_agent_steps_trained: 6553440\n",
      "    num_steps_sampled: 1638360\n",
      "    num_steps_trained: 1638360\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.079310344827586\n",
      "    gpu_util_percent0: 0.13772413793103447\n",
      "    ram_util_percent: 94.6696551724138\n",
      "    vram_util_percent0: 0.5158275748601515\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8783857376150699\n",
      "  policy_reward_mean:\n",
      "    main: 0.027781873480259813\n",
      "  policy_reward_min:\n",
      "    main: -1.781526938230034\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029401493326067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.877294896770906\n",
      "    mean_inference_ms: 4.40253256338065\n",
      "    mean_raw_obs_processing_ms: 1.2926026446071106\n",
      "  time_since_restore: 23504.647670269012\n",
      "  time_this_iter_s: 112.00847148895264\n",
      "  time_total_s: 23504.647670269012\n",
      "  timers:\n",
      "    learn_throughput: 92.884\n",
      "    learn_time_ms: 86042.864\n",
      "    sample_throughput: 307.053\n",
      "    sample_time_ms: 26028.087\n",
      "    update_time_ms: 2.888\n",
      "  timestamp: 1639181732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1638360\n",
      "  training_iteration: 205\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         23504.6</td><td style=\"text-align: right;\">1638360</td><td style=\"text-align: right;\">0.111127</td><td style=\"text-align: right;\">             1.01097</td><td style=\"text-align: right;\">            -1.40542</td><td style=\"text-align: right;\">             65.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6585408\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9634333432315554\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04493454260185326\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6087045254835853\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 56.41258741258741\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.190439146999133\n",
      "  episode_reward_mean: 0.12333754318505012\n",
      "  episode_reward_min: -1.1276773098119768\n",
      "  episodes_this_iter: 143\n",
      "  episodes_total: 22501\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7105919535160065\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017850091133266688\n",
      "          policy_loss: -0.10026791511103511\n",
      "          total_loss: 0.16762108197808265\n",
      "          vf_explained_var: 0.5621806979179382\n",
      "          vf_loss: 0.24981577920913697\n",
      "    num_agent_steps_sampled: 6585408\n",
      "    num_agent_steps_trained: 6585408\n",
      "    num_steps_sampled: 1646352\n",
      "    num_steps_trained: 1646352\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.966206896551725\n",
      "    gpu_util_percent0: 0.13882758620689656\n",
      "    ram_util_percent: 94.7648275862069\n",
      "    vram_util_percent0: 0.515802612020742\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9223090362524555\n",
      "  policy_reward_mean:\n",
      "    main: 0.03083438579626253\n",
      "  policy_reward_min:\n",
      "    main: -1.629014807063259\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032738979317645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.887546395958083\n",
      "    mean_inference_ms: 4.407291357438122\n",
      "    mean_raw_obs_processing_ms: 1.2930818108770765\n",
      "  time_since_restore: 23617.239782333374\n",
      "  time_this_iter_s: 112.59211206436157\n",
      "  time_total_s: 23617.239782333374\n",
      "  timers:\n",
      "    learn_throughput: 92.803\n",
      "    learn_time_ms: 86117.658\n",
      "    sample_throughput: 307.051\n",
      "    sample_time_ms: 26028.288\n",
      "    update_time_ms: 2.856\n",
      "  timestamp: 1639181845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1646352\n",
      "  training_iteration: 206\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         23617.2</td><td style=\"text-align: right;\">1646352</td><td style=\"text-align: right;\">0.123338</td><td style=\"text-align: right;\">             1.19044</td><td style=\"text-align: right;\">            -1.12768</td><td style=\"text-align: right;\">           56.4126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6617376\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8106323303544349\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0802512364837783\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4779090388780566\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 59.97794117647059\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8599877991388896\n",
      "  episode_reward_mean: 0.20651104098111253\n",
      "  episode_reward_min: -0.9609845047943981\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 22637\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7150586836338043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01799637221172452\n",
      "          policy_loss: -0.10064702297002077\n",
      "          total_loss: 0.193528733715415\n",
      "          vf_explained_var: 0.5139203071594238\n",
      "          vf_loss: 0.2759544275999069\n",
      "    num_agent_steps_sampled: 6617376\n",
      "    num_agent_steps_trained: 6617376\n",
      "    num_steps_sampled: 1654344\n",
      "    num_steps_trained: 1654344\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.015172413793103\n",
      "    gpu_util_percent0: 0.13841379310344828\n",
      "    ram_util_percent: 94.81241379310345\n",
      "    vram_util_percent0: 0.5158355175817817\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.956484168199391\n",
      "  policy_reward_mean:\n",
      "    main: 0.05162776024527814\n",
      "  policy_reward_min:\n",
      "    main: -1.7172543015492947\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029365178192166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.876635376788855\n",
      "    mean_inference_ms: 4.403045066546453\n",
      "    mean_raw_obs_processing_ms: 1.2932132161809158\n",
      "  time_since_restore: 23729.407086610794\n",
      "  time_this_iter_s: 112.16730427742004\n",
      "  time_total_s: 23729.407086610794\n",
      "  timers:\n",
      "    learn_throughput: 92.826\n",
      "    learn_time_ms: 86096.731\n",
      "    sample_throughput: 307.228\n",
      "    sample_time_ms: 26013.277\n",
      "    update_time_ms: 2.861\n",
      "  timestamp: 1639181957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1654344\n",
      "  training_iteration: 207\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         23729.4</td><td style=\"text-align: right;\">1654344</td><td style=\"text-align: right;\">0.206511</td><td style=\"text-align: right;\">             1.85999</td><td style=\"text-align: right;\">           -0.960985</td><td style=\"text-align: right;\">           59.9779</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6649344\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9988539670313838\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.054612316416947575\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6273929438335082\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-21-10\n",
      "  done: false\n",
      "  episode_len_mean: 58.48888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5265855215987492\n",
      "  episode_reward_mean: 0.10894584498933058\n",
      "  episode_reward_min: -1.564115782562073\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 22772\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7121994516849518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017838550247251988\n",
      "          policy_loss: -0.09879331120476127\n",
      "          total_loss: 0.17922571997717024\n",
      "          vf_explained_var: 0.5235762596130371\n",
      "          vf_loss: 0.2599575007557869\n",
      "    num_agent_steps_sampled: 6649344\n",
      "    num_agent_steps_trained: 6649344\n",
      "    num_steps_sampled: 1662336\n",
      "    num_steps_trained: 1662336\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.391095890410963\n",
      "    gpu_util_percent0: 0.13643835616438357\n",
      "    ram_util_percent: 94.86917808219178\n",
      "    vram_util_percent0: 0.5158048114240896\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.171648156578306\n",
      "  policy_reward_mean:\n",
      "    main: 0.02723646124733265\n",
      "  policy_reward_min:\n",
      "    main: -1.65335858523129\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30292199231854666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.87333648027086\n",
      "    mean_inference_ms: 4.401807412022753\n",
      "    mean_raw_obs_processing_ms: 1.293586294010403\n",
      "  time_since_restore: 23842.02974319458\n",
      "  time_this_iter_s: 112.62265658378601\n",
      "  time_total_s: 23842.02974319458\n",
      "  timers:\n",
      "    learn_throughput: 92.77\n",
      "    learn_time_ms: 86148.552\n",
      "    sample_throughput: 306.891\n",
      "    sample_time_ms: 26041.861\n",
      "    update_time_ms: 2.818\n",
      "  timestamp: 1639182070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1662336\n",
      "  training_iteration: 208\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">           23842</td><td style=\"text-align: right;\">1662336</td><td style=\"text-align: right;\">0.108946</td><td style=\"text-align: right;\">             1.52659</td><td style=\"text-align: right;\">            -1.56412</td><td style=\"text-align: right;\">           58.4889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6681312\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9569578674202209\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10224859674132486\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5290815161066862\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 58.786259541984734\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3641705245258142\n",
      "  episode_reward_mean: 0.22605182991873904\n",
      "  episode_reward_min: -1.2065838232360213\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 22903\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.717892256975174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01825779517367482\n",
      "          policy_loss: -0.10219089267402888\n",
      "          total_loss: 0.19067835810035466\n",
      "          vf_explained_var: 0.4931320250034332\n",
      "          vf_loss: 0.27438323366642\n",
      "    num_agent_steps_sampled: 6681312\n",
      "    num_agent_steps_trained: 6681312\n",
      "    num_steps_sampled: 1670328\n",
      "    num_steps_trained: 1670328\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.07602739726027\n",
      "    gpu_util_percent0: 0.13623287671232878\n",
      "    ram_util_percent: 94.96506849315068\n",
      "    vram_util_percent0: 0.5158081921324156\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8934897632954142\n",
      "  policy_reward_mean:\n",
      "    main: 0.05651295747968475\n",
      "  policy_reward_min:\n",
      "    main: -1.8171031696991484\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30306287900591256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.877058159090613\n",
      "    mean_inference_ms: 4.403791818560541\n",
      "    mean_raw_obs_processing_ms: 1.293860102201691\n",
      "  time_since_restore: 23954.185160160065\n",
      "  time_this_iter_s: 112.15541696548462\n",
      "  time_total_s: 23954.185160160065\n",
      "  timers:\n",
      "    learn_throughput: 92.825\n",
      "    learn_time_ms: 86097.255\n",
      "    sample_throughput: 306.982\n",
      "    sample_time_ms: 26034.126\n",
      "    update_time_ms: 2.838\n",
      "  timestamp: 1639182182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1670328\n",
      "  training_iteration: 209\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         23954.2</td><td style=\"text-align: right;\">1670328</td><td style=\"text-align: right;\">0.226052</td><td style=\"text-align: right;\">             1.36417</td><td style=\"text-align: right;\">            -1.20658</td><td style=\"text-align: right;\">           58.7863</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6713280\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8177600537600292\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05879896826261081\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5700144639242787\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 61.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.2253081043738066\n",
      "  episode_reward_mean: 0.08760817715315285\n",
      "  episode_reward_min: -0.6911552679447546\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 23043\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7095148017406464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018356354143470525\n",
      "          policy_loss: -0.1012146359756589\n",
      "          total_loss: 0.1878395377881825\n",
      "          vf_explained_var: 0.5182269215583801\n",
      "          vf_loss: 0.27046836441755295\n",
      "    num_agent_steps_sampled: 6713280\n",
      "    num_agent_steps_trained: 6713280\n",
      "    num_steps_sampled: 1678320\n",
      "    num_steps_trained: 1678320\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.04344827586207\n",
      "    gpu_util_percent0: 0.13827586206896555\n",
      "    ram_util_percent: 94.95931034482759\n",
      "    vram_util_percent0: 0.5159296955668267\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.910096408376626\n",
      "  policy_reward_mean:\n",
      "    main: 0.021902044288288205\n",
      "  policy_reward_min:\n",
      "    main: -1.9258421417407385\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033102884867294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.88000031074532\n",
      "    mean_inference_ms: 4.405398696728665\n",
      "    mean_raw_obs_processing_ms: 1.2943871934905007\n",
      "  time_since_restore: 24066.410904169083\n",
      "  time_this_iter_s: 112.22574400901794\n",
      "  time_total_s: 24066.410904169083\n",
      "  timers:\n",
      "    learn_throughput: 92.803\n",
      "    learn_time_ms: 86118.087\n",
      "    sample_throughput: 307.159\n",
      "    sample_time_ms: 26019.087\n",
      "    update_time_ms: 2.82\n",
      "  timestamp: 1639182294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1678320\n",
      "  training_iteration: 210\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         24066.4</td><td style=\"text-align: right;\">1678320</td><td style=\"text-align: right;\">0.0876082</td><td style=\"text-align: right;\">             2.22531</td><td style=\"text-align: right;\">           -0.691155</td><td style=\"text-align: right;\">              61.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6745248\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.2444268202490278\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08367796668932474\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5771234347430797\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 55.347517730496456\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7222177726836188\n",
      "  episode_reward_mean: 0.1467549577104265\n",
      "  episode_reward_min: -0.7886394076174994\n",
      "  episodes_this_iter: 141\n",
      "  episodes_total: 23184\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7075126523971558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01810001961514354\n",
      "          policy_loss: -0.10080108425021171\n",
      "          total_loss: 0.19100116795301436\n",
      "          vf_explained_var: 0.4981366991996765\n",
      "          vf_loss: 0.2734759839773178\n",
      "    num_agent_steps_sampled: 6745248\n",
      "    num_agent_steps_trained: 6745248\n",
      "    num_steps_sampled: 1686312\n",
      "    num_steps_trained: 1686312\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.41241379310345\n",
      "    gpu_util_percent0: 0.13889655172413792\n",
      "    ram_util_percent: 94.91103448275861\n",
      "    vram_util_percent0: 0.5161884013570708\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.979542466964414\n",
      "  policy_reward_mean:\n",
      "    main: 0.03668873942760661\n",
      "  policy_reward_min:\n",
      "    main: -1.5771234347430796\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029831580408937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.869700285212797\n",
      "    mean_inference_ms: 4.401913327592454\n",
      "    mean_raw_obs_processing_ms: 1.2946976089581559\n",
      "  time_since_restore: 24178.891134500504\n",
      "  time_this_iter_s: 112.4802303314209\n",
      "  time_total_s: 24178.891134500504\n",
      "  timers:\n",
      "    learn_throughput: 92.738\n",
      "    learn_time_ms: 86177.848\n",
      "    sample_throughput: 307.553\n",
      "    sample_time_ms: 25985.768\n",
      "    update_time_ms: 2.819\n",
      "  timestamp: 1639182407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1686312\n",
      "  training_iteration: 211\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         24178.9</td><td style=\"text-align: right;\">1686312</td><td style=\"text-align: right;\">0.146755</td><td style=\"text-align: right;\">             1.72222</td><td style=\"text-align: right;\">           -0.788639</td><td style=\"text-align: right;\">           55.3475</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6777216\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8864379807783496\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08541583324144542\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6853299324986571\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-28-45\n",
      "  done: false\n",
      "  episode_len_mean: 60.51538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2407190921859916\n",
      "  episode_reward_mean: 0.10531681869186367\n",
      "  episode_reward_min: -1.097625447070454\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 23314\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7126084234714508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01831233461201191\n",
      "          policy_loss: -0.10168946817517281\n",
      "          total_loss: 0.2006720933392644\n",
      "          vf_explained_var: 0.4805304706096649\n",
      "          vf_loss: 0.28382032269239427\n",
      "    num_agent_steps_sampled: 6777216\n",
      "    num_agent_steps_trained: 6777216\n",
      "    num_steps_sampled: 1694304\n",
      "    num_steps_trained: 1694304\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.26381578947369\n",
      "    gpu_util_percent0: 0.17447368421052628\n",
      "    ram_util_percent: 95.13815789473684\n",
      "    vram_util_percent0: 0.5300771981780711\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.822371300243919\n",
      "  policy_reward_mean:\n",
      "    main: 0.02632920467296593\n",
      "  policy_reward_min:\n",
      "    main: -1.81023199993126\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30356157008966167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.891652287895127\n",
      "    mean_inference_ms: 4.408747658505948\n",
      "    mean_raw_obs_processing_ms: 1.2954254347863365\n",
      "  time_since_restore: 24296.96498966217\n",
      "  time_this_iter_s: 118.07385516166687\n",
      "  time_total_s: 24296.96498966217\n",
      "  timers:\n",
      "    learn_throughput: 92.288\n",
      "    learn_time_ms: 86598.223\n",
      "    sample_throughput: 304.858\n",
      "    sample_time_ms: 26215.491\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639182525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1694304\n",
      "  training_iteration: 212\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">           24297</td><td style=\"text-align: right;\">1694304</td><td style=\"text-align: right;\">0.105317</td><td style=\"text-align: right;\">             1.24072</td><td style=\"text-align: right;\">            -1.09763</td><td style=\"text-align: right;\">           60.5154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6809184\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9196706681713788\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06814795040051262\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8720525439491441\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 57.31617647058823\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8609479078622342\n",
      "  episode_reward_mean: 0.1688711192582194\n",
      "  episode_reward_min: -1.5073218338568042\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 23450\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.705511536359787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018010820265859366\n",
      "          policy_loss: -0.101177216604352\n",
      "          total_loss: 0.20383320759236812\n",
      "          vf_explained_var: 0.4847719073295593\n",
      "          vf_loss: 0.28677447003126144\n",
      "    num_agent_steps_sampled: 6809184\n",
      "    num_agent_steps_trained: 6809184\n",
      "    num_steps_sampled: 1702296\n",
      "    num_steps_trained: 1702296\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.971527777777776\n",
      "    gpu_util_percent0: 0.13062500000000002\n",
      "    ram_util_percent: 94.49791666666665\n",
      "    vram_util_percent0: 0.5309803572081458\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.2202783443355187\n",
      "  policy_reward_mean:\n",
      "    main: 0.042217779814554844\n",
      "  policy_reward_min:\n",
      "    main: -1.872052543949144\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3028705232451518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.87486476572198\n",
      "    mean_inference_ms: 4.403651395598977\n",
      "    mean_raw_obs_processing_ms: 1.295143106846045\n",
      "  time_since_restore: 24408.497765779495\n",
      "  time_this_iter_s: 111.53277611732483\n",
      "  time_total_s: 24408.497765779495\n",
      "  timers:\n",
      "    learn_throughput: 92.334\n",
      "    learn_time_ms: 86555.762\n",
      "    sample_throughput: 306.351\n",
      "    sample_time_ms: 26087.688\n",
      "    update_time_ms: 2.824\n",
      "  timestamp: 1639182636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1702296\n",
      "  training_iteration: 213\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         24408.5</td><td style=\"text-align: right;\">1702296</td><td style=\"text-align: right;\">0.168871</td><td style=\"text-align: right;\">             1.86095</td><td style=\"text-align: right;\">            -1.50732</td><td style=\"text-align: right;\">           57.3162</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6841152\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8116532218033053\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06117653317309846\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.546506121887758\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 59.02962962962963\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4690371562420337\n",
      "  episode_reward_mean: 0.09766188110129656\n",
      "  episode_reward_min: -1.2118118334977326\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 23585\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7055134615898132\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01841324419528246\n",
      "          policy_loss: -0.10100243118777871\n",
      "          total_loss: 0.187482110414654\n",
      "          vf_explained_var: 0.5016636252403259\n",
      "          vf_loss: 0.2698411294221878\n",
      "    num_agent_steps_sampled: 6841152\n",
      "    num_agent_steps_trained: 6841152\n",
      "    num_steps_sampled: 1710288\n",
      "    num_steps_trained: 1710288\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.977777777777785\n",
      "    gpu_util_percent0: 0.13229166666666667\n",
      "    ram_util_percent: 94.55416666666666\n",
      "    vram_util_percent0: 0.530505054659793\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9802549673065417\n",
      "  policy_reward_mean:\n",
      "    main: 0.024415470275324133\n",
      "  policy_reward_min:\n",
      "    main: -1.7399372670896858\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032969391814446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.88094656976734\n",
      "    mean_inference_ms: 4.405731705111805\n",
      "    mean_raw_obs_processing_ms: 1.295788277078945\n",
      "  time_since_restore: 24519.393729686737\n",
      "  time_this_iter_s: 110.89596390724182\n",
      "  time_total_s: 24519.393729686737\n",
      "  timers:\n",
      "    learn_throughput: 92.611\n",
      "    learn_time_ms: 86296.891\n",
      "    sample_throughput: 307.008\n",
      "    sample_time_ms: 26031.927\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1639182747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1710288\n",
      "  training_iteration: 214\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         24519.4</td><td style=\"text-align: right;\">1710288</td><td style=\"text-align: right;\">0.0976619</td><td style=\"text-align: right;\">             1.46904</td><td style=\"text-align: right;\">            -1.21181</td><td style=\"text-align: right;\">           59.0296</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6873120\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8153750503104372\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10715902287588815\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.777226827392601\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-34-18\n",
      "  done: false\n",
      "  episode_len_mean: 59.859154929577464\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5540408986510945\n",
      "  episode_reward_mean: 0.16027469378717155\n",
      "  episode_reward_min: -1.2660369764606938\n",
      "  episodes_this_iter: 142\n",
      "  episodes_total: 23727\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7009641768932342\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01792060324922204\n",
      "          policy_loss: -0.10041654788702727\n",
      "          total_loss: 0.18530680759251117\n",
      "          vf_explained_var: 0.5403836965560913\n",
      "          vf_loss: 0.267578743994236\n",
      "    num_agent_steps_sampled: 6873120\n",
      "    num_agent_steps_trained: 6873120\n",
      "    num_steps_sampled: 1718280\n",
      "    num_steps_trained: 1718280\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.774125874125865\n",
      "    gpu_util_percent0: 0.13132867132867135\n",
      "    ram_util_percent: 94.50279720279721\n",
      "    vram_util_percent0: 0.5251336356963207\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.96850565296733\n",
      "  policy_reward_mean:\n",
      "    main: 0.040068673446792887\n",
      "  policy_reward_min:\n",
      "    main: -1.7821240469328608\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031430273662777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.87412421857575\n",
      "    mean_inference_ms: 4.403487989494686\n",
      "    mean_raw_obs_processing_ms: 1.2960283385689513\n",
      "  time_since_restore: 24630.178929567337\n",
      "  time_this_iter_s: 110.78519988059998\n",
      "  time_total_s: 24630.178929567337\n",
      "  timers:\n",
      "    learn_throughput: 92.726\n",
      "    learn_time_ms: 86189.374\n",
      "    sample_throughput: 307.117\n",
      "    sample_time_ms: 26022.678\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1639182858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1718280\n",
      "  training_iteration: 215\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         24630.2</td><td style=\"text-align: right;\">1718280</td><td style=\"text-align: right;\">0.160275</td><td style=\"text-align: right;\">             1.55404</td><td style=\"text-align: right;\">            -1.26604</td><td style=\"text-align: right;\">           59.8592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6905088\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8074688814332172\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08085246551824822\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5587654308846043\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-36-09\n",
      "  done: false\n",
      "  episode_len_mean: 52.1578947368421\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6455423308862267\n",
      "  episode_reward_mean: 0.18424787331607093\n",
      "  episode_reward_min: -0.9586775361759581\n",
      "  episodes_this_iter: 152\n",
      "  episodes_total: 23879\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.700858094215393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01823213281109929\n",
      "          policy_loss: -0.10181450021639467\n",
      "          total_loss: 0.1991736472696066\n",
      "          vf_explained_var: 0.5290791988372803\n",
      "          vf_loss: 0.2825281108021736\n",
      "    num_agent_steps_sampled: 6905088\n",
      "    num_agent_steps_trained: 6905088\n",
      "    num_steps_sampled: 1726272\n",
      "    num_steps_trained: 1726272\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.83888888888889\n",
      "    gpu_util_percent0: 0.13270833333333332\n",
      "    ram_util_percent: 94.72152777777778\n",
      "    vram_util_percent0: 0.5175770538554348\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9460371351099608\n",
      "  policy_reward_mean:\n",
      "    main: 0.04606196832901773\n",
      "  policy_reward_min:\n",
      "    main: -1.568326095321654\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30302835956440183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.870185746668987\n",
      "    mean_inference_ms: 4.4025811952273015\n",
      "    mean_raw_obs_processing_ms: 1.29626569384235\n",
      "  time_since_restore: 24740.754771709442\n",
      "  time_this_iter_s: 110.5758421421051\n",
      "  time_total_s: 24740.754771709442\n",
      "  timers:\n",
      "    learn_throughput: 92.891\n",
      "    learn_time_ms: 86036.663\n",
      "    sample_throughput: 307.752\n",
      "    sample_time_ms: 25968.949\n",
      "    update_time_ms: 2.988\n",
      "  timestamp: 1639182969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1726272\n",
      "  training_iteration: 216\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         24740.8</td><td style=\"text-align: right;\">1726272</td><td style=\"text-align: right;\">0.184248</td><td style=\"text-align: right;\">             1.64554</td><td style=\"text-align: right;\">           -0.958678</td><td style=\"text-align: right;\">           52.1579</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6937056\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7834902045301706\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06463363219298898\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5144293176872412\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 58.05343511450382\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2546250546840025\n",
      "  episode_reward_mean: 0.11095756568022705\n",
      "  episode_reward_min: -0.9114199508161245\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 24010\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7044024934768677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018196208797395228\n",
      "          policy_loss: -0.10043917552009225\n",
      "          total_loss: 0.18924978272616863\n",
      "          vf_explained_var: 0.5015947818756104\n",
      "          vf_loss: 0.2712652968168259\n",
      "    num_agent_steps_sampled: 6937056\n",
      "    num_agent_steps_trained: 6937056\n",
      "    num_steps_sampled: 1734264\n",
      "    num_steps_trained: 1734264\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.306338028169016\n",
      "    gpu_util_percent0: 0.13154929577464788\n",
      "    ram_util_percent: 94.66760563380281\n",
      "    vram_util_percent0: 0.5166740820043658\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8649995178534495\n",
      "  policy_reward_mean:\n",
      "    main: 0.027739391420056755\n",
      "  policy_reward_min:\n",
      "    main: -1.8192105997999028\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3026959945290432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.857290612362753\n",
      "    mean_inference_ms: 4.398616885235071\n",
      "    mean_raw_obs_processing_ms: 1.2961634779417504\n",
      "  time_since_restore: 24850.6136405468\n",
      "  time_this_iter_s: 109.85886883735657\n",
      "  time_total_s: 24850.6136405468\n",
      "  timers:\n",
      "    learn_throughput: 93.097\n",
      "    learn_time_ms: 85846.255\n",
      "    sample_throughput: 308.206\n",
      "    sample_time_ms: 25930.708\n",
      "    update_time_ms: 3.055\n",
      "  timestamp: 1639183079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1734264\n",
      "  training_iteration: 217\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         24850.6</td><td style=\"text-align: right;\">1734264</td><td style=\"text-align: right;\">0.110958</td><td style=\"text-align: right;\">             1.25463</td><td style=\"text-align: right;\">            -0.91142</td><td style=\"text-align: right;\">           58.0534</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 6969024\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8085831654271352\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05197897896455986\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7906724580091812\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-39-49\n",
      "  done: false\n",
      "  episode_len_mean: 69.23333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6084582142652866\n",
      "  episode_reward_mean: 0.06340145566913687\n",
      "  episode_reward_min: -1.1088365745398152\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 24130\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7046656746864319\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018139296151697636\n",
      "          policy_loss: -0.1002706610262394\n",
      "          total_loss: 0.13893225169181825\n",
      "          vf_explained_var: 0.5477412343025208\n",
      "          vf_loss: 0.2208368738293648\n",
      "    num_agent_steps_sampled: 6969024\n",
      "    num_agent_steps_trained: 6969024\n",
      "    num_steps_sampled: 1742256\n",
      "    num_steps_trained: 1742256\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.78194444444445\n",
      "    gpu_util_percent0: 0.13118055555555555\n",
      "    ram_util_percent: 94.71458333333332\n",
      "    vram_util_percent0: 0.5150657197177434\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8227274453202362\n",
      "  policy_reward_mean:\n",
      "    main: 0.01585036391728422\n",
      "  policy_reward_min:\n",
      "    main: -1.790672458009181\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032633143305272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.866386725116648\n",
      "    mean_inference_ms: 4.401811707626215\n",
      "    mean_raw_obs_processing_ms: 1.2965028175908544\n",
      "  time_since_restore: 24961.132940530777\n",
      "  time_this_iter_s: 110.51929998397827\n",
      "  time_total_s: 24961.132940530777\n",
      "  timers:\n",
      "    learn_throughput: 93.307\n",
      "    learn_time_ms: 85652.503\n",
      "    sample_throughput: 308.416\n",
      "    sample_time_ms: 25913.02\n",
      "    update_time_ms: 3.054\n",
      "  timestamp: 1639183189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1742256\n",
      "  training_iteration: 218\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         24961.1</td><td style=\"text-align: right;\">1742256</td><td style=\"text-align: right;\">0.0634015</td><td style=\"text-align: right;\">             1.60846</td><td style=\"text-align: right;\">            -1.10884</td><td style=\"text-align: right;\">           69.2333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7000992\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6582507072998811\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08115297689471317\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6248413977727573\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 62.853658536585364\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.542111347173448\n",
      "  episode_reward_mean: 0.1803675419819834\n",
      "  episode_reward_min: -0.9387173863104756\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 24253\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6951756830215454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01834162527322769\n",
      "          policy_loss: -0.10219860207661986\n",
      "          total_loss: 0.19323257764428853\n",
      "          vf_explained_var: 0.4766799211502075\n",
      "          vf_loss: 0.2768602837324142\n",
      "    num_agent_steps_sampled: 7000992\n",
      "    num_agent_steps_trained: 7000992\n",
      "    num_steps_sampled: 1750248\n",
      "    num_steps_trained: 1750248\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.252777777777776\n",
      "    gpu_util_percent0: 0.13249999999999998\n",
      "    ram_util_percent: 94.82152777777777\n",
      "    vram_util_percent0: 0.5152005411136705\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.916199030967121\n",
      "  policy_reward_mean:\n",
      "    main: 0.04509188549549586\n",
      "  policy_reward_min:\n",
      "    main: -1.6275938827000682\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30294342898102616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.859827102815792\n",
      "    mean_inference_ms: 4.400436944492473\n",
      "    mean_raw_obs_processing_ms: 1.2962759999397482\n",
      "  time_since_restore: 25071.890261888504\n",
      "  time_this_iter_s: 110.75732135772705\n",
      "  time_total_s: 25071.890261888504\n",
      "  timers:\n",
      "    learn_throughput: 93.474\n",
      "    learn_time_ms: 85499.524\n",
      "    sample_throughput: 308.337\n",
      "    sample_time_ms: 25919.683\n",
      "    update_time_ms: 3.042\n",
      "  timestamp: 1639183300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1750248\n",
      "  training_iteration: 219\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         25071.9</td><td style=\"text-align: right;\">1750248</td><td style=\"text-align: right;\">0.180368</td><td style=\"text-align: right;\">             1.54211</td><td style=\"text-align: right;\">           -0.938717</td><td style=\"text-align: right;\">           62.8537</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7032960\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7982308013308559\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0558579650821901\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5616629417529944\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-43-32\n",
      "  done: false\n",
      "  episode_len_mean: 68.29411764705883\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.1900629287699633\n",
      "  episode_reward_mean: 0.1069614728739123\n",
      "  episode_reward_min: -1.5228397666317384\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 24372\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.7001201612949371\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01831368860602379\n",
      "          policy_loss: -0.1004838819950819\n",
      "          total_loss: 0.13926313659176232\n",
      "          vf_explained_var: 0.5515769124031067\n",
      "          vf_loss: 0.22120440798997879\n",
      "    num_agent_steps_sampled: 7032960\n",
      "    num_agent_steps_trained: 7032960\n",
      "    num_steps_sampled: 1758240\n",
      "    num_steps_trained: 1758240\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.296527777777776\n",
      "    gpu_util_percent0: 0.1298611111111111\n",
      "    ram_util_percent: 94.82361111111112\n",
      "    vram_util_percent0: 0.5151605517165735\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0198329939155464\n",
      "  policy_reward_mean:\n",
      "    main: 0.02674036821847808\n",
      "  policy_reward_min:\n",
      "    main: -2.0535185720171203\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3026221242661103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.849798169411837\n",
      "    mean_inference_ms: 4.396464803702212\n",
      "    mean_raw_obs_processing_ms: 1.2963001420815257\n",
      "  time_since_restore: 25183.921721220016\n",
      "  time_this_iter_s: 112.03145933151245\n",
      "  time_total_s: 25183.921721220016\n",
      "  timers:\n",
      "    learn_throughput: 93.595\n",
      "    learn_time_ms: 85389.158\n",
      "    sample_throughput: 307.215\n",
      "    sample_time_ms: 26014.38\n",
      "    update_time_ms: 3.048\n",
      "  timestamp: 1639183412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1758240\n",
      "  training_iteration: 220\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         25183.9</td><td style=\"text-align: right;\">1758240</td><td style=\"text-align: right;\">0.106961</td><td style=\"text-align: right;\">             2.19006</td><td style=\"text-align: right;\">            -1.52284</td><td style=\"text-align: right;\">           68.2941</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7064928\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8147384192870939\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08535416038180932\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4999106585823296\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-45-23\n",
      "  done: false\n",
      "  episode_len_mean: 59.6764705882353\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1458089751098717\n",
      "  episode_reward_mean: 0.07427492761603864\n",
      "  episode_reward_min: -1.5071731268761495\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 24508\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6880358655452729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01837816895917058\n",
      "          policy_loss: -0.10135942769423127\n",
      "          total_loss: 0.18231304661929607\n",
      "          vf_explained_var: 0.5189111232757568\n",
      "          vf_loss: 0.265064577460289\n",
      "    num_agent_steps_sampled: 7064928\n",
      "    num_agent_steps_trained: 7064928\n",
      "    num_steps_sampled: 1766232\n",
      "    num_steps_trained: 1766232\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.784027777777776\n",
      "    gpu_util_percent0: 0.12958333333333333\n",
      "    ram_util_percent: 94.95138888888889\n",
      "    vram_util_percent0: 0.5151411282951263\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8007221909157938\n",
      "  policy_reward_mean:\n",
      "    main: 0.01856873190400966\n",
      "  policy_reward_min:\n",
      "    main: -1.8328155091513532\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032031333608881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.863418013379228\n",
      "    mean_inference_ms: 4.402039815621599\n",
      "    mean_raw_obs_processing_ms: 1.2966982610282052\n",
      "  time_since_restore: 25294.83896756172\n",
      "  time_this_iter_s: 110.91724634170532\n",
      "  time_total_s: 25294.83896756172\n",
      "  timers:\n",
      "    learn_throughput: 93.734\n",
      "    learn_time_ms: 85262.861\n",
      "    sample_throughput: 307.589\n",
      "    sample_time_ms: 25982.687\n",
      "    update_time_ms: 3.045\n",
      "  timestamp: 1639183523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1766232\n",
      "  training_iteration: 221\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         25294.8</td><td style=\"text-align: right;\">1766232</td><td style=\"text-align: right;\">0.0742749</td><td style=\"text-align: right;\">             1.14581</td><td style=\"text-align: right;\">            -1.50717</td><td style=\"text-align: right;\">           59.6765</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7096896\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8075151313217074\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07393180952423607\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6184956689516838\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 63.46341463414634\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1363660356931422\n",
      "  episode_reward_mean: 0.1404625973304141\n",
      "  episode_reward_min: -0.7911226294368221\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 24631\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.692458987236023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018342729281634093\n",
      "          policy_loss: -0.10095310690067709\n",
      "          total_loss: 0.1918721370473504\n",
      "          vf_explained_var: 0.4704076051712036\n",
      "          vf_loss: 0.27425322920084\n",
      "    num_agent_steps_sampled: 7096896\n",
      "    num_agent_steps_trained: 7096896\n",
      "    num_steps_sampled: 1774224\n",
      "    num_steps_trained: 1774224\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.58541666666666\n",
      "    gpu_util_percent0: 0.1307638888888889\n",
      "    ram_util_percent: 94.94236111111111\n",
      "    vram_util_percent0: 0.5151502687287485\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9338849390157127\n",
      "  policy_reward_mean:\n",
      "    main: 0.035115649332603534\n",
      "  policy_reward_min:\n",
      "    main: -1.6716609293491587\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30259619426736034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.84474370577594\n",
      "    mean_inference_ms: 4.3958420751425695\n",
      "    mean_raw_obs_processing_ms: 1.2964167340816417\n",
      "  time_since_restore: 25406.327999830246\n",
      "  time_this_iter_s: 111.48903226852417\n",
      "  time_total_s: 25406.327999830246\n",
      "  timers:\n",
      "    learn_throughput: 94.223\n",
      "    learn_time_ms: 84820.456\n",
      "    sample_throughput: 310.158\n",
      "    sample_time_ms: 25767.509\n",
      "    update_time_ms: 3.012\n",
      "  timestamp: 1639183635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1774224\n",
      "  training_iteration: 222\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         25406.3</td><td style=\"text-align: right;\">1774224</td><td style=\"text-align: right;\">0.140463</td><td style=\"text-align: right;\">             1.13637</td><td style=\"text-align: right;\">           -0.791123</td><td style=\"text-align: right;\">           63.4634</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7128864\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7550923293213093\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06471701397864221\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6250973624977455\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-49-06\n",
      "  done: false\n",
      "  episode_len_mean: 64.488\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4796464686662039\n",
      "  episode_reward_mean: 0.12270015168563875\n",
      "  episode_reward_min: -1.1559589409273474\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 24756\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6976969771385193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01843933491781354\n",
      "          policy_loss: -0.10123704774864017\n",
      "          total_loss: 0.16947926821559667\n",
      "          vf_explained_var: 0.5208960175514221\n",
      "          vf_loss: 0.25204648745059965\n",
      "    num_agent_steps_sampled: 7128864\n",
      "    num_agent_steps_trained: 7128864\n",
      "    num_steps_sampled: 1782216\n",
      "    num_steps_trained: 1782216\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.367132867132867\n",
      "    gpu_util_percent0: 0.1312587412587413\n",
      "    ram_util_percent: 94.65664335664337\n",
      "    vram_util_percent0: 0.515150364607423\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8276379749436578\n",
      "  policy_reward_mean:\n",
      "    main: 0.030675037921409676\n",
      "  policy_reward_min:\n",
      "    main: -1.7331893559008418\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031405984694394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.8569715207314\n",
      "    mean_inference_ms: 4.400853452285087\n",
      "    mean_raw_obs_processing_ms: 1.296686235708759\n",
      "  time_since_restore: 25516.891140699387\n",
      "  time_this_iter_s: 110.56314086914062\n",
      "  time_total_s: 25516.891140699387\n",
      "  timers:\n",
      "    learn_throughput: 94.293\n",
      "    learn_time_ms: 84757.176\n",
      "    sample_throughput: 310.631\n",
      "    sample_time_ms: 25728.271\n",
      "    update_time_ms: 3.002\n",
      "  timestamp: 1639183746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1782216\n",
      "  training_iteration: 223\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         25516.9</td><td style=\"text-align: right;\">1782216</td><td style=\"text-align: right;\">  0.1227</td><td style=\"text-align: right;\">             1.47965</td><td style=\"text-align: right;\">            -1.15596</td><td style=\"text-align: right;\">            64.488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7160832\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9580889224039272\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07019888080771297\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5077522973390485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 58.992592592592594\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6227265669786508\n",
      "  episode_reward_mean: 0.1057243670237602\n",
      "  episode_reward_min: -1.0711601395044545\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 24891\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6854904997348785\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018268907483667135\n",
      "          policy_loss: -0.09915027761645615\n",
      "          total_loss: 0.17992213489487768\n",
      "          vf_explained_var: 0.5337033867835999\n",
      "          vf_loss: 0.2605751439929008\n",
      "    num_agent_steps_sampled: 7160832\n",
      "    num_agent_steps_trained: 7160832\n",
      "    num_steps_sampled: 1790208\n",
      "    num_steps_trained: 1790208\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.555172413793102\n",
      "    gpu_util_percent0: 0.13027586206896552\n",
      "    ram_util_percent: 94.8393103448276\n",
      "    vram_util_percent0: 0.5151456354744641\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.063048644682753\n",
      "  policy_reward_mean:\n",
      "    main: 0.02643109175594005\n",
      "  policy_reward_min:\n",
      "    main: -1.7400949497457319\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30266973881430764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.841578914124117\n",
      "    mean_inference_ms: 4.3954380280931575\n",
      "    mean_raw_obs_processing_ms: 1.2968230593258196\n",
      "  time_since_restore: 25629.049310684204\n",
      "  time_this_iter_s: 112.1581699848175\n",
      "  time_total_s: 25629.049310684204\n",
      "  timers:\n",
      "    learn_throughput: 94.172\n",
      "    learn_time_ms: 84866.39\n",
      "    sample_throughput: 310.387\n",
      "    sample_time_ms: 25748.516\n",
      "    update_time_ms: 2.878\n",
      "  timestamp: 1639183858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1790208\n",
      "  training_iteration: 224\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">           25629</td><td style=\"text-align: right;\">1790208</td><td style=\"text-align: right;\">0.105724</td><td style=\"text-align: right;\">             1.62273</td><td style=\"text-align: right;\">            -1.07116</td><td style=\"text-align: right;\">           58.9926</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7192800\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9660661654100143\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07740807610839841\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.116694560243697\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-52-49\n",
      "  done: false\n",
      "  episode_len_mean: 63.951219512195124\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.288173959946672\n",
      "  episode_reward_mean: 0.15141127492808998\n",
      "  episode_reward_min: -1.3133698447512043\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 25014\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6898350551128387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018562828235328198\n",
      "          policy_loss: -0.10169415567815304\n",
      "          total_loss: 0.17478270414471625\n",
      "          vf_explained_var: 0.5082874298095703\n",
      "          vf_loss: 0.257681994497776\n",
      "    num_agent_steps_sampled: 7192800\n",
      "    num_agent_steps_trained: 7192800\n",
      "    num_steps_sampled: 1798200\n",
      "    num_steps_trained: 1798200\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.502097902097905\n",
      "    gpu_util_percent0: 0.13146853146853146\n",
      "    ram_util_percent: 94.69160839160841\n",
      "    vram_util_percent0: 0.5152101929002224\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9027975966133328\n",
      "  policy_reward_mean:\n",
      "    main: 0.03785281873202248\n",
      "  policy_reward_min:\n",
      "    main: -2.116694560243697\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30261282525555616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.839214959809745\n",
      "    mean_inference_ms: 4.395116604733125\n",
      "    mean_raw_obs_processing_ms: 1.2968043908183575\n",
      "  time_since_restore: 25739.77344059944\n",
      "  time_this_iter_s: 110.72412991523743\n",
      "  time_total_s: 25739.77344059944\n",
      "  timers:\n",
      "    learn_throughput: 94.132\n",
      "    learn_time_ms: 84902.445\n",
      "    sample_throughput: 310.895\n",
      "    sample_time_ms: 25706.408\n",
      "    update_time_ms: 2.873\n",
      "  timestamp: 1639183969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1798200\n",
      "  training_iteration: 225\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         25739.8</td><td style=\"text-align: right;\">1798200</td><td style=\"text-align: right;\">0.151411</td><td style=\"text-align: right;\">             1.28817</td><td style=\"text-align: right;\">            -1.31337</td><td style=\"text-align: right;\">           63.9512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7224768\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7884858842323236\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07380560289887021\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5871653278726663\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 68.59166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1367895562017856\n",
      "  episode_reward_mean: 0.14786241723571186\n",
      "  episode_reward_min: -1.006399595309615\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 25134\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6862305314540863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01842881354317069\n",
      "          policy_loss: -0.10022512100636959\n",
      "          total_loss: 0.18417391845583916\n",
      "          vf_explained_var: 0.4923124313354492\n",
      "          vf_loss: 0.2657398664951324\n",
      "    num_agent_steps_sampled: 7224768\n",
      "    num_agent_steps_trained: 7224768\n",
      "    num_steps_sampled: 1806192\n",
      "    num_steps_trained: 1806192\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.557342657342662\n",
      "    gpu_util_percent0: 0.13132867132867135\n",
      "    ram_util_percent: 94.75734265734268\n",
      "    vram_util_percent0: 0.5151975369152071\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.868629022982494\n",
      "  policy_reward_mean:\n",
      "    main: 0.03696560430892795\n",
      "  policy_reward_min:\n",
      "    main: -1.5871653278726663\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30291645385186833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.843789749109746\n",
      "    mean_inference_ms: 4.3970998814971\n",
      "    mean_raw_obs_processing_ms: 1.296716067288916\n",
      "  time_since_restore: 25850.59921693802\n",
      "  time_this_iter_s: 110.82577633857727\n",
      "  time_total_s: 25850.59921693802\n",
      "  timers:\n",
      "    learn_throughput: 94.085\n",
      "    learn_time_ms: 84944.068\n",
      "    sample_throughput: 311.02\n",
      "    sample_time_ms: 25696.136\n",
      "    update_time_ms: 2.91\n",
      "  timestamp: 1639184079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1806192\n",
      "  training_iteration: 226\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         25850.6</td><td style=\"text-align: right;\">1806192</td><td style=\"text-align: right;\">0.147862</td><td style=\"text-align: right;\">             1.13679</td><td style=\"text-align: right;\">             -1.0064</td><td style=\"text-align: right;\">           68.5917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7256736\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6747834310594484\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.030085630159588008\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7792198217808122\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 70.26548672566372\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1564005092798055\n",
      "  episode_reward_mean: -0.01824557365603832\n",
      "  episode_reward_min: -2.2368831610848305\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 25247\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6885086243152618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01866606453806162\n",
      "          policy_loss: -0.10260542245209217\n",
      "          total_loss: 0.16264383279532194\n",
      "          vf_explained_var: 0.4802123010158539\n",
      "          vf_loss: 0.24634986370801926\n",
      "    num_agent_steps_sampled: 7256736\n",
      "    num_agent_steps_trained: 7256736\n",
      "    num_steps_sampled: 1814184\n",
      "    num_steps_trained: 1814184\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.548251748251747\n",
      "    gpu_util_percent0: 0.13146853146853146\n",
      "    ram_util_percent: 94.80629370629373\n",
      "    vram_util_percent0: 0.5150836330500695\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.935125651020916\n",
      "  policy_reward_mean:\n",
      "    main: -0.004561393414009578\n",
      "  policy_reward_min:\n",
      "    main: -1.7487926511707967\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.302895895945599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.845555106156436\n",
      "    mean_inference_ms: 4.39931017262579\n",
      "    mean_raw_obs_processing_ms: 1.2963392909166618\n",
      "  time_since_restore: 25960.992483854294\n",
      "  time_this_iter_s: 110.39326691627502\n",
      "  time_total_s: 25960.992483854294\n",
      "  timers:\n",
      "    learn_throughput: 94.035\n",
      "    learn_time_ms: 84989.326\n",
      "    sample_throughput: 310.932\n",
      "    sample_time_ms: 25703.336\n",
      "    update_time_ms: 2.84\n",
      "  timestamp: 1639184190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1814184\n",
      "  training_iteration: 227\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">           25961</td><td style=\"text-align: right;\">1814184</td><td style=\"text-align: right;\">-0.0182456</td><td style=\"text-align: right;\">              1.1564</td><td style=\"text-align: right;\">            -2.23688</td><td style=\"text-align: right;\">           70.2655</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7288704\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9582170319554817\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10337328914028576\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49083816444110795\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_21-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 61.784615384615385\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3615999098039528\n",
      "  episode_reward_mean: 0.1976844064455411\n",
      "  episode_reward_min: -1.3994982315519735\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 25377\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6792340457439423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018665407460182904\n",
      "          policy_loss: -0.10172900356724858\n",
      "          total_loss: 0.18989284570142625\n",
      "          vf_explained_var: 0.5028148293495178\n",
      "          vf_loss: 0.2727231248021126\n",
      "    num_agent_steps_sampled: 7288704\n",
      "    num_agent_steps_trained: 7288704\n",
      "    num_steps_sampled: 1822176\n",
      "    num_steps_trained: 1822176\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.514583333333334\n",
      "    gpu_util_percent0: 0.13118055555555555\n",
      "    ram_util_percent: 94.85416666666667\n",
      "    vram_util_percent0: 0.5152062538846843\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9187394408032565\n",
      "  policy_reward_mean:\n",
      "    main: 0.04942110161138528\n",
      "  policy_reward_min:\n",
      "    main: -1.9779663156895824\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3025842624807127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.82958877588474\n",
      "    mean_inference_ms: 4.393431547693203\n",
      "    mean_raw_obs_processing_ms: 1.2964664917151727\n",
      "  time_since_restore: 26071.939077854156\n",
      "  time_this_iter_s: 110.94659399986267\n",
      "  time_total_s: 26071.939077854156\n",
      "  timers:\n",
      "    learn_throughput: 93.955\n",
      "    learn_time_ms: 85061.988\n",
      "    sample_throughput: 311.359\n",
      "    sample_time_ms: 25668.159\n",
      "    update_time_ms: 2.848\n",
      "  timestamp: 1639184301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1822176\n",
      "  training_iteration: 228\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         26071.9</td><td style=\"text-align: right;\">1822176</td><td style=\"text-align: right;\">0.197684</td><td style=\"text-align: right;\">              1.3616</td><td style=\"text-align: right;\">             -1.3995</td><td style=\"text-align: right;\">           61.7846</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7320672\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7272176287814502\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06846243066549457\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.579720353000593\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 59.06153846153846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4097549634485014\n",
      "  episode_reward_mean: 0.11064569372021947\n",
      "  episode_reward_min: -0.9629107363097158\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 25507\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6842001292705536\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018272700130939484\n",
      "          policy_loss: -0.10140735357627273\n",
      "          total_loss: 0.15635536137223244\n",
      "          vf_explained_var: 0.542290449142456\n",
      "          vf_loss: 0.2392616041302681\n",
      "    num_agent_steps_sampled: 7320672\n",
      "    num_agent_steps_trained: 7320672\n",
      "    num_steps_sampled: 1830168\n",
      "    num_steps_trained: 1830168\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.709722222222222\n",
      "    gpu_util_percent0: 0.1307638888888889\n",
      "    ram_util_percent: 94.80486111111112\n",
      "    vram_util_percent0: 0.5151948283426565\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.721445033431275\n",
      "  policy_reward_mean:\n",
      "    main: 0.027661423430054885\n",
      "  policy_reward_min:\n",
      "    main: -1.594107017492251\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030074100912325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.839267545179553\n",
      "    mean_inference_ms: 4.397869103662035\n",
      "    mean_raw_obs_processing_ms: 1.2966636131607756\n",
      "  time_since_restore: 26183.187393903732\n",
      "  time_this_iter_s: 111.2483160495758\n",
      "  time_total_s: 26183.187393903732\n",
      "  timers:\n",
      "    learn_throughput: 93.879\n",
      "    learn_time_ms: 85130.994\n",
      "    sample_throughput: 311.583\n",
      "    sample_time_ms: 25649.706\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1639184412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1830168\n",
      "  training_iteration: 229\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         26183.2</td><td style=\"text-align: right;\">1830168</td><td style=\"text-align: right;\">0.110646</td><td style=\"text-align: right;\">             1.40975</td><td style=\"text-align: right;\">           -0.962911</td><td style=\"text-align: right;\">           59.0615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7352640\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1864398398006923\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.058402606225518405\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8150131770947856\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 62.8421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.531396514216072\n",
      "  episode_reward_mean: 0.04267189079860399\n",
      "  episode_reward_min: -1.1575026369015577\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 25640\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6811867773532867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01868172061815858\n",
      "          policy_loss: -0.1008420070335269\n",
      "          total_loss: 0.18472300427407026\n",
      "          vf_explained_var: 0.5123403072357178\n",
      "          vf_loss: 0.2666497681736946\n",
      "    num_agent_steps_sampled: 7352640\n",
      "    num_agent_steps_trained: 7352640\n",
      "    num_steps_sampled: 1838160\n",
      "    num_steps_trained: 1838160\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.493055555555557\n",
      "    gpu_util_percent0: 0.12986111111111112\n",
      "    ram_util_percent: 94.94444444444444\n",
      "    vram_util_percent0: 0.5151891155716428\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.015068413899552\n",
      "  policy_reward_mean:\n",
      "    main: 0.010667972699650996\n",
      "  policy_reward_min:\n",
      "    main: -2.0153694141584215\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30265524272028654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.8281567727943\n",
      "    mean_inference_ms: 4.394167071366502\n",
      "    mean_raw_obs_processing_ms: 1.296529431200818\n",
      "  time_since_restore: 26294.677444458008\n",
      "  time_this_iter_s: 111.49005055427551\n",
      "  time_total_s: 26294.677444458008\n",
      "  timers:\n",
      "    learn_throughput: 93.84\n",
      "    learn_time_ms: 85166.007\n",
      "    sample_throughput: 312.716\n",
      "    sample_time_ms: 25556.773\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1639184524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1838160\n",
      "  training_iteration: 230\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         26294.7</td><td style=\"text-align: right;\">1838160</td><td style=\"text-align: right;\">0.0426719</td><td style=\"text-align: right;\">              1.5314</td><td style=\"text-align: right;\">             -1.1575</td><td style=\"text-align: right;\">           62.8421</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7384608\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9184277990160296\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09481778835984794\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6797176158267736\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 59.92481203007519\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.403301949325401\n",
      "  episode_reward_mean: 0.23515577703482654\n",
      "  episode_reward_min: -1.2184142125676276\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 25773\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6745049769878387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018307975076138974\n",
      "          policy_loss: -0.10087964946776629\n",
      "          total_loss: 0.18262642082944514\n",
      "          vf_explained_var: 0.5321718454360962\n",
      "          vf_loss: 0.2649692450165749\n",
      "    num_agent_steps_sampled: 7384608\n",
      "    num_agent_steps_trained: 7384608\n",
      "    num_steps_sampled: 1846152\n",
      "    num_steps_trained: 1846152\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.793055555555554\n",
      "    gpu_util_percent0: 0.13291666666666668\n",
      "    ram_util_percent: 94.81875\n",
      "    vram_util_percent0: 0.5151102793316515\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.908205799446229\n",
      "  policy_reward_mean:\n",
      "    main: 0.058788944258706635\n",
      "  policy_reward_min:\n",
      "    main: -1.7819212785024776\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3026215635837119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.830081541034005\n",
      "    mean_inference_ms: 4.395357147955419\n",
      "    mean_raw_obs_processing_ms: 1.296457825135425\n",
      "  time_since_restore: 26406.181220293045\n",
      "  time_this_iter_s: 111.50377583503723\n",
      "  time_total_s: 26406.181220293045\n",
      "  timers:\n",
      "    learn_throughput: 93.795\n",
      "    learn_time_ms: 85206.684\n",
      "    sample_throughput: 312.51\n",
      "    sample_time_ms: 25573.575\n",
      "    update_time_ms: 2.885\n",
      "  timestamp: 1639184635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1846152\n",
      "  training_iteration: 231\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         26406.2</td><td style=\"text-align: right;\">1846152</td><td style=\"text-align: right;\">0.235156</td><td style=\"text-align: right;\">              1.4033</td><td style=\"text-align: right;\">            -1.21841</td><td style=\"text-align: right;\">           59.9248</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7416576\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8069945262997134\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.059165111779593445\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4786269025572367\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 63.877049180327866\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6959444817742866\n",
      "  episode_reward_mean: 0.19765947460346492\n",
      "  episode_reward_min: -0.701032106826625\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 25895\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6797853264808654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018389665476977825\n",
      "          policy_loss: -0.10062244540080428\n",
      "          total_loss: 0.1696702412441373\n",
      "          vf_explained_var: 0.5090246200561523\n",
      "          vf_loss: 0.25167315095663073\n",
      "    num_agent_steps_sampled: 7416576\n",
      "    num_agent_steps_trained: 7416576\n",
      "    num_steps_sampled: 1854144\n",
      "    num_steps_trained: 1854144\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.517241379310345\n",
      "    gpu_util_percent0: 0.1299310344827586\n",
      "    ram_util_percent: 94.84344827586206\n",
      "    vram_util_percent0: 0.5151467701489828\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7638929229939806\n",
      "  policy_reward_mean:\n",
      "    main: 0.049414868650866244\n",
      "  policy_reward_min:\n",
      "    main: -1.505396654680938\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3024695346030529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.819297541527444\n",
      "    mean_inference_ms: 4.3914198278109255\n",
      "    mean_raw_obs_processing_ms: 1.2965929316376141\n",
      "  time_since_restore: 26518.110476017\n",
      "  time_this_iter_s: 111.92925572395325\n",
      "  time_total_s: 26518.110476017\n",
      "  timers:\n",
      "    learn_throughput: 93.729\n",
      "    learn_time_ms: 85266.869\n",
      "    sample_throughput: 312.702\n",
      "    sample_time_ms: 25557.91\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1639184747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1854144\n",
      "  training_iteration: 232\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         26518.1</td><td style=\"text-align: right;\">1854144</td><td style=\"text-align: right;\">0.197659</td><td style=\"text-align: right;\">             1.69594</td><td style=\"text-align: right;\">           -0.701032</td><td style=\"text-align: right;\">            63.877</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7448544\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.758488791215665\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07428779687897871\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5376094168368639\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 74.3177570093458\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.9630394169868045\n",
      "  episode_reward_mean: 0.053949282915294834\n",
      "  episode_reward_min: -1.8809354288134343\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 26002\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6763628079891205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01836385380104184\n",
      "          policy_loss: -0.10043921453878284\n",
      "          total_loss: 0.16172298543900251\n",
      "          vf_explained_var: 0.46958768367767334\n",
      "          vf_loss: 0.24356879782676696\n",
      "    num_agent_steps_sampled: 7448544\n",
      "    num_agent_steps_trained: 7448544\n",
      "    num_steps_sampled: 1862136\n",
      "    num_steps_trained: 1862136\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.244755244755243\n",
      "    gpu_util_percent0: 0.13384615384615384\n",
      "    ram_util_percent: 94.98531468531468\n",
      "    vram_util_percent0: 0.5150755792414236\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8017474249669592\n",
      "  policy_reward_mean:\n",
      "    main: 0.013487320728823735\n",
      "  policy_reward_min:\n",
      "    main: -1.9126368998793333\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029147895860147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.83034519147207\n",
      "    mean_inference_ms: 4.396433619228271\n",
      "    mean_raw_obs_processing_ms: 1.296357283957077\n",
      "  time_since_restore: 26628.939895391464\n",
      "  time_this_iter_s: 110.82941937446594\n",
      "  time_total_s: 26628.939895391464\n",
      "  timers:\n",
      "    learn_throughput: 93.696\n",
      "    learn_time_ms: 85297.571\n",
      "    sample_throughput: 312.696\n",
      "    sample_time_ms: 25558.362\n",
      "    update_time_ms: 2.925\n",
      "  timestamp: 1639184858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1862136\n",
      "  training_iteration: 233\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         26628.9</td><td style=\"text-align: right;\">1862136</td><td style=\"text-align: right;\">0.0539493</td><td style=\"text-align: right;\">            0.963039</td><td style=\"text-align: right;\">            -1.88094</td><td style=\"text-align: right;\">           74.3178</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7480512\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9065821273246266\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09403095483584024\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.0911136525318927\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-09-36\n",
      "  done: false\n",
      "  episode_len_mean: 64.51587301587301\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2386593136517057\n",
      "  episode_reward_mean: 0.1025895019484336\n",
      "  episode_reward_min: -1.2802411739623318\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 26128\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6653821184635162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01894971550628543\n",
      "          policy_loss: -0.10355575093254447\n",
      "          total_loss: 0.17100865667313336\n",
      "          vf_explained_var: 0.5218490958213806\n",
      "          vf_loss: 0.25537782019376754\n",
      "    num_agent_steps_sampled: 7480512\n",
      "    num_agent_steps_trained: 7480512\n",
      "    num_steps_sampled: 1870128\n",
      "    num_steps_trained: 1870128\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.51315789473684\n",
      "    gpu_util_percent0: 0.17486842105263156\n",
      "    ram_util_percent: 94.9296052631579\n",
      "    vram_util_percent0: 0.5292339931764257\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8502997351622679\n",
      "  policy_reward_mean:\n",
      "    main: 0.0256473754871084\n",
      "  policy_reward_min:\n",
      "    main: -2.095423688510898\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30270276604749086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.825172559187966\n",
      "    mean_inference_ms: 4.393613442930897\n",
      "    mean_raw_obs_processing_ms: 1.2967153451448457\n",
      "  time_since_restore: 26746.161811113358\n",
      "  time_this_iter_s: 117.22191572189331\n",
      "  time_total_s: 26746.161811113358\n",
      "  timers:\n",
      "    learn_throughput: 93.245\n",
      "    learn_time_ms: 85710.122\n",
      "    sample_throughput: 311.529\n",
      "    sample_time_ms: 25654.139\n",
      "    update_time_ms: 2.945\n",
      "  timestamp: 1639184976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1870128\n",
      "  training_iteration: 234\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         26746.2</td><td style=\"text-align: right;\">1870128</td><td style=\"text-align: right;\"> 0.10259</td><td style=\"text-align: right;\">             1.23866</td><td style=\"text-align: right;\">            -1.28024</td><td style=\"text-align: right;\">           64.5159</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7512480\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6577681287015015\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04708956529515675\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49618763381902375\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 58.65413533834587\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.437014576704553\n",
      "  episode_reward_mean: 0.0983671333386803\n",
      "  episode_reward_min: -1.5353785549613328\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 26261\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6682367551326752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01870486557111144\n",
      "          policy_loss: -0.10096903662383556\n",
      "          total_loss: 0.18429741279035808\n",
      "          vf_explained_var: 0.5177104473114014\n",
      "          vf_loss: 0.2663277714252472\n",
      "    num_agent_steps_sampled: 7512480\n",
      "    num_agent_steps_trained: 7512480\n",
      "    num_steps_sampled: 1878120\n",
      "    num_steps_trained: 1878120\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.874000000000002\n",
      "    gpu_util_percent0: 0.17833333333333334\n",
      "    ram_util_percent: 94.86199999999997\n",
      "    vram_util_percent0: 0.5326620598881212\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9488538129599327\n",
      "  policy_reward_mean:\n",
      "    main: 0.024591783334670075\n",
      "  policy_reward_min:\n",
      "    main: -1.9619061165000402\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30284313170335675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.82726413199036\n",
      "    mean_inference_ms: 4.393839545503924\n",
      "    mean_raw_obs_processing_ms: 1.2972875975431652\n",
      "  time_since_restore: 26861.766298532486\n",
      "  time_this_iter_s: 115.60448741912842\n",
      "  time_total_s: 26861.766298532486\n",
      "  timers:\n",
      "    learn_throughput: 92.861\n",
      "    learn_time_ms: 86064.258\n",
      "    sample_throughput: 309.932\n",
      "    sample_time_ms: 25786.28\n",
      "    update_time_ms: 2.985\n",
      "  timestamp: 1639185091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1878120\n",
      "  training_iteration: 235\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         26861.8</td><td style=\"text-align: right;\">1878120</td><td style=\"text-align: right;\">0.0983671</td><td style=\"text-align: right;\">             1.43701</td><td style=\"text-align: right;\">            -1.53538</td><td style=\"text-align: right;\">           58.6541</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7544448\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.937303726436473\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0730198694692965\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7940432637463284\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 63.07462686567164\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2038022552408492\n",
      "  episode_reward_mean: 0.07198172827885167\n",
      "  episode_reward_min: -1.7412017304230463\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 26395\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6666726455688476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018604702189564705\n",
      "          policy_loss: -0.10132227684929968\n",
      "          total_loss: 0.18310982707887888\n",
      "          vf_explained_var: 0.5165843367576599\n",
      "          vf_loss: 0.2655948413014412\n",
      "    num_agent_steps_sampled: 7544448\n",
      "    num_agent_steps_trained: 7544448\n",
      "    num_steps_sampled: 1886112\n",
      "    num_steps_trained: 1886112\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.84736842105263\n",
      "    gpu_util_percent0: 0.18855263157894733\n",
      "    ram_util_percent: 94.79671052631578\n",
      "    vram_util_percent0: 0.5356051159488059\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.872807458703166\n",
      "  policy_reward_mean:\n",
      "    main: 0.017995432069712918\n",
      "  policy_reward_min:\n",
      "    main: -1.9286469117296308\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3025666695349173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.825053335221227\n",
      "    mean_inference_ms: 4.393266763155698\n",
      "    mean_raw_obs_processing_ms: 1.2971823692522189\n",
      "  time_since_restore: 26980.849638700485\n",
      "  time_this_iter_s: 119.08334016799927\n",
      "  time_total_s: 26980.849638700485\n",
      "  timers:\n",
      "    learn_throughput: 92.106\n",
      "    learn_time_ms: 86769.801\n",
      "    sample_throughput: 308.486\n",
      "    sample_time_ms: 25907.189\n",
      "    update_time_ms: 2.926\n",
      "  timestamp: 1639185210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1886112\n",
      "  training_iteration: 236\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         26980.8</td><td style=\"text-align: right;\">1886112</td><td style=\"text-align: right;\">0.0719817</td><td style=\"text-align: right;\">              1.2038</td><td style=\"text-align: right;\">             -1.7412</td><td style=\"text-align: right;\">           63.0746</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7576416\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5761034223578197\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02913697972579112\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6528825410866874\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 61.06153846153846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8685065016710314\n",
      "  episode_reward_mean: 0.12925568039029478\n",
      "  episode_reward_min: -0.9113881295041393\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 26525\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6619699873924255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018823840111494065\n",
      "          policy_loss: -0.10277179590240121\n",
      "          total_loss: 0.1876179485693574\n",
      "          vf_explained_var: 0.5166813135147095\n",
      "          vf_loss: 0.2713306067585945\n",
      "    num_agent_steps_sampled: 7576416\n",
      "    num_agent_steps_trained: 7576416\n",
      "    num_steps_sampled: 1894104\n",
      "    num_steps_trained: 1894104\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.17218543046358\n",
      "    gpu_util_percent0: 0.18556291390728477\n",
      "    ram_util_percent: 94.1364238410596\n",
      "    vram_util_percent0: 0.5320458760179478\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9212686976664606\n",
      "  policy_reward_mean:\n",
      "    main: 0.03231392009757371\n",
      "  policy_reward_min:\n",
      "    main: -1.6528825410866874\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3024694740198178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.82385455536223\n",
      "    mean_inference_ms: 4.392558333344453\n",
      "    mean_raw_obs_processing_ms: 1.2975182373996903\n",
      "  time_since_restore: 27097.606324911118\n",
      "  time_this_iter_s: 116.75668621063232\n",
      "  time_total_s: 27097.606324911118\n",
      "  timers:\n",
      "    learn_throughput: 91.552\n",
      "    learn_time_ms: 87295.053\n",
      "    sample_throughput: 307.18\n",
      "    sample_time_ms: 26017.305\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1639185327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1894104\n",
      "  training_iteration: 237\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         27097.6</td><td style=\"text-align: right;\">1894104</td><td style=\"text-align: right;\">0.129256</td><td style=\"text-align: right;\">             1.86851</td><td style=\"text-align: right;\">           -0.911388</td><td style=\"text-align: right;\">           61.0615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7608384\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.776443221983426\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06901983110091084\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7482316001008463\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 58.52554744525548\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.252004325228726\n",
      "  episode_reward_mean: 0.1480480882442724\n",
      "  episode_reward_min: -1.0747800560965177\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 26662\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6590824880599976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018810600213706494\n",
      "          policy_loss: -0.10125116710364819\n",
      "          total_loss: 0.21120555471628905\n",
      "          vf_explained_var: 0.4883483946323395\n",
      "          vf_loss: 0.29341098707914354\n",
      "    num_agent_steps_sampled: 7608384\n",
      "    num_agent_steps_trained: 7608384\n",
      "    num_steps_sampled: 1902096\n",
      "    num_steps_trained: 1902096\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.76027397260274\n",
      "    gpu_util_percent0: 0.13554794520547944\n",
      "    ram_util_percent: 92.85068493150685\n",
      "    vram_util_percent0: 0.5240447245173476\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0695668713325874\n",
      "  policy_reward_mean:\n",
      "    main: 0.0370120220610681\n",
      "  policy_reward_min:\n",
      "    main: -1.7627397146969077\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3025862446456648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.829513962967077\n",
      "    mean_inference_ms: 4.394282605195575\n",
      "    mean_raw_obs_processing_ms: 1.2979534209751384\n",
      "  time_since_restore: 27210.386110782623\n",
      "  time_this_iter_s: 112.77978587150574\n",
      "  time_total_s: 27210.386110782623\n",
      "  timers:\n",
      "    learn_throughput: 91.494\n",
      "    learn_time_ms: 87349.808\n",
      "    sample_throughput: 305.601\n",
      "    sample_time_ms: 26151.71\n",
      "    update_time_ms: 3.066\n",
      "  timestamp: 1639185440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1902096\n",
      "  training_iteration: 238\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         27210.4</td><td style=\"text-align: right;\">1902096</td><td style=\"text-align: right;\">0.148048</td><td style=\"text-align: right;\">               1.252</td><td style=\"text-align: right;\">            -1.07478</td><td style=\"text-align: right;\">           58.5255</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7640352\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.800294896137697\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07096118882153969\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6464591313394432\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 58.72992700729927\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2514413738732073\n",
      "  episode_reward_mean: 0.1890030646216073\n",
      "  episode_reward_min: -1.440719560768184\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 26799\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.65768235039711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018610079247504473\n",
      "          policy_loss: -0.10206638227775693\n",
      "          total_loss: 0.19762352456152438\n",
      "          vf_explained_var: 0.5028541684150696\n",
      "          vf_loss: 0.2808471994400024\n",
      "    num_agent_steps_sampled: 7640352\n",
      "    num_agent_steps_trained: 7640352\n",
      "    num_steps_sampled: 1910088\n",
      "    num_steps_trained: 1910088\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.34829931972789\n",
      "    gpu_util_percent0: 0.13727891156462585\n",
      "    ram_util_percent: 92.76802721088437\n",
      "    vram_util_percent0: 0.5225604555741349\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.078535689804743\n",
      "  policy_reward_mean:\n",
      "    main: 0.047250766155401816\n",
      "  policy_reward_min:\n",
      "    main: -2.0778267653010634\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30264498137809354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.83447728173317\n",
      "    mean_inference_ms: 4.3934568285230196\n",
      "    mean_raw_obs_processing_ms: 1.2989976917306152\n",
      "  time_since_restore: 27325.01340818405\n",
      "  time_this_iter_s: 114.62729740142822\n",
      "  time_total_s: 27325.01340818405\n",
      "  timers:\n",
      "    learn_throughput: 91.409\n",
      "    learn_time_ms: 87430.747\n",
      "    sample_throughput: 302.568\n",
      "    sample_time_ms: 26413.907\n",
      "    update_time_ms: 3.029\n",
      "  timestamp: 1639185555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1910088\n",
      "  training_iteration: 239\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">           27325</td><td style=\"text-align: right;\">1910088</td><td style=\"text-align: right;\">0.189003</td><td style=\"text-align: right;\">             1.25144</td><td style=\"text-align: right;\">            -1.44072</td><td style=\"text-align: right;\">           58.7299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7672320\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7193850709822371\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07259255078503193\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6198392983923872\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 62.552\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5159232700580834\n",
      "  episode_reward_mean: 0.20269629335848116\n",
      "  episode_reward_min: -0.8148196282474105\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 26924\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6582971694469452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0186650683991611\n",
      "          policy_loss: -0.1007310311831534\n",
      "          total_loss: 0.15329818232357503\n",
      "          vf_explained_var: 0.558272123336792\n",
      "          vf_loss: 0.23513082891702652\n",
      "    num_agent_steps_sampled: 7672320\n",
      "    num_agent_steps_trained: 7672320\n",
      "    num_steps_sampled: 1918080\n",
      "    num_steps_trained: 1918080\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.251369863013696\n",
      "    gpu_util_percent0: 0.138013698630137\n",
      "    ram_util_percent: 92.80753424657533\n",
      "    vram_util_percent0: 0.5205220264416468\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9982643618105753\n",
      "  policy_reward_mean:\n",
      "    main: 0.05067407333962029\n",
      "  policy_reward_min:\n",
      "    main: -1.6635858771897452\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30287310366237036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.842536212259287\n",
      "    mean_inference_ms: 4.3972536446535235\n",
      "    mean_raw_obs_processing_ms: 1.2989885573378461\n",
      "  time_since_restore: 27437.18607854843\n",
      "  time_this_iter_s: 112.17267036437988\n",
      "  time_total_s: 27437.18607854843\n",
      "  timers:\n",
      "    learn_throughput: 91.381\n",
      "    learn_time_ms: 87457.85\n",
      "    sample_throughput: 302.097\n",
      "    sample_time_ms: 26455.061\n",
      "    update_time_ms: 3.013\n",
      "  timestamp: 1639185667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1918080\n",
      "  training_iteration: 240\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         27437.2</td><td style=\"text-align: right;\">1918080</td><td style=\"text-align: right;\">0.202696</td><td style=\"text-align: right;\">             1.51592</td><td style=\"text-align: right;\">            -0.81482</td><td style=\"text-align: right;\">            62.552</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7704288\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1449105058713933\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05835602961781185\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5492304538865619\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 64.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3852174559752144\n",
      "  episode_reward_mean: 0.12192533947462317\n",
      "  episode_reward_min: -0.7474669482630681\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 27049\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6625881123542786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01848279272019863\n",
      "          policy_loss: -0.10044872096925973\n",
      "          total_loss: 0.18556570784002543\n",
      "          vf_explained_var: 0.4879417419433594\n",
      "          vf_loss: 0.2673005995750427\n",
      "    num_agent_steps_sampled: 7704288\n",
      "    num_agent_steps_trained: 7704288\n",
      "    num_steps_sampled: 1926072\n",
      "    num_steps_trained: 1926072\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.432876712328767\n",
      "    gpu_util_percent0: 0.13513698630136986\n",
      "    ram_util_percent: 92.86232876712327\n",
      "    vram_util_percent0: 0.51940977340239\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0594978048624992\n",
      "  policy_reward_mean:\n",
      "    main: 0.030481334868655795\n",
      "  policy_reward_min:\n",
      "    main: -1.8977339176396444\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30294820887991936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.84135542380708\n",
      "    mean_inference_ms: 4.396276708966996\n",
      "    mean_raw_obs_processing_ms: 1.2993441402662957\n",
      "  time_since_restore: 27549.92169046402\n",
      "  time_this_iter_s: 112.73561191558838\n",
      "  time_total_s: 27549.92169046402\n",
      "  timers:\n",
      "    learn_throughput: 91.29\n",
      "    learn_time_ms: 87544.992\n",
      "    sample_throughput: 301.637\n",
      "    sample_time_ms: 26495.389\n",
      "    update_time_ms: 2.998\n",
      "  timestamp: 1639185780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1926072\n",
      "  training_iteration: 241\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         27549.9</td><td style=\"text-align: right;\">1926072</td><td style=\"text-align: right;\">0.121925</td><td style=\"text-align: right;\">             1.38522</td><td style=\"text-align: right;\">           -0.747467</td><td style=\"text-align: right;\">             64.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7736256\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5939351468219266\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.060049901442348196\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5242134749280513\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 59.315384615384616\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5265373454820672\n",
      "  episode_reward_mean: 0.16835338899815672\n",
      "  episode_reward_min: -1.1461545323536464\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 27179\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.648540241241455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0188186368457973\n",
      "          policy_loss: -0.10125575691461564\n",
      "          total_loss: 0.17322805649787187\n",
      "          vf_explained_var: 0.5238836407661438\n",
      "          vf_loss: 0.2554299428462982\n",
      "    num_agent_steps_sampled: 7736256\n",
      "    num_agent_steps_trained: 7736256\n",
      "    num_steps_sampled: 1934064\n",
      "    num_steps_trained: 1934064\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.50620689655172\n",
      "    gpu_util_percent0: 0.13717241379310346\n",
      "    ram_util_percent: 93.01034482758621\n",
      "    vram_util_percent0: 0.516643405839035\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0687096449300344\n",
      "  policy_reward_mean:\n",
      "    main: 0.04208834724953917\n",
      "  policy_reward_min:\n",
      "    main: -1.7192411655122888\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30289160655054065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.84208874080502\n",
      "    mean_inference_ms: 4.39711626795477\n",
      "    mean_raw_obs_processing_ms: 1.2992962215633572\n",
      "  time_since_restore: 27662.050467967987\n",
      "  time_this_iter_s: 112.12877750396729\n",
      "  time_total_s: 27662.050467967987\n",
      "  timers:\n",
      "    learn_throughput: 91.313\n",
      "    learn_time_ms: 87523.066\n",
      "    sample_throughput: 301.116\n",
      "    sample_time_ms: 26541.228\n",
      "    update_time_ms: 2.975\n",
      "  timestamp: 1639185892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1934064\n",
      "  training_iteration: 242\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         27662.1</td><td style=\"text-align: right;\">1934064</td><td style=\"text-align: right;\">0.168353</td><td style=\"text-align: right;\">             1.52654</td><td style=\"text-align: right;\">            -1.14615</td><td style=\"text-align: right;\">           59.3154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7768224\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8569586226746093\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08099218235019225\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5180253036619643\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 63.671875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6342702825983144\n",
      "  episode_reward_mean: 0.1497689263385349\n",
      "  episode_reward_min: -1.394262197791523\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 27307\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.654421311378479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018677011929452418\n",
      "          policy_loss: -0.1013698721602559\n",
      "          total_loss: 0.17203743756189943\n",
      "          vf_explained_var: 0.5239202976226807\n",
      "          vf_loss: 0.25449683660268785\n",
      "    num_agent_steps_sampled: 7768224\n",
      "    num_agent_steps_trained: 7768224\n",
      "    num_steps_sampled: 1942056\n",
      "    num_steps_trained: 1942056\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.43972602739726\n",
      "    gpu_util_percent0: 0.13595890410958905\n",
      "    ram_util_percent: 93.0876712328767\n",
      "    vram_util_percent0: 0.5150137256758036\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9191351835193313\n",
      "  policy_reward_mean:\n",
      "    main: 0.03744223158463374\n",
      "  policy_reward_min:\n",
      "    main: -1.8351267885092208\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30288250864140254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.83722888384509\n",
      "    mean_inference_ms: 4.395574969862674\n",
      "    mean_raw_obs_processing_ms: 1.2996187306480784\n",
      "  time_since_restore: 27774.44972372055\n",
      "  time_this_iter_s: 112.39925575256348\n",
      "  time_total_s: 27774.44972372055\n",
      "  timers:\n",
      "    learn_throughput: 91.199\n",
      "    learn_time_ms: 87632.797\n",
      "    sample_throughput: 300.578\n",
      "    sample_time_ms: 26588.794\n",
      "    update_time_ms: 2.981\n",
      "  timestamp: 1639186004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1942056\n",
      "  training_iteration: 243\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         27774.4</td><td style=\"text-align: right;\">1942056</td><td style=\"text-align: right;\">0.149769</td><td style=\"text-align: right;\">             1.63427</td><td style=\"text-align: right;\">            -1.39426</td><td style=\"text-align: right;\">           63.6719</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7800192\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8441664052740089\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06920204244955667\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7138246022695522\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 62.207692307692305\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.380575585325306\n",
      "  episode_reward_mean: 0.19141491797536375\n",
      "  episode_reward_min: -0.9517957790304468\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 27437\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.657899115562439\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01871162812039256\n",
      "          policy_loss: -0.10263282282277941\n",
      "          total_loss: 0.18319368967413902\n",
      "          vf_explained_var: 0.5056434273719788\n",
      "          vf_loss: 0.2668809903860092\n",
      "    num_agent_steps_sampled: 7800192\n",
      "    num_agent_steps_trained: 7800192\n",
      "    num_steps_sampled: 1950048\n",
      "    num_steps_trained: 1950048\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.29379310344828\n",
      "    gpu_util_percent0: 0.13793103448275862\n",
      "    ram_util_percent: 93.07379310344828\n",
      "    vram_util_percent0: 0.5149833770183022\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.042286418640648\n",
      "  policy_reward_mean:\n",
      "    main: 0.047853729493840945\n",
      "  policy_reward_min:\n",
      "    main: -1.7475004951679294\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030850775101407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.843414844746935\n",
      "    mean_inference_ms: 4.398144394332765\n",
      "    mean_raw_obs_processing_ms: 1.2997367082847682\n",
      "  time_since_restore: 27886.413162708282\n",
      "  time_this_iter_s: 111.96343898773193\n",
      "  time_total_s: 27886.413162708282\n",
      "  timers:\n",
      "    learn_throughput: 91.652\n",
      "    learn_time_ms: 87199.442\n",
      "    sample_throughput: 301.605\n",
      "    sample_time_ms: 26498.245\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1639186116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1950048\n",
      "  training_iteration: 244\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         27886.4</td><td style=\"text-align: right;\">1950048</td><td style=\"text-align: right;\">0.191415</td><td style=\"text-align: right;\">             1.38058</td><td style=\"text-align: right;\">           -0.951796</td><td style=\"text-align: right;\">           62.2077</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7832160\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6638139579775688\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07003563680537027\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4478663614602481\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-30-29\n",
      "  done: false\n",
      "  episode_len_mean: 56.716312056737586\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4127578826974414\n",
      "  episode_reward_mean: 0.15718200020133016\n",
      "  episode_reward_min: -0.8167984874534836\n",
      "  episodes_this_iter: 141\n",
      "  episodes_total: 27578\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6490068290233612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01875260532647371\n",
      "          policy_loss: -0.10111836837977171\n",
      "          total_loss: 0.17679158436506986\n",
      "          vf_explained_var: 0.5401075482368469\n",
      "          vf_loss: 0.2589229404330254\n",
      "    num_agent_steps_sampled: 7832160\n",
      "    num_agent_steps_trained: 7832160\n",
      "    num_steps_sampled: 1958040\n",
      "    num_steps_trained: 1958040\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.2\n",
      "    gpu_util_percent0: 0.1366896551724138\n",
      "    ram_util_percent: 93.15724137931034\n",
      "    vram_util_percent0: 0.5149788383202277\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9045460950141964\n",
      "  policy_reward_mean:\n",
      "    main: 0.03929550005033254\n",
      "  policy_reward_min:\n",
      "    main: -1.6114367517171608\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3027784619198359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.833656851155098\n",
      "    mean_inference_ms: 4.394280925057406\n",
      "    mean_raw_obs_processing_ms: 1.2999593153930697\n",
      "  time_since_restore: 27998.589022874832\n",
      "  time_this_iter_s: 112.17586016654968\n",
      "  time_total_s: 27998.589022874832\n",
      "  timers:\n",
      "    learn_throughput: 91.944\n",
      "    learn_time_ms: 86922.583\n",
      "    sample_throughput: 302.361\n",
      "    sample_time_ms: 26431.941\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1639186229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1958040\n",
      "  training_iteration: 245\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         27998.6</td><td style=\"text-align: right;\">1958040</td><td style=\"text-align: right;\">0.157182</td><td style=\"text-align: right;\">             1.41276</td><td style=\"text-align: right;\">           -0.816798</td><td style=\"text-align: right;\">           56.7163</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7864128\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8356873925904312\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0733049280155631\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6724157655511258\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 62.3515625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6683545392402817\n",
      "  episode_reward_mean: 0.20805327040704027\n",
      "  episode_reward_min: -0.7410118369865673\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 27706\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6548163697719575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018977161645889282\n",
      "          policy_loss: -0.10250206108018756\n",
      "          total_loss: 0.19352066710963844\n",
      "          vf_explained_var: 0.48982253670692444\n",
      "          vf_loss: 0.2768083505630493\n",
      "    num_agent_steps_sampled: 7864128\n",
      "    num_agent_steps_trained: 7864128\n",
      "    num_steps_sampled: 1966032\n",
      "    num_steps_trained: 1966032\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.492465753424653\n",
      "    gpu_util_percent0: 0.13904109589041094\n",
      "    ram_util_percent: 93.32465753424657\n",
      "    vram_util_percent0: 0.5149866800091955\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9046200568349245\n",
      "  policy_reward_mean:\n",
      "    main: 0.05201331760176006\n",
      "  policy_reward_min:\n",
      "    main: -1.9308527818594683\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.302464846092031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.826392076642467\n",
      "    mean_inference_ms: 4.391241063976674\n",
      "    mean_raw_obs_processing_ms: 1.2999569636604287\n",
      "  time_since_restore: 28111.22391629219\n",
      "  time_this_iter_s: 112.6348934173584\n",
      "  time_total_s: 28111.22391629219\n",
      "  timers:\n",
      "    learn_throughput: 92.595\n",
      "    learn_time_ms: 86311.298\n",
      "    sample_throughput: 302.769\n",
      "    sample_time_ms: 26396.369\n",
      "    update_time_ms: 2.995\n",
      "  timestamp: 1639186341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1966032\n",
      "  training_iteration: 246\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         28111.2</td><td style=\"text-align: right;\">1966032</td><td style=\"text-align: right;\">0.208053</td><td style=\"text-align: right;\">             1.66835</td><td style=\"text-align: right;\">           -0.741012</td><td style=\"text-align: right;\">           62.3516</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7896096\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.661513962233254\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05241085639340283\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5332448576687334\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-34-19\n",
      "  done: false\n",
      "  episode_len_mean: 58.722627737226276\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7186821544708135\n",
      "  episode_reward_mean: 0.1277896551327605\n",
      "  episode_reward_min: -1.3990502860775815\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 27843\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6544964141845703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01914496195688844\n",
      "          policy_loss: -0.10277502002939581\n",
      "          total_loss: 0.19495057612657546\n",
      "          vf_explained_var: 0.5055065155029297\n",
      "          vf_loss: 0.2783413204550743\n",
      "    num_agent_steps_sampled: 7896096\n",
      "    num_agent_steps_trained: 7896096\n",
      "    num_steps_sampled: 1974024\n",
      "    num_steps_trained: 1974024\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.92666666666667\n",
      "    gpu_util_percent0: 0.1592\n",
      "    ram_util_percent: 93.654\n",
      "    vram_util_percent0: 0.5155303279587582\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.790983721571007\n",
      "  policy_reward_mean:\n",
      "    main: 0.031947413783190125\n",
      "  policy_reward_min:\n",
      "    main: -1.7887720955426283\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029175328348467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.837893667436795\n",
      "    mean_inference_ms: 4.396563940441219\n",
      "    mean_raw_obs_processing_ms: 1.3000755799885297\n",
      "  time_since_restore: 28228.351521253586\n",
      "  time_this_iter_s: 117.12760496139526\n",
      "  time_total_s: 28228.351521253586\n",
      "  timers:\n",
      "    learn_throughput: 92.508\n",
      "    learn_time_ms: 86392.595\n",
      "    sample_throughput: 303.374\n",
      "    sample_time_ms: 26343.707\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1639186459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1974024\n",
      "  training_iteration: 247\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         28228.4</td><td style=\"text-align: right;\">1974024</td><td style=\"text-align: right;\"> 0.12779</td><td style=\"text-align: right;\">             1.71868</td><td style=\"text-align: right;\">            -1.39905</td><td style=\"text-align: right;\">           58.7226</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7928064\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1960649241716803\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10438941616423425\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5185254956427805\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 66.51666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5050506715168632\n",
      "  episode_reward_mean: 0.19834166206375486\n",
      "  episode_reward_min: -0.8028501028674402\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 27963\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.658333960056305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01884639995917678\n",
      "          policy_loss: -0.10110802733525634\n",
      "          total_loss: 0.16107493030652403\n",
      "          vf_explained_var: 0.5258693695068359\n",
      "          vf_loss: 0.24310097742080689\n",
      "    num_agent_steps_sampled: 7928064\n",
      "    num_agent_steps_trained: 7928064\n",
      "    num_steps_sampled: 1982016\n",
      "    num_steps_trained: 1982016\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.182165605095534\n",
      "    gpu_util_percent0: 0.19515923566878984\n",
      "    ram_util_percent: 94.05796178343948\n",
      "    vram_util_percent0: 0.5224197953148351\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9586649288446845\n",
      "  policy_reward_mean:\n",
      "    main: 0.049585415515938695\n",
      "  policy_reward_min:\n",
      "    main: -1.7810498529753103\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3028822613234181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.85187053235827\n",
      "    mean_inference_ms: 4.398481718761881\n",
      "    mean_raw_obs_processing_ms: 1.3006329570720696\n",
      "  time_since_restore: 28350.358239889145\n",
      "  time_this_iter_s: 122.00671863555908\n",
      "  time_total_s: 28350.358239889145\n",
      "  timers:\n",
      "    learn_throughput: 91.848\n",
      "    learn_time_ms: 87013.737\n",
      "    sample_throughput: 299.986\n",
      "    sample_time_ms: 26641.244\n",
      "    update_time_ms: 2.921\n",
      "  timestamp: 1639186581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1982016\n",
      "  training_iteration: 248\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         28350.4</td><td style=\"text-align: right;\">1982016</td><td style=\"text-align: right;\">0.198342</td><td style=\"text-align: right;\">             1.50505</td><td style=\"text-align: right;\">            -0.80285</td><td style=\"text-align: right;\">           66.5167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7960032\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8972322375948149\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07555217324822112\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5688028040028049\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 57.61764705882353\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8520507336292966\n",
      "  episode_reward_mean: 0.15322398729820796\n",
      "  episode_reward_min: -0.9759647690631914\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 28099\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.653917720079422\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019046314414590596\n",
      "          policy_loss: -0.10208930143713951\n",
      "          total_loss: 0.17589139914140106\n",
      "          vf_explained_var: 0.5321575999259949\n",
      "          vf_loss: 0.2586963056325913\n",
      "    num_agent_steps_sampled: 7960032\n",
      "    num_agent_steps_trained: 7960032\n",
      "    num_steps_sampled: 1990008\n",
      "    num_steps_trained: 1990008\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.574324324324323\n",
      "    gpu_util_percent0: 0.17898648648648646\n",
      "    ram_util_percent: 93.4331081081081\n",
      "    vram_util_percent0: 0.5237020090179024\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9201890531436745\n",
      "  policy_reward_mean:\n",
      "    main: 0.038305996824551984\n",
      "  policy_reward_min:\n",
      "    main: -1.5884645113562539\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30270033753513464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.844703184640718\n",
      "    mean_inference_ms: 4.395602377726797\n",
      "    mean_raw_obs_processing_ms: 1.3010196267454128\n",
      "  time_since_restore: 28464.886057138443\n",
      "  time_this_iter_s: 114.5278172492981\n",
      "  time_total_s: 28464.886057138443\n",
      "  timers:\n",
      "    learn_throughput: 91.668\n",
      "    learn_time_ms: 87183.995\n",
      "    sample_throughput: 302.08\n",
      "    sample_time_ms: 26456.542\n",
      "    update_time_ms: 2.921\n",
      "  timestamp: 1639186695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1990008\n",
      "  training_iteration: 249\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         28464.9</td><td style=\"text-align: right;\">1990008</td><td style=\"text-align: right;\">0.153224</td><td style=\"text-align: right;\">             1.85205</td><td style=\"text-align: right;\">           -0.975965</td><td style=\"text-align: right;\">           57.6176</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 7992000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0697354808142683\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08932080637314034\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5148871139777683\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 63.09448818897638\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8509078937355832\n",
      "  episode_reward_mean: 0.24466088049800574\n",
      "  episode_reward_min: -0.6642885699877081\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 28226\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6613833038806916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018869786366820334\n",
      "          policy_loss: -0.10061220398172736\n",
      "          total_loss: 0.1765961646735668\n",
      "          vf_explained_var: 0.5198094844818115\n",
      "          vf_loss: 0.25810270810127256\n",
      "    num_agent_steps_sampled: 7992000\n",
      "    num_agent_steps_trained: 7992000\n",
      "    num_steps_sampled: 1998000\n",
      "    num_steps_trained: 1998000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.01292517006802\n",
      "    gpu_util_percent0: 0.1827891156462585\n",
      "    ram_util_percent: 93.51496598639456\n",
      "    vram_util_percent0: 0.5230831391457538\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0356453154138796\n",
      "  policy_reward_mean:\n",
      "    main: 0.061165220124501436\n",
      "  policy_reward_min:\n",
      "    main: -1.5819727157522496\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3028425444101323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.848911176079312\n",
      "    mean_inference_ms: 4.397147426798076\n",
      "    mean_raw_obs_processing_ms: 1.301260519556066\n",
      "  time_since_restore: 28579.26953101158\n",
      "  time_this_iter_s: 114.38347387313843\n",
      "  time_total_s: 28579.26953101158\n",
      "  timers:\n",
      "    learn_throughput: 91.479\n",
      "    learn_time_ms: 87364.208\n",
      "    sample_throughput: 301.56\n",
      "    sample_time_ms: 26502.232\n",
      "    update_time_ms: 2.922\n",
      "  timestamp: 1639186810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1998000\n",
      "  training_iteration: 250\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         28579.3</td><td style=\"text-align: right;\">1998000</td><td style=\"text-align: right;\">0.244661</td><td style=\"text-align: right;\">             1.85091</td><td style=\"text-align: right;\">           -0.664289</td><td style=\"text-align: right;\">           63.0945</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8023968\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.748665258321999\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07510336539160263\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5226072063739465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 64.88983050847457\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8588632845368496\n",
      "  episode_reward_mean: 0.22405739409629463\n",
      "  episode_reward_min: -0.943235312043865\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 28344\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6568138229846955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018991883538663386\n",
      "          policy_loss: -0.10277097241580486\n",
      "          total_loss: 0.16415868361853064\n",
      "          vf_explained_var: 0.5064939260482788\n",
      "          vf_loss: 0.24770037209987641\n",
      "    num_agent_steps_sampled: 8023968\n",
      "    num_agent_steps_trained: 8023968\n",
      "    num_steps_sampled: 2005992\n",
      "    num_steps_trained: 2005992\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.08657718120806\n",
      "    gpu_util_percent0: 0.17912751677852348\n",
      "    ram_util_percent: 93.51879194630871\n",
      "    vram_util_percent0: 0.5230659149181448\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9390029343473048\n",
      "  policy_reward_mean:\n",
      "    main: 0.056014348524073665\n",
      "  policy_reward_min:\n",
      "    main: -1.6336511520484822\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30279326447434596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.847569851412107\n",
      "    mean_inference_ms: 4.397354940848926\n",
      "    mean_raw_obs_processing_ms: 1.301254448185325\n",
      "  time_since_restore: 28693.944464445114\n",
      "  time_this_iter_s: 114.67493343353271\n",
      "  time_total_s: 28693.944464445114\n",
      "  timers:\n",
      "    learn_throughput: 91.263\n",
      "    learn_time_ms: 87571.087\n",
      "    sample_throughput: 301.7\n",
      "    sample_time_ms: 26489.916\n",
      "    update_time_ms: 2.925\n",
      "  timestamp: 1639186925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2005992\n",
      "  training_iteration: 251\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         28693.9</td><td style=\"text-align: right;\">2005992</td><td style=\"text-align: right;\">0.224057</td><td style=\"text-align: right;\">             1.85886</td><td style=\"text-align: right;\">           -0.943235</td><td style=\"text-align: right;\">           64.8898</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8055936\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.017933683288444\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05904781275092533\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5704172996070912\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 70.47413793103448\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4281537832257651\n",
      "  episode_reward_mean: 0.2020923027656204\n",
      "  episode_reward_min: -0.8095436183183289\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 28460\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6514719910621644\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01900635129585862\n",
      "          policy_loss: -0.10171053514629602\n",
      "          total_loss: 0.17893735869973898\n",
      "          vf_explained_var: 0.48118922114372253\n",
      "          vf_loss: 0.2614039622545242\n",
      "    num_agent_steps_sampled: 8055936\n",
      "    num_agent_steps_trained: 8055936\n",
      "    num_steps_sampled: 2013984\n",
      "    num_steps_trained: 2013984\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.95503355704698\n",
      "    gpu_util_percent0: 0.18134228187919463\n",
      "    ram_util_percent: 93.53355704697985\n",
      "    vram_util_percent0: 0.5198625916773222\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9702210515033174\n",
      "  policy_reward_mean:\n",
      "    main: 0.0505230756914051\n",
      "  policy_reward_min:\n",
      "    main: -1.5891452909828527\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3027858347769042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.842575469318056\n",
      "    mean_inference_ms: 4.3957123399292986\n",
      "    mean_raw_obs_processing_ms: 1.3014352091367454\n",
      "  time_since_restore: 28808.858608722687\n",
      "  time_this_iter_s: 114.91414427757263\n",
      "  time_total_s: 28808.858608722687\n",
      "  timers:\n",
      "    learn_throughput: 90.999\n",
      "    learn_time_ms: 87825.109\n",
      "    sample_throughput: 301.405\n",
      "    sample_time_ms: 26515.777\n",
      "    update_time_ms: 2.908\n",
      "  timestamp: 1639187040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2013984\n",
      "  training_iteration: 252\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         28808.9</td><td style=\"text-align: right;\">2013984</td><td style=\"text-align: right;\">0.202092</td><td style=\"text-align: right;\">             1.42815</td><td style=\"text-align: right;\">           -0.809544</td><td style=\"text-align: right;\">           70.4741</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8087904\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9643630077139946\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05801816550178241\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5795457606413672\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 59.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.887548488518675\n",
      "  episode_reward_mean: 0.15268085707654294\n",
      "  episode_reward_min: -1.1219097429809128\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 28600\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6563312051296234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01953964461013675\n",
      "          policy_loss: -0.10494373055920005\n",
      "          total_loss: 0.20343648195639252\n",
      "          vf_explained_var: 0.5033335089683533\n",
      "          vf_loss: 0.28859632074832914\n",
      "    num_agent_steps_sampled: 8087904\n",
      "    num_agent_steps_trained: 8087904\n",
      "    num_steps_sampled: 2021976\n",
      "    num_steps_trained: 2021976\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.61744966442953\n",
      "    gpu_util_percent0: 0.18201342281879193\n",
      "    ram_util_percent: 93.62013422818794\n",
      "    vram_util_percent0: 0.5163511928817984\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0708082089915196\n",
      "  policy_reward_mean:\n",
      "    main: 0.03817021426913574\n",
      "  policy_reward_min:\n",
      "    main: -1.6889072209727052\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30311167729262606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.8537156835283\n",
      "    mean_inference_ms: 4.400644348612952\n",
      "    mean_raw_obs_processing_ms: 1.3015178922238233\n",
      "  time_since_restore: 28923.500603437424\n",
      "  time_this_iter_s: 114.64199471473694\n",
      "  time_total_s: 28923.500603437424\n",
      "  timers:\n",
      "    learn_throughput: 90.772\n",
      "    learn_time_ms: 88044.499\n",
      "    sample_throughput: 301.41\n",
      "    sample_time_ms: 26515.409\n",
      "    update_time_ms: 2.888\n",
      "  timestamp: 1639187154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2021976\n",
      "  training_iteration: 253\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         28923.5</td><td style=\"text-align: right;\">2021976</td><td style=\"text-align: right;\">0.152681</td><td style=\"text-align: right;\">             1.88755</td><td style=\"text-align: right;\">            -1.12191</td><td style=\"text-align: right;\">             59.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8119872\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9394548121876118\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06797879528578275\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49720029596755505\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-47-50\n",
      "  done: false\n",
      "  episode_len_mean: 58.82442748091603\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.519289083796659\n",
      "  episode_reward_mean: 0.17112561632112896\n",
      "  episode_reward_min: -1.26275816251959\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 28731\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6507649183273315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018946556221693753\n",
      "          policy_loss: -0.10179052734747529\n",
      "          total_loss: 0.18023467338085175\n",
      "          vf_explained_var: 0.5056901574134827\n",
      "          vf_loss: 0.26284181040525434\n",
      "    num_agent_steps_sampled: 8119872\n",
      "    num_agent_steps_trained: 8119872\n",
      "    num_steps_sampled: 2029968\n",
      "    num_steps_trained: 2029968\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.502013422818784\n",
      "    gpu_util_percent0: 0.18120805369127516\n",
      "    ram_util_percent: 93.6510067114094\n",
      "    vram_util_percent0: 0.5156809353129673\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8754797888784527\n",
      "  policy_reward_mean:\n",
      "    main: 0.042781404080282255\n",
      "  policy_reward_min:\n",
      "    main: -1.749738023839936\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30323439984747025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.856998792370096\n",
      "    mean_inference_ms: 4.401783182858359\n",
      "    mean_raw_obs_processing_ms: 1.301962949852315\n",
      "  time_since_restore: 29038.77197432518\n",
      "  time_this_iter_s: 115.27137088775635\n",
      "  time_total_s: 29038.77197432518\n",
      "  timers:\n",
      "    learn_throughput: 90.513\n",
      "    learn_time_ms: 88296.314\n",
      "    sample_throughput: 300.529\n",
      "    sample_time_ms: 26593.098\n",
      "    update_time_ms: 2.893\n",
      "  timestamp: 1639187270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2029968\n",
      "  training_iteration: 254\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         29038.8</td><td style=\"text-align: right;\">2029968</td><td style=\"text-align: right;\">0.171126</td><td style=\"text-align: right;\">             1.51929</td><td style=\"text-align: right;\">            -1.26276</td><td style=\"text-align: right;\">           58.8244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8151840\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7032519356038088\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02187124317330797\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6324414416685039\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 60.79259259259259\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.695035053338433\n",
      "  episode_reward_mean: 0.15549023600175507\n",
      "  episode_reward_min: -0.9055600159076258\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 28866\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6519154732227326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01913174033910036\n",
      "          policy_loss: -0.10350985465571284\n",
      "          total_loss: 0.17987079387903213\n",
      "          vf_explained_var: 0.5156069397926331\n",
      "          vf_loss: 0.26400976091623307\n",
      "    num_agent_steps_sampled: 8151840\n",
      "    num_agent_steps_trained: 8151840\n",
      "    num_steps_sampled: 2037960\n",
      "    num_steps_trained: 2037960\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.954362416107383\n",
      "    gpu_util_percent0: 0.17885906040268457\n",
      "    ram_util_percent: 93.69999999999999\n",
      "    vram_util_percent0: 0.5156721016053056\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9648707524936746\n",
      "  policy_reward_mean:\n",
      "    main: 0.03887255900043877\n",
      "  policy_reward_min:\n",
      "    main: -1.6877713011246611\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30270915586292835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.840500120946004\n",
      "    mean_inference_ms: 4.395521494064149\n",
      "    mean_raw_obs_processing_ms: 1.3021692996906302\n",
      "  time_since_restore: 29153.179552316666\n",
      "  time_this_iter_s: 114.4075779914856\n",
      "  time_total_s: 29153.179552316666\n",
      "  timers:\n",
      "    learn_throughput: 90.274\n",
      "    learn_time_ms: 88530.183\n",
      "    sample_throughput: 300.694\n",
      "    sample_time_ms: 26578.503\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1639187384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2037960\n",
      "  training_iteration: 255\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         29153.2</td><td style=\"text-align: right;\">2037960</td><td style=\"text-align: right;\"> 0.15549</td><td style=\"text-align: right;\">             1.69504</td><td style=\"text-align: right;\">            -0.90556</td><td style=\"text-align: right;\">           60.7926</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8183808\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.944477679984608\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04272852951040872\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7450951435270184\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 57.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7287068616661019\n",
      "  episode_reward_mean: 0.18374916708099404\n",
      "  episode_reward_min: -1.3469352453060852\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 29002\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6553272421360016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019186939425766467\n",
      "          policy_loss: -0.10305497886613012\n",
      "          total_loss: 0.16500273269414903\n",
      "          vf_explained_var: 0.5433830618858337\n",
      "          vf_loss: 0.24863093411922454\n",
      "    num_agent_steps_sampled: 8183808\n",
      "    num_agent_steps_trained: 8183808\n",
      "    num_steps_sampled: 2045952\n",
      "    num_steps_trained: 2045952\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.7\n",
      "    gpu_util_percent0: 0.1783221476510067\n",
      "    ram_util_percent: 93.8221476510067\n",
      "    vram_util_percent0: 0.5156566426168977\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9062962759513775\n",
      "  policy_reward_mean:\n",
      "    main: 0.0459372917702485\n",
      "  policy_reward_min:\n",
      "    main: -1.7450951435270183\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032679807778572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.85721588264239\n",
      "    mean_inference_ms: 4.401867403119386\n",
      "    mean_raw_obs_processing_ms: 1.3024061898465507\n",
      "  time_since_restore: 29268.715159893036\n",
      "  time_this_iter_s: 115.53560757637024\n",
      "  time_total_s: 29268.715159893036\n",
      "  timers:\n",
      "    learn_throughput: 90.021\n",
      "    learn_time_ms: 88779.429\n",
      "    sample_throughput: 300.269\n",
      "    sample_time_ms: 26616.173\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639187500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2045952\n",
      "  training_iteration: 256\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         29268.7</td><td style=\"text-align: right;\">2045952</td><td style=\"text-align: right;\">0.183749</td><td style=\"text-align: right;\">             1.72871</td><td style=\"text-align: right;\">            -1.34694</td><td style=\"text-align: right;\">             57.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8215776\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7081108393333247\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.044301383362770154\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.689778036002478\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 57.81021897810219\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4479186611684383\n",
      "  episode_reward_mean: 0.12607779973764296\n",
      "  episode_reward_min: -0.9928921451502162\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 29139\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6507884345054626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018736815232783557\n",
      "          policy_loss: -0.10069668739289045\n",
      "          total_loss: 0.1850496962144971\n",
      "          vf_explained_var: 0.5151283144950867\n",
      "          vf_loss: 0.26677535873651503\n",
      "    num_agent_steps_sampled: 8215776\n",
      "    num_agent_steps_trained: 8215776\n",
      "    num_steps_sampled: 2053944\n",
      "    num_steps_trained: 2053944\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.94832214765101\n",
      "    gpu_util_percent0: 0.18067114093959732\n",
      "    ram_util_percent: 93.88724832214766\n",
      "    vram_util_percent0: 0.5156599552572707\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8578631987860952\n",
      "  policy_reward_mean:\n",
      "    main: 0.03151944993441074\n",
      "  policy_reward_min:\n",
      "    main: -1.6981265737018147\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3035039934367886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.868384722425077\n",
      "    mean_inference_ms: 4.406232077596517\n",
      "    mean_raw_obs_processing_ms: 1.3028015720348132\n",
      "  time_since_restore: 29383.776829004288\n",
      "  time_this_iter_s: 115.06166911125183\n",
      "  time_total_s: 29383.776829004288\n",
      "  timers:\n",
      "    learn_throughput: 90.314\n",
      "    learn_time_ms: 88491.233\n",
      "    sample_throughput: 299.3\n",
      "    sample_time_ms: 26702.266\n",
      "    update_time_ms: 2.804\n",
      "  timestamp: 1639187615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2053944\n",
      "  training_iteration: 257\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         29383.8</td><td style=\"text-align: right;\">2053944</td><td style=\"text-align: right;\">0.126078</td><td style=\"text-align: right;\">             1.44792</td><td style=\"text-align: right;\">           -0.992892</td><td style=\"text-align: right;\">           57.8102</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8247744\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8982292238337444\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06174091126181444\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8110635827279115\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 70.2695652173913\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7944915948925604\n",
      "  episode_reward_mean: 0.11972870779822395\n",
      "  episode_reward_min: -1.3134281240605024\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 29254\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6494387288093567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01899708542972803\n",
      "          policy_loss: -0.10034019564837217\n",
      "          total_loss: 0.17231267708539963\n",
      "          vf_explained_var: 0.49370360374450684\n",
      "          vf_loss: 0.2534183220267296\n",
      "    num_agent_steps_sampled: 8247744\n",
      "    num_agent_steps_trained: 8247744\n",
      "    num_steps_sampled: 2061936\n",
      "    num_steps_trained: 2061936\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.863513513513514\n",
      "    gpu_util_percent0: 0.18155405405405403\n",
      "    ram_util_percent: 93.9418918918919\n",
      "    vram_util_percent0: 0.5156468166093043\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0285053536266853\n",
      "  policy_reward_mean:\n",
      "    main: 0.029932176949555987\n",
      "  policy_reward_min:\n",
      "    main: -1.8110635827279116\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030219628781036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.852051276201955\n",
      "    mean_inference_ms: 4.39988289701606\n",
      "    mean_raw_obs_processing_ms: 1.302859278536748\n",
      "  time_since_restore: 29498.08920788765\n",
      "  time_this_iter_s: 114.31237888336182\n",
      "  time_total_s: 29498.08920788765\n",
      "  timers:\n",
      "    learn_throughput: 90.727\n",
      "    learn_time_ms: 88088.634\n",
      "    sample_throughput: 303.478\n",
      "    sample_time_ms: 26334.706\n",
      "    update_time_ms: 2.72\n",
      "  timestamp: 1639187729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2061936\n",
      "  training_iteration: 258\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         29498.1</td><td style=\"text-align: right;\">2061936</td><td style=\"text-align: right;\">0.119729</td><td style=\"text-align: right;\">             1.79449</td><td style=\"text-align: right;\">            -1.31343</td><td style=\"text-align: right;\">           70.2696</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8279712\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6564728376705812\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09416044522259216\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5336977985107081\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 62.267716535433074\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5315454414441914\n",
      "  episode_reward_mean: 0.18621456441171838\n",
      "  episode_reward_min: -0.9662068407891535\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 29381\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6478175508975983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019242284528911115\n",
      "          policy_loss: -0.10380407532304525\n",
      "          total_loss: 0.1750960500538349\n",
      "          vf_explained_var: 0.5207259654998779\n",
      "          vf_loss: 0.25941730964183807\n",
      "    num_agent_steps_sampled: 8279712\n",
      "    num_agent_steps_trained: 8279712\n",
      "    num_steps_sampled: 2069928\n",
      "    num_steps_trained: 2069928\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.69194630872484\n",
      "    gpu_util_percent0: 0.17852348993288591\n",
      "    ram_util_percent: 93.99261744966441\n",
      "    vram_util_percent0: 0.5156610594707285\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9511118183506295\n",
      "  policy_reward_mean:\n",
      "    main: 0.046553641102929595\n",
      "  policy_reward_min:\n",
      "    main: -1.5919629960403119\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30294930929249403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.853552268956644\n",
      "    mean_inference_ms: 4.400661328885551\n",
      "    mean_raw_obs_processing_ms: 1.3031962816415608\n",
      "  time_since_restore: 29613.351173877716\n",
      "  time_this_iter_s: 115.26196599006653\n",
      "  time_total_s: 29613.351173877716\n",
      "  timers:\n",
      "    learn_throughput: 90.709\n",
      "    learn_time_ms: 88106.299\n",
      "    sample_throughput: 302.85\n",
      "    sample_time_ms: 26389.273\n",
      "    update_time_ms: 2.763\n",
      "  timestamp: 1639187845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2069928\n",
      "  training_iteration: 259\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         29613.4</td><td style=\"text-align: right;\">2069928</td><td style=\"text-align: right;\">0.186215</td><td style=\"text-align: right;\">             1.53155</td><td style=\"text-align: right;\">           -0.966207</td><td style=\"text-align: right;\">           62.2677</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8311680\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.817119288098433\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07608261292700616\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47901925136187873\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_22-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 67.18032786885246\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.426377688017154\n",
      "  episode_reward_mean: 0.1919718841963222\n",
      "  episode_reward_min: -1.1891638373096232\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 29503\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.643993064403534\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01906659948825836\n",
      "          policy_loss: -0.10222240315005184\n",
      "          total_loss: 0.18975131245329976\n",
      "          vf_explained_var: 0.48891523480415344\n",
      "          vf_loss: 0.2726687838435173\n",
      "    num_agent_steps_sampled: 8311680\n",
      "    num_agent_steps_trained: 8311680\n",
      "    num_steps_sampled: 2077920\n",
      "    num_steps_trained: 2077920\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.14729729729729\n",
      "    gpu_util_percent0: 0.17898648648648646\n",
      "    ram_util_percent: 94.00675675675676\n",
      "    vram_util_percent0: 0.5156379232144287\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9103736202859998\n",
      "  policy_reward_mean:\n",
      "    main: 0.047992971049080534\n",
      "  policy_reward_min:\n",
      "    main: -1.6554313161356728\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30295904942901286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.85266888301126\n",
      "    mean_inference_ms: 4.400066499924447\n",
      "    mean_raw_obs_processing_ms: 1.303320861473925\n",
      "  time_since_restore: 29727.855791807175\n",
      "  time_this_iter_s: 114.50461792945862\n",
      "  time_total_s: 29727.855791807175\n",
      "  timers:\n",
      "    learn_throughput: 90.693\n",
      "    learn_time_ms: 88121.207\n",
      "    sample_throughput: 302.924\n",
      "    sample_time_ms: 26382.843\n",
      "    update_time_ms: 2.752\n",
      "  timestamp: 1639187959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2077920\n",
      "  training_iteration: 260\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         29727.9</td><td style=\"text-align: right;\">2077920</td><td style=\"text-align: right;\">0.191972</td><td style=\"text-align: right;\">             1.42638</td><td style=\"text-align: right;\">            -1.18916</td><td style=\"text-align: right;\">           67.1803</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8343648\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8761728752788344\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10294337366178366\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5375558747039767\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 61.125984251968504\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8076869866313634\n",
      "  episode_reward_mean: 0.20619827048746256\n",
      "  episode_reward_min: -0.7849213811616802\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 29630\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6453724820613861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019364870671182872\n",
      "          policy_loss: -0.10221030102856457\n",
      "          total_loss: 0.17957462611049413\n",
      "          vf_explained_var: 0.4948032796382904\n",
      "          vf_loss: 0.26217799603939057\n",
      "    num_agent_steps_sampled: 8343648\n",
      "    num_agent_steps_trained: 8343648\n",
      "    num_steps_sampled: 2085912\n",
      "    num_steps_trained: 2085912\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.137837837837836\n",
      "    gpu_util_percent0: 0.1823648648648649\n",
      "    ram_util_percent: 94.09391891891892\n",
      "    vram_util_percent0: 0.5156379232144287\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9867574943770614\n",
      "  policy_reward_mean:\n",
      "    main: 0.05154956762186564\n",
      "  policy_reward_min:\n",
      "    main: -1.632006666920058\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30327823625618694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.858509742058732\n",
      "    mean_inference_ms: 4.402173209009706\n",
      "    mean_raw_obs_processing_ms: 1.3035957529913587\n",
      "  time_since_restore: 29842.670520067215\n",
      "  time_this_iter_s: 114.81472826004028\n",
      "  time_total_s: 29842.670520067215\n",
      "  timers:\n",
      "    learn_throughput: 90.723\n",
      "    learn_time_ms: 88091.977\n",
      "    sample_throughput: 302.422\n",
      "    sample_time_ms: 26426.685\n",
      "    update_time_ms: 2.742\n",
      "  timestamp: 1639188074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2085912\n",
      "  training_iteration: 261\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         29842.7</td><td style=\"text-align: right;\">2085912</td><td style=\"text-align: right;\">0.206198</td><td style=\"text-align: right;\">             1.80769</td><td style=\"text-align: right;\">           -0.784921</td><td style=\"text-align: right;\">            61.126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8375616\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6954541312156595\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.13174820617633612\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9397455692018303\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 62.92248062015504\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9383064841312145\n",
      "  episode_reward_mean: 0.21435692661308145\n",
      "  episode_reward_min: -0.682159861099795\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 29759\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6387804565429688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01933281396329403\n",
      "          policy_loss: -0.10267414193972946\n",
      "          total_loss: 0.17612045403569937\n",
      "          vf_explained_var: 0.5235337615013123\n",
      "          vf_loss: 0.259220122218132\n",
      "    num_agent_steps_sampled: 8375616\n",
      "    num_agent_steps_trained: 8375616\n",
      "    num_steps_sampled: 2093904\n",
      "    num_steps_trained: 2093904\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.836241610738256\n",
      "    gpu_util_percent0: 0.17946308724832213\n",
      "    ram_util_percent: 94.05234899328859\n",
      "    vram_util_percent0: 0.5156456004823206\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8479627173234294\n",
      "  policy_reward_mean:\n",
      "    main: 0.053589231653270376\n",
      "  policy_reward_min:\n",
      "    main: -1.9452153614638052\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031676800787743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.857081810351946\n",
      "    mean_inference_ms: 4.402292517436927\n",
      "    mean_raw_obs_processing_ms: 1.3036021358494656\n",
      "  time_since_restore: 29957.814700603485\n",
      "  time_this_iter_s: 115.14418053627014\n",
      "  time_total_s: 29957.814700603485\n",
      "  timers:\n",
      "    learn_throughput: 90.677\n",
      "    learn_time_ms: 88136.902\n",
      "    sample_throughput: 302.761\n",
      "    sample_time_ms: 26397.03\n",
      "    update_time_ms: 2.78\n",
      "  timestamp: 1639188189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2093904\n",
      "  training_iteration: 262\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         29957.8</td><td style=\"text-align: right;\">2093904</td><td style=\"text-align: right;\">0.214357</td><td style=\"text-align: right;\">             1.93831</td><td style=\"text-align: right;\">            -0.68216</td><td style=\"text-align: right;\">           62.9225</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8407584\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.23606422556653\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05499539031721782\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6646212096834029\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 67.05982905982906\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3841809870745991\n",
      "  episode_reward_mean: 0.14248453485936352\n",
      "  episode_reward_min: -1.0931459337215417\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 29876\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6438814001083374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018791593488305808\n",
      "          policy_loss: -0.10157102221623063\n",
      "          total_loss: 0.1796420461013913\n",
      "          vf_explained_var: 0.4820325970649719\n",
      "          vf_loss: 0.26218657797574996\n",
      "    num_agent_steps_sampled: 8407584\n",
      "    num_agent_steps_trained: 8407584\n",
      "    num_steps_sampled: 2101896\n",
      "    num_steps_trained: 2101896\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.21045751633987\n",
      "    gpu_util_percent0: 0.18738562091503266\n",
      "    ram_util_percent: 94.00718954248366\n",
      "    vram_util_percent0: 0.5181045106426907\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.092287955667093\n",
      "  policy_reward_mean:\n",
      "    main: 0.03562113371484088\n",
      "  policy_reward_min:\n",
      "    main: -1.6915367279975666\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30287951551972214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.85610558068863\n",
      "    mean_inference_ms: 4.402404699700462\n",
      "    mean_raw_obs_processing_ms: 1.3034396441326817\n",
      "  time_since_restore: 30076.487505435944\n",
      "  time_this_iter_s: 118.6728048324585\n",
      "  time_total_s: 30076.487505435944\n",
      "  timers:\n",
      "    learn_throughput: 90.324\n",
      "    learn_time_ms: 88481.503\n",
      "    sample_throughput: 302.046\n",
      "    sample_time_ms: 26459.508\n",
      "    update_time_ms: 2.808\n",
      "  timestamp: 1639188308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2101896\n",
      "  training_iteration: 263\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         30076.5</td><td style=\"text-align: right;\">2101896</td><td style=\"text-align: right;\">0.142485</td><td style=\"text-align: right;\">             1.38418</td><td style=\"text-align: right;\">            -1.09315</td><td style=\"text-align: right;\">           67.0598</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8439552\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7757893377484415\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05881876387245156\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.524213935460663\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 67.85123966942149\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4062851565949637\n",
      "  episode_reward_mean: 0.2198044771830643\n",
      "  episode_reward_min: -1.04538210349541\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 29997\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6449731593132019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019101666379719974\n",
      "          policy_loss: -0.10315792390704155\n",
      "          total_loss: 0.16080182429030537\n",
      "          vf_explained_var: 0.522386372089386\n",
      "          vf_loss: 0.24461930924654007\n",
      "    num_agent_steps_sampled: 8439552\n",
      "    num_agent_steps_trained: 8439552\n",
      "    num_steps_sampled: 2109888\n",
      "    num_steps_trained: 2109888\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.140136054421774\n",
      "    gpu_util_percent0: 0.14265306122448979\n",
      "    ram_util_percent: 94.06462585034014\n",
      "    vram_util_percent0: 0.5203130281398509\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.963936091506799\n",
      "  policy_reward_mean:\n",
      "    main: 0.054951119295766085\n",
      "  policy_reward_min:\n",
      "    main: -1.6074245188561436\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303296004588264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.863751860644697\n",
      "    mean_inference_ms: 4.404830721641625\n",
      "    mean_raw_obs_processing_ms: 1.3037070159611956\n",
      "  time_since_restore: 30190.164254188538\n",
      "  time_this_iter_s: 113.676748752594\n",
      "  time_total_s: 30190.164254188538\n",
      "  timers:\n",
      "    learn_throughput: 90.465\n",
      "    learn_time_ms: 88343.577\n",
      "    sample_throughput: 302.284\n",
      "    sample_time_ms: 26438.731\n",
      "    update_time_ms: 2.794\n",
      "  timestamp: 1639188422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2109888\n",
      "  training_iteration: 264\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         30190.2</td><td style=\"text-align: right;\">2109888</td><td style=\"text-align: right;\">0.219804</td><td style=\"text-align: right;\">             1.40629</td><td style=\"text-align: right;\">            -1.04538</td><td style=\"text-align: right;\">           67.8512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8471520\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7311335625480034\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10296794499074927\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5244350528445436\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 67.84210526315789\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6463508650383472\n",
      "  episode_reward_mean: 0.20682604942758379\n",
      "  episode_reward_min: -2.25301061157007\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 30111\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6431311540603638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019139621164649723\n",
      "          policy_loss: -0.10124928184971213\n",
      "          total_loss: 0.15902117885649203\n",
      "          vf_explained_var: 0.503636360168457\n",
      "          vf_loss: 0.2408915938138962\n",
      "    num_agent_steps_sampled: 8471520\n",
      "    num_agent_steps_trained: 8471520\n",
      "    num_steps_sampled: 2117880\n",
      "    num_steps_trained: 2117880\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.993877551020404\n",
      "    gpu_util_percent0: 0.13714285714285712\n",
      "    ram_util_percent: 94.01496598639456\n",
      "    vram_util_percent0: 0.5200197881060947\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8646236982819508\n",
      "  policy_reward_mean:\n",
      "    main: 0.05170651235689595\n",
      "  policy_reward_min:\n",
      "    main: -1.5918243005770831\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3035555324226058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.86615140329862\n",
      "    mean_inference_ms: 4.405088011920273\n",
      "    mean_raw_obs_processing_ms: 1.3039271376921817\n",
      "  time_since_restore: 30303.44100356102\n",
      "  time_this_iter_s: 113.2767493724823\n",
      "  time_total_s: 30303.44100356102\n",
      "  timers:\n",
      "    learn_throughput: 90.597\n",
      "    learn_time_ms: 88214.407\n",
      "    sample_throughput: 302.098\n",
      "    sample_time_ms: 26455.013\n",
      "    update_time_ms: 2.808\n",
      "  timestamp: 1639188535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2117880\n",
      "  training_iteration: 265\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         30303.4</td><td style=\"text-align: right;\">2117880</td><td style=\"text-align: right;\">0.206826</td><td style=\"text-align: right;\">             1.64635</td><td style=\"text-align: right;\">            -2.25301</td><td style=\"text-align: right;\">           67.8421</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8503488\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9589432207871624\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09109435214012548\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5515983087793563\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 66.78688524590164\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3928912053402542\n",
      "  episode_reward_mean: 0.18765369373868213\n",
      "  episode_reward_min: -1.036734463638199\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 30233\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6392914958000183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019304550249129533\n",
      "          policy_loss: -0.10275988735072315\n",
      "          total_loss: 0.163152057826519\n",
      "          vf_explained_var: 0.5298871397972107\n",
      "          vf_loss: 0.2463660840988159\n",
      "    num_agent_steps_sampled: 8503488\n",
      "    num_agent_steps_trained: 8503488\n",
      "    num_steps_sampled: 2125872\n",
      "    num_steps_trained: 2125872\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.961643835616435\n",
      "    gpu_util_percent0: 0.13760273972602743\n",
      "    ram_util_percent: 93.95890410958904\n",
      "    vram_util_percent0: 0.5193139866664864\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.865343245678062\n",
      "  policy_reward_mean:\n",
      "    main: 0.046913423434670545\n",
      "  policy_reward_min:\n",
      "    main: -1.7885632432057506\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30325818096711404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.862678202032676\n",
      "    mean_inference_ms: 4.404220501105554\n",
      "    mean_raw_obs_processing_ms: 1.3038121367164177\n",
      "  time_since_restore: 30416.772820949554\n",
      "  time_this_iter_s: 113.33181738853455\n",
      "  time_total_s: 30416.772820949554\n",
      "  timers:\n",
      "    learn_throughput: 90.811\n",
      "    learn_time_ms: 88006.966\n",
      "    sample_throughput: 302.256\n",
      "    sample_time_ms: 26441.13\n",
      "    update_time_ms: 2.804\n",
      "  timestamp: 1639188648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2125872\n",
      "  training_iteration: 266\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         30416.8</td><td style=\"text-align: right;\">2125872</td><td style=\"text-align: right;\">0.187654</td><td style=\"text-align: right;\">             1.39289</td><td style=\"text-align: right;\">            -1.03673</td><td style=\"text-align: right;\">           66.7869</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8535456\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7746156496723752\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.026081152926802565\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5684045817398155\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-12-44\n",
      "  done: false\n",
      "  episode_len_mean: 67.42735042735043\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1401464268725379\n",
      "  episode_reward_mean: 0.08610638091198422\n",
      "  episode_reward_min: -1.9054034684869383\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 30350\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6447191865444183\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01956269782781601\n",
      "          policy_loss: -0.10198358589783312\n",
      "          total_loss: 0.17709141551330684\n",
      "          vf_explained_var: 0.49042007327079773\n",
      "          vf_loss: 0.2592677701115608\n",
      "    num_agent_steps_sampled: 8535456\n",
      "    num_agent_steps_trained: 8535456\n",
      "    num_steps_sampled: 2133864\n",
      "    num_steps_trained: 2133864\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.32972972972973\n",
      "    gpu_util_percent0: 0.15925675675675677\n",
      "    ram_util_percent: 94.13986486486486\n",
      "    vram_util_percent0: 0.5201613261830439\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0768967543721115\n",
      "  policy_reward_mean:\n",
      "    main: 0.021526595227996062\n",
      "  policy_reward_min:\n",
      "    main: -1.880163715718692\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3030595281264129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.85491544403261\n",
      "    mean_inference_ms: 4.401053804354786\n",
      "    mean_raw_obs_processing_ms: 1.3038641456965545\n",
      "  time_since_restore: 30531.962957382202\n",
      "  time_this_iter_s: 115.1901364326477\n",
      "  time_total_s: 30531.962957382202\n",
      "  timers:\n",
      "    learn_throughput: 90.691\n",
      "    learn_time_ms: 88123.763\n",
      "    sample_throughput: 303.391\n",
      "    sample_time_ms: 26342.225\n",
      "    update_time_ms: 2.825\n",
      "  timestamp: 1639188764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2133864\n",
      "  training_iteration: 267\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">           30532</td><td style=\"text-align: right;\">2133864</td><td style=\"text-align: right;\">0.0861064</td><td style=\"text-align: right;\">             1.14015</td><td style=\"text-align: right;\">             -1.9054</td><td style=\"text-align: right;\">           67.4274</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8567424\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9645475124863266\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0711775440919229\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.52040718693576\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-14-43\n",
      "  done: false\n",
      "  episode_len_mean: 62.152671755725194\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4475664362208154\n",
      "  episode_reward_mean: 0.21244962182126437\n",
      "  episode_reward_min: -0.9378657416464202\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 30481\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6407130150794983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018961535707116128\n",
      "          policy_loss: -0.10217916740849614\n",
      "          total_loss: 0.18333790718019008\n",
      "          vf_explained_var: 0.5115683078765869\n",
      "          vf_loss: 0.26631851863861083\n",
      "    num_agent_steps_sampled: 8567424\n",
      "    num_agent_steps_trained: 8567424\n",
      "    num_steps_sampled: 2141856\n",
      "    num_steps_trained: 2141856\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.964285714285715\n",
      "    gpu_util_percent0: 0.1907142857142857\n",
      "    ram_util_percent: 94.00194805194805\n",
      "    vram_util_percent0: 0.5199634192724024\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8792452849150552\n",
      "  policy_reward_mean:\n",
      "    main: 0.053112405455316106\n",
      "  policy_reward_min:\n",
      "    main: -1.712720758239819\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30303516356865784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.863619551700257\n",
      "    mean_inference_ms: 4.403751365035321\n",
      "    mean_raw_obs_processing_ms: 1.3040396524531501\n",
      "  time_since_restore: 30651.227532863617\n",
      "  time_this_iter_s: 119.2645754814148\n",
      "  time_total_s: 30651.227532863617\n",
      "  timers:\n",
      "    learn_throughput: 90.325\n",
      "    learn_time_ms: 88480.046\n",
      "    sample_throughput: 301.756\n",
      "    sample_time_ms: 26485.002\n",
      "    update_time_ms: 2.878\n",
      "  timestamp: 1639188883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2141856\n",
      "  training_iteration: 268\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         30651.2</td><td style=\"text-align: right;\">2141856</td><td style=\"text-align: right;\"> 0.21245</td><td style=\"text-align: right;\">             1.44757</td><td style=\"text-align: right;\">           -0.937866</td><td style=\"text-align: right;\">           62.1527</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8599392\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0463526476911724\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1262573099510323\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5251248601295924\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-16-38\n",
      "  done: false\n",
      "  episode_len_mean: 61.40769230769231\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0521935633034203\n",
      "  episode_reward_mean: 0.22937916971612746\n",
      "  episode_reward_min: -1.2458129931400608\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 30611\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6389470145702362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019459787003695966\n",
      "          policy_loss: -0.10231610187888146\n",
      "          total_loss: 0.20834204389899968\n",
      "          vf_explained_var: 0.4816644489765167\n",
      "          vf_loss: 0.2909551120400429\n",
      "    num_agent_steps_sampled: 8599392\n",
      "    num_agent_steps_trained: 8599392\n",
      "    num_steps_sampled: 2149848\n",
      "    num_steps_trained: 2149848\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.639597315436248\n",
      "    gpu_util_percent0: 0.18073825503355703\n",
      "    ram_util_percent: 94.03020134228188\n",
      "    vram_util_percent0: 0.5194816380344117\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0677365329490183\n",
      "  policy_reward_mean:\n",
      "    main: 0.05734479242903188\n",
      "  policy_reward_min:\n",
      "    main: -1.6090841624585144\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303404012855518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.86941234346468\n",
      "    mean_inference_ms: 4.405771097153127\n",
      "    mean_raw_obs_processing_ms: 1.3042870069437493\n",
      "  time_since_restore: 30765.818681955338\n",
      "  time_this_iter_s: 114.59114909172058\n",
      "  time_total_s: 30765.818681955338\n",
      "  timers:\n",
      "    learn_throughput: 90.296\n",
      "    learn_time_ms: 88508.578\n",
      "    sample_throughput: 302.882\n",
      "    sample_time_ms: 26386.512\n",
      "    update_time_ms: 2.951\n",
      "  timestamp: 1639188998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2149848\n",
      "  training_iteration: 269\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         30765.8</td><td style=\"text-align: right;\">2149848</td><td style=\"text-align: right;\">0.229379</td><td style=\"text-align: right;\">             2.05219</td><td style=\"text-align: right;\">            -1.24581</td><td style=\"text-align: right;\">           61.4077</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8631360\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7999360489019982\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0680916780793821\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6570286218986097\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 60.54135338345865\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3493466392701654\n",
      "  episode_reward_mean: 0.10981847257880376\n",
      "  episode_reward_min: -1.2286570809313548\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 30744\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6427342109680175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018988354101777075\n",
      "          policy_loss: -0.10209343107789755\n",
      "          total_loss: 0.18386297111958266\n",
      "          vf_explained_var: 0.5037633776664734\n",
      "          vf_loss: 0.2667306914925575\n",
      "    num_agent_steps_sampled: 8631360\n",
      "    num_agent_steps_trained: 8631360\n",
      "    num_steps_sampled: 2157840\n",
      "    num_steps_trained: 2157840\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.00066666666667\n",
      "    gpu_util_percent0: 0.191\n",
      "    ram_util_percent: 93.96399999999998\n",
      "    vram_util_percent0: 0.5195097071405067\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8897359244799072\n",
      "  policy_reward_mean:\n",
      "    main: 0.027454618144700932\n",
      "  policy_reward_min:\n",
      "    main: -1.7846457398318707\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30393061799934945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.879664877215113\n",
      "    mean_inference_ms: 4.409763022266538\n",
      "    mean_raw_obs_processing_ms: 1.3046981588034798\n",
      "  time_since_restore: 30882.471560001373\n",
      "  time_this_iter_s: 116.65287804603577\n",
      "  time_total_s: 30882.471560001373\n",
      "  timers:\n",
      "    learn_throughput: 90.071\n",
      "    learn_time_ms: 88729.685\n",
      "    sample_throughput: 302.922\n",
      "    sample_time_ms: 26383.003\n",
      "    update_time_ms: 2.981\n",
      "  timestamp: 1639189114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2157840\n",
      "  training_iteration: 270\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         30882.5</td><td style=\"text-align: right;\">2157840</td><td style=\"text-align: right;\">0.109818</td><td style=\"text-align: right;\">             1.34935</td><td style=\"text-align: right;\">            -1.22866</td><td style=\"text-align: right;\">           60.5414</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8663328\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8729942763134965\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09459587920808332\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5607566074241067\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-20-30\n",
      "  done: false\n",
      "  episode_len_mean: 68.44347826086957\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6568508954795909\n",
      "  episode_reward_mean: 0.24542296099549496\n",
      "  episode_reward_min: -0.831085735215721\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 30859\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6443667759895325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019227591164410113\n",
      "          policy_loss: -0.10114980628341437\n",
      "          total_loss: 0.14690392187982798\n",
      "          vf_explained_var: 0.5399776697158813\n",
      "          vf_loss: 0.22858579099178314\n",
      "    num_agent_steps_sampled: 8663328\n",
      "    num_agent_steps_trained: 8663328\n",
      "    num_steps_sampled: 2165832\n",
      "    num_steps_trained: 2165832\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.874496644295302\n",
      "    gpu_util_percent0: 0.18328859060402683\n",
      "    ram_util_percent: 93.9973154362416\n",
      "    vram_util_percent0: 0.5180991627853564\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9135360285759693\n",
      "  policy_reward_mean:\n",
      "    main: 0.06135574024887375\n",
      "  policy_reward_min:\n",
      "    main: -1.6574384514654548\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30326640333739874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.868279503450182\n",
      "    mean_inference_ms: 4.405522700408407\n",
      "    mean_raw_obs_processing_ms: 1.3044561770795695\n",
      "  time_since_restore: 30997.82889342308\n",
      "  time_this_iter_s: 115.35733342170715\n",
      "  time_total_s: 30997.82889342308\n",
      "  timers:\n",
      "    learn_throughput: 90.045\n",
      "    learn_time_ms: 88755.471\n",
      "    sample_throughput: 302.595\n",
      "    sample_time_ms: 26411.541\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1639189230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2165832\n",
      "  training_iteration: 271\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         30997.8</td><td style=\"text-align: right;\">2165832</td><td style=\"text-align: right;\">0.245423</td><td style=\"text-align: right;\">             1.65685</td><td style=\"text-align: right;\">           -0.831086</td><td style=\"text-align: right;\">           68.4435</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8695296\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9788035178347299\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10279352599762867\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6792436415959745\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 69.10344827586206\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4456389182805074\n",
      "  episode_reward_mean: 0.12472315727096545\n",
      "  episode_reward_min: -0.9287740378687371\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 30975\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6392884917259216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018995227228850126\n",
      "          policy_loss: -0.10248048961907626\n",
      "          total_loss: 0.1940658316835761\n",
      "          vf_explained_var: 0.46073001623153687\n",
      "          vf_loss: 0.27731365180015566\n",
      "    num_agent_steps_sampled: 8695296\n",
      "    num_agent_steps_trained: 8695296\n",
      "    num_steps_sampled: 2173824\n",
      "    num_steps_trained: 2173824\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.344\n",
      "    gpu_util_percent0: 0.17646666666666666\n",
      "    ram_util_percent: 94.01333333333334\n",
      "    vram_util_percent0: 0.5180914774596908\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8813262250796479\n",
      "  policy_reward_mean:\n",
      "    main: 0.03118078931774135\n",
      "  policy_reward_min:\n",
      "    main: -1.6792436415959746\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30303849308910835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.862497815980724\n",
      "    mean_inference_ms: 4.403246584609811\n",
      "    mean_raw_obs_processing_ms: 1.3044428665964858\n",
      "  time_since_restore: 31113.946951150894\n",
      "  time_this_iter_s: 116.11805772781372\n",
      "  time_total_s: 31113.946951150894\n",
      "  timers:\n",
      "    learn_throughput: 90.013\n",
      "    learn_time_ms: 88787.32\n",
      "    sample_throughput: 301.773\n",
      "    sample_time_ms: 26483.524\n",
      "    update_time_ms: 2.978\n",
      "  timestamp: 1639189346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2173824\n",
      "  training_iteration: 272\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         31113.9</td><td style=\"text-align: right;\">2173824</td><td style=\"text-align: right;\">0.124723</td><td style=\"text-align: right;\">             1.44564</td><td style=\"text-align: right;\">           -0.928774</td><td style=\"text-align: right;\">           69.1034</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8727264\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0516671768772556\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10007349452378765\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6012429430569115\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 66.4274193548387\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5372616232550107\n",
      "  episode_reward_mean: 0.18070926029620776\n",
      "  episode_reward_min: -1.317192242594984\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 31099\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6410253548622131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019495456639677285\n",
      "          policy_loss: -0.10288516698405147\n",
      "          total_loss: 0.17459545402228832\n",
      "          vf_explained_var: 0.49387526512145996\n",
      "          vf_loss: 0.25774146926403046\n",
      "    num_agent_steps_sampled: 8727264\n",
      "    num_agent_steps_trained: 8727264\n",
      "    num_steps_sampled: 2181816\n",
      "    num_steps_trained: 2181816\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.33533333333333\n",
      "    gpu_util_percent0: 0.17953333333333332\n",
      "    ram_util_percent: 94.00599999999997\n",
      "    vram_util_percent0: 0.5153800592300098\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.058156747143649\n",
      "  policy_reward_mean:\n",
      "    main: 0.045177315074051955\n",
      "  policy_reward_min:\n",
      "    main: -1.6622705774281714\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031485560893275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.868359464396192\n",
      "    mean_inference_ms: 4.405561340539569\n",
      "    mean_raw_obs_processing_ms: 1.3045685581373128\n",
      "  time_since_restore: 31229.739675045013\n",
      "  time_this_iter_s: 115.79272389411926\n",
      "  time_total_s: 31229.739675045013\n",
      "  timers:\n",
      "    learn_throughput: 90.336\n",
      "    learn_time_ms: 88469.764\n",
      "    sample_throughput: 301.475\n",
      "    sample_time_ms: 26509.634\n",
      "    update_time_ms: 2.94\n",
      "  timestamp: 1639189462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2181816\n",
      "  training_iteration: 273\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         31229.7</td><td style=\"text-align: right;\">2181816</td><td style=\"text-align: right;\">0.180709</td><td style=\"text-align: right;\">             1.53726</td><td style=\"text-align: right;\">            -1.31719</td><td style=\"text-align: right;\">           66.4274</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8759232\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7701148090626992\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06917696661421496\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5442979823940227\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 60.8062015503876\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.428652453032973\n",
      "  episode_reward_mean: 0.1622917115424876\n",
      "  episode_reward_min: -0.8833493081292803\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 31228\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6405677227973938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019262785725295543\n",
      "          policy_loss: -0.10249769457429647\n",
      "          total_loss: 0.176128199191764\n",
      "          vf_explained_var: 0.52000892162323\n",
      "          vf_loss: 0.25912232261896134\n",
      "    num_agent_steps_sampled: 8759232\n",
      "    num_agent_steps_trained: 8759232\n",
      "    num_steps_sampled: 2189808\n",
      "    num_steps_trained: 2189808\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.724999999999994\n",
      "    gpu_util_percent0: 0.17972972972972973\n",
      "    ram_util_percent: 94.05810810810813\n",
      "    vram_util_percent0: 0.5148386293499818\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9298506547636098\n",
      "  policy_reward_mean:\n",
      "    main: 0.040572927885621904\n",
      "  policy_reward_min:\n",
      "    main: -1.55041499704243\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303601936674849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.877561725220954\n",
      "    mean_inference_ms: 4.408911525239819\n",
      "    mean_raw_obs_processing_ms: 1.304897511113566\n",
      "  time_since_restore: 31344.72402358055\n",
      "  time_this_iter_s: 114.98434853553772\n",
      "  time_total_s: 31344.72402358055\n",
      "  timers:\n",
      "    learn_throughput: 90.184\n",
      "    learn_time_ms: 88618.36\n",
      "    sample_throughput: 301.683\n",
      "    sample_time_ms: 26491.422\n",
      "    update_time_ms: 2.948\n",
      "  timestamp: 1639189577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2189808\n",
      "  training_iteration: 274\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         31344.7</td><td style=\"text-align: right;\">2189808</td><td style=\"text-align: right;\">0.162292</td><td style=\"text-align: right;\">             1.42865</td><td style=\"text-align: right;\">           -0.883349</td><td style=\"text-align: right;\">           60.8062</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8791200\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.720848907143071\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.054315966892881526\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7330288737927125\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 65.31666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.40971765764308\n",
      "  episode_reward_mean: 0.2003524347174654\n",
      "  episode_reward_min: -1.4758575693121152\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 31348\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6475865550041199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01908965900912881\n",
      "          policy_loss: -0.10097460731118918\n",
      "          total_loss: 0.13879808393865825\n",
      "          vf_explained_var: 0.5621029734611511\n",
      "          vf_loss: 0.2204444102048874\n",
      "    num_agent_steps_sampled: 8791200\n",
      "    num_agent_steps_trained: 8791200\n",
      "    num_steps_sampled: 2197800\n",
      "    num_steps_trained: 2197800\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.639597315436244\n",
      "    gpu_util_percent0: 0.17966442953020137\n",
      "    ram_util_percent: 94.0543624161074\n",
      "    vram_util_percent0: 0.5148141277486634\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7758353103208586\n",
      "  policy_reward_mean:\n",
      "    main: 0.05008810867936635\n",
      "  policy_reward_min:\n",
      "    main: -1.7330288737927124\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033358811448259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.871862796986495\n",
      "    mean_inference_ms: 4.40739815913437\n",
      "    mean_raw_obs_processing_ms: 1.3048576851802243\n",
      "  time_since_restore: 31459.25774526596\n",
      "  time_this_iter_s: 114.53372168540955\n",
      "  time_total_s: 31459.25774526596\n",
      "  timers:\n",
      "    learn_throughput: 90.069\n",
      "    learn_time_ms: 88732.337\n",
      "    sample_throughput: 301.561\n",
      "    sample_time_ms: 26502.118\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1639189691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2197800\n",
      "  training_iteration: 275\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         31459.3</td><td style=\"text-align: right;\">2197800</td><td style=\"text-align: right;\">0.200352</td><td style=\"text-align: right;\">             1.40972</td><td style=\"text-align: right;\">            -1.47586</td><td style=\"text-align: right;\">           65.3167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8823168\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9448522635958501\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.056988842995805845\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.2286186127676044\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 67.15044247787611\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0691735321777118\n",
      "  episode_reward_mean: 0.19914975117811193\n",
      "  episode_reward_min: -1.0845970819571282\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 31461\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6413476777076721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01900969520583749\n",
      "          policy_loss: -0.10144354040175677\n",
      "          total_loss: 0.15705517410486936\n",
      "          vf_explained_var: 0.5045108795166016\n",
      "          vf_loss: 0.23925139677524568\n",
      "    num_agent_steps_sampled: 8823168\n",
      "    num_agent_steps_trained: 8823168\n",
      "    num_steps_sampled: 2205792\n",
      "    num_steps_trained: 2205792\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.970270270270273\n",
      "    gpu_util_percent0: 0.1789189189189189\n",
      "    ram_util_percent: 94.11756756756756\n",
      "    vram_util_percent0: 0.5148330709781845\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.889452249672205\n",
      "  policy_reward_mean:\n",
      "    main: 0.04978743779452799\n",
      "  policy_reward_min:\n",
      "    main: -2.2987435238680396\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30296852266901814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.86145562955596\n",
      "    mean_inference_ms: 4.403023589463228\n",
      "    mean_raw_obs_processing_ms: 1.304803841106189\n",
      "  time_since_restore: 31574.27330303192\n",
      "  time_this_iter_s: 115.0155577659607\n",
      "  time_total_s: 31574.27330303192\n",
      "  timers:\n",
      "    learn_throughput: 89.895\n",
      "    learn_time_ms: 88903.271\n",
      "    sample_throughput: 301.535\n",
      "    sample_time_ms: 26504.422\n",
      "    update_time_ms: 3.021\n",
      "  timestamp: 1639189807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2205792\n",
      "  training_iteration: 276\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         31574.3</td><td style=\"text-align: right;\">2205792</td><td style=\"text-align: right;\"> 0.19915</td><td style=\"text-align: right;\">             1.06917</td><td style=\"text-align: right;\">             -1.0846</td><td style=\"text-align: right;\">           67.1504</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8855136\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8803136090114152\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06841488544177225\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4917852654433208\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-32-04\n",
      "  done: false\n",
      "  episode_len_mean: 71.77310924369748\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.796434046618561\n",
      "  episode_reward_mean: 0.13368497281617994\n",
      "  episode_reward_min: -0.9574408482318377\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 31580\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6397659122943878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01947747565433383\n",
      "          policy_loss: -0.10455525602400303\n",
      "          total_loss: 0.16718147775530814\n",
      "          vf_explained_var: 0.4993358254432678\n",
      "          vf_loss: 0.2520157883763313\n",
      "    num_agent_steps_sampled: 8855136\n",
      "    num_agent_steps_trained: 8855136\n",
      "    num_steps_sampled: 2213784\n",
      "    num_steps_trained: 2213784\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.217333333333336\n",
      "    gpu_util_percent0: 0.18519999999999998\n",
      "    ram_util_percent: 94.25466666666667\n",
      "    vram_util_percent0: 0.5176571240539651\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.232688048860849\n",
      "  policy_reward_mean:\n",
      "    main: 0.033421243204044986\n",
      "  policy_reward_min:\n",
      "    main: -1.5797924998973345\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034187800617451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.874925780834193\n",
      "    mean_inference_ms: 4.408314057288467\n",
      "    mean_raw_obs_processing_ms: 1.3051771160630332\n",
      "  time_since_restore: 31691.32225561142\n",
      "  time_this_iter_s: 117.04895257949829\n",
      "  time_total_s: 31691.32225561142\n",
      "  timers:\n",
      "    learn_throughput: 89.773\n",
      "    learn_time_ms: 89024.851\n",
      "    sample_throughput: 300.82\n",
      "    sample_time_ms: 26567.408\n",
      "    update_time_ms: 3.07\n",
      "  timestamp: 1639189924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2213784\n",
      "  training_iteration: 277\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         31691.3</td><td style=\"text-align: right;\">2213784</td><td style=\"text-align: right;\">0.133685</td><td style=\"text-align: right;\">             1.79643</td><td style=\"text-align: right;\">           -0.957441</td><td style=\"text-align: right;\">           71.7731</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8887104\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0618217947682012\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05283544613455858\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6408992330396717\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 64.77049180327869\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3798723730347493\n",
      "  episode_reward_mean: 0.17121455266298136\n",
      "  episode_reward_min: -1.4102379865185715\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 31702\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6374128496646881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019230634763836862\n",
      "          policy_loss: -0.10200370152294636\n",
      "          total_loss: 0.16338548823446036\n",
      "          vf_explained_var: 0.5152508020401001\n",
      "          vf_loss: 0.24591816979646683\n",
      "    num_agent_steps_sampled: 8887104\n",
      "    num_agent_steps_trained: 8887104\n",
      "    num_steps_sampled: 2221776\n",
      "    num_steps_trained: 2221776\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.583333333333336\n",
      "    gpu_util_percent0: 0.18166666666666667\n",
      "    ram_util_percent: 94.08533333333332\n",
      "    vram_util_percent0: 0.520307118569705\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.008439979839962\n",
      "  policy_reward_mean:\n",
      "    main: 0.04280363816574533\n",
      "  policy_reward_min:\n",
      "    main: -1.7082669776005703\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3029907064620522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.867750464564303\n",
      "    mean_inference_ms: 4.405784344424015\n",
      "    mean_raw_obs_processing_ms: 1.305134212440052\n",
      "  time_since_restore: 31807.402940511703\n",
      "  time_this_iter_s: 116.08068490028381\n",
      "  time_total_s: 31807.402940511703\n",
      "  timers:\n",
      "    learn_throughput: 90.014\n",
      "    learn_time_ms: 88786.367\n",
      "    sample_throughput: 301.776\n",
      "    sample_time_ms: 26483.238\n",
      "    update_time_ms: 3.026\n",
      "  timestamp: 1639190040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2221776\n",
      "  training_iteration: 278\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         31807.4</td><td style=\"text-align: right;\">2221776</td><td style=\"text-align: right;\">0.171215</td><td style=\"text-align: right;\">             1.37987</td><td style=\"text-align: right;\">            -1.41024</td><td style=\"text-align: right;\">           64.7705</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8919072\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7878645322429029\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06374367016218545\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8618687899583741\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 56.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.615634210089558\n",
      "  episode_reward_mean: 0.1805221414538844\n",
      "  episode_reward_min: -0.7227515194310206\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 31842\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6199939985275269\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0191695467941463\n",
      "          policy_loss: -0.1028958711028099\n",
      "          total_loss: 0.17761325086280705\n",
      "          vf_explained_var: 0.529475212097168\n",
      "          vf_loss: 0.26109995585680007\n",
      "    num_agent_steps_sampled: 8919072\n",
      "    num_agent_steps_trained: 8919072\n",
      "    num_steps_sampled: 2229768\n",
      "    num_steps_trained: 2229768\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.083561643835615\n",
      "    gpu_util_percent0: 0.14993150684931508\n",
      "    ram_util_percent: 94.23835616438357\n",
      "    vram_util_percent0: 0.5206707776079911\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1277490570889563\n",
      "  policy_reward_mean:\n",
      "    main: 0.04513053536347111\n",
      "  policy_reward_min:\n",
      "    main: -1.8618687899583741\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30320646171331245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.877959367575368\n",
      "    mean_inference_ms: 4.408732854142413\n",
      "    mean_raw_obs_processing_ms: 1.3057516461256085\n",
      "  time_since_restore: 31921.11709189415\n",
      "  time_this_iter_s: 113.71415138244629\n",
      "  time_total_s: 31921.11709189415\n",
      "  timers:\n",
      "    learn_throughput: 90.266\n",
      "    learn_time_ms: 88537.969\n",
      "    sample_throughput: 299.877\n",
      "    sample_time_ms: 26650.933\n",
      "    update_time_ms: 2.958\n",
      "  timestamp: 1639190154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2229768\n",
      "  training_iteration: 279\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         31921.1</td><td style=\"text-align: right;\">2229768</td><td style=\"text-align: right;\">0.180522</td><td style=\"text-align: right;\">             1.61563</td><td style=\"text-align: right;\">           -0.722752</td><td style=\"text-align: right;\">              56.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8951040\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7838810317401008\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.068840286103585\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4278972853016015\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 62.261538461538464\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7008147863413576\n",
      "  episode_reward_mean: 0.16916473061089782\n",
      "  episode_reward_min: -1.0065646041996665\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 31972\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6315143015384674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019147242341190578\n",
      "          policy_loss: -0.10094412076473236\n",
      "          total_loss: 0.16742915768548847\n",
      "          vf_explained_var: 0.545315682888031\n",
      "          vf_loss: 0.2489866944551468\n",
      "    num_agent_steps_sampled: 8951040\n",
      "    num_agent_steps_trained: 8951040\n",
      "    num_steps_sampled: 2237760\n",
      "    num_steps_trained: 2237760\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.471917808219175\n",
      "    gpu_util_percent0: 0.13767123287671235\n",
      "    ram_util_percent: 94.22397260273974\n",
      "    vram_util_percent0: 0.5192046770972788\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9611190773699432\n",
      "  policy_reward_mean:\n",
      "    main: 0.042291182652724454\n",
      "  policy_reward_min:\n",
      "    main: -1.7229765005951743\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032202502022087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.873392952160977\n",
      "    mean_inference_ms: 4.406587175464006\n",
      "    mean_raw_obs_processing_ms: 1.3060103185805623\n",
      "  time_since_restore: 32034.042789936066\n",
      "  time_this_iter_s: 112.9256980419159\n",
      "  time_total_s: 32034.042789936066\n",
      "  timers:\n",
      "    learn_throughput: 90.653\n",
      "    learn_time_ms: 88160.164\n",
      "    sample_throughput: 299.868\n",
      "    sample_time_ms: 26651.695\n",
      "    update_time_ms: 2.976\n",
      "  timestamp: 1639190267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2237760\n",
      "  training_iteration: 280\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">           32034</td><td style=\"text-align: right;\">2237760</td><td style=\"text-align: right;\">0.169165</td><td style=\"text-align: right;\">             1.70081</td><td style=\"text-align: right;\">            -1.00656</td><td style=\"text-align: right;\">           62.2615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 8983008\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8290130164212968\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0517736391881413\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6063548209263947\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 58.02158273381295\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0458542281426855\n",
      "  episode_reward_mean: 0.13041544424407112\n",
      "  episode_reward_min: -0.6023033458792253\n",
      "  episodes_this_iter: 139\n",
      "  episodes_total: 32111\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6244958691596985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019327565789222716\n",
      "          policy_loss: -0.10092461244389415\n",
      "          total_loss: 0.18581405618041755\n",
      "          vf_explained_var: 0.5267012715339661\n",
      "          vf_loss: 0.2671695057153702\n",
      "    num_agent_steps_sampled: 8983008\n",
      "    num_agent_steps_trained: 8983008\n",
      "    num_steps_sampled: 2245752\n",
      "    num_steps_trained: 2245752\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.998000000000005\n",
      "    gpu_util_percent0: 0.15046666666666667\n",
      "    ram_util_percent: 94.16400000000002\n",
      "    vram_util_percent0: 0.5173609740046067\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.3136524783913703\n",
      "  policy_reward_mean:\n",
      "    main: 0.03260386106101779\n",
      "  policy_reward_min:\n",
      "    main: -1.6063548209263947\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034748333413117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.886957373943314\n",
      "    mean_inference_ms: 4.4103084065647655\n",
      "    mean_raw_obs_processing_ms: 1.3067336859773344\n",
      "  time_since_restore: 32151.41146492958\n",
      "  time_this_iter_s: 117.36867499351501\n",
      "  time_total_s: 32151.41146492958\n",
      "  timers:\n",
      "    learn_throughput: 90.567\n",
      "    learn_time_ms: 88243.855\n",
      "    sample_throughput: 298.585\n",
      "    sample_time_ms: 26766.278\n",
      "    update_time_ms: 2.997\n",
      "  timestamp: 1639190384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2245752\n",
      "  training_iteration: 281\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         32151.4</td><td style=\"text-align: right;\">2245752</td><td style=\"text-align: right;\">0.130415</td><td style=\"text-align: right;\">             2.04585</td><td style=\"text-align: right;\">           -0.602303</td><td style=\"text-align: right;\">           58.0216</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9014976\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7974422435385267\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.042191540634561736\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5830993012549824\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 62.1796875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3000848978557178\n",
      "  episode_reward_mean: 0.10435491338065954\n",
      "  episode_reward_min: -1.1615409233652272\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 32239\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6362437653541565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01917650556564331\n",
      "          policy_loss: -0.10117994919791817\n",
      "          total_loss: 0.20077579287067054\n",
      "          vf_explained_var: 0.47005629539489746\n",
      "          vf_loss: 0.2825395302772522\n",
      "    num_agent_steps_sampled: 9014976\n",
      "    num_agent_steps_trained: 9014976\n",
      "    num_steps_sampled: 2253744\n",
      "    num_steps_trained: 2253744\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.68445945945946\n",
      "    gpu_util_percent0: 0.14358108108108109\n",
      "    ram_util_percent: 94.23918918918919\n",
      "    vram_util_percent0: 0.5197199914623409\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0440247411498573\n",
      "  policy_reward_mean:\n",
      "    main: 0.02608872834516488\n",
      "  policy_reward_min:\n",
      "    main: -1.7626453428615112\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3037970301425168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.895096652213212\n",
      "    mean_inference_ms: 4.413055030503811\n",
      "    mean_raw_obs_processing_ms: 1.3071450610021929\n",
      "  time_since_restore: 32266.13425707817\n",
      "  time_this_iter_s: 114.72279214859009\n",
      "  time_total_s: 32266.13425707817\n",
      "  timers:\n",
      "    learn_throughput: 90.75\n",
      "    learn_time_ms: 88066.271\n",
      "    sample_throughput: 298.172\n",
      "    sample_time_ms: 26803.333\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1639190499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2253744\n",
      "  training_iteration: 282\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         32266.1</td><td style=\"text-align: right;\">2253744</td><td style=\"text-align: right;\">0.104355</td><td style=\"text-align: right;\">             1.30008</td><td style=\"text-align: right;\">            -1.16154</td><td style=\"text-align: right;\">           62.1797</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9046944\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8504510651939408\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.050825261435005764\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5496171843996188\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-43-39\n",
      "  done: false\n",
      "  episode_len_mean: 61.9296875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3880316789207612\n",
      "  episode_reward_mean: 0.13337580024324888\n",
      "  episode_reward_min: -1.357220841203362\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 32367\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6258971059322357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019351167909801008\n",
      "          policy_loss: -0.10158545894920826\n",
      "          total_loss: 0.19343154494464398\n",
      "          vf_explained_var: 0.5006651282310486\n",
      "          vf_loss: 0.27542394542694093\n",
      "    num_agent_steps_sampled: 9046944\n",
      "    num_agent_steps_trained: 9046944\n",
      "    num_steps_sampled: 2261736\n",
      "    num_steps_trained: 2261736\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.02903225806452\n",
      "    gpu_util_percent0: 0.19780645161290325\n",
      "    ram_util_percent: 94.27290322580646\n",
      "    vram_util_percent0: 0.5227515417847551\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.907559390369697\n",
      "  policy_reward_mean:\n",
      "    main: 0.03334395006081219\n",
      "  policy_reward_min:\n",
      "    main: -1.7786613659255863\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3032219102877304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.88081029660644\n",
      "    mean_inference_ms: 4.407642084120351\n",
      "    mean_raw_obs_processing_ms: 1.3072056704683508\n",
      "  time_since_restore: 32385.91050195694\n",
      "  time_this_iter_s: 119.77624487876892\n",
      "  time_total_s: 32385.91050195694\n",
      "  timers:\n",
      "    learn_throughput: 90.3\n",
      "    learn_time_ms: 88504.496\n",
      "    sample_throughput: 298.621\n",
      "    sample_time_ms: 26763.041\n",
      "    update_time_ms: 2.979\n",
      "  timestamp: 1639190619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2261736\n",
      "  training_iteration: 283\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         32385.9</td><td style=\"text-align: right;\">2261736</td><td style=\"text-align: right;\">0.133376</td><td style=\"text-align: right;\">             1.38803</td><td style=\"text-align: right;\">            -1.35722</td><td style=\"text-align: right;\">           61.9297</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9078912\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7497865535886857\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.059494234802636436\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7318373988321416\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 61.51162790697674\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2867079403498676\n",
      "  episode_reward_mean: 0.14102911063878035\n",
      "  episode_reward_min: -0.9911518694876884\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 32496\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6339413337707519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01958185650408268\n",
      "          policy_loss: -0.10213342666998505\n",
      "          total_loss: 0.17703681077063083\n",
      "          vf_explained_var: 0.509976327419281\n",
      "          vf_loss: 0.25934360587596894\n",
      "    num_agent_steps_sampled: 9078912\n",
      "    num_agent_steps_trained: 9078912\n",
      "    num_steps_sampled: 2269728\n",
      "    num_steps_trained: 2269728\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.01133333333334\n",
      "    gpu_util_percent0: 0.17806666666666668\n",
      "    ram_util_percent: 94.22666666666669\n",
      "    vram_util_percent0: 0.5217747065920806\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8605788810790544\n",
      "  policy_reward_mean:\n",
      "    main: 0.03525727765969511\n",
      "  policy_reward_min:\n",
      "    main: -1.7020363604019675\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039316347445918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.900585343756536\n",
      "    mean_inference_ms: 4.414967070404065\n",
      "    mean_raw_obs_processing_ms: 1.307627138806102\n",
      "  time_since_restore: 32501.970559358597\n",
      "  time_this_iter_s: 116.0600574016571\n",
      "  time_total_s: 32501.970559358597\n",
      "  timers:\n",
      "    learn_throughput: 90.246\n",
      "    learn_time_ms: 88557.633\n",
      "    sample_throughput: 298.055\n",
      "    sample_time_ms: 26813.832\n",
      "    update_time_ms: 2.958\n",
      "  timestamp: 1639190735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2269728\n",
      "  training_iteration: 284\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">           32502</td><td style=\"text-align: right;\">2269728</td><td style=\"text-align: right;\">0.141029</td><td style=\"text-align: right;\">             1.28671</td><td style=\"text-align: right;\">           -0.991152</td><td style=\"text-align: right;\">           61.5116</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9110880\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6806326283270259\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.040736193240381864\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7177364027199344\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-47-31\n",
      "  done: false\n",
      "  episode_len_mean: 65.6311475409836\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4244335985707397\n",
      "  episode_reward_mean: 0.07714958867682399\n",
      "  episode_reward_min: -0.7865293119592511\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 32618\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6322631177902222\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019524878185242414\n",
      "          policy_loss: -0.10149416806548833\n",
      "          total_loss: 0.16061985180899502\n",
      "          vf_explained_var: 0.5305484533309937\n",
      "          vf_loss: 0.24234508109092712\n",
      "    num_agent_steps_sampled: 9110880\n",
      "    num_agent_steps_trained: 9110880\n",
      "    num_steps_sampled: 2277720\n",
      "    num_steps_trained: 2277720\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.704\n",
      "    gpu_util_percent0: 0.1806\n",
      "    ram_util_percent: 94.31866666666669\n",
      "    vram_util_percent0: 0.5212844137325876\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8776214472866282\n",
      "  policy_reward_mean:\n",
      "    main: 0.01928739716920601\n",
      "  policy_reward_min:\n",
      "    main: -1.7177364027199344\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3031368455145636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.88092568303785\n",
      "    mean_inference_ms: 4.406917560852797\n",
      "    mean_raw_obs_processing_ms: 1.3077205126851703\n",
      "  time_since_restore: 32618.081569433212\n",
      "  time_this_iter_s: 116.11101007461548\n",
      "  time_total_s: 32618.081569433212\n",
      "  timers:\n",
      "    learn_throughput: 90.147\n",
      "    learn_time_ms: 88655.219\n",
      "    sample_throughput: 297.325\n",
      "    sample_time_ms: 26879.657\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1639190851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2277720\n",
      "  training_iteration: 285\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         32618.1</td><td style=\"text-align: right;\">2277720</td><td style=\"text-align: right;\">0.0771496</td><td style=\"text-align: right;\">             1.42443</td><td style=\"text-align: right;\">           -0.786529</td><td style=\"text-align: right;\">           65.6311</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9142848\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6766398870740914\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04813107025549652\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.627951043927362\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-49-26\n",
      "  done: false\n",
      "  episode_len_mean: 66.21138211382114\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4689435702888731\n",
      "  episode_reward_mean: 0.1437265569763308\n",
      "  episode_reward_min: -1.2234687053837696\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 32741\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.631692667722702\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019229850675910713\n",
      "          policy_loss: -0.10160479231178761\n",
      "          total_loss: 0.15961001613363623\n",
      "          vf_explained_var: 0.5442957282066345\n",
      "          vf_loss: 0.24174458467960358\n",
      "    num_agent_steps_sampled: 9142848\n",
      "    num_agent_steps_trained: 9142848\n",
      "    num_steps_sampled: 2285712\n",
      "    num_steps_trained: 2285712\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.53288590604026\n",
      "    gpu_util_percent0: 0.18107382550335568\n",
      "    ram_util_percent: 94.23087248322148\n",
      "    vram_util_percent0: 0.5175934330217243\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9923021839072743\n",
      "  policy_reward_mean:\n",
      "    main: 0.03593163924408268\n",
      "  policy_reward_min:\n",
      "    main: -1.5889668527376202\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30348714523578413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89402816482956\n",
      "    mean_inference_ms: 4.412665762424504\n",
      "    mean_raw_obs_processing_ms: 1.3076994436039606\n",
      "  time_since_restore: 32733.212536096573\n",
      "  time_this_iter_s: 115.1309666633606\n",
      "  time_total_s: 32733.212536096573\n",
      "  timers:\n",
      "    learn_throughput: 90.136\n",
      "    learn_time_ms: 88666.031\n",
      "    sample_throughput: 297.309\n",
      "    sample_time_ms: 26881.162\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1639190966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2285712\n",
      "  training_iteration: 286\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         32733.2</td><td style=\"text-align: right;\">2285712</td><td style=\"text-align: right;\">0.143727</td><td style=\"text-align: right;\">             1.46894</td><td style=\"text-align: right;\">            -1.22347</td><td style=\"text-align: right;\">           66.2114</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9174816\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8650116900088283\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05198219815522884\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5067628737662538\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-51-23\n",
      "  done: false\n",
      "  episode_len_mean: 64.232\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5821291682303154\n",
      "  episode_reward_mean: 0.07605592212685827\n",
      "  episode_reward_min: -0.8555544689316394\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 32866\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6264503207206726\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019489274900406597\n",
      "          policy_loss: -0.10147344485670329\n",
      "          total_loss: 0.18750719190761447\n",
      "          vf_explained_var: 0.5039597749710083\n",
      "          vf_loss: 0.269247745513916\n",
      "    num_agent_steps_sampled: 9174816\n",
      "    num_agent_steps_trained: 9174816\n",
      "    num_steps_sampled: 2293704\n",
      "    num_steps_trained: 2293704\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.858278145695365\n",
      "    gpu_util_percent0: 0.17781456953642386\n",
      "    ram_util_percent: 94.34768211920532\n",
      "    vram_util_percent0: 0.5148380109351064\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.132536883711704\n",
      "  policy_reward_mean:\n",
      "    main: 0.01901398053171458\n",
      "  policy_reward_min:\n",
      "    main: -1.5724621521545714\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3036714735753733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.895114956847177\n",
      "    mean_inference_ms: 4.412871467834731\n",
      "    mean_raw_obs_processing_ms: 1.3080084909243002\n",
      "  time_since_restore: 32850.133427381516\n",
      "  time_this_iter_s: 116.92089128494263\n",
      "  time_total_s: 32850.133427381516\n",
      "  timers:\n",
      "    learn_throughput: 90.181\n",
      "    learn_time_ms: 88622.11\n",
      "    sample_throughput: 296.963\n",
      "    sample_time_ms: 26912.437\n",
      "    update_time_ms: 2.865\n",
      "  timestamp: 1639191083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2293704\n",
      "  training_iteration: 287\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         32850.1</td><td style=\"text-align: right;\">2293704</td><td style=\"text-align: right;\">0.0760559</td><td style=\"text-align: right;\">             1.58213</td><td style=\"text-align: right;\">           -0.855554</td><td style=\"text-align: right;\">            64.232</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9206784\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6663904186999997\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.027165626184395216\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.496366314852371\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-53-19\n",
      "  done: false\n",
      "  episode_len_mean: 60.77272727272727\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.200783241502629\n",
      "  episode_reward_mean: 0.1325790289392442\n",
      "  episode_reward_min: -1.0746234329663324\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 32998\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6268656358718873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019467792220413685\n",
      "          policy_loss: -0.1021133816242218\n",
      "          total_loss: 0.19427046255767347\n",
      "          vf_explained_var: 0.4963682293891907\n",
      "          vf_loss: 0.2766727039217949\n",
      "    num_agent_steps_sampled: 9206784\n",
      "    num_agent_steps_trained: 9206784\n",
      "    num_steps_sampled: 2301696\n",
      "    num_steps_trained: 2301696\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.675496688741724\n",
      "    gpu_util_percent0: 0.17927152317880796\n",
      "    ram_util_percent: 94.19801324503314\n",
      "    vram_util_percent0: 0.514853265168701\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9817450407442339\n",
      "  policy_reward_mean:\n",
      "    main: 0.03314475723481105\n",
      "  policy_reward_min:\n",
      "    main: -1.6109642339996781\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.303673478165769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89682421877978\n",
      "    mean_inference_ms: 4.413562336733195\n",
      "    mean_raw_obs_processing_ms: 1.3081291636203778\n",
      "  time_since_restore: 32966.33883500099\n",
      "  time_this_iter_s: 116.20540761947632\n",
      "  time_total_s: 32966.33883500099\n",
      "  timers:\n",
      "    learn_throughput: 90.176\n",
      "    learn_time_ms: 88626.659\n",
      "    sample_throughput: 296.881\n",
      "    sample_time_ms: 26919.856\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1639191199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2301696\n",
      "  training_iteration: 288\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         32966.3</td><td style=\"text-align: right;\">2301696</td><td style=\"text-align: right;\">0.132579</td><td style=\"text-align: right;\">             2.20078</td><td style=\"text-align: right;\">            -1.07462</td><td style=\"text-align: right;\">           60.7727</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9238752\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8738484475266671\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.031321683995474435\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5472425054673437\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 59.746153846153845\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5870515638365075\n",
      "  episode_reward_mean: 0.07549601559977322\n",
      "  episode_reward_min: -0.809115853449045\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 33128\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.621432463169098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019646230079233648\n",
      "          policy_loss: -0.10295553139969707\n",
      "          total_loss: 0.17545451778918505\n",
      "          vf_explained_var: 0.530644416809082\n",
      "          vf_loss: 0.2585182398557663\n",
      "    num_agent_steps_sampled: 9238752\n",
      "    num_agent_steps_trained: 9238752\n",
      "    num_steps_sampled: 2309688\n",
      "    num_steps_trained: 2309688\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.88\n",
      "    gpu_util_percent0: 0.1844\n",
      "    ram_util_percent: 94.29533333333335\n",
      "    vram_util_percent0: 0.5152626960623014\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8351959836167082\n",
      "  policy_reward_mean:\n",
      "    main: 0.018874003899943304\n",
      "  policy_reward_min:\n",
      "    main: -1.984077033096421\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3033934609904062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.887596436196333\n",
      "    mean_inference_ms: 4.409787423473453\n",
      "    mean_raw_obs_processing_ms: 1.3083777938064944\n",
      "  time_since_restore: 33082.67270183563\n",
      "  time_this_iter_s: 116.3338668346405\n",
      "  time_total_s: 33082.67270183563\n",
      "  timers:\n",
      "    learn_throughput: 89.798\n",
      "    learn_time_ms: 89000.141\n",
      "    sample_throughput: 298.154\n",
      "    sample_time_ms: 26804.903\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639191316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2309688\n",
      "  training_iteration: 289\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         33082.7</td><td style=\"text-align: right;\">2309688</td><td style=\"text-align: right;\">0.075496</td><td style=\"text-align: right;\">             1.58705</td><td style=\"text-align: right;\">           -0.809116</td><td style=\"text-align: right;\">           59.7462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9270720\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0720527198745264\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05556676199490437\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7625102653565271\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 65.99186991869918\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3578227436177328\n",
      "  episode_reward_mean: 0.1325792233389558\n",
      "  episode_reward_min: -1.5116165947571563\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 33251\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6220711541175842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019249724064022304\n",
      "          policy_loss: -0.10042819911986589\n",
      "          total_loss: 0.17964336458966135\n",
      "          vf_explained_var: 0.49796080589294434\n",
      "          vf_loss: 0.260581217110157\n",
      "    num_agent_steps_sampled: 9270720\n",
      "    num_agent_steps_trained: 9270720\n",
      "    num_steps_sampled: 2317680\n",
      "    num_steps_trained: 2317680\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.60529801324503\n",
      "    gpu_util_percent0: 0.18695364238410594\n",
      "    ram_util_percent: 94.25033112582783\n",
      "    vram_util_percent0: 0.51792808282613\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9040527370406641\n",
      "  policy_reward_mean:\n",
      "    main: 0.03314480583473895\n",
      "  policy_reward_min:\n",
      "    main: -1.7841500764783804\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30346568574589183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89467339553187\n",
      "    mean_inference_ms: 4.412728730228605\n",
      "    mean_raw_obs_processing_ms: 1.3087041966800146\n",
      "  time_since_restore: 33200.05112743378\n",
      "  time_this_iter_s: 117.37842559814453\n",
      "  time_total_s: 33200.05112743378\n",
      "  timers:\n",
      "    learn_throughput: 89.443\n",
      "    learn_time_ms: 89353.163\n",
      "    sample_throughput: 297.153\n",
      "    sample_time_ms: 26895.198\n",
      "    update_time_ms: 2.826\n",
      "  timestamp: 1639191433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2317680\n",
      "  training_iteration: 290\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         33200.1</td><td style=\"text-align: right;\">2317680</td><td style=\"text-align: right;\">0.132579</td><td style=\"text-align: right;\">             1.35782</td><td style=\"text-align: right;\">            -1.51162</td><td style=\"text-align: right;\">           65.9919</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9302688\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6328641687304076\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06846751924309087\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.3941988122864386\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-10_23-59-14\n",
      "  done: false\n",
      "  episode_len_mean: 67.49579831932773\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5075958075864058\n",
      "  episode_reward_mean: 0.12231862582316592\n",
      "  episode_reward_min: -0.7837935594964971\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 33370\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.629621933221817\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01962239468842745\n",
      "          policy_loss: -0.10209483435750008\n",
      "          total_loss: 0.18619201747700573\n",
      "          vf_explained_var: 0.47752276062965393\n",
      "          vf_loss: 0.2684191761016846\n",
      "    num_agent_steps_sampled: 9302688\n",
      "    num_agent_steps_trained: 9302688\n",
      "    num_steps_sampled: 2325672\n",
      "    num_steps_trained: 2325672\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.09225806451613\n",
      "    gpu_util_percent0: 0.20012903225806453\n",
      "    ram_util_percent: 94.22000000000001\n",
      "    vram_util_percent0: 0.5211614601577345\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9810913938559258\n",
      "  policy_reward_mean:\n",
      "    main: 0.030579656455791467\n",
      "  policy_reward_min:\n",
      "    main: -1.5837684296309575\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30358702764289297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.894558323916087\n",
      "    mean_inference_ms: 4.412812104542835\n",
      "    mean_raw_obs_processing_ms: 1.3088967981973911\n",
      "  time_since_restore: 33320.69482302666\n",
      "  time_this_iter_s: 120.64369559288025\n",
      "  time_total_s: 33320.69482302666\n",
      "  timers:\n",
      "    learn_throughput: 88.95\n",
      "    learn_time_ms: 89848.482\n",
      "    sample_throughput: 299.004\n",
      "    sample_time_ms: 26728.743\n",
      "    update_time_ms: 2.826\n",
      "  timestamp: 1639191554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2325672\n",
      "  training_iteration: 291\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         33320.7</td><td style=\"text-align: right;\">2325672</td><td style=\"text-align: right;\">0.122319</td><td style=\"text-align: right;\">              1.5076</td><td style=\"text-align: right;\">           -0.783794</td><td style=\"text-align: right;\">           67.4958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9334656\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.920676887800925\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09650147999026241\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5842164977501771\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 60.15555555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3906545511467905\n",
      "  episode_reward_mean: 0.18215317318336513\n",
      "  episode_reward_min: -1.1175228835406843\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 33505\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6233107702732086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019757005754858257\n",
      "          policy_loss: -0.10317381670698524\n",
      "          total_loss: 0.192064971357584\n",
      "          vf_explained_var: 0.5058692693710327\n",
      "          vf_loss: 0.2752348175644875\n",
      "    num_agent_steps_sampled: 9334656\n",
      "    num_agent_steps_trained: 9334656\n",
      "    num_steps_sampled: 2333664\n",
      "    num_steps_trained: 2333664\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.775838926174497\n",
      "    gpu_util_percent0: 0.1801342281879195\n",
      "    ram_util_percent: 94.25637583892617\n",
      "    vram_util_percent0: 0.5206896475571486\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9942282841234318\n",
      "  policy_reward_mean:\n",
      "    main: 0.045538293295841276\n",
      "  policy_reward_min:\n",
      "    main: -1.6812955794528013\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034741305402271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89421714408359\n",
      "    mean_inference_ms: 4.412544352471408\n",
      "    mean_raw_obs_processing_ms: 1.3090615424750545\n",
      "  time_since_restore: 33436.22812008858\n",
      "  time_this_iter_s: 115.53329706192017\n",
      "  time_total_s: 33436.22812008858\n",
      "  timers:\n",
      "    learn_throughput: 88.864\n",
      "    learn_time_ms: 89935.578\n",
      "    sample_throughput: 299.06\n",
      "    sample_time_ms: 26723.775\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1639191670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2333664\n",
      "  training_iteration: 292\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         33436.2</td><td style=\"text-align: right;\">2333664</td><td style=\"text-align: right;\">0.182153</td><td style=\"text-align: right;\">             1.39065</td><td style=\"text-align: right;\">            -1.11752</td><td style=\"text-align: right;\">           60.1556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9366624\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8536680630775519\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0930438423725069\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.52944089079449\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-03-05\n",
      "  done: false\n",
      "  episode_len_mean: 62.475806451612904\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4257623420737597\n",
      "  episode_reward_mean: 0.16390998142446309\n",
      "  episode_reward_min: -0.6976986354183721\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 33629\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6244314968585968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019702125772833826\n",
      "          policy_loss: -0.10162942397966981\n",
      "          total_loss: 0.16864119943231345\n",
      "          vf_explained_var: 0.5147008299827576\n",
      "          vf_loss: 0.25032222145795824\n",
      "    num_agent_steps_sampled: 9366624\n",
      "    num_agent_steps_trained: 9366624\n",
      "    num_steps_sampled: 2341656\n",
      "    num_steps_trained: 2341656\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.28456375838927\n",
      "    gpu_util_percent0: 0.17973154362416108\n",
      "    ram_util_percent: 94.27315436241611\n",
      "    vram_util_percent0: 0.5206112484016511\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9420435088948458\n",
      "  policy_reward_mean:\n",
      "    main: 0.04097749535611577\n",
      "  policy_reward_min:\n",
      "    main: -1.5294408907944899\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3037547193829662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.898987651983038\n",
      "    mean_inference_ms: 4.414383913811065\n",
      "    mean_raw_obs_processing_ms: 1.3092240368621506\n",
      "  time_since_restore: 33551.22054553032\n",
      "  time_this_iter_s: 114.99242544174194\n",
      "  time_total_s: 33551.22054553032\n",
      "  timers:\n",
      "    learn_throughput: 89.28\n",
      "    learn_time_ms: 89515.787\n",
      "    sample_throughput: 299.668\n",
      "    sample_time_ms: 26669.529\n",
      "    update_time_ms: 2.868\n",
      "  timestamp: 1639191785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2341656\n",
      "  training_iteration: 293\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         33551.2</td><td style=\"text-align: right;\">2341656</td><td style=\"text-align: right;\"> 0.16391</td><td style=\"text-align: right;\">             1.42576</td><td style=\"text-align: right;\">           -0.697699</td><td style=\"text-align: right;\">           62.4758</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9398592\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7173659136450413\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.048037483362639105\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8785934290311014\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 62.42063492063492\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6267566814357588\n",
      "  episode_reward_mean: 0.14025463911586777\n",
      "  episode_reward_min: -1.378383677111545\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 33755\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6205927083492279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019653340119868516\n",
      "          policy_loss: -0.10247923707962037\n",
      "          total_loss: 0.15565592131763697\n",
      "          vf_explained_var: 0.5296234488487244\n",
      "          vf_loss: 0.23823615020513533\n",
      "    num_agent_steps_sampled: 9398592\n",
      "    num_agent_steps_trained: 9398592\n",
      "    num_steps_sampled: 2349648\n",
      "    num_steps_trained: 2349648\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.649664429530205\n",
      "    gpu_util_percent0: 0.18053691275167785\n",
      "    ram_util_percent: 94.29127516778523\n",
      "    vram_util_percent0: 0.5194926801689889\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6400642830764207\n",
      "  policy_reward_mean:\n",
      "    main: 0.035063659778966944\n",
      "  policy_reward_min:\n",
      "    main: -1.8881866965686416\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30378478703134704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.90598551672879\n",
      "    mean_inference_ms: 4.417959249510207\n",
      "    mean_raw_obs_processing_ms: 1.30929398624516\n",
      "  time_since_restore: 33666.44982171059\n",
      "  time_this_iter_s: 115.22927618026733\n",
      "  time_total_s: 33666.44982171059\n",
      "  timers:\n",
      "    learn_throughput: 89.337\n",
      "    learn_time_ms: 89458.888\n",
      "    sample_throughput: 299.911\n",
      "    sample_time_ms: 26647.904\n",
      "    update_time_ms: 2.887\n",
      "  timestamp: 1639191900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2349648\n",
      "  training_iteration: 294\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         33666.4</td><td style=\"text-align: right;\">2349648</td><td style=\"text-align: right;\">0.140255</td><td style=\"text-align: right;\">             1.62676</td><td style=\"text-align: right;\">            -1.37838</td><td style=\"text-align: right;\">           62.4206</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9430560\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1581639329429396\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06348228847303398\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5053502281893423\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 62.19847328244275\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5221710508451745\n",
      "  episode_reward_mean: 0.11429030482678237\n",
      "  episode_reward_min: -0.922410211763194\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 33886\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6184326348304748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01934561067074537\n",
      "          policy_loss: -0.09988535434007645\n",
      "          total_loss: 0.16229710030928254\n",
      "          vf_explained_var: 0.5413697361946106\n",
      "          vf_loss: 0.24259502297639846\n",
      "    num_agent_steps_sampled: 9430560\n",
      "    num_agent_steps_trained: 9430560\n",
      "    num_steps_sampled: 2357640\n",
      "    num_steps_trained: 2357640\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.513422818791945\n",
      "    gpu_util_percent0: 0.17986577181208055\n",
      "    ram_util_percent: 94.35906040268456\n",
      "    vram_util_percent0: 0.5176784574579681\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9048244260046334\n",
      "  policy_reward_mean:\n",
      "    main: 0.0285725762066956\n",
      "  policy_reward_min:\n",
      "    main: -1.782734659280327\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034369647188465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.89483601693716\n",
      "    mean_inference_ms: 4.413491274231599\n",
      "    mean_raw_obs_processing_ms: 1.3095089529999342\n",
      "  time_since_restore: 33781.525710344315\n",
      "  time_this_iter_s: 115.07588863372803\n",
      "  time_total_s: 33781.525710344315\n",
      "  timers:\n",
      "    learn_throughput: 89.415\n",
      "    learn_time_ms: 89380.858\n",
      "    sample_throughput: 300.246\n",
      "    sample_time_ms: 26618.165\n",
      "    update_time_ms: 2.901\n",
      "  timestamp: 1639192015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2357640\n",
      "  training_iteration: 295\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         33781.5</td><td style=\"text-align: right;\">2357640</td><td style=\"text-align: right;\"> 0.11429</td><td style=\"text-align: right;\">             1.52217</td><td style=\"text-align: right;\">            -0.92241</td><td style=\"text-align: right;\">           62.1985</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9462528\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6327095048756181\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06418206302226635\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5380503184619537\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-08-51\n",
      "  done: false\n",
      "  episode_len_mean: 59.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5017484567983432\n",
      "  episode_reward_mean: 0.19044415064641815\n",
      "  episode_reward_min: -1.560300257978371\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 34021\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6222650685310364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019587984684854745\n",
      "          policy_loss: -0.10096304296329618\n",
      "          total_loss: 0.1931447058841586\n",
      "          vf_explained_var: 0.499108225107193\n",
      "          vf_loss: 0.2742749141454697\n",
      "    num_agent_steps_sampled: 9462528\n",
      "    num_agent_steps_trained: 9462528\n",
      "    num_steps_sampled: 2365632\n",
      "    num_steps_trained: 2365632\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.12133333333333\n",
      "    gpu_util_percent0: 0.1838\n",
      "    ram_util_percent: 94.37866666666666\n",
      "    vram_util_percent0: 0.5155401996270702\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8467339379220677\n",
      "  policy_reward_mean:\n",
      "    main: 0.04761103766160454\n",
      "  policy_reward_min:\n",
      "    main: -2.0184370074825972\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039021902780329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.90487252665147\n",
      "    mean_inference_ms: 4.4168140246405425\n",
      "    mean_raw_obs_processing_ms: 1.309970239705137\n",
      "  time_since_restore: 33897.28324890137\n",
      "  time_this_iter_s: 115.75753855705261\n",
      "  time_total_s: 33897.28324890137\n",
      "  timers:\n",
      "    learn_throughput: 89.347\n",
      "    learn_time_ms: 89448.991\n",
      "    sample_throughput: 300.316\n",
      "    sample_time_ms: 26612.01\n",
      "    update_time_ms: 2.922\n",
      "  timestamp: 1639192131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2365632\n",
      "  training_iteration: 296\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         33897.3</td><td style=\"text-align: right;\">2365632</td><td style=\"text-align: right;\">0.190444</td><td style=\"text-align: right;\">             1.50175</td><td style=\"text-align: right;\">             -1.5603</td><td style=\"text-align: right;\">              59.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9494496\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8338946414253428\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02941215958046121\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8869392949961852\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 63.66393442622951\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.141109056646624\n",
      "  episode_reward_mean: 0.1322163011109317\n",
      "  episode_reward_min: -1.010346674509538\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 34143\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6165747122764588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018999042328447104\n",
      "          policy_loss: -0.09995998189225792\n",
      "          total_loss: 0.1663162463903427\n",
      "          vf_explained_var: 0.5218011736869812\n",
      "          vf_loss: 0.24703969830274583\n",
      "    num_agent_steps_sampled: 9494496\n",
      "    num_agent_steps_trained: 9494496\n",
      "    num_steps_sampled: 2373624\n",
      "    num_steps_trained: 2373624\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.49161290322581\n",
      "    gpu_util_percent0: 0.1978064516129032\n",
      "    ram_util_percent: 94.53225806451613\n",
      "    vram_util_percent0: 0.5205649141801739\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8496386407749714\n",
      "  policy_reward_mean:\n",
      "    main: 0.03305407527773292\n",
      "  policy_reward_min:\n",
      "    main: -1.893253882975287\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3035989413419948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.904631561837558\n",
      "    mean_inference_ms: 4.416264326474753\n",
      "    mean_raw_obs_processing_ms: 1.3102586806461627\n",
      "  time_since_restore: 34017.92375612259\n",
      "  time_this_iter_s: 120.64050722122192\n",
      "  time_total_s: 34017.92375612259\n",
      "  timers:\n",
      "    learn_throughput: 89.135\n",
      "    learn_time_ms: 89661.879\n",
      "    sample_throughput: 298.567\n",
      "    sample_time_ms: 26767.817\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1639192252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2373624\n",
      "  training_iteration: 297\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         34017.9</td><td style=\"text-align: right;\">2373624</td><td style=\"text-align: right;\">0.132216</td><td style=\"text-align: right;\">             1.14111</td><td style=\"text-align: right;\">            -1.01035</td><td style=\"text-align: right;\">           63.6639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9526464\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.729445917693011\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.02819239051402832\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6672266231894434\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-12-50\n",
      "  done: false\n",
      "  episode_len_mean: 63.91338582677165\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1288700529024323\n",
      "  episode_reward_mean: 0.03518974969077218\n",
      "  episode_reward_min: -2.308015364212506\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 34270\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.620395884513855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019231765516102315\n",
      "          policy_loss: -0.09892794596776366\n",
      "          total_loss: 0.16715087492018937\n",
      "          vf_explained_var: 0.5226837992668152\n",
      "          vf_loss: 0.24660665678977967\n",
      "    num_agent_steps_sampled: 9526464\n",
      "    num_agent_steps_trained: 9526464\n",
      "    num_steps_sampled: 2381616\n",
      "    num_steps_trained: 2381616\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.47960526315789\n",
      "    gpu_util_percent0: 0.17717105263157895\n",
      "    ram_util_percent: 94.52499999999999\n",
      "    vram_util_percent0: 0.5213323288477859\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7871283968144835\n",
      "  policy_reward_mean:\n",
      "    main: 0.008797437422693052\n",
      "  policy_reward_min:\n",
      "    main: -1.800400088454687\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30366260093266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.910541621030582\n",
      "    mean_inference_ms: 4.417566562437401\n",
      "    mean_raw_obs_processing_ms: 1.3107901991749775\n",
      "  time_since_restore: 34136.43474316597\n",
      "  time_this_iter_s: 118.51098704338074\n",
      "  time_total_s: 34136.43474316597\n",
      "  timers:\n",
      "    learn_throughput: 89.018\n",
      "    learn_time_ms: 89779.7\n",
      "    sample_throughput: 297.24\n",
      "    sample_time_ms: 26887.348\n",
      "    update_time_ms: 2.901\n",
      "  timestamp: 1639192370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2381616\n",
      "  training_iteration: 298\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         34136.4</td><td style=\"text-align: right;\">2381616</td><td style=\"text-align: right;\">0.0351897</td><td style=\"text-align: right;\">             1.12887</td><td style=\"text-align: right;\">            -2.30802</td><td style=\"text-align: right;\">           63.9134</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9558432\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6037821389513803\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04843206218570955\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6007495381321343\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 62.40298507462686\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6384369749006815\n",
      "  episode_reward_mean: 0.20897477595430483\n",
      "  episode_reward_min: -0.970920953565247\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 34404\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6166097514629364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01948277123272419\n",
      "          policy_loss: -0.10133725458011031\n",
      "          total_loss: 0.18975937723368405\n",
      "          vf_explained_var: 0.5116675496101379\n",
      "          vf_loss: 0.2713703251481056\n",
      "    num_agent_steps_sampled: 9558432\n",
      "    num_agent_steps_trained: 9558432\n",
      "    num_steps_sampled: 2389608\n",
      "    num_steps_trained: 2389608\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.47074829931972\n",
      "    gpu_util_percent0: 0.13578231292517007\n",
      "    ram_util_percent: 94.44625850340135\n",
      "    vram_util_percent0: 0.5204059247917661\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8812396719375812\n",
      "  policy_reward_mean:\n",
      "    main: 0.05224369398857621\n",
      "  policy_reward_min:\n",
      "    main: -1.6229872953144733\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30436632431244787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.92677494842643\n",
      "    mean_inference_ms: 4.422991204725738\n",
      "    mean_raw_obs_processing_ms: 1.3114668906943523\n",
      "  time_since_restore: 34249.86561894417\n",
      "  time_this_iter_s: 113.43087577819824\n",
      "  time_total_s: 34249.86561894417\n",
      "  timers:\n",
      "    learn_throughput: 89.305\n",
      "    learn_time_ms: 89491.322\n",
      "    sample_throughput: 297.2\n",
      "    sample_time_ms: 26891.015\n",
      "    update_time_ms: 2.895\n",
      "  timestamp: 1639192484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2389608\n",
      "  training_iteration: 299\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         34249.9</td><td style=\"text-align: right;\">2389608</td><td style=\"text-align: right;\">0.208975</td><td style=\"text-align: right;\">             1.63844</td><td style=\"text-align: right;\">           -0.970921</td><td style=\"text-align: right;\">            62.403</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9590400\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6680043616118752\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03820526869623546\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49989663344552765\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-16-39\n",
      "  done: false\n",
      "  episode_len_mean: 57.15107913669065\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.411642440308475\n",
      "  episode_reward_mean: 0.1798375848577445\n",
      "  episode_reward_min: -1.2750371364469988\n",
      "  episodes_this_iter: 139\n",
      "  episodes_total: 34543\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6126400314569473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019359661180526018\n",
      "          policy_loss: -0.10082460901141167\n",
      "          total_loss: 0.1882164563834667\n",
      "          vf_explained_var: 0.5355620384216309\n",
      "          vf_loss: 0.26943940836191177\n",
      "    num_agent_steps_sampled: 9590400\n",
      "    num_agent_steps_trained: 9590400\n",
      "    num_steps_sampled: 2397600\n",
      "    num_steps_trained: 2397600\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.42953020134228\n",
      "    gpu_util_percent0: 0.15879194630872484\n",
      "    ram_util_percent: 94.42080536912752\n",
      "    vram_util_percent0: 0.5211467919286413\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.916439113823118\n",
      "  policy_reward_mean:\n",
      "    main: 0.04495939621443613\n",
      "  policy_reward_min:\n",
      "    main: -1.5674095719639283\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3040935916594661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.920730070799443\n",
      "    mean_inference_ms: 4.421364905442176\n",
      "    mean_raw_obs_processing_ms: 1.311425586079685\n",
      "  time_since_restore: 34365.32475233078\n",
      "  time_this_iter_s: 115.45913338661194\n",
      "  time_total_s: 34365.32475233078\n",
      "  timers:\n",
      "    learn_throughput: 89.365\n",
      "    learn_time_ms: 89431.115\n",
      "    sample_throughput: 298.648\n",
      "    sample_time_ms: 26760.618\n",
      "    update_time_ms: 2.866\n",
      "  timestamp: 1639192599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2397600\n",
      "  training_iteration: 300\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         34365.3</td><td style=\"text-align: right;\">2397600</td><td style=\"text-align: right;\">0.179838</td><td style=\"text-align: right;\">             1.41164</td><td style=\"text-align: right;\">            -1.27504</td><td style=\"text-align: right;\">           57.1511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9622368\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7414560678002079\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04717394883008761\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5114185948635197\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-18-36\n",
      "  done: false\n",
      "  episode_len_mean: 56.35507246376812\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6145761734229804\n",
      "  episode_reward_mean: 0.13712067497741617\n",
      "  episode_reward_min: -1.0184514885633997\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 34681\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6193319497108459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019603414468467237\n",
      "          policy_loss: -0.09921362837962806\n",
      "          total_loss: 0.19969839860498906\n",
      "          vf_explained_var: 0.48984000086784363\n",
      "          vf_loss: 0.2790635677576065\n",
      "    num_agent_steps_sampled: 9622368\n",
      "    num_agent_steps_trained: 9622368\n",
      "    num_steps_sampled: 2405592\n",
      "    num_steps_trained: 2405592\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.716556291390724\n",
      "    gpu_util_percent0: 0.18503311258278146\n",
      "    ram_util_percent: 94.34172185430464\n",
      "    vram_util_percent0: 0.5196202131670186\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0831938561641787\n",
      "  policy_reward_mean:\n",
      "    main: 0.03428016874435404\n",
      "  policy_reward_min:\n",
      "    main: -1.5665149028415954\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30424746893019966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.92262113931163\n",
      "    mean_inference_ms: 4.420526058650091\n",
      "    mean_raw_obs_processing_ms: 1.3121057357764485\n",
      "  time_since_restore: 34482.545634031296\n",
      "  time_this_iter_s: 117.22088170051575\n",
      "  time_total_s: 34482.545634031296\n",
      "  timers:\n",
      "    learn_throughput: 89.84\n",
      "    learn_time_ms: 88958.543\n",
      "    sample_throughput: 297.247\n",
      "    sample_time_ms: 26886.705\n",
      "    update_time_ms: 2.958\n",
      "  timestamp: 1639192716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2405592\n",
      "  training_iteration: 301\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         34482.5</td><td style=\"text-align: right;\">2405592</td><td style=\"text-align: right;\">0.137121</td><td style=\"text-align: right;\">             1.61458</td><td style=\"text-align: right;\">            -1.01845</td><td style=\"text-align: right;\">           56.3551</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9654336\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8261584588859475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0613541965861319\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9327935820823282\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 59.25581395348837\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4555659134739884\n",
      "  episode_reward_mean: 0.06546690352847861\n",
      "  episode_reward_min: -2.5609359769390005\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 34810\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6198442897796631\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019721342649310827\n",
      "          policy_loss: -0.10347135420888662\n",
      "          total_loss: 0.18128763290494682\n",
      "          vf_explained_var: 0.4975557327270508\n",
      "          vf_loss: 0.2647911266684532\n",
      "    num_agent_steps_sampled: 9654336\n",
      "    num_agent_steps_trained: 9654336\n",
      "    num_steps_sampled: 2413584\n",
      "    num_steps_trained: 2413584\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.65878378378378\n",
      "    gpu_util_percent0: 0.18027027027027026\n",
      "    ram_util_percent: 94.35135135135135\n",
      "    vram_util_percent0: 0.5162382273685333\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8523633051087856\n",
      "  policy_reward_mean:\n",
      "    main: 0.01636672588211965\n",
      "  policy_reward_min:\n",
      "    main: -1.9327935820823283\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039237830211963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.917105070977925\n",
      "    mean_inference_ms: 4.41870974214651\n",
      "    mean_raw_obs_processing_ms: 1.31218693147019\n",
      "  time_since_restore: 34597.67263174057\n",
      "  time_this_iter_s: 115.12699770927429\n",
      "  time_total_s: 34597.67263174057\n",
      "  timers:\n",
      "    learn_throughput: 89.843\n",
      "    learn_time_ms: 88955.469\n",
      "    sample_throughput: 297.657\n",
      "    sample_time_ms: 26849.721\n",
      "    update_time_ms: 2.925\n",
      "  timestamp: 1639192832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2413584\n",
      "  training_iteration: 302\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         34597.7</td><td style=\"text-align: right;\">2413584</td><td style=\"text-align: right;\">0.0654669</td><td style=\"text-align: right;\">             1.45557</td><td style=\"text-align: right;\">            -2.56094</td><td style=\"text-align: right;\">           59.2558</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9686304\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.195578459280388\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06941468541878966\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.594186228779082\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 70.42241379310344\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7542245513247292\n",
      "  episode_reward_mean: 0.2496538974093145\n",
      "  episode_reward_min: -0.8143133051302933\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 34926\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6211060428619385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019751191634684803\n",
      "          policy_loss: -0.102600863981992\n",
      "          total_loss: 0.17520534561574458\n",
      "          vf_explained_var: 0.48676037788391113\n",
      "          vf_loss: 0.2578081251382828\n",
      "    num_agent_steps_sampled: 9686304\n",
      "    num_agent_steps_trained: 9686304\n",
      "    num_steps_sampled: 2421576\n",
      "    num_steps_trained: 2421576\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.211920529801326\n",
      "    gpu_util_percent0: 0.1804635761589404\n",
      "    ram_util_percent: 94.46490066225164\n",
      "    vram_util_percent0: 0.5155756620882175\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9736114066041002\n",
      "  policy_reward_mean:\n",
      "    main: 0.06241347435232865\n",
      "  policy_reward_min:\n",
      "    main: -1.594186228779082\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3040205540843206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.922762506396303\n",
      "    mean_inference_ms: 4.421105705791756\n",
      "    mean_raw_obs_processing_ms: 1.312159520896772\n",
      "  time_since_restore: 34714.80727148056\n",
      "  time_this_iter_s: 117.13463973999023\n",
      "  time_total_s: 34714.80727148056\n",
      "  timers:\n",
      "    learn_throughput: 89.71\n",
      "    learn_time_ms: 89087.232\n",
      "    sample_throughput: 296.734\n",
      "    sample_time_ms: 26933.249\n",
      "    update_time_ms: 2.889\n",
      "  timestamp: 1639192949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2421576\n",
      "  training_iteration: 303\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         34714.8</td><td style=\"text-align: right;\">2421576</td><td style=\"text-align: right;\">0.249654</td><td style=\"text-align: right;\">             1.75422</td><td style=\"text-align: right;\">           -0.814313</td><td style=\"text-align: right;\">           70.4224</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9718272\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9857802071684535\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09217527205203024\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5917649542893307\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-24-24\n",
      "  done: false\n",
      "  episode_len_mean: 64.7578125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3903393401097577\n",
      "  episode_reward_mean: 0.17878878040810264\n",
      "  episode_reward_min: -1.0998270673986044\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 35054\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6237432647943497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020057440005242823\n",
      "          policy_loss: -0.10233519260957837\n",
      "          total_loss: 0.1671257127672434\n",
      "          vf_explained_var: 0.5164555311203003\n",
      "          vf_loss: 0.24915274488925934\n",
      "    num_agent_steps_sampled: 9718272\n",
      "    num_agent_steps_trained: 9718272\n",
      "    num_steps_sampled: 2429568\n",
      "    num_steps_trained: 2429568\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.41148648648648\n",
      "    gpu_util_percent0: 0.1762837837837838\n",
      "    ram_util_percent: 94.34797297297297\n",
      "    vram_util_percent0: 0.5145373655985699\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.830318583338478\n",
      "  policy_reward_mean:\n",
      "    main: 0.044697195102025675\n",
      "  policy_reward_min:\n",
      "    main: -1.6398256021277338\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30439007321963585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.933279037784335\n",
      "    mean_inference_ms: 4.4254671328795165\n",
      "    mean_raw_obs_processing_ms: 1.3125251451301088\n",
      "  time_since_restore: 34829.50029754639\n",
      "  time_this_iter_s: 114.69302606582642\n",
      "  time_total_s: 34829.50029754639\n",
      "  timers:\n",
      "    learn_throughput: 89.761\n",
      "    learn_time_ms: 89036.382\n",
      "    sample_throughput: 296.758\n",
      "    sample_time_ms: 26931.012\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1639193064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2429568\n",
      "  training_iteration: 304\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         34829.5</td><td style=\"text-align: right;\">2429568</td><td style=\"text-align: right;\">0.178789</td><td style=\"text-align: right;\">             1.39034</td><td style=\"text-align: right;\">            -1.09983</td><td style=\"text-align: right;\">           64.7578</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9750240\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8998015391773505\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05716896405920822\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.692847974117064\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 60.08661417322835\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.224814612288053\n",
      "  episode_reward_mean: 0.16998101535187532\n",
      "  episode_reward_min: -1.1333485580858502\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 35181\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6147405726909637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014901762440800667\n",
      "          policy_loss: -0.09515771853923798\n",
      "          total_loss: 0.2089739391133189\n",
      "          vf_explained_var: 0.45805951952934265\n",
      "          vf_loss: 0.281499605178833\n",
      "    num_agent_steps_sampled: 9750240\n",
      "    num_agent_steps_trained: 9750240\n",
      "    num_steps_sampled: 2437560\n",
      "    num_steps_trained: 2437560\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.81111111111111\n",
      "    gpu_util_percent0: 0.18915032679738564\n",
      "    ram_util_percent: 94.47777777777777\n",
      "    vram_util_percent0: 0.5153462503790592\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8356048018120625\n",
      "  policy_reward_mean:\n",
      "    main: 0.04249525383796882\n",
      "  policy_reward_min:\n",
      "    main: -1.8464536403120462\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30382127274169546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.921410746754052\n",
      "    mean_inference_ms: 4.420428394987038\n",
      "    mean_raw_obs_processing_ms: 1.3123698421291523\n",
      "  time_since_restore: 34947.539398908615\n",
      "  time_this_iter_s: 118.0391013622284\n",
      "  time_total_s: 34947.539398908615\n",
      "  timers:\n",
      "    learn_throughput: 89.54\n",
      "    learn_time_ms: 89256.644\n",
      "    sample_throughput: 295.954\n",
      "    sample_time_ms: 27004.213\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1639193182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2437560\n",
      "  training_iteration: 305\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         34947.5</td><td style=\"text-align: right;\">2437560</td><td style=\"text-align: right;\">0.169981</td><td style=\"text-align: right;\">             1.22481</td><td style=\"text-align: right;\">            -1.13335</td><td style=\"text-align: right;\">           60.0866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9782208\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9737484928329673\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.052678226126439166\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5383289139816569\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-28-18\n",
      "  done: false\n",
      "  episode_len_mean: 63.11278195488722\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3260541983894578\n",
      "  episode_reward_mean: 0.2635520122996095\n",
      "  episode_reward_min: -1.117119736090896\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 35314\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6130141174793243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014847363639622926\n",
      "          policy_loss: -0.0952939186040312\n",
      "          total_loss: 0.1974223391674459\n",
      "          vf_explained_var: 0.5114665627479553\n",
      "          vf_loss: 0.2701668258905411\n",
      "    num_agent_steps_sampled: 9782208\n",
      "    num_agent_steps_trained: 9782208\n",
      "    num_steps_sampled: 2445552\n",
      "    num_steps_trained: 2445552\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.254362416107384\n",
      "    gpu_util_percent0: 0.18248322147651005\n",
      "    ram_util_percent: 94.448322147651\n",
      "    vram_util_percent0: 0.5149300701617232\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9505340733082037\n",
      "  policy_reward_mean:\n",
      "    main: 0.06588800307490239\n",
      "  policy_reward_min:\n",
      "    main: -1.745061897598507\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.304016772997124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.92616954607619\n",
      "    mean_inference_ms: 4.421792435814576\n",
      "    mean_raw_obs_processing_ms: 1.3128399038600722\n",
      "  time_since_restore: 35063.443334817886\n",
      "  time_this_iter_s: 115.90393590927124\n",
      "  time_total_s: 35063.443334817886\n",
      "  timers:\n",
      "    learn_throughput: 89.597\n",
      "    learn_time_ms: 89199.346\n",
      "    sample_throughput: 295.162\n",
      "    sample_time_ms: 27076.657\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639193298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2445552\n",
      "  training_iteration: 306\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         35063.4</td><td style=\"text-align: right;\">2445552</td><td style=\"text-align: right;\">0.263552</td><td style=\"text-align: right;\">             1.32605</td><td style=\"text-align: right;\">            -1.11712</td><td style=\"text-align: right;\">           63.1128</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9814176\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9095034207051844\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.050425969867914956\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7150308299768452\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-30-13\n",
      "  done: false\n",
      "  episode_len_mean: 64.16260162601625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.127447667106674\n",
      "  episode_reward_mean: 0.19230395571707462\n",
      "  episode_reward_min: -1.1164243462052728\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 35437\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6122920184135437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014944233521819115\n",
      "          policy_loss: -0.09594280540570617\n",
      "          total_loss: 0.2143080970570445\n",
      "          vf_explained_var: 0.47250497341156006\n",
      "          vf_loss: 0.2875543466210365\n",
      "    num_agent_steps_sampled: 9814176\n",
      "    num_agent_steps_trained: 9814176\n",
      "    num_steps_sampled: 2453544\n",
      "    num_steps_trained: 2453544\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.493959731543626\n",
      "    gpu_util_percent0: 0.1793288590604027\n",
      "    ram_util_percent: 94.4261744966443\n",
      "    vram_util_percent0: 0.5143757550059517\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8373594540790352\n",
      "  policy_reward_mean:\n",
      "    main: 0.04807598892926864\n",
      "  policy_reward_min:\n",
      "    main: -1.75249819913316\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043863752976566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.93651005697436\n",
      "    mean_inference_ms: 4.425763450059512\n",
      "    mean_raw_obs_processing_ms: 1.3131757660001675\n",
      "  time_since_restore: 35178.86394739151\n",
      "  time_this_iter_s: 115.42061257362366\n",
      "  time_total_s: 35178.86394739151\n",
      "  timers:\n",
      "    learn_throughput: 89.95\n",
      "    learn_time_ms: 88849.743\n",
      "    sample_throughput: 297.01\n",
      "    sample_time_ms: 26908.147\n",
      "    update_time_ms: 2.78\n",
      "  timestamp: 1639193413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2453544\n",
      "  training_iteration: 307\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         35178.9</td><td style=\"text-align: right;\">2453544</td><td style=\"text-align: right;\">0.192304</td><td style=\"text-align: right;\">             1.12745</td><td style=\"text-align: right;\">            -1.11642</td><td style=\"text-align: right;\">           64.1626</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9846144\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9168746855516415\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10491333790888283\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4625919688292012\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 55.55172413793103\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3622539287891455\n",
      "  episode_reward_mean: 0.2236288585114427\n",
      "  episode_reward_min: -1.0518881613997744\n",
      "  episodes_this_iter: 145\n",
      "  episodes_total: 35582\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6096376724243164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01492447217926383\n",
      "          policy_loss: -0.09590127026289702\n",
      "          total_loss: 0.20593703938275576\n",
      "          vf_explained_var: 0.5226387977600098\n",
      "          vf_loss: 0.279171768784523\n",
      "    num_agent_steps_sampled: 9846144\n",
      "    num_agent_steps_trained: 9846144\n",
      "    num_steps_sampled: 2461536\n",
      "    num_steps_trained: 2461536\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.41610738255034\n",
      "    gpu_util_percent0: 0.18181208053691275\n",
      "    ram_util_percent: 94.46375838926174\n",
      "    vram_util_percent0: 0.5143591918040861\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9320043141288838\n",
      "  policy_reward_mean:\n",
      "    main: 0.05590721462786068\n",
      "  policy_reward_min:\n",
      "    main: -1.6154995673456027\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30441943501289453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.93766283493101\n",
      "    mean_inference_ms: 4.426074747907073\n",
      "    mean_raw_obs_processing_ms: 1.3134219933726485\n",
      "  time_since_restore: 35294.09107661247\n",
      "  time_this_iter_s: 115.22712922096252\n",
      "  time_total_s: 35294.09107661247\n",
      "  timers:\n",
      "    learn_throughput: 90.171\n",
      "    learn_time_ms: 88631.46\n",
      "    sample_throughput: 298.293\n",
      "    sample_time_ms: 26792.463\n",
      "    update_time_ms: 2.796\n",
      "  timestamp: 1639193528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2461536\n",
      "  training_iteration: 308\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         35294.1</td><td style=\"text-align: right;\">2461536</td><td style=\"text-align: right;\">0.223629</td><td style=\"text-align: right;\">             1.36225</td><td style=\"text-align: right;\">            -1.05189</td><td style=\"text-align: right;\">           55.5517</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9878112\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8429393252265265\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09080899919752629\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7055993174737523\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 64.58196721311475\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2350405732747656\n",
      "  episode_reward_mean: 0.2068716703660121\n",
      "  episode_reward_min: -0.8379069690577383\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 35704\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6181747789382934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01460020001605153\n",
      "          policy_loss: -0.09356370901316405\n",
      "          total_loss: 0.1900853293314576\n",
      "          vf_explained_var: 0.503421425819397\n",
      "          vf_loss: 0.26147498381137846\n",
      "    num_agent_steps_sampled: 9878112\n",
      "    num_agent_steps_trained: 9878112\n",
      "    num_steps_sampled: 2469528\n",
      "    num_steps_trained: 2469528\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.154\n",
      "    gpu_util_percent0: 0.181\n",
      "    ram_util_percent: 94.46\n",
      "    vram_util_percent0: 0.5148842821103432\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8115361931881533\n",
      "  policy_reward_mean:\n",
      "    main: 0.05171791759150302\n",
      "  policy_reward_min:\n",
      "    main: -1.7141307377106543\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3040105408096196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.927757560840107\n",
      "    mean_inference_ms: 4.4230661125831015\n",
      "    mean_raw_obs_processing_ms: 1.313121110371016\n",
      "  time_since_restore: 35410.60601091385\n",
      "  time_this_iter_s: 116.51493430137634\n",
      "  time_total_s: 35410.60601091385\n",
      "  timers:\n",
      "    learn_throughput: 89.842\n",
      "    learn_time_ms: 88955.745\n",
      "    sample_throughput: 298.542\n",
      "    sample_time_ms: 26770.076\n",
      "    update_time_ms: 2.792\n",
      "  timestamp: 1639193645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2469528\n",
      "  training_iteration: 309\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         35410.6</td><td style=\"text-align: right;\">2469528</td><td style=\"text-align: right;\">0.206872</td><td style=\"text-align: right;\">             1.23504</td><td style=\"text-align: right;\">           -0.837907</td><td style=\"text-align: right;\">            64.582</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9910080\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0135121103352027\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09251787522342957\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4793350086992086\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 67.73504273504274\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3080560212963752\n",
      "  episode_reward_mean: 0.17598901056224983\n",
      "  episode_reward_min: -0.7793561367567827\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 35821\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.615296306848526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014734018601477146\n",
      "          policy_loss: -0.0943187354367692\n",
      "          total_loss: 0.1769763690456748\n",
      "          vf_explained_var: 0.4917857050895691\n",
      "          vf_loss: 0.24891781455278397\n",
      "    num_agent_steps_sampled: 9910080\n",
      "    num_agent_steps_trained: 9910080\n",
      "    num_steps_sampled: 2477520\n",
      "    num_steps_trained: 2477520\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.68648648648649\n",
      "    gpu_util_percent0: 0.1787837837837838\n",
      "    ram_util_percent: 94.52364864864865\n",
      "    vram_util_percent0: 0.5149931520859456\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9202085298854483\n",
      "  policy_reward_mean:\n",
      "    main: 0.04399725264056248\n",
      "  policy_reward_min:\n",
      "    main: -1.5402213591230591\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3041533323607409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.93410409011228\n",
      "    mean_inference_ms: 4.425845816275926\n",
      "    mean_raw_obs_processing_ms: 1.3132082597732797\n",
      "  time_since_restore: 35525.780128240585\n",
      "  time_this_iter_s: 115.17411732673645\n",
      "  time_total_s: 35525.780128240585\n",
      "  timers:\n",
      "    learn_throughput: 89.936\n",
      "    learn_time_ms: 88863.276\n",
      "    sample_throughput: 297.796\n",
      "    sample_time_ms: 26837.165\n",
      "    update_time_ms: 2.781\n",
      "  timestamp: 1639193760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2477520\n",
      "  training_iteration: 310\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         35525.8</td><td style=\"text-align: right;\">2477520</td><td style=\"text-align: right;\">0.175989</td><td style=\"text-align: right;\">             1.30806</td><td style=\"text-align: right;\">           -0.779356</td><td style=\"text-align: right;\">            67.735</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9942048\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7516175473084983\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04666462576288718\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7928606709075302\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 65.984\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5547533393571018\n",
      "  episode_reward_mean: 0.01348321106210295\n",
      "  episode_reward_min: -1.7029173887746234\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 35946\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6112587840557099\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014856591165065765\n",
      "          policy_loss: -0.09488863904774189\n",
      "          total_loss: 0.21104777270555497\n",
      "          vf_explained_var: 0.48050040006637573\n",
      "          vf_loss: 0.28337296748161317\n",
      "    num_agent_steps_sampled: 9942048\n",
      "    num_agent_steps_trained: 9942048\n",
      "    num_steps_sampled: 2485512\n",
      "    num_steps_trained: 2485512\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.36375838926175\n",
      "    gpu_util_percent0: 0.18093959731543624\n",
      "    ram_util_percent: 94.51476510067113\n",
      "    vram_util_percent0: 0.5149996356095587\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7297284405489333\n",
      "  policy_reward_mean:\n",
      "    main: 0.0033708027655257383\n",
      "  policy_reward_min:\n",
      "    main: -1.8624742565389636\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043371780785879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.935357976781773\n",
      "    mean_inference_ms: 4.425742353429304\n",
      "    mean_raw_obs_processing_ms: 1.3133971912356577\n",
      "  time_since_restore: 35641.06697869301\n",
      "  time_this_iter_s: 115.2868504524231\n",
      "  time_total_s: 35641.06697869301\n",
      "  timers:\n",
      "    learn_throughput: 90.031\n",
      "    learn_time_ms: 88769.461\n",
      "    sample_throughput: 298.859\n",
      "    sample_time_ms: 26741.716\n",
      "    update_time_ms: 2.776\n",
      "  timestamp: 1639193876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2485512\n",
      "  training_iteration: 311\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         35641.1</td><td style=\"text-align: right;\">2485512</td><td style=\"text-align: right;\">0.0134832</td><td style=\"text-align: right;\">             1.55475</td><td style=\"text-align: right;\">            -1.70292</td><td style=\"text-align: right;\">            65.984</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 9974016\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9173645796092771\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06791037399224194\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5561511297624347\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 62.072\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4985053980638217\n",
      "  episode_reward_mean: 0.15985984809409934\n",
      "  episode_reward_min: -0.7522163642924785\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 36071\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6155141098499298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014717360354959965\n",
      "          policy_loss: -0.09491624222695827\n",
      "          total_loss: 0.1740837604291737\n",
      "          vf_explained_var: 0.5062257051467896\n",
      "          vf_loss: 0.24664801174402237\n",
      "    num_agent_steps_sampled: 9974016\n",
      "    num_agent_steps_trained: 9974016\n",
      "    num_steps_sampled: 2493504\n",
      "    num_steps_trained: 2493504\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.814666666666675\n",
      "    gpu_util_percent0: 0.18066666666666667\n",
      "    ram_util_percent: 94.624\n",
      "    vram_util_percent0: 0.5150828123286169\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.977609038617381\n",
      "  policy_reward_mean:\n",
      "    main: 0.03996496202352485\n",
      "  policy_reward_min:\n",
      "    main: -1.672747623899004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3040949027523028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.930594243679856\n",
      "    mean_inference_ms: 4.424048765334437\n",
      "    mean_raw_obs_processing_ms: 1.313304271575825\n",
      "  time_since_restore: 35756.69091510773\n",
      "  time_this_iter_s: 115.62393641471863\n",
      "  time_total_s: 35756.69091510773\n",
      "  timers:\n",
      "    learn_throughput: 89.969\n",
      "    learn_time_ms: 88830.801\n",
      "    sample_throughput: 299.029\n",
      "    sample_time_ms: 26726.479\n",
      "    update_time_ms: 2.776\n",
      "  timestamp: 1639193991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2493504\n",
      "  training_iteration: 312\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         35756.7</td><td style=\"text-align: right;\">2493504</td><td style=\"text-align: right;\"> 0.15986</td><td style=\"text-align: right;\">             1.49851</td><td style=\"text-align: right;\">           -0.752216</td><td style=\"text-align: right;\">            62.072</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10005984\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7586527167772058\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05698774340540993\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.460454956975032\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 65.31451612903226\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2638573751080862\n",
      "  episode_reward_mean: 0.21874024771076558\n",
      "  episode_reward_min: -0.743132246055346\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 36195\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6145706720352173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014696755532175303\n",
      "          policy_loss: -0.09452520698308944\n",
      "          total_loss: 0.1882996083907783\n",
      "          vf_explained_var: 0.5169930458068848\n",
      "          vf_loss: 0.2605041170120239\n",
      "    num_agent_steps_sampled: 10005984\n",
      "    num_agent_steps_trained: 10005984\n",
      "    num_steps_sampled: 2501496\n",
      "    num_steps_trained: 2501496\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.76447368421053\n",
      "    gpu_util_percent0: 0.19072368421052632\n",
      "    ram_util_percent: 94.78552631578948\n",
      "    vram_util_percent0: 0.5159516201659133\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9604310925998398\n",
      "  policy_reward_mean:\n",
      "    main: 0.05468506192769138\n",
      "  policy_reward_min:\n",
      "    main: -1.556841248189214\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3041676393870133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.93441347290699\n",
      "    mean_inference_ms: 4.426019504658746\n",
      "    mean_raw_obs_processing_ms: 1.31327608714107\n",
      "  time_since_restore: 35874.97361111641\n",
      "  time_this_iter_s: 118.28269600868225\n",
      "  time_total_s: 35874.97361111641\n",
      "  timers:\n",
      "    learn_throughput: 89.849\n",
      "    learn_time_ms: 88948.929\n",
      "    sample_throughput: 299.146\n",
      "    sample_time_ms: 26716.008\n",
      "    update_time_ms: 2.794\n",
      "  timestamp: 1639194110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2501496\n",
      "  training_iteration: 313\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">           35875</td><td style=\"text-align: right;\">2501496</td><td style=\"text-align: right;\"> 0.21874</td><td style=\"text-align: right;\">             1.26386</td><td style=\"text-align: right;\">           -0.743132</td><td style=\"text-align: right;\">           65.3145</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10037952\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9916192059876803\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09812419058781331\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5948850914900251\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-43-48\n",
      "  done: false\n",
      "  episode_len_mean: 68.16964285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1991877293556805\n",
      "  episode_reward_mean: 0.19912999231324588\n",
      "  episode_reward_min: -0.7636649095671326\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 36307\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6211894288063049\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014705260522663592\n",
      "          policy_loss: -0.09441403512656689\n",
      "          total_loss: 0.1749291943386197\n",
      "          vf_explained_var: 0.4815179109573364\n",
      "          vf_loss: 0.24700961470603944\n",
      "    num_agent_steps_sampled: 10037952\n",
      "    num_agent_steps_trained: 10037952\n",
      "    num_steps_sampled: 2509488\n",
      "    num_steps_trained: 2509488\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.24473684210526\n",
      "    gpu_util_percent0: 0.18749999999999997\n",
      "    ram_util_percent: 94.76381578947368\n",
      "    vram_util_percent0: 0.516570764274952\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8311906586959061\n",
      "  policy_reward_mean:\n",
      "    main: 0.04978249807831146\n",
      "  policy_reward_min:\n",
      "    main: -1.6618004584867252\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039125575018461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.92778926169105\n",
      "    mean_inference_ms: 4.422546107135253\n",
      "    mean_raw_obs_processing_ms: 1.3133448854110412\n",
      "  time_since_restore: 35993.50805878639\n",
      "  time_this_iter_s: 118.53444766998291\n",
      "  time_total_s: 35993.50805878639\n",
      "  timers:\n",
      "    learn_throughput: 89.518\n",
      "    learn_time_ms: 89277.981\n",
      "    sample_throughput: 298.543\n",
      "    sample_time_ms: 26769.987\n",
      "    update_time_ms: 2.821\n",
      "  timestamp: 1639194228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2509488\n",
      "  training_iteration: 314\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         35993.5</td><td style=\"text-align: right;\">2509488</td><td style=\"text-align: right;\"> 0.19913</td><td style=\"text-align: right;\">             1.19919</td><td style=\"text-align: right;\">           -0.763665</td><td style=\"text-align: right;\">           68.1696</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10069920\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.010636930951309\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03751899157103175\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6819372789261404\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 66.8267716535433\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4963308997750722\n",
      "  episode_reward_mean: 0.13838289499300285\n",
      "  episode_reward_min: -0.9500926583503237\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 36434\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6134092404842376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014868149321526289\n",
      "          policy_loss: -0.09681029700487852\n",
      "          total_loss: 0.20437941312789917\n",
      "          vf_explained_var: 0.4742262065410614\n",
      "          vf_loss: 0.2786087095141411\n",
      "    num_agent_steps_sampled: 10069920\n",
      "    num_agent_steps_trained: 10069920\n",
      "    num_steps_sampled: 2517480\n",
      "    num_steps_trained: 2517480\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.683018867924524\n",
      "    gpu_util_percent0: 0.19635220125786165\n",
      "    ram_util_percent: 94.80503144654088\n",
      "    vram_util_percent0: 0.5183267418734647\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9095082197627598\n",
      "  policy_reward_mean:\n",
      "    main: 0.03459572374825071\n",
      "  policy_reward_min:\n",
      "    main: -1.7946551997621678\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30424953655507886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.93971297997257\n",
      "    mean_inference_ms: 4.426950850861067\n",
      "    mean_raw_obs_processing_ms: 1.3138095193383819\n",
      "  time_since_restore: 36116.71637892723\n",
      "  time_this_iter_s: 123.20832014083862\n",
      "  time_total_s: 36116.71637892723\n",
      "  timers:\n",
      "    learn_throughput: 88.981\n",
      "    learn_time_ms: 89817.278\n",
      "    sample_throughput: 298.714\n",
      "    sample_time_ms: 26754.659\n",
      "    update_time_ms: 2.917\n",
      "  timestamp: 1639194352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2517480\n",
      "  training_iteration: 315\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         36116.7</td><td style=\"text-align: right;\">2517480</td><td style=\"text-align: right;\">0.138383</td><td style=\"text-align: right;\">             1.49633</td><td style=\"text-align: right;\">           -0.950093</td><td style=\"text-align: right;\">           66.8268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10101888\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0973917322947322\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.038709725709719145\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47858304469556917\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 63.07086614173228\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8215389886910303\n",
      "  episode_reward_mean: 0.15898771405263343\n",
      "  episode_reward_min: -0.9115507706449764\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 36561\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6056524639129639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014790998045355082\n",
      "          policy_loss: -0.09585182257369161\n",
      "          total_loss: 0.17417158111929892\n",
      "          vf_explained_var: 0.531741201877594\n",
      "          vf_loss: 0.24755957424640657\n",
      "    num_agent_steps_sampled: 10101888\n",
      "    num_agent_steps_trained: 10101888\n",
      "    num_steps_sampled: 2525472\n",
      "    num_steps_trained: 2525472\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.16423841059603\n",
      "    gpu_util_percent0: 0.18304635761589405\n",
      "    ram_util_percent: 94.68476821192054\n",
      "    vram_util_percent0: 0.5191048379891434\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9892023289742835\n",
      "  policy_reward_mean:\n",
      "    main: 0.03974692851315837\n",
      "  policy_reward_min:\n",
      "    main: -1.5873469090448311\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039681383888333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.936030940432225\n",
      "    mean_inference_ms: 4.425429046946177\n",
      "    mean_raw_obs_processing_ms: 1.3137659441206593\n",
      "  time_since_restore: 36233.58774256706\n",
      "  time_this_iter_s: 116.87136363983154\n",
      "  time_total_s: 36233.58774256706\n",
      "  timers:\n",
      "    learn_throughput: 88.887\n",
      "    learn_time_ms: 89912.149\n",
      "    sample_throughput: 298.748\n",
      "    sample_time_ms: 26751.653\n",
      "    update_time_ms: 2.893\n",
      "  timestamp: 1639194468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2525472\n",
      "  training_iteration: 316\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         36233.6</td><td style=\"text-align: right;\">2525472</td><td style=\"text-align: right;\">0.158988</td><td style=\"text-align: right;\">             1.82154</td><td style=\"text-align: right;\">           -0.911551</td><td style=\"text-align: right;\">           63.0709</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10133856\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6676143419293513\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04763386070036245\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6131085264821031\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 57.85606060606061\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3061357196021435\n",
      "  episode_reward_mean: 0.1570030144102402\n",
      "  episode_reward_min: -0.9450875312822589\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 36693\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6156780698299408\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014779651083052158\n",
      "          policy_loss: -0.0946622040309012\n",
      "          total_loss: 0.19048547578603028\n",
      "          vf_explained_var: 0.526595413684845\n",
      "          vf_loss: 0.2627010865211487\n",
      "    num_agent_steps_sampled: 10133856\n",
      "    num_agent_steps_trained: 10133856\n",
      "    num_steps_sampled: 2533464\n",
      "    num_steps_trained: 2533464\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.56953642384106\n",
      "    gpu_util_percent0: 0.18940397350993374\n",
      "    ram_util_percent: 94.70463576158939\n",
      "    vram_util_percent0: 0.5197542325050285\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8751681481115219\n",
      "  policy_reward_mean:\n",
      "    main: 0.03925075360256006\n",
      "  policy_reward_min:\n",
      "    main: -1.870588647196631\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039981795864379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.936400723186896\n",
      "    mean_inference_ms: 4.425956497832472\n",
      "    mean_raw_obs_processing_ms: 1.3139225739204974\n",
      "  time_since_restore: 36350.895011901855\n",
      "  time_this_iter_s: 117.30726933479309\n",
      "  time_total_s: 36350.895011901855\n",
      "  timers:\n",
      "    learn_throughput: 88.663\n",
      "    learn_time_ms: 90139.219\n",
      "    sample_throughput: 299.199\n",
      "    sample_time_ms: 26711.285\n",
      "    update_time_ms: 2.94\n",
      "  timestamp: 1639194586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2533464\n",
      "  training_iteration: 317\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         36350.9</td><td style=\"text-align: right;\">2533464</td><td style=\"text-align: right;\">0.157003</td><td style=\"text-align: right;\">             1.30614</td><td style=\"text-align: right;\">           -0.945088</td><td style=\"text-align: right;\">           57.8561</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10165824\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.870001288107234\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06012557760378779\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5362000665686273\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-51-45\n",
      "  done: false\n",
      "  episode_len_mean: 65.39516129032258\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7810436736718365\n",
      "  episode_reward_mean: 0.15525471955763204\n",
      "  episode_reward_min: -1.6209357193152396\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 36817\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6061108772754669\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01465468380600214\n",
      "          policy_loss: -0.09365614707022905\n",
      "          total_loss: 0.20709277191758155\n",
      "          vf_explained_var: 0.4816543161869049\n",
      "          vf_loss: 0.2784921184182167\n",
      "    num_agent_steps_sampled: 10165824\n",
      "    num_agent_steps_trained: 10165824\n",
      "    num_steps_sampled: 2541456\n",
      "    num_steps_trained: 2541456\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.53071895424837\n",
      "    gpu_util_percent0: 0.1961437908496732\n",
      "    ram_util_percent: 94.93790849673202\n",
      "    vram_util_percent0: 0.5192217942348597\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.089555659478922\n",
      "  policy_reward_mean:\n",
      "    main: 0.038813679889407975\n",
      "  policy_reward_min:\n",
      "    main: -1.9119513547169453\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3034255663125996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.922066946912405\n",
      "    mean_inference_ms: 4.419446706013723\n",
      "    mean_raw_obs_processing_ms: 1.314012306824688\n",
      "  time_since_restore: 36469.975897789\n",
      "  time_this_iter_s: 119.080885887146\n",
      "  time_total_s: 36469.975897789\n",
      "  timers:\n",
      "    learn_throughput: 88.348\n",
      "    learn_time_ms: 90460.111\n",
      "    sample_throughput: 298.451\n",
      "    sample_time_ms: 26778.302\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1639194705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2541456\n",
      "  training_iteration: 318\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">           36470</td><td style=\"text-align: right;\">2541456</td><td style=\"text-align: right;\">0.155255</td><td style=\"text-align: right;\">             1.78104</td><td style=\"text-align: right;\">            -1.62094</td><td style=\"text-align: right;\">           65.3952</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10197792\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7455352058492947\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05320791419833695\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6799727963014541\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 67.2892561983471\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.238872112584924\n",
      "  episode_reward_mean: 0.11686421765040278\n",
      "  episode_reward_min: -1.3612677663989894\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 36938\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6068947141170502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014783739749342203\n",
      "          policy_loss: -0.09465328436344862\n",
      "          total_loss: 0.1820796234421432\n",
      "          vf_explained_var: 0.5039994120597839\n",
      "          vf_loss: 0.25428010284900665\n",
      "    num_agent_steps_sampled: 10197792\n",
      "    num_agent_steps_trained: 10197792\n",
      "    num_steps_sampled: 2549448\n",
      "    num_steps_trained: 2549448\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.234640522875814\n",
      "    gpu_util_percent0: 0.1845098039215686\n",
      "    ram_util_percent: 94.93921568627451\n",
      "    vram_util_percent0: 0.5176066258465655\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8939650053874095\n",
      "  policy_reward_mean:\n",
      "    main: 0.029216054412600702\n",
      "  policy_reward_min:\n",
      "    main: -1.9622410733282547\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3044590047917883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.95227659779998\n",
      "    mean_inference_ms: 4.429248549370697\n",
      "    mean_raw_obs_processing_ms: 1.3149476511069271\n",
      "  time_since_restore: 36588.04335165024\n",
      "  time_this_iter_s: 118.06745386123657\n",
      "  time_total_s: 36588.04335165024\n",
      "  timers:\n",
      "    learn_throughput: 88.415\n",
      "    learn_time_ms: 90391.899\n",
      "    sample_throughput: 295.995\n",
      "    sample_time_ms: 27000.444\n",
      "    update_time_ms: 3.007\n",
      "  timestamp: 1639194823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2549448\n",
      "  training_iteration: 319\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">           36588</td><td style=\"text-align: right;\">2549448</td><td style=\"text-align: right;\">0.116864</td><td style=\"text-align: right;\">             1.23887</td><td style=\"text-align: right;\">            -1.36127</td><td style=\"text-align: right;\">           67.2893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10229760\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0104810058666354\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.055674387246210995\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6749102448451515\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 68.98181818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.441816840363626\n",
      "  episode_reward_mean: 0.17687993133561994\n",
      "  episode_reward_min: -1.0783754127998706\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37048\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6168802881240845\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0149926078915596\n",
      "          policy_loss: -0.09748396938294172\n",
      "          total_loss: 0.18187756663933397\n",
      "          vf_explained_var: 0.47522613406181335\n",
      "          vf_loss: 0.256591513633728\n",
      "    num_agent_steps_sampled: 10229760\n",
      "    num_agent_steps_trained: 10229760\n",
      "    num_steps_sampled: 2557440\n",
      "    num_steps_trained: 2557440\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.409868421052636\n",
      "    gpu_util_percent0: 0.18210526315789474\n",
      "    ram_util_percent: 94.96513157894736\n",
      "    vram_util_percent0: 0.5148183266656275\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9631718133045395\n",
      "  policy_reward_mean:\n",
      "    main: 0.044219982833905\n",
      "  policy_reward_min:\n",
      "    main: -1.6373373060357674\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30410618144711626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.943963923570674\n",
      "    mean_inference_ms: 4.425867748024676\n",
      "    mean_raw_obs_processing_ms: 1.314686689242908\n",
      "  time_since_restore: 36705.769126176834\n",
      "  time_this_iter_s: 117.72577452659607\n",
      "  time_total_s: 36705.769126176834\n",
      "  timers:\n",
      "    learn_throughput: 88.157\n",
      "    learn_time_ms: 90656.811\n",
      "    sample_throughput: 296.083\n",
      "    sample_time_ms: 26992.419\n",
      "    update_time_ms: 3.022\n",
      "  timestamp: 1639194941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2557440\n",
      "  training_iteration: 320\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         36705.8</td><td style=\"text-align: right;\">2557440</td><td style=\"text-align: right;\"> 0.17688</td><td style=\"text-align: right;\">             1.44182</td><td style=\"text-align: right;\">            -1.07838</td><td style=\"text-align: right;\">           68.9818</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10261728\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8878553297301713\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08918770461745032\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6548058313899651\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 59.31386861313869\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3955474479760737\n",
      "  episode_reward_mean: 0.1786604089120061\n",
      "  episode_reward_min: -0.95324604777504\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 37185\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6016603008508682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014737760722637177\n",
      "          policy_loss: -0.09573219554126262\n",
      "          total_loss: 0.20517635039240123\n",
      "          vf_explained_var: 0.4845736026763916\n",
      "          vf_loss: 0.2785255724191666\n",
      "    num_agent_steps_sampled: 10261728\n",
      "    num_agent_steps_trained: 10261728\n",
      "    num_steps_sampled: 2565432\n",
      "    num_steps_trained: 2565432\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.309395973154366\n",
      "    gpu_util_percent0: 0.1740268456375839\n",
      "    ram_util_percent: 95.03288590604025\n",
      "    vram_util_percent0: 0.5142145398411259\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.968685786075447\n",
      "  policy_reward_mean:\n",
      "    main: 0.04466510222800153\n",
      "  policy_reward_min:\n",
      "    main: -1.654805831389965\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3045047540889746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.95591265333303\n",
      "    mean_inference_ms: 4.43090009342639\n",
      "    mean_raw_obs_processing_ms: 1.3150990458277756\n",
      "  time_since_restore: 36821.0541806221\n",
      "  time_this_iter_s: 115.28505444526672\n",
      "  time_total_s: 36821.0541806221\n",
      "  timers:\n",
      "    learn_throughput: 88.168\n",
      "    learn_time_ms: 90645.06\n",
      "    sample_throughput: 295.95\n",
      "    sample_time_ms: 27004.602\n",
      "    update_time_ms: 2.993\n",
      "  timestamp: 1639195056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2565432\n",
      "  training_iteration: 321\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         36821.1</td><td style=\"text-align: right;\">2565432</td><td style=\"text-align: right;\"> 0.17866</td><td style=\"text-align: right;\">             1.39555</td><td style=\"text-align: right;\">           -0.953246</td><td style=\"text-align: right;\">           59.3139</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10293696\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8501936987485846\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08712181524869887\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8280550760785268\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_00-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 68.30508474576271\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2013086292547719\n",
      "  episode_reward_mean: 0.14526262521870775\n",
      "  episode_reward_min: -0.9820026573690015\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 37303\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.605624504327774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014891880463808776\n",
      "          policy_loss: -0.09542775424197317\n",
      "          total_loss: 0.18437712045013904\n",
      "          vf_explained_var: 0.49533912539482117\n",
      "          vf_loss: 0.2571878324151039\n",
      "    num_agent_steps_sampled: 10293696\n",
      "    num_agent_steps_trained: 10293696\n",
      "    num_steps_sampled: 2573424\n",
      "    num_steps_trained: 2573424\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.48333333333333\n",
      "    gpu_util_percent0: 0.1741333333333333\n",
      "    ram_util_percent: 95.07466666666666\n",
      "    vram_util_percent0: 0.5141932653285072\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8236036236177258\n",
      "  policy_reward_mean:\n",
      "    main: 0.03631565630467692\n",
      "  policy_reward_min:\n",
      "    main: -1.832880540013426\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3049958268888203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.968125627796464\n",
      "    mean_inference_ms: 4.435957513098887\n",
      "    mean_raw_obs_processing_ms: 1.3153772935839156\n",
      "  time_since_restore: 36936.41189932823\n",
      "  time_this_iter_s: 115.35771870613098\n",
      "  time_total_s: 36936.41189932823\n",
      "  timers:\n",
      "    learn_throughput: 88.179\n",
      "    learn_time_ms: 90633.959\n",
      "    sample_throughput: 296.155\n",
      "    sample_time_ms: 26985.861\n",
      "    update_time_ms: 3.059\n",
      "  timestamp: 1639195172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2573424\n",
      "  training_iteration: 322\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         36936.4</td><td style=\"text-align: right;\">2573424</td><td style=\"text-align: right;\">0.145263</td><td style=\"text-align: right;\">             1.20131</td><td style=\"text-align: right;\">           -0.982003</td><td style=\"text-align: right;\">           68.3051</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10325664\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.984207796867755\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11995307008335299\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48802043840960624\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 69.82456140350877\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.526211686471163\n",
      "  episode_reward_mean: 0.12595076019912166\n",
      "  episode_reward_min: -1.669207424964832\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 37417\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6081269822120666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014855509165674447\n",
      "          policy_loss: -0.09557921308279037\n",
      "          total_loss: 0.1795978929065168\n",
      "          vf_explained_var: 0.4738522171974182\n",
      "          vf_loss: 0.25261530274152755\n",
      "    num_agent_steps_sampled: 10325664\n",
      "    num_agent_steps_trained: 10325664\n",
      "    num_steps_sampled: 2581416\n",
      "    num_steps_trained: 2581416\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.64630872483221\n",
      "    gpu_util_percent0: 0.17563758389261747\n",
      "    ram_util_percent: 95.18993288590603\n",
      "    vram_util_percent0: 0.5138280651309267\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.932062200941929\n",
      "  policy_reward_mean:\n",
      "    main: 0.031487690049780415\n",
      "  policy_reward_min:\n",
      "    main: -1.703466219698621\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30466323152791064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.95732259256693\n",
      "    mean_inference_ms: 4.430931190518377\n",
      "    mean_raw_obs_processing_ms: 1.315288230384604\n",
      "  time_since_restore: 37052.30151391029\n",
      "  time_this_iter_s: 115.88961458206177\n",
      "  time_total_s: 37052.30151391029\n",
      "  timers:\n",
      "    learn_throughput: 88.419\n",
      "    learn_time_ms: 90388.149\n",
      "    sample_throughput: 296.064\n",
      "    sample_time_ms: 26994.147\n",
      "    update_time_ms: 3.037\n",
      "  timestamp: 1639195288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2581416\n",
      "  training_iteration: 323\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         37052.3</td><td style=\"text-align: right;\">2581416</td><td style=\"text-align: right;\">0.125951</td><td style=\"text-align: right;\">             1.52621</td><td style=\"text-align: right;\">            -1.66921</td><td style=\"text-align: right;\">           69.8246</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10357632\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.007747157577559\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09245938465312627\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.40110317382603\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-03-23\n",
      "  done: false\n",
      "  episode_len_mean: 61.20472440944882\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1789179417757332\n",
      "  episode_reward_mean: 0.14957918345838317\n",
      "  episode_reward_min: -0.6244427189116237\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 37544\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.607130126953125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015267281129956246\n",
      "          policy_loss: -0.09740333228558302\n",
      "          total_loss: 0.19361460592225194\n",
      "          vf_explained_var: 0.49493151903152466\n",
      "          vf_loss: 0.2678307556509972\n",
      "    num_agent_steps_sampled: 10357632\n",
      "    num_agent_steps_trained: 10357632\n",
      "    num_steps_sampled: 2589408\n",
      "    num_steps_trained: 2589408\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.683892617449665\n",
      "    gpu_util_percent0: 0.17691275167785236\n",
      "    ram_util_percent: 95.13087248322145\n",
      "    vram_util_percent0: 0.513830273557842\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.877896115331657\n",
      "  policy_reward_mean:\n",
      "    main: 0.03739479586459578\n",
      "  policy_reward_min:\n",
      "    main: -1.5592164879617605\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043330344021558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.951084807425154\n",
      "    mean_inference_ms: 4.428842115103041\n",
      "    mean_raw_obs_processing_ms: 1.3152079960270122\n",
      "  time_since_restore: 37167.861592531204\n",
      "  time_this_iter_s: 115.56007862091064\n",
      "  time_total_s: 37167.861592531204\n",
      "  timers:\n",
      "    learn_throughput: 88.669\n",
      "    learn_time_ms: 90133.192\n",
      "    sample_throughput: 296.578\n",
      "    sample_time_ms: 26947.356\n",
      "    update_time_ms: 3.089\n",
      "  timestamp: 1639195403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2589408\n",
      "  training_iteration: 324\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         37167.9</td><td style=\"text-align: right;\">2589408</td><td style=\"text-align: right;\">0.149579</td><td style=\"text-align: right;\">             1.17892</td><td style=\"text-align: right;\">           -0.624443</td><td style=\"text-align: right;\">           61.2047</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10389600\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6437946554135335\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04769507430677154\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7054419386643433\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-05-19\n",
      "  done: false\n",
      "  episode_len_mean: 54.83225806451613\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5111068679696067\n",
      "  episode_reward_mean: 0.14105785551460526\n",
      "  episode_reward_min: -1.2138547740869523\n",
      "  episodes_this_iter: 155\n",
      "  episodes_total: 37699\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6049013195037842\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014912043683230876\n",
      "          policy_loss: -0.09468439601361751\n",
      "          total_loss: 0.2236858813762665\n",
      "          vf_explained_var: 0.5028720498085022\n",
      "          vf_loss: 0.29572261148691176\n",
      "    num_agent_steps_sampled: 10389600\n",
      "    num_agent_steps_trained: 10389600\n",
      "    num_steps_sampled: 2597400\n",
      "    num_steps_trained: 2597400\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.64093959731544\n",
      "    gpu_util_percent0: 0.17597315436241612\n",
      "    ram_util_percent: 95.03892617449664\n",
      "    vram_util_percent0: 0.513696663729459\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8754754644623364\n",
      "  policy_reward_mean:\n",
      "    main: 0.035264463878651314\n",
      "  policy_reward_min:\n",
      "    main: -1.709850572004516\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3039633977176182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.94322713425368\n",
      "    mean_inference_ms: 4.426075464134047\n",
      "    mean_raw_obs_processing_ms: 1.31535761979528\n",
      "  time_since_restore: 37283.062529325485\n",
      "  time_this_iter_s: 115.200936794281\n",
      "  time_total_s: 37283.062529325485\n",
      "  timers:\n",
      "    learn_throughput: 89.405\n",
      "    learn_time_ms: 89390.768\n",
      "    sample_throughput: 297.346\n",
      "    sample_time_ms: 26877.748\n",
      "    update_time_ms: 2.951\n",
      "  timestamp: 1639195519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2597400\n",
      "  training_iteration: 325\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         37283.1</td><td style=\"text-align: right;\">2597400</td><td style=\"text-align: right;\">0.141058</td><td style=\"text-align: right;\">             1.51111</td><td style=\"text-align: right;\">            -1.21385</td><td style=\"text-align: right;\">           54.8323</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10421568\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9207833189309222\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06262392326266625\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6019466042081707\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 57.948905109489054\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6657449965948583\n",
      "  episode_reward_mean: 0.11245271186384465\n",
      "  episode_reward_min: -1.087569169744786\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 37836\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.60441858959198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014559205997735261\n",
      "          policy_loss: -0.0917502650283277\n",
      "          total_loss: 0.18801925939321518\n",
      "          vf_explained_var: 0.5369267463684082\n",
      "          vf_loss: 0.2576577323079109\n",
      "    num_agent_steps_sampled: 10421568\n",
      "    num_agent_steps_trained: 10421568\n",
      "    num_steps_sampled: 2605392\n",
      "    num_steps_trained: 2605392\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.7744966442953\n",
      "    gpu_util_percent0: 0.1755704697986577\n",
      "    ram_util_percent: 95.26040268456376\n",
      "    vram_util_percent0: 0.5135608454741604\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8834205596665288\n",
      "  policy_reward_mean:\n",
      "    main: 0.028113177965961172\n",
      "  policy_reward_min:\n",
      "    main: -1.606700046865874\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30442716300704364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.953414151110927\n",
      "    mean_inference_ms: 4.430458134999213\n",
      "    mean_raw_obs_processing_ms: 1.3158741911463536\n",
      "  time_since_restore: 37398.34320664406\n",
      "  time_this_iter_s: 115.280677318573\n",
      "  time_total_s: 37398.34320664406\n",
      "  timers:\n",
      "    learn_throughput: 89.497\n",
      "    learn_time_ms: 89299.545\n",
      "    sample_throughput: 298.123\n",
      "    sample_time_ms: 26807.769\n",
      "    update_time_ms: 2.978\n",
      "  timestamp: 1639195634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2605392\n",
      "  training_iteration: 326\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         37398.3</td><td style=\"text-align: right;\">2605392</td><td style=\"text-align: right;\">0.112453</td><td style=\"text-align: right;\">             1.66574</td><td style=\"text-align: right;\">            -1.08757</td><td style=\"text-align: right;\">           57.9489</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10453536\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9829979348517226\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1010902261454762\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5480203131490583\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 59.7557251908397\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6517624077131088\n",
      "  episode_reward_mean: 0.18758755505948727\n",
      "  episode_reward_min: -1.4573158509822388\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 37967\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6047462702989578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014408835984766484\n",
      "          policy_loss: -0.09307024826481938\n",
      "          total_loss: 0.20919081827998162\n",
      "          vf_explained_var: 0.5008434653282166\n",
      "          vf_loss: 0.2803776453733444\n",
      "    num_agent_steps_sampled: 10453536\n",
      "    num_agent_steps_trained: 10453536\n",
      "    num_steps_sampled: 2613384\n",
      "    num_steps_trained: 2613384\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.64697986577182\n",
      "    gpu_util_percent0: 0.17557046979865773\n",
      "    ram_util_percent: 95.11140939597314\n",
      "    vram_util_percent0: 0.513557532833787\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9846264870160728\n",
      "  policy_reward_mean:\n",
      "    main: 0.046896888764871816\n",
      "  policy_reward_min:\n",
      "    main: -1.6087240166435048\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.304277631030949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.9505418242402\n",
      "    mean_inference_ms: 4.4295536215075515\n",
      "    mean_raw_obs_processing_ms: 1.3159912058472703\n",
      "  time_since_restore: 37513.78302359581\n",
      "  time_this_iter_s: 115.43981695175171\n",
      "  time_total_s: 37513.78302359581\n",
      "  timers:\n",
      "    learn_throughput: 89.705\n",
      "    learn_time_ms: 89092.494\n",
      "    sample_throughput: 297.942\n",
      "    sample_time_ms: 26824.043\n",
      "    update_time_ms: 2.966\n",
      "  timestamp: 1639195749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2613384\n",
      "  training_iteration: 327\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         37513.8</td><td style=\"text-align: right;\">2613384</td><td style=\"text-align: right;\">0.187588</td><td style=\"text-align: right;\">             1.65176</td><td style=\"text-align: right;\">            -1.45732</td><td style=\"text-align: right;\">           59.7557</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10485504\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0570166058669461\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0640952742911989\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5601484075129457\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 74.34862385321101\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5344425873058505\n",
      "  episode_reward_mean: 0.1361162731344884\n",
      "  episode_reward_min: -1.1824868888377025\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 38076\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6117045929431916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015012489087879657\n",
      "          policy_loss: -0.09667619032412768\n",
      "          total_loss: 0.21375650107115507\n",
      "          vf_explained_var: 0.4322081208229065\n",
      "          vf_loss: 0.2876324728727341\n",
      "    num_agent_steps_sampled: 10485504\n",
      "    num_agent_steps_trained: 10485504\n",
      "    num_steps_sampled: 2621376\n",
      "    num_steps_trained: 2621376\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25033112582781\n",
      "    gpu_util_percent0: 0.18569536423841063\n",
      "    ram_util_percent: 95.24635761589404\n",
      "    vram_util_percent0: 0.515362102817893\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9304291049579492\n",
      "  policy_reward_mean:\n",
      "    main: 0.0340290682836221\n",
      "  policy_reward_min:\n",
      "    main: -1.6448983634344159\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3051035730841187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.969291861156332\n",
      "    mean_inference_ms: 4.4372106794559745\n",
      "    mean_raw_obs_processing_ms: 1.3165135209542445\n",
      "  time_since_restore: 37631.47951745987\n",
      "  time_this_iter_s: 117.69649386405945\n",
      "  time_total_s: 37631.47951745987\n",
      "  timers:\n",
      "    learn_throughput: 89.751\n",
      "    learn_time_ms: 89046.035\n",
      "    sample_throughput: 298.954\n",
      "    sample_time_ms: 26733.189\n",
      "    update_time_ms: 2.925\n",
      "  timestamp: 1639195867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2621376\n",
      "  training_iteration: 328\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         37631.5</td><td style=\"text-align: right;\">2621376</td><td style=\"text-align: right;\">0.136116</td><td style=\"text-align: right;\">             1.53444</td><td style=\"text-align: right;\">            -1.18249</td><td style=\"text-align: right;\">           74.3486</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10517472\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6834690920930631\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11997519656233424\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.491622778871283\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 57.391304347826086\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3498097447236554\n",
      "  episode_reward_mean: 0.2695890338447144\n",
      "  episode_reward_min: -0.6318386336194202\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 38214\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6067900562286377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0155231141038239\n",
      "          policy_loss: -0.09921050284802914\n",
      "          total_loss: 0.20061718760430813\n",
      "          vf_explained_var: 0.4988101124763489\n",
      "          vf_loss: 0.27625195968151095\n",
      "    num_agent_steps_sampled: 10517472\n",
      "    num_agent_steps_trained: 10517472\n",
      "    num_steps_sampled: 2629368\n",
      "    num_steps_trained: 2629368\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.54144736842105\n",
      "    gpu_util_percent0: 0.18743421052631576\n",
      "    ram_util_percent: 95.1921052631579\n",
      "    vram_util_percent0: 0.5172775843854454\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9540539060322222\n",
      "  policy_reward_mean:\n",
      "    main: 0.0673972584611786\n",
      "  policy_reward_min:\n",
      "    main: -1.5390386124478503\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30408122580034563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.944279258165057\n",
      "    mean_inference_ms: 4.426449496495232\n",
      "    mean_raw_obs_processing_ms: 1.3160940588700536\n",
      "  time_since_restore: 37749.27304291725\n",
      "  time_this_iter_s: 117.7935254573822\n",
      "  time_total_s: 37749.27304291725\n",
      "  timers:\n",
      "    learn_throughput: 89.608\n",
      "    learn_time_ms: 89188.257\n",
      "    sample_throughput: 300.8\n",
      "    sample_time_ms: 26569.128\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1639195985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2629368\n",
      "  training_iteration: 329\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         37749.3</td><td style=\"text-align: right;\">2629368</td><td style=\"text-align: right;\">0.269589</td><td style=\"text-align: right;\">             1.34981</td><td style=\"text-align: right;\">           -0.631839</td><td style=\"text-align: right;\">           57.3913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10549440\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9790884335550301\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07733467591301717\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5221744168096752\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 68.77118644067797\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.222302626313242\n",
      "  episode_reward_mean: 0.15745596053596514\n",
      "  episode_reward_min: -1.1842916129952754\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 38332\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6043518148660659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014980823785066604\n",
      "          policy_loss: -0.09352666176855565\n",
      "          total_loss: 0.17884026916325094\n",
      "          vf_explained_var: 0.4900929033756256\n",
      "          vf_loss: 0.2496148045659065\n",
      "    num_agent_steps_sampled: 10549440\n",
      "    num_agent_steps_trained: 10549440\n",
      "    num_steps_sampled: 2637360\n",
      "    num_steps_trained: 2637360\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.601333333333336\n",
      "    gpu_util_percent0: 0.1786\n",
      "    ram_util_percent: 94.73066666666665\n",
      "    vram_util_percent0: 0.5166326642535923\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.5032493821650714\n",
      "  policy_reward_mean:\n",
      "    main: 0.03936399013399129\n",
      "  policy_reward_min:\n",
      "    main: -1.7792536968583943\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3047694629852019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.962154365328175\n",
      "    mean_inference_ms: 4.433597401239425\n",
      "    mean_raw_obs_processing_ms: 1.316513728986148\n",
      "  time_since_restore: 37864.95195531845\n",
      "  time_this_iter_s: 115.67891240119934\n",
      "  time_total_s: 37864.95195531845\n",
      "  timers:\n",
      "    learn_throughput: 89.805\n",
      "    learn_time_ms: 88992.836\n",
      "    sample_throughput: 300.957\n",
      "    sample_time_ms: 26555.258\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1639196101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2637360\n",
      "  training_iteration: 330\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">           37865</td><td style=\"text-align: right;\">2637360</td><td style=\"text-align: right;\">0.157456</td><td style=\"text-align: right;\">              2.2223</td><td style=\"text-align: right;\">            -1.18429</td><td style=\"text-align: right;\">           68.7712</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10581408\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7844718922665291\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07014854910174524\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4814728937641011\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 61.768\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.753651598160579\n",
      "  episode_reward_mean: 0.20080064396031755\n",
      "  episode_reward_min: -1.1092932473498252\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 38457\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6050465292930604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015179265614598989\n",
      "          policy_loss: -0.09652253826335072\n",
      "          total_loss: 0.20705746419727802\n",
      "          vf_explained_var: 0.4614172577857971\n",
      "          vf_loss: 0.2805264941453934\n",
      "    num_agent_steps_sampled: 10581408\n",
      "    num_agent_steps_trained: 10581408\n",
      "    num_steps_sampled: 2645352\n",
      "    num_steps_trained: 2645352\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.86513157894737\n",
      "    gpu_util_percent0: 0.17710526315789474\n",
      "    ram_util_percent: 94.6078947368421\n",
      "    vram_util_percent0: 0.5166259676832754\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8614908780956818\n",
      "  policy_reward_mean:\n",
      "    main: 0.05020016099007938\n",
      "  policy_reward_min:\n",
      "    main: -1.7215847947677205\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30482117287775107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.967651790011217\n",
      "    mean_inference_ms: 4.43379147002463\n",
      "    mean_raw_obs_processing_ms: 1.3169266763909924\n",
      "  time_since_restore: 37983.26633501053\n",
      "  time_this_iter_s: 118.31437969207764\n",
      "  time_total_s: 37983.26633501053\n",
      "  timers:\n",
      "    learn_throughput: 89.76\n",
      "    learn_time_ms: 89037.386\n",
      "    sample_throughput: 298.226\n",
      "    sample_time_ms: 26798.456\n",
      "    update_time_ms: 3.344\n",
      "  timestamp: 1639196219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2645352\n",
      "  training_iteration: 331\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         37983.3</td><td style=\"text-align: right;\">2645352</td><td style=\"text-align: right;\">0.200801</td><td style=\"text-align: right;\">             1.75365</td><td style=\"text-align: right;\">            -1.10929</td><td style=\"text-align: right;\">            61.768</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10613376\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8535390599338128\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07648741793668658\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8573626028233414\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 61.35606060606061\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3623669730140358\n",
      "  episode_reward_mean: 0.1732586841656039\n",
      "  episode_reward_min: -1.2359448802671933\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 38589\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6036608227491379\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014799160797148943\n",
      "          policy_loss: -0.09540952394902706\n",
      "          total_loss: 0.19682752758823335\n",
      "          vf_explained_var: 0.5012593269348145\n",
      "          vf_loss: 0.2697608278989792\n",
      "    num_agent_steps_sampled: 10613376\n",
      "    num_agent_steps_trained: 10613376\n",
      "    num_steps_sampled: 2653344\n",
      "    num_steps_trained: 2653344\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.71946308724832\n",
      "    gpu_util_percent0: 0.17832214765100668\n",
      "    ram_util_percent: 94.58791946308723\n",
      "    vram_util_percent0: 0.5166338715269727\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.040601373924032\n",
      "  policy_reward_mean:\n",
      "    main: 0.04331467104140098\n",
      "  policy_reward_min:\n",
      "    main: -1.8621369284569236\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043718421231287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.958681859446386\n",
      "    mean_inference_ms: 4.431174254285246\n",
      "    mean_raw_obs_processing_ms: 1.3168142811510428\n",
      "  time_since_restore: 38099.016226530075\n",
      "  time_this_iter_s: 115.74989151954651\n",
      "  time_total_s: 38099.016226530075\n",
      "  timers:\n",
      "    learn_throughput: 89.728\n",
      "    learn_time_ms: 89069.342\n",
      "    sample_throughput: 298.088\n",
      "    sample_time_ms: 26810.886\n",
      "    update_time_ms: 3.303\n",
      "  timestamp: 1639196335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2653344\n",
      "  training_iteration: 332\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">           38099</td><td style=\"text-align: right;\">2653344</td><td style=\"text-align: right;\">0.173259</td><td style=\"text-align: right;\">             1.36237</td><td style=\"text-align: right;\">            -1.23594</td><td style=\"text-align: right;\">           61.3561</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10645344\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.822761661174116\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.055179721717348265\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6918030322533422\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-20-51\n",
      "  done: false\n",
      "  episode_len_mean: 58.76428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2195183005169512\n",
      "  episode_reward_mean: 0.11754821184007273\n",
      "  episode_reward_min: -1.1081508817827252\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 38729\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6067256124019623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015017546523362398\n",
      "          policy_loss: -0.09645673263445496\n",
      "          total_loss: 0.1888752214219421\n",
      "          vf_explained_var: 0.5329852104187012\n",
      "          vf_loss: 0.26252405589818956\n",
      "    num_agent_steps_sampled: 10645344\n",
      "    num_agent_steps_trained: 10645344\n",
      "    num_steps_sampled: 2661336\n",
      "    num_steps_trained: 2661336\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.911333333333324\n",
      "    gpu_util_percent0: 0.18\n",
      "    ram_util_percent: 94.74333333333334\n",
      "    vram_util_percent0: 0.5146638148513765\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.985776143843093\n",
      "  policy_reward_mean:\n",
      "    main: 0.029387052960018183\n",
      "  policy_reward_min:\n",
      "    main: -1.6918030322533422\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043202817387702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.958601654369\n",
      "    mean_inference_ms: 4.431391224038164\n",
      "    mean_raw_obs_processing_ms: 1.3170748637657599\n",
      "  time_since_restore: 38215.3541572094\n",
      "  time_this_iter_s: 116.33793067932129\n",
      "  time_total_s: 38215.3541572094\n",
      "  timers:\n",
      "    learn_throughput: 89.702\n",
      "    learn_time_ms: 89095.362\n",
      "    sample_throughput: 297.853\n",
      "    sample_time_ms: 26832.06\n",
      "    update_time_ms: 3.358\n",
      "  timestamp: 1639196451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2661336\n",
      "  training_iteration: 333\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         38215.4</td><td style=\"text-align: right;\">2661336</td><td style=\"text-align: right;\">0.117548</td><td style=\"text-align: right;\">             1.21952</td><td style=\"text-align: right;\">            -1.10815</td><td style=\"text-align: right;\">           58.7643</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10677312\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7739811875708207\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07448150295645294\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7277274848711217\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 56.18840579710145\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2311174645928942\n",
      "  episode_reward_mean: 0.1403161926139612\n",
      "  episode_reward_min: -1.6482556742340062\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 38867\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6096283428668976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01480191170424223\n",
      "          policy_loss: -0.09368716154620052\n",
      "          total_loss: 0.19317539132758976\n",
      "          vf_explained_var: 0.5221853852272034\n",
      "          vf_loss: 0.2643821480870247\n",
      "    num_agent_steps_sampled: 10677312\n",
      "    num_agent_steps_trained: 10677312\n",
      "    num_steps_sampled: 2669328\n",
      "    num_steps_trained: 2669328\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.815999999999995\n",
      "    gpu_util_percent0: 0.17900000000000002\n",
      "    ram_util_percent: 94.78200000000001\n",
      "    vram_util_percent0: 0.5140506745640012\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.955504267952953\n",
      "  policy_reward_mean:\n",
      "    main: 0.0350790481534903\n",
      "  policy_reward_min:\n",
      "    main: -1.7467949246710672\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30402038222116096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.954504363012177\n",
      "    mean_inference_ms: 4.430346927095777\n",
      "    mean_raw_obs_processing_ms: 1.317093158869331\n",
      "  time_since_restore: 38331.08910322189\n",
      "  time_this_iter_s: 115.73494601249695\n",
      "  time_total_s: 38331.08910322189\n",
      "  timers:\n",
      "    learn_throughput: 89.681\n",
      "    learn_time_ms: 89115.593\n",
      "    sample_throughput: 297.966\n",
      "    sample_time_ms: 26821.816\n",
      "    update_time_ms: 3.318\n",
      "  timestamp: 1639196567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2669328\n",
      "  training_iteration: 334\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         38331.1</td><td style=\"text-align: right;\">2669328</td><td style=\"text-align: right;\">0.140316</td><td style=\"text-align: right;\">             1.23112</td><td style=\"text-align: right;\">            -1.64826</td><td style=\"text-align: right;\">           56.1884</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10709280\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1216185960212202\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04410079533280974\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5938215687887557\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-24-43\n",
      "  done: false\n",
      "  episode_len_mean: 65.09836065573771\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.61472679038119\n",
      "  episode_reward_mean: 0.12646029476581575\n",
      "  episode_reward_min: -1.4718890006132415\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 38989\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6029817938804627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014624672066420317\n",
      "          policy_loss: -0.0938050590865314\n",
      "          total_loss: 0.20106911881640555\n",
      "          vf_explained_var: 0.4771926999092102\n",
      "          vf_loss: 0.27266295927762985\n",
      "    num_agent_steps_sampled: 10709280\n",
      "    num_agent_steps_trained: 10709280\n",
      "    num_steps_sampled: 2677320\n",
      "    num_steps_trained: 2677320\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.52885906040269\n",
      "    gpu_util_percent0: 0.17765100671140938\n",
      "    ram_util_percent: 94.71677852348992\n",
      "    vram_util_percent0: 0.514045595182096\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0392569032148913\n",
      "  policy_reward_mean:\n",
      "    main: 0.031615073691453936\n",
      "  policy_reward_min:\n",
      "    main: -1.9169077986619734\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3045199649653013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.960961784157572\n",
      "    mean_inference_ms: 4.43201021342975\n",
      "    mean_raw_obs_processing_ms: 1.317412566183846\n",
      "  time_since_restore: 38447.04490828514\n",
      "  time_this_iter_s: 115.95580506324768\n",
      "  time_total_s: 38447.04490828514\n",
      "  timers:\n",
      "    learn_throughput: 89.576\n",
      "    learn_time_ms: 89220.467\n",
      "    sample_throughput: 298.187\n",
      "    sample_time_ms: 26801.988\n",
      "    update_time_ms: 3.383\n",
      "  timestamp: 1639196683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2677320\n",
      "  training_iteration: 335\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">           38447</td><td style=\"text-align: right;\">2677320</td><td style=\"text-align: right;\"> 0.12646</td><td style=\"text-align: right;\">             1.61473</td><td style=\"text-align: right;\">            -1.47189</td><td style=\"text-align: right;\">           65.0984</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10741248\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6861053354966625\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05525365094532109\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5415436427783509\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-26-39\n",
      "  done: false\n",
      "  episode_len_mean: 65.77235772357723\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1861904223281075\n",
      "  episode_reward_mean: 0.11228576248831905\n",
      "  episode_reward_min: -0.8258640120725929\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 39112\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6060828392505646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015062440130859613\n",
      "          policy_loss: -0.09645883816480637\n",
      "          total_loss: 0.17560949248448016\n",
      "          vf_explained_var: 0.5142025351524353\n",
      "          vf_loss: 0.24919225066900252\n",
      "    num_agent_steps_sampled: 10741248\n",
      "    num_agent_steps_trained: 10741248\n",
      "    num_steps_sampled: 2685312\n",
      "    num_steps_trained: 2685312\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.836000000000006\n",
      "    gpu_util_percent0: 0.1808\n",
      "    ram_util_percent: 94.85000000000001\n",
      "    vram_util_percent0: 0.5140539651201051\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.6400089214152689\n",
      "  policy_reward_mean:\n",
      "    main: 0.028071440622079783\n",
      "  policy_reward_min:\n",
      "    main: -1.8655464444426793\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3045164977905564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.960363077112838\n",
      "    mean_inference_ms: 4.43247589489801\n",
      "    mean_raw_obs_processing_ms: 1.3175155873790234\n",
      "  time_since_restore: 38563.05336165428\n",
      "  time_this_iter_s: 116.00845336914062\n",
      "  time_total_s: 38563.05336165428\n",
      "  timers:\n",
      "    learn_throughput: 89.463\n",
      "    learn_time_ms: 89332.899\n",
      "    sample_throughput: 298.563\n",
      "    sample_time_ms: 26768.208\n",
      "    update_time_ms: 3.378\n",
      "  timestamp: 1639196799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2685312\n",
      "  training_iteration: 336\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         38563.1</td><td style=\"text-align: right;\">2685312</td><td style=\"text-align: right;\">0.112286</td><td style=\"text-align: right;\">             1.18619</td><td style=\"text-align: right;\">           -0.825864</td><td style=\"text-align: right;\">           65.7724</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10773216\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9904974508332337\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05842245862334817\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5677232608215772\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 62.09848484848485\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7770086255697022\n",
      "  episode_reward_mean: 0.1469592189725251\n",
      "  episode_reward_min: -1.3112768489508637\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 39244\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6059355555772782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014973688062280416\n",
      "          policy_loss: -0.0959740608409047\n",
      "          total_loss: 0.2091392095759511\n",
      "          vf_explained_var: 0.4883503019809723\n",
      "          vf_loss: 0.28237198263406754\n",
      "    num_agent_steps_sampled: 10773216\n",
      "    num_agent_steps_trained: 10773216\n",
      "    num_steps_sampled: 2693304\n",
      "    num_steps_trained: 2693304\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.532666666666664\n",
      "    gpu_util_percent0: 0.17940000000000003\n",
      "    ram_util_percent: 94.76933333333332\n",
      "    vram_util_percent0: 0.5140616430843479\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.871212491207267\n",
      "  policy_reward_mean:\n",
      "    main: 0.03673980474313126\n",
      "  policy_reward_min:\n",
      "    main: -1.7654745173142654\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30468537588962863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.965365614614846\n",
      "    mean_inference_ms: 4.434879839983618\n",
      "    mean_raw_obs_processing_ms: 1.3177504995146634\n",
      "  time_since_restore: 38678.731701374054\n",
      "  time_this_iter_s: 115.67833971977234\n",
      "  time_total_s: 38678.731701374054\n",
      "  timers:\n",
      "    learn_throughput: 89.457\n",
      "    learn_time_ms: 89338.834\n",
      "    sample_throughput: 298.37\n",
      "    sample_time_ms: 26785.517\n",
      "    update_time_ms: 3.404\n",
      "  timestamp: 1639196915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2693304\n",
      "  training_iteration: 337\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         38678.7</td><td style=\"text-align: right;\">2693304</td><td style=\"text-align: right;\">0.146959</td><td style=\"text-align: right;\">             1.77701</td><td style=\"text-align: right;\">            -1.31128</td><td style=\"text-align: right;\">           62.0985</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10805184\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8062494591079352\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07183475062657682\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5036952548606654\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 65.38655462184875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7427510736656489\n",
      "  episode_reward_mean: 0.15458456733049514\n",
      "  episode_reward_min: -1.4514950119892038\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 39363\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.604353376030922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015020023625344037\n",
      "          policy_loss: -0.0968011172413826\n",
      "          total_loss: 0.20841596276313065\n",
      "          vf_explained_var: 0.46380364894866943\n",
      "          vf_loss: 0.28240541988611223\n",
      "    num_agent_steps_sampled: 10805184\n",
      "    num_agent_steps_trained: 10805184\n",
      "    num_steps_sampled: 2701296\n",
      "    num_steps_trained: 2701296\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.43221476510067\n",
      "    gpu_util_percent0: 0.18114093959731542\n",
      "    ram_util_percent: 94.84630872483223\n",
      "    vram_util_percent0: 0.5140500120359266\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.183021801280363\n",
      "  policy_reward_mean:\n",
      "    main: 0.03864614183262377\n",
      "  policy_reward_min:\n",
      "    main: -1.7457549797559844\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3047522715805793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.970594650142658\n",
      "    mean_inference_ms: 4.4378409380071\n",
      "    mean_raw_obs_processing_ms: 1.31787097571548\n",
      "  time_since_restore: 38794.569311380386\n",
      "  time_this_iter_s: 115.8376100063324\n",
      "  time_total_s: 38794.569311380386\n",
      "  timers:\n",
      "    learn_throughput: 89.639\n",
      "    learn_time_ms: 89157.994\n",
      "    sample_throughput: 298.549\n",
      "    sample_time_ms: 26769.452\n",
      "    update_time_ms: 3.459\n",
      "  timestamp: 1639197031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2701296\n",
      "  training_iteration: 338\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         38794.6</td><td style=\"text-align: right;\">2701296</td><td style=\"text-align: right;\">0.154585</td><td style=\"text-align: right;\">             1.74275</td><td style=\"text-align: right;\">             -1.4515</td><td style=\"text-align: right;\">           65.3866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10837152\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0405050192279557\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08357102967939478\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8341510728177955\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 57.40875912408759\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.815517026623756\n",
      "  episode_reward_mean: 0.14296203883461933\n",
      "  episode_reward_min: -1.119498342206851\n",
      "  episodes_this_iter: 137\n",
      "  episodes_total: 39500\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.600850216627121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015291521091014146\n",
      "          policy_loss: -0.0986516329459846\n",
      "          total_loss: 0.21146932466328144\n",
      "          vf_explained_var: 0.48049649596214294\n",
      "          vf_loss: 0.286896959900856\n",
      "    num_agent_steps_sampled: 10837152\n",
      "    num_agent_steps_trained: 10837152\n",
      "    num_steps_sampled: 2709288\n",
      "    num_steps_trained: 2709288\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.62214765100671\n",
      "    gpu_util_percent0: 0.17845637583892615\n",
      "    ram_util_percent: 94.91677852348992\n",
      "    vram_util_percent0: 0.5140433867551805\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9979335682451806\n",
      "  policy_reward_mean:\n",
      "    main: 0.035740509708654826\n",
      "  policy_reward_min:\n",
      "    main: -1.8402147796959598\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30446856601835587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.958839003322403\n",
      "    mean_inference_ms: 4.4322886809983695\n",
      "    mean_raw_obs_processing_ms: 1.3176689536650006\n",
      "  time_since_restore: 38910.533202171326\n",
      "  time_this_iter_s: 115.96389079093933\n",
      "  time_total_s: 38910.533202171326\n",
      "  timers:\n",
      "    learn_throughput: 89.76\n",
      "    learn_time_ms: 89037.238\n",
      "    sample_throughput: 299.258\n",
      "    sample_time_ms: 26706.083\n",
      "    update_time_ms: 3.428\n",
      "  timestamp: 1639197147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2709288\n",
      "  training_iteration: 339\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         38910.5</td><td style=\"text-align: right;\">2709288</td><td style=\"text-align: right;\">0.142962</td><td style=\"text-align: right;\">             1.81552</td><td style=\"text-align: right;\">             -1.1195</td><td style=\"text-align: right;\">           57.4088</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10869120\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7943654849162384\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.085475980952915\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7956146818041298\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-34-23\n",
      "  done: false\n",
      "  episode_len_mean: 65.1015625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.978207476145279\n",
      "  episode_reward_mean: 0.18456206423020752\n",
      "  episode_reward_min: -1.7485493163026535\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 39628\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6003192105293274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01500702352449298\n",
      "          policy_loss: -0.09546540937572717\n",
      "          total_loss: 0.20233534326404334\n",
      "          vf_explained_var: 0.4990273118019104\n",
      "          vf_loss: 0.27500883567333223\n",
      "    num_agent_steps_sampled: 10869120\n",
      "    num_agent_steps_trained: 10869120\n",
      "    num_steps_sampled: 2717280\n",
      "    num_steps_trained: 2717280\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.48133333333333\n",
      "    gpu_util_percent0: 0.17859999999999998\n",
      "    ram_util_percent: 94.89600000000002\n",
      "    vram_util_percent0: 0.5140605462323133\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9883206706532386\n",
      "  policy_reward_mean:\n",
      "    main: 0.0461405160575519\n",
      "  policy_reward_min:\n",
      "    main: -1.8026650277268437\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30484173144959625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.969114805473865\n",
      "    mean_inference_ms: 4.436857402531307\n",
      "    mean_raw_obs_processing_ms: 1.3180546174536198\n",
      "  time_since_restore: 39026.39112305641\n",
      "  time_this_iter_s: 115.85792088508606\n",
      "  time_total_s: 39026.39112305641\n",
      "  timers:\n",
      "    learn_throughput: 89.733\n",
      "    learn_time_ms: 89064.601\n",
      "    sample_throughput: 299.302\n",
      "    sample_time_ms: 26702.088\n",
      "    update_time_ms: 3.413\n",
      "  timestamp: 1639197263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2717280\n",
      "  training_iteration: 340\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         39026.4</td><td style=\"text-align: right;\">2717280</td><td style=\"text-align: right;\">0.184562</td><td style=\"text-align: right;\">             1.97821</td><td style=\"text-align: right;\">            -1.74855</td><td style=\"text-align: right;\">           65.1016</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10901088\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7932959860338659\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05947630933710907\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6164662724448877\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-36-18\n",
      "  done: false\n",
      "  episode_len_mean: 56.286764705882355\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2462708614144602\n",
      "  episode_reward_mean: 0.061147409348625205\n",
      "  episode_reward_min: -1.4987961506009637\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 39764\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6046560943126679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0152290700674057\n",
      "          policy_loss: -0.09700451431795955\n",
      "          total_loss: 0.21093005553632974\n",
      "          vf_explained_var: 0.469986230134964\n",
      "          vf_loss: 0.2848054186105728\n",
      "    num_agent_steps_sampled: 10901088\n",
      "    num_agent_steps_trained: 10901088\n",
      "    num_steps_sampled: 2725272\n",
      "    num_steps_trained: 2725272\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.68533333333333\n",
      "    gpu_util_percent0: 0.17646666666666666\n",
      "    ram_util_percent: 94.99066666666667\n",
      "    vram_util_percent0: 0.5140649336404518\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8719150292223103\n",
      "  policy_reward_mean:\n",
      "    main: 0.015286852337156298\n",
      "  policy_reward_min:\n",
      "    main: -1.8700548333143958\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30431913493801943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.953306594822642\n",
      "    mean_inference_ms: 4.430056814973788\n",
      "    mean_raw_obs_processing_ms: 1.3177739357647373\n",
      "  time_since_restore: 39141.719868421555\n",
      "  time_this_iter_s: 115.32874536514282\n",
      "  time_total_s: 39141.719868421555\n",
      "  timers:\n",
      "    learn_throughput: 89.754\n",
      "    learn_time_ms: 89043.076\n",
      "    sample_throughput: 302.451\n",
      "    sample_time_ms: 26424.088\n",
      "    update_time_ms: 2.99\n",
      "  timestamp: 1639197378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2725272\n",
      "  training_iteration: 341\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         39141.7</td><td style=\"text-align: right;\">2725272</td><td style=\"text-align: right;\">0.0611474</td><td style=\"text-align: right;\">             1.24627</td><td style=\"text-align: right;\">             -1.4988</td><td style=\"text-align: right;\">           56.2868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10933056\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8624891797321591\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06980710638565772\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.45481095765787694\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 62.35658914728682\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3880914603956138\n",
      "  episode_reward_mean: 0.17290341217655886\n",
      "  episode_reward_min: -0.7295368721523623\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 39893\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6015405638217926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015231622438877821\n",
      "          policy_loss: -0.09529404553771019\n",
      "          total_loss: 0.1799198708459735\n",
      "          vf_explained_var: 0.5274650454521179\n",
      "          vf_loss: 0.25208089178800586\n",
      "    num_agent_steps_sampled: 10933056\n",
      "    num_agent_steps_trained: 10933056\n",
      "    num_steps_sampled: 2733264\n",
      "    num_steps_trained: 2733264\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.73866666666667\n",
      "    gpu_util_percent0: 0.18\n",
      "    ram_util_percent: 94.98200000000001\n",
      "    vram_util_percent0: 0.5140572556762091\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.879894662477986\n",
      "  policy_reward_mean:\n",
      "    main: 0.043225853044139693\n",
      "  policy_reward_min:\n",
      "    main: -1.5994531899294224\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3043689401373721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.95515395310479\n",
      "    mean_inference_ms: 4.431357237226785\n",
      "    mean_raw_obs_processing_ms: 1.3179463668159261\n",
      "  time_since_restore: 39257.60679125786\n",
      "  time_this_iter_s: 115.88692283630371\n",
      "  time_total_s: 39257.60679125786\n",
      "  timers:\n",
      "    learn_throughput: 89.772\n",
      "    learn_time_ms: 89025.517\n",
      "    sample_throughput: 302.148\n",
      "    sample_time_ms: 26450.654\n",
      "    update_time_ms: 3.026\n",
      "  timestamp: 1639197494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2733264\n",
      "  training_iteration: 342\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         39257.6</td><td style=\"text-align: right;\">2733264</td><td style=\"text-align: right;\">0.172903</td><td style=\"text-align: right;\">             1.38809</td><td style=\"text-align: right;\">           -0.729537</td><td style=\"text-align: right;\">           62.3566</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10965024\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8465113175661968\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0984489001834443\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6780400544040044\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 67.34426229508196\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.709128572505065\n",
      "  episode_reward_mean: 0.22099015769736036\n",
      "  episode_reward_min: -1.5909399522163181\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 40015\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6044762687683105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014937647067010403\n",
      "          policy_loss: -0.09530659140646458\n",
      "          total_loss: 0.20242736149951815\n",
      "          vf_explained_var: 0.4764102101325989\n",
      "          vf_loss: 0.27504740238189695\n",
      "    num_agent_steps_sampled: 10965024\n",
      "    num_agent_steps_trained: 10965024\n",
      "    num_steps_sampled: 2741256\n",
      "    num_steps_trained: 2741256\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.37702702702702\n",
      "    gpu_util_percent0: 0.18020270270270272\n",
      "    ram_util_percent: 94.96554054054054\n",
      "    vram_util_percent0: 0.51404934055477\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9449017219383213\n",
      "  policy_reward_mean:\n",
      "    main: 0.05524753942434006\n",
      "  policy_reward_min:\n",
      "    main: -1.6830323275891765\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3047781415752397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.966315184722152\n",
      "    mean_inference_ms: 4.436576281087834\n",
      "    mean_raw_obs_processing_ms: 1.31821612034233\n",
      "  time_since_restore: 39372.499363183975\n",
      "  time_this_iter_s: 114.89257192611694\n",
      "  time_total_s: 39372.499363183975\n",
      "  timers:\n",
      "    learn_throughput: 89.816\n",
      "    learn_time_ms: 88981.575\n",
      "    sample_throughput: 303.338\n",
      "    sample_time_ms: 26346.849\n",
      "    update_time_ms: 3.001\n",
      "  timestamp: 1639197609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2741256\n",
      "  training_iteration: 343\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         39372.5</td><td style=\"text-align: right;\">2741256</td><td style=\"text-align: right;\"> 0.22099</td><td style=\"text-align: right;\">             1.70913</td><td style=\"text-align: right;\">            -1.59094</td><td style=\"text-align: right;\">           67.3443</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 10996992\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.03171607072264\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.051009274682379695\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4635782502755382\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 61.49586776859504\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4791133429049672\n",
      "  episode_reward_mean: 0.13545943438588576\n",
      "  episode_reward_min: -1.216097564725404\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 40136\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6084787542819977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014885985180735589\n",
      "          policy_loss: -0.09467102343589068\n",
      "          total_loss: 0.20803009498119354\n",
      "          vf_explained_var: 0.4608520269393921\n",
      "          vf_loss: 0.28009302806854247\n",
      "    num_agent_steps_sampled: 10996992\n",
      "    num_agent_steps_trained: 10996992\n",
      "    num_steps_sampled: 2749248\n",
      "    num_steps_trained: 2749248\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.75503355704697\n",
      "    gpu_util_percent0: 0.1804697986577181\n",
      "    ram_util_percent: 95.01409395973154\n",
      "    vram_util_percent0: 0.5140566373166728\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9639246221368147\n",
      "  policy_reward_mean:\n",
      "    main: 0.03386485859647143\n",
      "  policy_reward_min:\n",
      "    main: -1.5643116985757002\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30462829430413796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.9619390383196\n",
      "    mean_inference_ms: 4.435054955400764\n",
      "    mean_raw_obs_processing_ms: 1.3180509627944346\n",
      "  time_since_restore: 39488.16389465332\n",
      "  time_this_iter_s: 115.66453146934509\n",
      "  time_total_s: 39488.16389465332\n",
      "  timers:\n",
      "    learn_throughput: 89.81\n",
      "    learn_time_ms: 88987.394\n",
      "    sample_throughput: 303.396\n",
      "    sample_time_ms: 26341.85\n",
      "    update_time_ms: 3.005\n",
      "  timestamp: 1639197725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2749248\n",
      "  training_iteration: 344\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         39488.2</td><td style=\"text-align: right;\">2749248</td><td style=\"text-align: right;\">0.135459</td><td style=\"text-align: right;\">             1.47911</td><td style=\"text-align: right;\">             -1.2161</td><td style=\"text-align: right;\">           61.4959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11028960\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.6465565868688852\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05746546910239865\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5060636889329511\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 69.1880341880342\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8846776717972107\n",
      "  episode_reward_mean: 0.09283881491602182\n",
      "  episode_reward_min: -0.8856573964730641\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 40253\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6098433678150177\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015352960500866175\n",
      "          policy_loss: -0.0966555223762989\n",
      "          total_loss: 0.1671338839903474\n",
      "          vf_explained_var: 0.49696648120880127\n",
      "          vf_loss: 0.24047209817171097\n",
      "    num_agent_steps_sampled: 11028960\n",
      "    num_agent_steps_trained: 11028960\n",
      "    num_steps_sampled: 2757240\n",
      "    num_steps_trained: 2757240\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.36979865771812\n",
      "    gpu_util_percent0: 0.1793288590604027\n",
      "    ram_util_percent: 95.00604026845639\n",
      "    vram_util_percent0: 0.5140433867551805\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.5328940878748973\n",
      "  policy_reward_mean:\n",
      "    main: 0.023209703729005444\n",
      "  policy_reward_min:\n",
      "    main: -1.5306791603318912\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30476348855846513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.962865315490685\n",
      "    mean_inference_ms: 4.434924142752999\n",
      "    mean_raw_obs_processing_ms: 1.318071319353322\n",
      "  time_since_restore: 39603.53149223328\n",
      "  time_this_iter_s: 115.36759757995605\n",
      "  time_total_s: 39603.53149223328\n",
      "  timers:\n",
      "    learn_throughput: 89.866\n",
      "    learn_time_ms: 88931.948\n",
      "    sample_throughput: 303.471\n",
      "    sample_time_ms: 26335.32\n",
      "    update_time_ms: 2.978\n",
      "  timestamp: 1639197840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2757240\n",
      "  training_iteration: 345\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         39603.5</td><td style=\"text-align: right;\">2757240</td><td style=\"text-align: right;\">0.0928388</td><td style=\"text-align: right;\">             1.88468</td><td style=\"text-align: right;\">           -0.885657</td><td style=\"text-align: right;\">            69.188</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11060928\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8436330147081503\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.042321771083618356\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5110827623466159\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 71.38392857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5916624377164816\n",
      "  episode_reward_mean: 0.09065002277333352\n",
      "  episode_reward_min: -0.9976720384391093\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 40365\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6069671516418457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015069913174957037\n",
      "          policy_loss: -0.0943607565164566\n",
      "          total_loss: 0.1625858704596758\n",
      "          vf_explained_var: 0.4901713728904724\n",
      "          vf_loss: 0.23405919682979584\n",
      "    num_agent_steps_sampled: 11060928\n",
      "    num_agent_steps_trained: 11060928\n",
      "    num_steps_sampled: 2765232\n",
      "    num_steps_trained: 2765232\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    gpu_util_percent0: 0.17741721854304635\n",
      "    ram_util_percent: 95.0205298013245\n",
      "    vram_util_percent0: 0.5140611346098949\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9666993030483775\n",
      "  policy_reward_mean:\n",
      "    main: 0.022662505693333373\n",
      "  policy_reward_min:\n",
      "    main: -1.617215231370408\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3053373499705297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.980709324422044\n",
      "    mean_inference_ms: 4.442402065716618\n",
      "    mean_raw_obs_processing_ms: 1.3184624368770332\n",
      "  time_since_restore: 39721.15345788002\n",
      "  time_this_iter_s: 117.62196564674377\n",
      "  time_total_s: 39721.15345788002\n",
      "  timers:\n",
      "    learn_throughput: 89.759\n",
      "    learn_time_ms: 89038.462\n",
      "    sample_throughput: 302.827\n",
      "    sample_time_ms: 26391.326\n",
      "    update_time_ms: 2.965\n",
      "  timestamp: 1639197958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2765232\n",
      "  training_iteration: 346\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         39721.2</td><td style=\"text-align: right;\">2765232</td><td style=\"text-align: right;\"> 0.09065</td><td style=\"text-align: right;\">             1.59166</td><td style=\"text-align: right;\">           -0.997672</td><td style=\"text-align: right;\">           71.3839</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11092896\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.6563421720631766\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1153308416619956\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47056326698515555\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 67.02459016393442\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6167722218268512\n",
      "  episode_reward_mean: 0.2161025005653562\n",
      "  episode_reward_min: -1.0110287977742467\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 40487\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6051875500679016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015253842767328024\n",
      "          policy_loss: -0.0958822357468307\n",
      "          total_loss: 0.19836511410027743\n",
      "          vf_explained_var: 0.46856603026390076\n",
      "          vf_loss: 0.2710805773139\n",
      "    num_agent_steps_sampled: 11092896\n",
      "    num_agent_steps_trained: 11092896\n",
      "    num_steps_sampled: 2773224\n",
      "    num_steps_trained: 2773224\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.85466666666667\n",
      "    gpu_util_percent0: 0.17819999999999997\n",
      "    ram_util_percent: 94.87666666666665\n",
      "    vram_util_percent0: 0.5140715147526599\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.4115643987404054\n",
      "  policy_reward_mean:\n",
      "    main: 0.054025625141339066\n",
      "  policy_reward_min:\n",
      "    main: -1.7989932478661104\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30502163237428215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.969767481749116\n",
      "    mean_inference_ms: 4.438174346565527\n",
      "    mean_raw_obs_processing_ms: 1.3182086474340875\n",
      "  time_since_restore: 39837.76795339584\n",
      "  time_this_iter_s: 116.61449551582336\n",
      "  time_total_s: 39837.76795339584\n",
      "  timers:\n",
      "    learn_throughput: 89.622\n",
      "    learn_time_ms: 89174.911\n",
      "    sample_throughput: 303.252\n",
      "    sample_time_ms: 26354.334\n",
      "    update_time_ms: 2.951\n",
      "  timestamp: 1639198075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2773224\n",
      "  training_iteration: 347\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         39837.8</td><td style=\"text-align: right;\">2773224</td><td style=\"text-align: right;\">0.216103</td><td style=\"text-align: right;\">             1.61677</td><td style=\"text-align: right;\">            -1.01103</td><td style=\"text-align: right;\">           67.0246</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11124864\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9123663229061905\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0727844929493738\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.46238523971079615\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-49-50\n",
      "  done: false\n",
      "  episode_len_mean: 65.50806451612904\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6701255281063623\n",
      "  episode_reward_mean: 0.1662494768048991\n",
      "  episode_reward_min: -0.6793200572786229\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 40611\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6002937951087952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015246412191540003\n",
      "          policy_loss: -0.09694643305987119\n",
      "          total_loss: 0.19564969453960657\n",
      "          vf_explained_var: 0.467132031917572\n",
      "          vf_loss: 0.2694406397342682\n",
      "    num_agent_steps_sampled: 11124864\n",
      "    num_agent_steps_trained: 11124864\n",
      "    num_steps_sampled: 2781216\n",
      "    num_steps_trained: 2781216\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.481208053691276\n",
      "    gpu_util_percent0: 0.1786577181208054\n",
      "    ram_util_percent: 94.68389261744964\n",
      "    vram_util_percent0: 0.5140422825417227\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7761448534524051\n",
      "  policy_reward_mean:\n",
      "    main: 0.04156236920122479\n",
      "  policy_reward_min:\n",
      "    main: -1.5171010728545922\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30494323378945426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.965722675427273\n",
      "    mean_inference_ms: 4.43653896120714\n",
      "    mean_raw_obs_processing_ms: 1.3182110095542203\n",
      "  time_since_restore: 39953.28720784187\n",
      "  time_this_iter_s: 115.51925444602966\n",
      "  time_total_s: 39953.28720784187\n",
      "  timers:\n",
      "    learn_throughput: 89.632\n",
      "    learn_time_ms: 89164.502\n",
      "    sample_throughput: 303.38\n",
      "    sample_time_ms: 26343.201\n",
      "    update_time_ms: 3.163\n",
      "  timestamp: 1639198190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2781216\n",
      "  training_iteration: 348\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         39953.3</td><td style=\"text-align: right;\">2781216</td><td style=\"text-align: right;\">0.166249</td><td style=\"text-align: right;\">             1.67013</td><td style=\"text-align: right;\">            -0.67932</td><td style=\"text-align: right;\">           65.5081</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11156832\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7155538030041088\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.050960430953419344\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4637088263293162\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 69.34210526315789\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6446867398079619\n",
      "  episode_reward_mean: 0.0829655796800195\n",
      "  episode_reward_min: -1.2675969442623671\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 40725\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6068609722852707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015007074616849423\n",
      "          policy_loss: -0.09623430064693093\n",
      "          total_loss: 0.18371264703571796\n",
      "          vf_explained_var: 0.48900189995765686\n",
      "          vf_loss: 0.25715495520830156\n",
      "    num_agent_steps_sampled: 11156832\n",
      "    num_agent_steps_trained: 11156832\n",
      "    num_steps_sampled: 2789208\n",
      "    num_steps_trained: 2789208\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.79127516778524\n",
      "    gpu_util_percent0: 0.1763758389261745\n",
      "    ram_util_percent: 94.51879194630874\n",
      "    vram_util_percent0: 0.5142123314142104\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1160709698108273\n",
      "  policy_reward_mean:\n",
      "    main: 0.020741394920004874\n",
      "  policy_reward_min:\n",
      "    main: -1.66299374771941\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30425283600805275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.952261960786107\n",
      "    mean_inference_ms: 4.432063519643515\n",
      "    mean_raw_obs_processing_ms: 1.317828367381098\n",
      "  time_since_restore: 40069.83528614044\n",
      "  time_this_iter_s: 116.54807829856873\n",
      "  time_total_s: 40069.83528614044\n",
      "  timers:\n",
      "    learn_throughput: 89.599\n",
      "    learn_time_ms: 89197.838\n",
      "    sample_throughput: 303.076\n",
      "    sample_time_ms: 26369.605\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1639198307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2789208\n",
      "  training_iteration: 349\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         40069.8</td><td style=\"text-align: right;\">2789208</td><td style=\"text-align: right;\">0.0829656</td><td style=\"text-align: right;\">             1.64469</td><td style=\"text-align: right;\">             -1.2676</td><td style=\"text-align: right;\">           69.3421</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11188800\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8434041318822915\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11060535800682135\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48730050253970175\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 67.31404958677686\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0434803486482194\n",
      "  episode_reward_mean: 0.29356703934743\n",
      "  episode_reward_min: -1.1656867256105823\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 40846\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6012043740749359\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015234079521149397\n",
      "          policy_loss: -0.09716077686846256\n",
      "          total_loss: 0.19687524879723786\n",
      "          vf_explained_var: 0.48229581117630005\n",
      "          vf_loss: 0.27089926779270174\n",
      "    num_agent_steps_sampled: 11188800\n",
      "    num_agent_steps_trained: 11188800\n",
      "    num_steps_sampled: 2797200\n",
      "    num_steps_trained: 2797200\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.75933333333333\n",
      "    gpu_util_percent0: 0.17686666666666667\n",
      "    ram_util_percent: 94.44933333333334\n",
      "    vram_util_percent0: 0.5143786333223648\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.021215876488715\n",
      "  policy_reward_mean:\n",
      "    main: 0.0733917598368575\n",
      "  policy_reward_min:\n",
      "    main: -1.5348570561939492\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30460945140175955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.956206015790976\n",
      "    mean_inference_ms: 4.433109891517081\n",
      "    mean_raw_obs_processing_ms: 1.3179374059284275\n",
      "  time_since_restore: 40185.216633319855\n",
      "  time_this_iter_s: 115.38134717941284\n",
      "  time_total_s: 40185.216633319855\n",
      "  timers:\n",
      "    learn_throughput: 89.601\n",
      "    learn_time_ms: 89195.215\n",
      "    sample_throughput: 303.657\n",
      "    sample_time_ms: 26319.132\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1639198422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2797200\n",
      "  training_iteration: 350\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         40185.2</td><td style=\"text-align: right;\">2797200</td><td style=\"text-align: right;\">0.293567</td><td style=\"text-align: right;\">             2.04348</td><td style=\"text-align: right;\">            -1.16569</td><td style=\"text-align: right;\">            67.314</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11220768\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8933564057627362\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04295263563656775\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47399934472299754\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 66.40495867768595\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2698867022442895\n",
      "  episode_reward_mean: 0.15099346894618768\n",
      "  episode_reward_min: -0.9469262302398926\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 40967\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6109227290153504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015187246955931186\n",
      "          policy_loss: -0.09679703615233302\n",
      "          total_loss: 0.19546432165428995\n",
      "          vf_explained_var: 0.481650710105896\n",
      "          vf_loss: 0.2691957269310951\n",
      "    num_agent_steps_sampled: 11220768\n",
      "    num_agent_steps_trained: 11220768\n",
      "    num_steps_sampled: 2805192\n",
      "    num_steps_trained: 2805192\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.27852348993289\n",
      "    gpu_util_percent0: 0.178993288590604\n",
      "    ram_util_percent: 94.58926174496644\n",
      "    vram_util_percent0: 0.5143845887136133\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8418986672825153\n",
      "  policy_reward_mean:\n",
      "    main: 0.03774836723654692\n",
      "  policy_reward_min:\n",
      "    main: -1.7941030327230403\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30486754215734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.966284369174343\n",
      "    mean_inference_ms: 4.438276892504116\n",
      "    mean_raw_obs_processing_ms: 1.3180753646868715\n",
      "  time_since_restore: 40300.875232458115\n",
      "  time_this_iter_s: 115.65859913825989\n",
      "  time_total_s: 40300.875232458115\n",
      "  timers:\n",
      "    learn_throughput: 89.531\n",
      "    learn_time_ms: 89265.348\n",
      "    sample_throughput: 303.916\n",
      "    sample_time_ms: 26296.71\n",
      "    update_time_ms: 3.213\n",
      "  timestamp: 1639198538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2805192\n",
      "  training_iteration: 351\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         40300.9</td><td style=\"text-align: right;\">2805192</td><td style=\"text-align: right;\">0.150993</td><td style=\"text-align: right;\">             1.26989</td><td style=\"text-align: right;\">           -0.946926</td><td style=\"text-align: right;\">            66.405</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11252736\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.3123967429387389\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04295094287194961\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4796522333593026\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 62.651162790697676\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0910477520296755\n",
      "  episode_reward_mean: 0.15442425380939967\n",
      "  episode_reward_min: -0.8911056760736846\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 41096\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6051414952278137\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01524898597598076\n",
      "          policy_loss: -0.09731935486942529\n",
      "          total_loss: 0.19386527305468917\n",
      "          vf_explained_var: 0.48047930002212524\n",
      "          vf_loss: 0.26802523076534274\n",
      "    num_agent_steps_sampled: 11252736\n",
      "    num_agent_steps_trained: 11252736\n",
      "    num_steps_sampled: 2813184\n",
      "    num_steps_trained: 2813184\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.77133333333333\n",
      "    gpu_util_percent0: 0.17820000000000003\n",
      "    ram_util_percent: 94.67399999999999\n",
      "    vram_util_percent0: 0.5152001754963255\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.181796767352801\n",
      "  policy_reward_mean:\n",
      "    main: 0.03860606345234991\n",
      "  policy_reward_min:\n",
      "    main: -1.5468820381038748\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3042477573053374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.951500726104083\n",
      "    mean_inference_ms: 4.432432270788607\n",
      "    mean_raw_obs_processing_ms: 1.317754952763577\n",
      "  time_since_restore: 40416.89861750603\n",
      "  time_this_iter_s: 116.0233850479126\n",
      "  time_total_s: 40416.89861750603\n",
      "  timers:\n",
      "    learn_throughput: 89.528\n",
      "    learn_time_ms: 89267.73\n",
      "    sample_throughput: 303.787\n",
      "    sample_time_ms: 26307.919\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1639198654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2813184\n",
      "  training_iteration: 352\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         40416.9</td><td style=\"text-align: right;\">2813184</td><td style=\"text-align: right;\">0.154424</td><td style=\"text-align: right;\">             2.09105</td><td style=\"text-align: right;\">           -0.891106</td><td style=\"text-align: right;\">           62.6512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11284704\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9797709788423491\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09311470250481237\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5437315263221159\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_01-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 67.99152542372882\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7681630081351676\n",
      "  episode_reward_mean: 0.24542403125302964\n",
      "  episode_reward_min: -1.1154178636844572\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 41214\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6024216417074203\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015061421893537045\n",
      "          policy_loss: -0.09603379861265421\n",
      "          total_loss: 0.18425208605080842\n",
      "          vf_explained_var: 0.4944670498371124\n",
      "          vf_loss: 0.25741135245561597\n",
      "    num_agent_steps_sampled: 11284704\n",
      "    num_agent_steps_trained: 11284704\n",
      "    num_steps_sampled: 2821176\n",
      "    num_steps_trained: 2821176\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.912\n",
      "    gpu_util_percent0: 0.18033333333333332\n",
      "    ram_util_percent: 94.55933333333334\n",
      "    vram_util_percent0: 0.5165865964681364\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8595403325923066\n",
      "  policy_reward_mean:\n",
      "    main: 0.06135600781325741\n",
      "  policy_reward_min:\n",
      "    main: -1.659567145050157\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3051460748930067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.97153250422681\n",
      "    mean_inference_ms: 4.439967619888193\n",
      "    mean_raw_obs_processing_ms: 1.3181690338851062\n",
      "  time_since_restore: 40533.110842227936\n",
      "  time_this_iter_s: 116.21222472190857\n",
      "  time_total_s: 40533.110842227936\n",
      "  timers:\n",
      "    learn_throughput: 89.441\n",
      "    learn_time_ms: 89354.751\n",
      "    sample_throughput: 303.32\n",
      "    sample_time_ms: 26348.412\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1639198771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2821176\n",
      "  training_iteration: 353\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         40533.1</td><td style=\"text-align: right;\">2821176</td><td style=\"text-align: right;\">0.245424</td><td style=\"text-align: right;\">             1.76816</td><td style=\"text-align: right;\">            -1.11542</td><td style=\"text-align: right;\">           67.9915</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11316672\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9090284928457413\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09373254707963755\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6493175096144819\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-01-41\n",
      "  done: false\n",
      "  episode_len_mean: 60.76190476190476\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7005087334509383\n",
      "  episode_reward_mean: 0.19340224333346054\n",
      "  episode_reward_min: -0.9281735524248573\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 41340\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6050345398187638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015491085100919007\n",
      "          policy_loss: -0.09719938503578306\n",
      "          total_loss: 0.19887222980335356\n",
      "          vf_explained_var: 0.4824655055999756\n",
      "          vf_loss: 0.27254453080892566\n",
      "    num_agent_steps_sampled: 11316672\n",
      "    num_agent_steps_trained: 11316672\n",
      "    num_steps_sampled: 2829168\n",
      "    num_steps_trained: 2829168\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.42650602409638\n",
      "    gpu_util_percent0: 0.21319277108433735\n",
      "    ram_util_percent: 95.52771084337348\n",
      "    vram_util_percent0: 0.5221616971340445\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9024570552144067\n",
      "  policy_reward_mean:\n",
      "    main: 0.048350560833365135\n",
      "  policy_reward_min:\n",
      "    main: -1.9274034719382116\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3048645551707022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.97051323678525\n",
      "    mean_inference_ms: 4.43765027585661\n",
      "    mean_raw_obs_processing_ms: 1.3184230043079928\n",
      "  time_since_restore: 40663.42391252518\n",
      "  time_this_iter_s: 130.3130702972412\n",
      "  time_total_s: 40663.42391252518\n",
      "  timers:\n",
      "    learn_throughput: 88.285\n",
      "    learn_time_ms: 90525.195\n",
      "    sample_throughput: 300.012\n",
      "    sample_time_ms: 26638.969\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1639198901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2829168\n",
      "  training_iteration: 354\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         40663.4</td><td style=\"text-align: right;\">2829168</td><td style=\"text-align: right;\">0.193402</td><td style=\"text-align: right;\">             1.70051</td><td style=\"text-align: right;\">           -0.928174</td><td style=\"text-align: right;\">           60.7619</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11348640\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1417768052510435\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08827932993861494\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5250392049447361\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-03-41\n",
      "  done: false\n",
      "  episode_len_mean: 57.572463768115945\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6847916633854982\n",
      "  episode_reward_mean: 0.1463275064549576\n",
      "  episode_reward_min: -0.86345661680879\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 41478\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6041019450426102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01520325030758977\n",
      "          policy_loss: -0.0969683614000678\n",
      "          total_loss: 0.17782056632637977\n",
      "          vf_explained_var: 0.54336017370224\n",
      "          vf_loss: 0.25169899308681487\n",
      "    num_agent_steps_sampled: 11348640\n",
      "    num_agent_steps_trained: 11348640\n",
      "    num_steps_sampled: 2837160\n",
      "    num_steps_trained: 2837160\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.335947712418296\n",
      "    gpu_util_percent0: 0.16098039215686277\n",
      "    ram_util_percent: 95.47712418300654\n",
      "    vram_util_percent0: 0.5237941617362091\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.071870065758532\n",
      "  policy_reward_mean:\n",
      "    main: 0.0365818766137394\n",
      "  policy_reward_min:\n",
      "    main: -1.756050353201719\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.305006745984202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.980551422565966\n",
      "    mean_inference_ms: 4.4396407179734325\n",
      "    mean_raw_obs_processing_ms: 1.3191452257475855\n",
      "  time_since_restore: 40783.00181603432\n",
      "  time_this_iter_s: 119.57790350914001\n",
      "  time_total_s: 40783.00181603432\n",
      "  timers:\n",
      "    learn_throughput: 88.203\n",
      "    learn_time_ms: 90609.064\n",
      "    sample_throughput: 296.283\n",
      "    sample_time_ms: 26974.173\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1639199021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2837160\n",
      "  training_iteration: 355\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">           40783</td><td style=\"text-align: right;\">2837160</td><td style=\"text-align: right;\">0.146328</td><td style=\"text-align: right;\">             1.68479</td><td style=\"text-align: right;\">           -0.863457</td><td style=\"text-align: right;\">           57.5725</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11380608\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9833956404091855\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07027617047259463\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6599216080567465\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 63.359375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4320581015764884\n",
      "  episode_reward_mean: 0.2193473566618028\n",
      "  episode_reward_min: -0.9732241534790322\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 41606\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.598079285621643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015148769438266754\n",
      "          policy_loss: -0.09517976276576519\n",
      "          total_loss: 0.20631957421451808\n",
      "          vf_explained_var: 0.4885523021221161\n",
      "          vf_loss: 0.27849214494228364\n",
      "    num_agent_steps_sampled: 11380608\n",
      "    num_agent_steps_trained: 11380608\n",
      "    num_steps_sampled: 2845152\n",
      "    num_steps_trained: 2845152\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7230303030303\n",
      "    gpu_util_percent0: 0.2113939393939394\n",
      "    ram_util_percent: 95.68909090909091\n",
      "    vram_util_percent0: 0.526851934946703\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.017919506203813\n",
      "  policy_reward_mean:\n",
      "    main: 0.054836839165450686\n",
      "  policy_reward_min:\n",
      "    main: -1.6715130873384783\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30484109620425026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.982185411955385\n",
      "    mean_inference_ms: 4.439240703415142\n",
      "    mean_raw_obs_processing_ms: 1.319340215262585\n",
      "  time_since_restore: 40912.40893602371\n",
      "  time_this_iter_s: 129.40711998939514\n",
      "  time_total_s: 40912.40893602371\n",
      "  timers:\n",
      "    learn_throughput: 87.229\n",
      "    learn_time_ms: 91621.283\n",
      "    sample_throughput: 294.591\n",
      "    sample_time_ms: 27129.124\n",
      "    update_time_ms: 3.328\n",
      "  timestamp: 1639199150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2845152\n",
      "  training_iteration: 356\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         40912.4</td><td style=\"text-align: right;\">2845152</td><td style=\"text-align: right;\">0.219347</td><td style=\"text-align: right;\">             1.43206</td><td style=\"text-align: right;\">           -0.973224</td><td style=\"text-align: right;\">           63.3594</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11412576\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9946551040016144\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.13287148630122453\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5421125312385419\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-07-59\n",
      "  done: false\n",
      "  episode_len_mean: 65.39495798319328\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4715271947789283\n",
      "  episode_reward_mean: 0.25504947178680365\n",
      "  episode_reward_min: -1.0177386606221304\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 41725\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5947714298963547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015289093039929867\n",
      "          policy_loss: -0.0967085779607296\n",
      "          total_loss: 0.21655826015770435\n",
      "          vf_explained_var: 0.44510921835899353\n",
      "          vf_loss: 0.290046529173851\n",
      "    num_agent_steps_sampled: 11412576\n",
      "    num_agent_steps_trained: 11412576\n",
      "    num_steps_sampled: 2853144\n",
      "    num_steps_trained: 2853144\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.58719512195122\n",
      "    gpu_util_percent0: 0.21591463414634143\n",
      "    ram_util_percent: 95.64268292682927\n",
      "    vram_util_percent0: 0.5262281398727117\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9709277954149962\n",
      "  policy_reward_mean:\n",
      "    main: 0.06376236794670091\n",
      "  policy_reward_min:\n",
      "    main: -1.5741258423759223\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3047890064672274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.989154320086726\n",
      "    mean_inference_ms: 4.43933043920951\n",
      "    mean_raw_obs_processing_ms: 1.3198210734714508\n",
      "  time_since_restore: 41041.26712369919\n",
      "  time_this_iter_s: 128.85818767547607\n",
      "  time_total_s: 41041.26712369919\n",
      "  timers:\n",
      "    learn_throughput: 86.448\n",
      "    learn_time_ms: 92448.156\n",
      "    sample_throughput: 290.39\n",
      "    sample_time_ms: 27521.565\n",
      "    update_time_ms: 3.384\n",
      "  timestamp: 1639199279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2853144\n",
      "  training_iteration: 357\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         41041.3</td><td style=\"text-align: right;\">2853144</td><td style=\"text-align: right;\">0.255049</td><td style=\"text-align: right;\">             1.47153</td><td style=\"text-align: right;\">            -1.01774</td><td style=\"text-align: right;\">            65.395</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11444544\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8583898725765525\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06387903194788047\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6137419037327977\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-10-08\n",
      "  done: false\n",
      "  episode_len_mean: 61.53623188405797\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2508879950192269\n",
      "  episode_reward_mean: 0.16753227171872564\n",
      "  episode_reward_min: -1.0463403212202658\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 41863\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5975227875709533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015248205687850713\n",
      "          policy_loss: -0.09644817067682743\n",
      "          total_loss: 0.22088984451442956\n",
      "          vf_explained_var: 0.48529472947120667\n",
      "          vf_loss: 0.2941798018813133\n",
      "    num_agent_steps_sampled: 11444544\n",
      "    num_agent_steps_trained: 11444544\n",
      "    num_steps_sampled: 2861136\n",
      "    num_steps_trained: 2861136\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.87212121212121\n",
      "    gpu_util_percent0: 0.2125454545454545\n",
      "    ram_util_percent: 95.69696969696969\n",
      "    vram_util_percent0: 0.5260462472703342\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7865327965054747\n",
      "  policy_reward_mean:\n",
      "    main: 0.041883067929681404\n",
      "  policy_reward_min:\n",
      "    main: -1.7011684125685171\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30538344929634703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.013958898333446\n",
      "    mean_inference_ms: 4.446065291209848\n",
      "    mean_raw_obs_processing_ms: 1.321072144195822\n",
      "  time_since_restore: 41170.42616677284\n",
      "  time_this_iter_s: 129.15904307365417\n",
      "  time_total_s: 41170.42616677284\n",
      "  timers:\n",
      "    learn_throughput: 85.661\n",
      "    learn_time_ms: 93297.542\n",
      "    sample_throughput: 285.083\n",
      "    sample_time_ms: 28033.901\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1639199408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2861136\n",
      "  training_iteration: 358\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         41170.4</td><td style=\"text-align: right;\">2861136</td><td style=\"text-align: right;\">0.167532</td><td style=\"text-align: right;\">             1.25089</td><td style=\"text-align: right;\">            -1.04634</td><td style=\"text-align: right;\">           61.5362</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11476512\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8412439077950478\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03102725095965949\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6697514908679637\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 65.63865546218487\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5358513916045093\n",
      "  episode_reward_mean: 0.10451131987732167\n",
      "  episode_reward_min: -1.1802023730652378\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 41982\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5977951563596725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015460758935660124\n",
      "          policy_loss: -0.09735540122538805\n",
      "          total_loss: 0.22086380786448717\n",
      "          vf_explained_var: 0.45138341188430786\n",
      "          vf_loss: 0.29473818045854566\n",
      "    num_agent_steps_sampled: 11476512\n",
      "    num_agent_steps_trained: 11476512\n",
      "    num_steps_sampled: 2869128\n",
      "    num_steps_trained: 2869128\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.76134969325154\n",
      "    gpu_util_percent0: 0.21190184049079755\n",
      "    ram_util_percent: 95.69999999999999\n",
      "    vram_util_percent0: 0.5248346142277185\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.069353795273517\n",
      "  policy_reward_mean:\n",
      "    main: 0.026127829969330406\n",
      "  policy_reward_min:\n",
      "    main: -1.6788759709840004\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30541227474456917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.023327129995252\n",
      "    mean_inference_ms: 4.447651902731749\n",
      "    mean_raw_obs_processing_ms: 1.3215129548225164\n",
      "  time_since_restore: 41297.72326827049\n",
      "  time_this_iter_s: 127.29710149765015\n",
      "  time_total_s: 41297.72326827049\n",
      "  timers:\n",
      "    learn_throughput: 85.01\n",
      "    learn_time_ms: 94012.078\n",
      "    sample_throughput: 281.521\n",
      "    sample_time_ms: 28388.675\n",
      "    update_time_ms: 3.42\n",
      "  timestamp: 1639199536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2869128\n",
      "  training_iteration: 359\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         41297.7</td><td style=\"text-align: right;\">2869128</td><td style=\"text-align: right;\">0.104511</td><td style=\"text-align: right;\">             1.53585</td><td style=\"text-align: right;\">             -1.1802</td><td style=\"text-align: right;\">           65.6387</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11508480\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8498326678913407\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05503760545607696\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5214307685296836\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 64.9920634920635\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.587183284391611\n",
      "  episode_reward_mean: 0.22149663271366044\n",
      "  episode_reward_min: -0.8343752100326411\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 42108\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.599666476726532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015054685089737178\n",
      "          policy_loss: -0.0966437492519617\n",
      "          total_loss: 0.19156614432483912\n",
      "          vf_explained_var: 0.4898937940597534\n",
      "          vf_loss: 0.2653455898761749\n",
      "    num_agent_steps_sampled: 11508480\n",
      "    num_agent_steps_trained: 11508480\n",
      "    num_steps_sampled: 2877120\n",
      "    num_steps_trained: 2877120\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.203105590062115\n",
      "    gpu_util_percent0: 0.20527950310559004\n",
      "    ram_util_percent: 95.01739130434783\n",
      "    vram_util_percent0: 0.5212731386386908\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.981950552969256\n",
      "  policy_reward_mean:\n",
      "    main: 0.05537415817841511\n",
      "  policy_reward_min:\n",
      "    main: -1.7152962884494916\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30520162640291826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.02298896175287\n",
      "    mean_inference_ms: 4.445819238015771\n",
      "    mean_raw_obs_processing_ms: 1.3217213581503673\n",
      "  time_since_restore: 41424.80061459541\n",
      "  time_this_iter_s: 127.07734632492065\n",
      "  time_total_s: 41424.80061459541\n",
      "  timers:\n",
      "    learn_throughput: 84.251\n",
      "    learn_time_ms: 94859.044\n",
      "    sample_throughput: 278.32\n",
      "    sample_time_ms: 28715.124\n",
      "    update_time_ms: 3.412\n",
      "  timestamp: 1639199663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2877120\n",
      "  training_iteration: 360\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         41424.8</td><td style=\"text-align: right;\">2877120</td><td style=\"text-align: right;\">0.221497</td><td style=\"text-align: right;\">             1.58718</td><td style=\"text-align: right;\">           -0.834375</td><td style=\"text-align: right;\">           64.9921</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11540448\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8282101679560361\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.058383639053859994\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.43114760083727566\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 64.05833333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6031430611474327\n",
      "  episode_reward_mean: 0.16867421713097103\n",
      "  episode_reward_min: -0.7271426014028255\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 42228\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6044450387954712\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015202340219169856\n",
      "          policy_loss: -0.09670319773070514\n",
      "          total_loss: 0.21116518047451974\n",
      "          vf_explained_var: 0.4529077708721161\n",
      "          vf_loss: 0.2847798257470131\n",
      "    num_agent_steps_sampled: 11540448\n",
      "    num_agent_steps_trained: 11540448\n",
      "    num_steps_sampled: 2885112\n",
      "    num_steps_trained: 2885112\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.91161290322581\n",
      "    gpu_util_percent0: 0.15954838709677419\n",
      "    ram_util_percent: 94.72903225806452\n",
      "    vram_util_percent0: 0.52420257087964\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9312109202417003\n",
      "  policy_reward_mean:\n",
      "    main: 0.04216855428274277\n",
      "  policy_reward_min:\n",
      "    main: -1.6039883704248958\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3056202576297314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.042561422178824\n",
      "    mean_inference_ms: 4.451009578512892\n",
      "    mean_raw_obs_processing_ms: 1.322381252196841\n",
      "  time_since_restore: 41545.24737691879\n",
      "  time_this_iter_s: 120.44676232337952\n",
      "  time_total_s: 41545.24737691879\n",
      "  timers:\n",
      "    learn_throughput: 84.178\n",
      "    learn_time_ms: 94941.998\n",
      "    sample_throughput: 274.569\n",
      "    sample_time_ms: 29107.396\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1639199783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2885112\n",
      "  training_iteration: 361\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         41545.2</td><td style=\"text-align: right;\">2885112</td><td style=\"text-align: right;\">0.168674</td><td style=\"text-align: right;\">             1.60314</td><td style=\"text-align: right;\">           -0.727143</td><td style=\"text-align: right;\">           64.0583</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11572416\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9227841749643493\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0853493381458455\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4865132477012601\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-18-22\n",
      "  done: false\n",
      "  episode_len_mean: 71.25233644859813\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2915931187402379\n",
      "  episode_reward_mean: 0.19141879461233885\n",
      "  episode_reward_min: -0.8894253035764997\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 42335\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6025840833187103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015652741204947234\n",
      "          policy_loss: -0.09708200279995799\n",
      "          total_loss: 0.1862894544750452\n",
      "          vf_explained_var: 0.44265031814575195\n",
      "          vf_loss: 0.2595988575220108\n",
      "    num_agent_steps_sampled: 11572416\n",
      "    num_agent_steps_trained: 11572416\n",
      "    num_steps_sampled: 2893104\n",
      "    num_steps_trained: 2893104\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.32549019607843\n",
      "    gpu_util_percent0: 0.14359477124183007\n",
      "    ram_util_percent: 94.52483660130719\n",
      "    vram_util_percent0: 0.5250899526202935\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8573570310140337\n",
      "  policy_reward_mean:\n",
      "    main: 0.04785469865308471\n",
      "  policy_reward_min:\n",
      "    main: -1.5636484260178616\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3056019868915604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.04374343502302\n",
      "    mean_inference_ms: 4.450408737340615\n",
      "    mean_raw_obs_processing_ms: 1.3224786861084563\n",
      "  time_since_restore: 41663.428793907166\n",
      "  time_this_iter_s: 118.1814169883728\n",
      "  time_total_s: 41663.428793907166\n",
      "  timers:\n",
      "    learn_throughput: 84.086\n",
      "    learn_time_ms: 95045.0\n",
      "    sample_throughput: 273.5\n",
      "    sample_time_ms: 29221.163\n",
      "    update_time_ms: 3.538\n",
      "  timestamp: 1639199902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2893104\n",
      "  training_iteration: 362\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         41663.4</td><td style=\"text-align: right;\">2893104</td><td style=\"text-align: right;\">0.191419</td><td style=\"text-align: right;\">             1.29159</td><td style=\"text-align: right;\">           -0.889425</td><td style=\"text-align: right;\">           71.2523</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11604384\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.5353519665957254\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08342752162317325\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5314912012271967\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 67.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0800073830087964\n",
      "  episode_reward_mean: 0.22977899215047806\n",
      "  episode_reward_min: -1.376346460703345\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 42460\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5939208394289017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015465454000979663\n",
      "          policy_loss: -0.09794366866722704\n",
      "          total_loss: 0.1967249877937138\n",
      "          vf_explained_var: 0.48422771692276\n",
      "          vf_loss: 0.2711805002093315\n",
      "    num_agent_steps_sampled: 11604384\n",
      "    num_agent_steps_trained: 11604384\n",
      "    num_steps_sampled: 2901096\n",
      "    num_steps_trained: 2901096\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.31180124223602\n",
      "    gpu_util_percent0: 0.1898136645962733\n",
      "    ram_util_percent: 94.67826086956521\n",
      "    vram_util_percent0: 0.5250143578612612\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.397351955151634\n",
      "  policy_reward_mean:\n",
      "    main: 0.05744474803761951\n",
      "  policy_reward_min:\n",
      "    main: -1.5718542577115189\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30572152141799697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.050697042032006\n",
      "    mean_inference_ms: 4.452622933531129\n",
      "    mean_raw_obs_processing_ms: 1.3227832782150128\n",
      "  time_since_restore: 41789.66320824623\n",
      "  time_this_iter_s: 126.23441433906555\n",
      "  time_total_s: 41789.66320824623\n",
      "  timers:\n",
      "    learn_throughput: 83.302\n",
      "    learn_time_ms: 95939.961\n",
      "    sample_throughput: 272.424\n",
      "    sample_time_ms: 29336.625\n",
      "    update_time_ms: 3.535\n",
      "  timestamp: 1639200028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2901096\n",
      "  training_iteration: 363\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         41789.7</td><td style=\"text-align: right;\">2901096</td><td style=\"text-align: right;\">0.229779</td><td style=\"text-align: right;\">             2.08001</td><td style=\"text-align: right;\">            -1.37635</td><td style=\"text-align: right;\">              67.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11636352\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9344977564237367\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05045248779622813\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5343014989432104\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-22-33\n",
      "  done: false\n",
      "  episode_len_mean: 66.16666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4075747487337924\n",
      "  episode_reward_mean: 0.14400746827153757\n",
      "  episode_reward_min: -1.4473433186792244\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 42580\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5974507921934128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01512738062813878\n",
      "          policy_loss: -0.09504695576056837\n",
      "          total_loss: 0.18859922805428506\n",
      "          vf_explained_var: 0.4793964922428131\n",
      "          vf_loss: 0.26067147505283356\n",
      "    num_agent_steps_sampled: 11636352\n",
      "    num_agent_steps_trained: 11636352\n",
      "    num_steps_sampled: 2909088\n",
      "    num_steps_trained: 2909088\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.400000000000006\n",
      "    gpu_util_percent0: 0.2015723270440252\n",
      "    ram_util_percent: 94.59874213836478\n",
      "    vram_util_percent0: 0.5225661784640347\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8319440736843302\n",
      "  policy_reward_mean:\n",
      "    main: 0.03600186706788438\n",
      "  policy_reward_min:\n",
      "    main: -1.5343014989432104\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3055000714464698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.05181087663578\n",
      "    mean_inference_ms: 4.451064719965639\n",
      "    mean_raw_obs_processing_ms: 1.323171962373365\n",
      "  time_since_restore: 41914.72755289078\n",
      "  time_this_iter_s: 125.06434464454651\n",
      "  time_total_s: 41914.72755289078\n",
      "  timers:\n",
      "    learn_throughput: 83.763\n",
      "    learn_time_ms: 95412.279\n",
      "    sample_throughput: 272.333\n",
      "    sample_time_ms: 29346.434\n",
      "    update_time_ms: 3.506\n",
      "  timestamp: 1639200153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2909088\n",
      "  training_iteration: 364\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         41914.7</td><td style=\"text-align: right;\">2909088</td><td style=\"text-align: right;\">0.144007</td><td style=\"text-align: right;\">             1.40757</td><td style=\"text-align: right;\">            -1.44734</td><td style=\"text-align: right;\">           66.1667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11668320\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9436376015715582\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05604960089849741\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5041500047318722\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 63.65384615384615\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6174206387492096\n",
      "  episode_reward_mean: 0.24769790180078677\n",
      "  episode_reward_min: -1.032106283195155\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 42710\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5937744137048722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01563248571753502\n",
      "          policy_loss: -0.09697001520730555\n",
      "          total_loss: 0.22767053004354237\n",
      "          vf_explained_var: 0.46231743693351746\n",
      "          vf_loss: 0.30089870566129684\n",
      "    num_agent_steps_sampled: 11668320\n",
      "    num_agent_steps_trained: 11668320\n",
      "    num_steps_sampled: 2917080\n",
      "    num_steps_trained: 2917080\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.07055214723926\n",
      "    gpu_util_percent0: 0.20343558282208588\n",
      "    ram_util_percent: 94.78711656441719\n",
      "    vram_util_percent0: 0.520411541575066\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.330197671532919\n",
      "  policy_reward_mean:\n",
      "    main: 0.06192447545019668\n",
      "  policy_reward_min:\n",
      "    main: -1.797104816197027\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30571637437793414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.06501566290642\n",
      "    mean_inference_ms: 4.454719875270906\n",
      "    mean_raw_obs_processing_ms: 1.3236489277522334\n",
      "  time_since_restore: 42041.70475268364\n",
      "  time_this_iter_s: 126.97719979286194\n",
      "  time_total_s: 42041.70475268364\n",
      "  timers:\n",
      "    learn_throughput: 83.055\n",
      "    learn_time_ms: 96224.926\n",
      "    sample_throughput: 272.954\n",
      "    sample_time_ms: 29279.653\n",
      "    update_time_ms: 3.449\n",
      "  timestamp: 1639200280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2917080\n",
      "  training_iteration: 365\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         42041.7</td><td style=\"text-align: right;\">2917080</td><td style=\"text-align: right;\">0.247698</td><td style=\"text-align: right;\">             1.61742</td><td style=\"text-align: right;\">            -1.03211</td><td style=\"text-align: right;\">           63.6538</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11700288\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9110840927974329\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06713375477400263\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5780770633963273\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 63.37903225806452\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5648714749215853\n",
      "  episode_reward_mean: 0.18160156755053\n",
      "  episode_reward_min: -1.3033669481428036\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 42834\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6018578159809113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01554511161148548\n",
      "          policy_loss: -0.09885426016151905\n",
      "          total_loss: 0.2354324796050787\n",
      "          vf_explained_var: 0.4362734258174896\n",
      "          vf_loss: 0.31067760175466536\n",
      "    num_agent_steps_sampled: 11700288\n",
      "    num_agent_steps_trained: 11700288\n",
      "    num_steps_sampled: 2925072\n",
      "    num_steps_trained: 2925072\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.452597402597405\n",
      "    gpu_util_percent0: 0.1711038961038961\n",
      "    ram_util_percent: 94.87597402597402\n",
      "    vram_util_percent0: 0.5214986560001367\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9421988452812604\n",
      "  policy_reward_mean:\n",
      "    main: 0.04540039188763251\n",
      "  policy_reward_min:\n",
      "    main: -1.732698811019827\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30568041821796477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.070032024914717\n",
      "    mean_inference_ms: 4.454155587474056\n",
      "    mean_raw_obs_processing_ms: 1.3241137906899836\n",
      "  time_since_restore: 42164.33984541893\n",
      "  time_this_iter_s: 122.63509273529053\n",
      "  time_total_s: 42164.33984541893\n",
      "  timers:\n",
      "    learn_throughput: 83.743\n",
      "    learn_time_ms: 95435.38\n",
      "    sample_throughput: 271.812\n",
      "    sample_time_ms: 29402.71\n",
      "    update_time_ms: 3.319\n",
      "  timestamp: 1639200403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2925072\n",
      "  training_iteration: 366\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         42164.3</td><td style=\"text-align: right;\">2925072</td><td style=\"text-align: right;\">0.181602</td><td style=\"text-align: right;\">             1.56487</td><td style=\"text-align: right;\">            -1.30337</td><td style=\"text-align: right;\">            63.379</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11732256\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.6159515687999289\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09181400138198642\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5633025916076921\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-28-45\n",
      "  done: false\n",
      "  episode_len_mean: 68.78947368421052\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9696195050257534\n",
      "  episode_reward_mean: 0.18107902155622196\n",
      "  episode_reward_min: -1.3086538070413236\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 42948\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6037188150882721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01538612238317728\n",
      "          policy_loss: -0.09824320536851883\n",
      "          total_loss: 0.17490609358251094\n",
      "          vf_explained_var: 0.49624553322792053\n",
      "          vf_loss: 0.24978162401914597\n",
      "    num_agent_steps_sampled: 11732256\n",
      "    num_agent_steps_trained: 11732256\n",
      "    num_steps_sampled: 2933064\n",
      "    num_steps_trained: 2933064\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.97948717948718\n",
      "    gpu_util_percent0: 0.17205128205128206\n",
      "    ram_util_percent: 94.72948717948718\n",
      "    vram_util_percent0: 0.5218653234447904\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.5013790031718175\n",
      "  policy_reward_mean:\n",
      "    main: 0.045269755389055505\n",
      "  policy_reward_min:\n",
      "    main: -1.5660896503620458\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30601079405166925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.079519126572002\n",
      "    mean_inference_ms: 4.457581229184289\n",
      "    mean_raw_obs_processing_ms: 1.324407768760557\n",
      "  time_since_restore: 42285.99796295166\n",
      "  time_this_iter_s: 121.6581175327301\n",
      "  time_total_s: 42285.99796295166\n",
      "  timers:\n",
      "    learn_throughput: 84.091\n",
      "    learn_time_ms: 95040.399\n",
      "    sample_throughput: 274.82\n",
      "    sample_time_ms: 29080.897\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1639200525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2933064\n",
      "  training_iteration: 367\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">           42286</td><td style=\"text-align: right;\">2933064</td><td style=\"text-align: right;\">0.181079</td><td style=\"text-align: right;\">             1.96962</td><td style=\"text-align: right;\">            -1.30865</td><td style=\"text-align: right;\">           68.7895</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11764224\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8351128530331011\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09011303968551508\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5697739355033655\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 63.80487804878049\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1236753430889264\n",
      "  episode_reward_mean: 0.16073978534577377\n",
      "  episode_reward_min: -1.0459257155418495\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 43071\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5949693827629089\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014964920613914728\n",
      "          policy_loss: -0.0952299209330231\n",
      "          total_loss: 0.21356843752786517\n",
      "          vf_explained_var: 0.44926291704177856\n",
      "          vf_loss: 0.28607038646936417\n",
      "    num_agent_steps_sampled: 11764224\n",
      "    num_agent_steps_trained: 11764224\n",
      "    num_steps_sampled: 2941056\n",
      "    num_steps_trained: 2941056\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.34085365853658\n",
      "    gpu_util_percent0: 0.20792682926829265\n",
      "    ram_util_percent: 94.92926829268292\n",
      "    vram_util_percent0: 0.5233539193733497\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7783969332657272\n",
      "  policy_reward_mean:\n",
      "    main: 0.04018494633644342\n",
      "  policy_reward_min:\n",
      "    main: -1.707395044259563\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30622630453335414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.08924995579919\n",
      "    mean_inference_ms: 4.458856523012335\n",
      "    mean_raw_obs_processing_ms: 1.325011248172708\n",
      "  time_since_restore: 42413.737308740616\n",
      "  time_this_iter_s: 127.73934578895569\n",
      "  time_total_s: 42413.737308740616\n",
      "  timers:\n",
      "    learn_throughput: 84.055\n",
      "    learn_time_ms: 95080.873\n",
      "    sample_throughput: 276.536\n",
      "    sample_time_ms: 28900.441\n",
      "    update_time_ms: 3.106\n",
      "  timestamp: 1639200652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2941056\n",
      "  training_iteration: 368\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         42413.7</td><td style=\"text-align: right;\">2941056</td><td style=\"text-align: right;\"> 0.16074</td><td style=\"text-align: right;\">             1.12368</td><td style=\"text-align: right;\">            -1.04593</td><td style=\"text-align: right;\">           63.8049</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11796192\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.092624380780552\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07105126157616555\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48841514323550556\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 65.24409448818898\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.5591442194921754\n",
      "  episode_reward_mean: 0.25704144082523106\n",
      "  episode_reward_min: -0.8373800039745722\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 43198\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.585382976770401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0156425467915833\n",
      "          policy_loss: -0.09872640982642769\n",
      "          total_loss: 0.20946355598792435\n",
      "          vf_explained_var: 0.4899313151836395\n",
      "          vf_loss: 0.28443284672498703\n",
      "    num_agent_steps_sampled: 11796192\n",
      "    num_agent_steps_trained: 11796192\n",
      "    num_steps_sampled: 2949048\n",
      "    num_steps_trained: 2949048\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.068098159509205\n",
      "    gpu_util_percent0: 0.19509202453987734\n",
      "    ram_util_percent: 95.18895705521471\n",
      "    vram_util_percent0: 0.52319337366788\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0713682901096435\n",
      "  policy_reward_mean:\n",
      "    main: 0.06426036020630775\n",
      "  policy_reward_min:\n",
      "    main: -1.6159266156516188\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3063647709112758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.10125320060149\n",
      "    mean_inference_ms: 4.461840850559953\n",
      "    mean_raw_obs_processing_ms: 1.3256719790671745\n",
      "  time_since_restore: 42540.9645178318\n",
      "  time_this_iter_s: 127.22720909118652\n",
      "  time_total_s: 42540.9645178318\n",
      "  timers:\n",
      "    learn_throughput: 84.051\n",
      "    learn_time_ms: 95084.744\n",
      "    sample_throughput: 276.596\n",
      "    sample_time_ms: 28894.092\n",
      "    update_time_ms: 2.954\n",
      "  timestamp: 1639200780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2949048\n",
      "  training_iteration: 369\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">           42541</td><td style=\"text-align: right;\">2949048</td><td style=\"text-align: right;\">0.257041</td><td style=\"text-align: right;\">             2.55914</td><td style=\"text-align: right;\">            -0.83738</td><td style=\"text-align: right;\">           65.2441</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11828160\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8526647846351637\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05373671040207177\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6252873291526687\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-34-59\n",
      "  done: false\n",
      "  episode_len_mean: 63.04838709677419\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9233153991985712\n",
      "  episode_reward_mean: 0.17234923364742089\n",
      "  episode_reward_min: -1.2216396743696007\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 43322\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5985433623790741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015450489815324545\n",
      "          policy_loss: -0.09705068677663803\n",
      "          total_loss: 0.20613232054561376\n",
      "          vf_explained_var: 0.4784771203994751\n",
      "          vf_loss: 0.2797175769209862\n",
      "    num_agent_steps_sampled: 11828160\n",
      "    num_agent_steps_trained: 11828160\n",
      "    num_steps_sampled: 2957040\n",
      "    num_steps_trained: 2957040\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.88506493506494\n",
      "    gpu_util_percent0: 0.18064935064935067\n",
      "    ram_util_percent: 94.43441558441559\n",
      "    vram_util_percent0: 0.5236150818579248\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8612976249756548\n",
      "  policy_reward_mean:\n",
      "    main: 0.04308730841185523\n",
      "  policy_reward_min:\n",
      "    main: -1.8852190912563682\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3063527408933042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.104135185940454\n",
      "    mean_inference_ms: 4.462542922530714\n",
      "    mean_raw_obs_processing_ms: 1.3258930293307956\n",
      "  time_since_restore: 42659.8947019577\n",
      "  time_this_iter_s: 118.93018412590027\n",
      "  time_total_s: 42659.8947019577\n",
      "  timers:\n",
      "    learn_throughput: 84.631\n",
      "    learn_time_ms: 94433.151\n",
      "    sample_throughput: 278.201\n",
      "    sample_time_ms: 28727.413\n",
      "    update_time_ms: 2.944\n",
      "  timestamp: 1639200899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2957040\n",
      "  training_iteration: 370\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         42659.9</td><td style=\"text-align: right;\">2957040</td><td style=\"text-align: right;\">0.172349</td><td style=\"text-align: right;\">             1.92332</td><td style=\"text-align: right;\">            -1.22164</td><td style=\"text-align: right;\">           63.0484</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11860128\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9422092530159815\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06909247379979375\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.507148925903228\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 60.53488372093023\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7167718737933413\n",
      "  episode_reward_mean: 0.20704620985167135\n",
      "  episode_reward_min: -0.9998796880381153\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 43451\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5974526815414428\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015508430376648903\n",
      "          policy_loss: -0.09832998107373714\n",
      "          total_loss: 0.21668099668622018\n",
      "          vf_explained_var: 0.45231103897094727\n",
      "          vf_loss: 0.2914575515985489\n",
      "    num_agent_steps_sampled: 11860128\n",
      "    num_agent_steps_trained: 11860128\n",
      "    num_steps_sampled: 2965032\n",
      "    num_steps_trained: 2965032\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.294666666666664\n",
      "    gpu_util_percent0: 0.1814\n",
      "    ram_util_percent: 93.91266666666668\n",
      "    vram_util_percent0: 0.5234145003838981\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8872776063021184\n",
      "  policy_reward_mean:\n",
      "    main: 0.051761552462917845\n",
      "  policy_reward_min:\n",
      "    main: -1.8626388491200028\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30652915317762935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.104600334728712\n",
      "    mean_inference_ms: 4.462524205834551\n",
      "    mean_raw_obs_processing_ms: 1.3260977080567893\n",
      "  time_since_restore: 42775.83738517761\n",
      "  time_this_iter_s: 115.94268321990967\n",
      "  time_total_s: 42775.83738517761\n",
      "  timers:\n",
      "    learn_throughput: 84.722\n",
      "    learn_time_ms: 94332.399\n",
      "    sample_throughput: 281.577\n",
      "    sample_time_ms: 28382.967\n",
      "    update_time_ms: 2.791\n",
      "  timestamp: 1639201015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2965032\n",
      "  training_iteration: 371\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         42775.8</td><td style=\"text-align: right;\">2965032</td><td style=\"text-align: right;\">0.207046</td><td style=\"text-align: right;\">             1.71677</td><td style=\"text-align: right;\">            -0.99988</td><td style=\"text-align: right;\">           60.5349</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11892096\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8686839403700796\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04998349947960845\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5393437113059093\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-38-50\n",
      "  done: false\n",
      "  episode_len_mean: 66.2741935483871\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3417179407245294\n",
      "  episode_reward_mean: 0.19448641739461667\n",
      "  episode_reward_min: -1.5099799588630611\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 43575\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.594753647685051\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015418463077396154\n",
      "          policy_loss: -0.09809651265293359\n",
      "          total_loss: 0.20581894529610872\n",
      "          vf_explained_var: 0.464135080575943\n",
      "          vf_loss: 0.28049866724014283\n",
      "    num_agent_steps_sampled: 11892096\n",
      "    num_agent_steps_trained: 11892096\n",
      "    num_steps_sampled: 2973024\n",
      "    num_steps_trained: 2973024\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.387248322147656\n",
      "    gpu_util_percent0: 0.18234899328859058\n",
      "    ram_util_percent: 94.01275167785234\n",
      "    vram_util_percent0: 0.5222134621287909\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8375857307220318\n",
      "  policy_reward_mean:\n",
      "    main: 0.048621604348654174\n",
      "  policy_reward_min:\n",
      "    main: -1.7346067341351512\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3060835551636432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.09678540303932\n",
      "    mean_inference_ms: 4.460017179695001\n",
      "    mean_raw_obs_processing_ms: 1.3258545346966768\n",
      "  time_since_restore: 42891.41857838631\n",
      "  time_this_iter_s: 115.58119320869446\n",
      "  time_total_s: 42891.41857838631\n",
      "  timers:\n",
      "    learn_throughput: 84.835\n",
      "    learn_time_ms: 94206.035\n",
      "    sample_throughput: 282.856\n",
      "    sample_time_ms: 28254.637\n",
      "    update_time_ms: 2.782\n",
      "  timestamp: 1639201130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2973024\n",
      "  training_iteration: 372\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         42891.4</td><td style=\"text-align: right;\">2973024</td><td style=\"text-align: right;\">0.194486</td><td style=\"text-align: right;\">             1.34172</td><td style=\"text-align: right;\">            -1.50998</td><td style=\"text-align: right;\">           66.2742</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11924064\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7178195103179922\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08353864919917377\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.559238132734965\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 64.73333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5472664855209581\n",
      "  episode_reward_mean: 0.17852699669917918\n",
      "  episode_reward_min: -0.8313981327670172\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 43695\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5913634233474732\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015310622744262219\n",
      "          policy_loss: -0.09699168958887458\n",
      "          total_loss: 0.196570438914001\n",
      "          vf_explained_var: 0.46102067828178406\n",
      "          vf_loss: 0.27030912083387376\n",
      "    num_agent_steps_sampled: 11924064\n",
      "    num_agent_steps_trained: 11924064\n",
      "    num_steps_sampled: 2981016\n",
      "    num_steps_trained: 2981016\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.551315789473676\n",
      "    gpu_util_percent0: 0.1819078947368421\n",
      "    ram_util_percent: 94.15328947368421\n",
      "    vram_util_percent0: 0.5195084515335724\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9596977009997885\n",
      "  policy_reward_mean:\n",
      "    main: 0.04463174917479481\n",
      "  policy_reward_min:\n",
      "    main: -1.6684360777982676\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3063488813448355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.1035117164046\n",
      "    mean_inference_ms: 4.462585387417651\n",
      "    mean_raw_obs_processing_ms: 1.3260825470514177\n",
      "  time_since_restore: 43008.862105846405\n",
      "  time_this_iter_s: 117.44352746009827\n",
      "  time_total_s: 43008.862105846405\n",
      "  timers:\n",
      "    learn_throughput: 85.583\n",
      "    learn_time_ms: 93383.52\n",
      "    sample_throughput: 283.409\n",
      "    sample_time_ms: 28199.576\n",
      "    update_time_ms: 2.814\n",
      "  timestamp: 1639201248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2981016\n",
      "  training_iteration: 373\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         43008.9</td><td style=\"text-align: right;\">2981016</td><td style=\"text-align: right;\">0.178527</td><td style=\"text-align: right;\">             1.54727</td><td style=\"text-align: right;\">           -0.831398</td><td style=\"text-align: right;\">           64.7333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11956032\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9549316036851022\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.018249967002112674\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7945101106879469\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-42-44\n",
      "  done: false\n",
      "  episode_len_mean: 66.28688524590164\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3319034443188924\n",
      "  episode_reward_mean: 0.09064684207369357\n",
      "  episode_reward_min: -2.0354467349400944\n",
      "  episodes_this_iter: 122\n",
      "  episodes_total: 43817\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5986401250362396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015567532438784839\n",
      "          policy_loss: -0.0984013820886612\n",
      "          total_loss: 0.19038428957760334\n",
      "          vf_explained_var: 0.47110244631767273\n",
      "          vf_loss: 0.2651424831748009\n",
      "    num_agent_steps_sampled: 11956032\n",
      "    num_agent_steps_trained: 11956032\n",
      "    num_steps_sampled: 2989008\n",
      "    num_steps_trained: 2989008\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.12317880794701\n",
      "    gpu_util_percent0: 0.1810596026490066\n",
      "    ram_util_percent: 94.14635761589403\n",
      "    vram_util_percent0: 0.5181983006783776\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9096831541517183\n",
      "  policy_reward_mean:\n",
      "    main: 0.022661710518423403\n",
      "  policy_reward_min:\n",
      "    main: -1.6003924483962282\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30572612536307464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.087309863986302\n",
      "    mean_inference_ms: 4.456476167400624\n",
      "    mean_raw_obs_processing_ms: 1.3257343821780236\n",
      "  time_since_restore: 43125.07440042496\n",
      "  time_this_iter_s: 116.21229457855225\n",
      "  time_total_s: 43125.07440042496\n",
      "  timers:\n",
      "    learn_throughput: 86.093\n",
      "    learn_time_ms: 92830.023\n",
      "    sample_throughput: 286.838\n",
      "    sample_time_ms: 27862.394\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1639201364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2989008\n",
      "  training_iteration: 374\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         43125.1</td><td style=\"text-align: right;\">2989008</td><td style=\"text-align: right;\">0.0906468</td><td style=\"text-align: right;\">              1.3319</td><td style=\"text-align: right;\">            -2.03545</td><td style=\"text-align: right;\">           66.2869</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 11988000\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0255020365529004\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.041827296638479915\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5793420087520877\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-44-41\n",
      "  done: false\n",
      "  episode_len_mean: 56.787234042553195\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5453507067882528\n",
      "  episode_reward_mean: 0.12507882906648277\n",
      "  episode_reward_min: -1.4071125025992353\n",
      "  episodes_this_iter: 141\n",
      "  episodes_total: 43958\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5971491260528564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015531853701919318\n",
      "          policy_loss: -0.09818898491933942\n",
      "          total_loss: 0.22719139649719\n",
      "          vf_explained_var: 0.4760538935661316\n",
      "          vf_loss: 0.3017913790345192\n",
      "    num_agent_steps_sampled: 11988000\n",
      "    num_agent_steps_trained: 11988000\n",
      "    num_steps_sampled: 2997000\n",
      "    num_steps_trained: 2997000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.372185430463574\n",
      "    gpu_util_percent0: 0.18\n",
      "    ram_util_percent: 94.31059602649007\n",
      "    vram_util_percent0: 0.5125531446602557\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9297972762795776\n",
      "  policy_reward_mean:\n",
      "    main: 0.03126970726662069\n",
      "  policy_reward_min:\n",
      "    main: -1.5858127001400422\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3061478710026909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.100848346504918\n",
      "    mean_inference_ms: 4.462172718617269\n",
      "    mean_raw_obs_processing_ms: 1.3262096961242476\n",
      "  time_since_restore: 43241.561047554016\n",
      "  time_this_iter_s: 116.48664712905884\n",
      "  time_total_s: 43241.561047554016\n",
      "  timers:\n",
      "    learn_throughput: 86.873\n",
      "    learn_time_ms: 91996.033\n",
      "    sample_throughput: 289.129\n",
      "    sample_time_ms: 27641.644\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1639201481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2997000\n",
      "  training_iteration: 375\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         43241.6</td><td style=\"text-align: right;\">2997000</td><td style=\"text-align: right;\">0.125079</td><td style=\"text-align: right;\">             1.54535</td><td style=\"text-align: right;\">            -1.40711</td><td style=\"text-align: right;\">           56.7872</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12019968\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.2297840034225098\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.034848149182403726\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5526377264122591\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-46-39\n",
      "  done: false\n",
      "  episode_len_mean: 67.3821138211382\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.3276821328656556\n",
      "  episode_reward_mean: 0.1615142344594038\n",
      "  episode_reward_min: -1.0038238880255201\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 44081\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5994445351362229\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0152826405800879\n",
      "          policy_loss: -0.0964374976977706\n",
      "          total_loss: 0.17564243274182081\n",
      "          vf_explained_var: 0.5085448026657104\n",
      "          vf_loss: 0.24886941993236542\n",
      "    num_agent_steps_sampled: 12019968\n",
      "    num_agent_steps_trained: 12019968\n",
      "    num_steps_sampled: 3004992\n",
      "    num_steps_trained: 3004992\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.2171052631579\n",
      "    gpu_util_percent0: 0.17927631578947367\n",
      "    ram_util_percent: 94.75657894736842\n",
      "    vram_util_percent0: 0.5102180426386795\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8872792647989\n",
      "  policy_reward_mean:\n",
      "    main: 0.04037855861485093\n",
      "  policy_reward_min:\n",
      "    main: -1.6676265761869065\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30596082226118065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.09520296632186\n",
      "    mean_inference_ms: 4.458883598515844\n",
      "    mean_raw_obs_processing_ms: 1.3264439166044912\n",
      "  time_since_restore: 43359.51470065117\n",
      "  time_this_iter_s: 117.95365309715271\n",
      "  time_total_s: 43359.51470065117\n",
      "  timers:\n",
      "    learn_throughput: 87.118\n",
      "    learn_time_ms: 91737.34\n",
      "    sample_throughput: 291.389\n",
      "    sample_time_ms: 27427.281\n",
      "    update_time_ms: 2.865\n",
      "  timestamp: 1639201599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3004992\n",
      "  training_iteration: 376\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         43359.5</td><td style=\"text-align: right;\">3004992</td><td style=\"text-align: right;\">0.161514</td><td style=\"text-align: right;\">             2.32768</td><td style=\"text-align: right;\">            -1.00382</td><td style=\"text-align: right;\">           67.3821</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12051936\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8545779585307396\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09649121687265552\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5072782973033835\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 62.41269841269841\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3502353222138925\n",
      "  episode_reward_mean: 0.2228751223873147\n",
      "  episode_reward_min: -0.8745785980158005\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 44207\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6019621493816376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015020104449242353\n",
      "          policy_loss: -0.09638751389831304\n",
      "          total_loss: 0.20928593730181455\n",
      "          vf_explained_var: 0.47633233666419983\n",
      "          vf_loss: 0.2828616670370102\n",
      "    num_agent_steps_sampled: 12051936\n",
      "    num_agent_steps_trained: 12051936\n",
      "    num_steps_sampled: 3012984\n",
      "    num_steps_trained: 3012984\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.42266666666667\n",
      "    gpu_util_percent0: 0.1796666666666667\n",
      "    ram_util_percent: 94.58\n",
      "    vram_util_percent0: 0.510200723922343\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9906674071563173\n",
      "  policy_reward_mean:\n",
      "    main: 0.05571878059682865\n",
      "  policy_reward_min:\n",
      "    main: -1.613006716901737\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3059333165068243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.09209281195245\n",
      "    mean_inference_ms: 4.457773969757117\n",
      "    mean_raw_obs_processing_ms: 1.3264622820825918\n",
      "  time_since_restore: 43475.88299369812\n",
      "  time_this_iter_s: 116.3682930469513\n",
      "  time_total_s: 43475.88299369812\n",
      "  timers:\n",
      "    learn_throughput: 87.566\n",
      "    learn_time_ms: 91268.045\n",
      "    sample_throughput: 292.059\n",
      "    sample_time_ms: 27364.337\n",
      "    update_time_ms: 2.867\n",
      "  timestamp: 1639201715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3012984\n",
      "  training_iteration: 377\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         43475.9</td><td style=\"text-align: right;\">3012984</td><td style=\"text-align: right;\">0.222875</td><td style=\"text-align: right;\">             1.35024</td><td style=\"text-align: right;\">           -0.874579</td><td style=\"text-align: right;\">           62.4127</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12083904\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0033895615133541\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0809819799284412\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.47378913468410017\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 63.66129032258065\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3211301364195394\n",
      "  episode_reward_mean: 0.31688102690428677\n",
      "  episode_reward_min: -1.0002917686451855\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 44331\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5985835440158844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015523045282810927\n",
      "          policy_loss: -0.09722933143377305\n",
      "          total_loss: 0.20948563329130412\n",
      "          vf_explained_var: 0.4848172664642334\n",
      "          vf_loss: 0.2831393406391144\n",
      "    num_agent_steps_sampled: 12083904\n",
      "    num_agent_steps_trained: 12083904\n",
      "    num_steps_sampled: 3020976\n",
      "    num_steps_trained: 3020976\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.568211920529805\n",
      "    gpu_util_percent0: 0.18291390728476822\n",
      "    ram_util_percent: 94.76953642384106\n",
      "    vram_util_percent0: 0.51023341156576\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1681547494971323\n",
      "  policy_reward_mean:\n",
      "    main: 0.0792202567260717\n",
      "  policy_reward_min:\n",
      "    main: -1.9087901143314259\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30649254668442405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.10814171743438\n",
      "    mean_inference_ms: 4.464529406025784\n",
      "    mean_raw_obs_processing_ms: 1.3269772111717086\n",
      "  time_since_restore: 43592.65414810181\n",
      "  time_this_iter_s: 116.77115440368652\n",
      "  time_total_s: 43592.65414810181\n",
      "  timers:\n",
      "    learn_throughput: 88.339\n",
      "    learn_time_ms: 90469.786\n",
      "    sample_throughput: 295.269\n",
      "    sample_time_ms: 27066.839\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1639201832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3020976\n",
      "  training_iteration: 378\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         43592.7</td><td style=\"text-align: right;\">3020976</td><td style=\"text-align: right;\">0.316881</td><td style=\"text-align: right;\">             1.32113</td><td style=\"text-align: right;\">            -1.00029</td><td style=\"text-align: right;\">           63.6613</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12115872\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.746312098623675\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.056170072364832006\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6395297744507245\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-52-29\n",
      "  done: false\n",
      "  episode_len_mean: 55.38620689655173\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8461564363442187\n",
      "  episode_reward_mean: 0.1971203604031745\n",
      "  episode_reward_min: -0.8892706691222134\n",
      "  episodes_this_iter: 145\n",
      "  episodes_total: 44476\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5896548454761505\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015256955243647098\n",
      "          policy_loss: -0.09589623281359673\n",
      "          total_loss: 0.2209154335409403\n",
      "          vf_explained_var: 0.4856441020965576\n",
      "          vf_loss: 0.29364016592502595\n",
      "    num_agent_steps_sampled: 12115872\n",
      "    num_agent_steps_trained: 12115872\n",
      "    num_steps_sampled: 3028968\n",
      "    num_steps_trained: 3028968\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.686184210526314\n",
      "    gpu_util_percent0: 0.18059210526315786\n",
      "    ram_util_percent: 94.82565789473684\n",
      "    vram_util_percent0: 0.5102158777991374\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8089381234478656\n",
      "  policy_reward_mean:\n",
      "    main: 0.04928009010079362\n",
      "  policy_reward_min:\n",
      "    main: -1.817108926759516\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3062160724722161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.10106954160044\n",
      "    mean_inference_ms: 4.461236974260097\n",
      "    mean_raw_obs_processing_ms: 1.3270470597848967\n",
      "  time_since_restore: 43709.783606529236\n",
      "  time_this_iter_s: 117.1294584274292\n",
      "  time_total_s: 43709.783606529236\n",
      "  timers:\n",
      "    learn_throughput: 89.046\n",
      "    learn_time_ms: 89751.847\n",
      "    sample_throughput: 298.539\n",
      "    sample_time_ms: 26770.407\n",
      "    update_time_ms: 2.843\n",
      "  timestamp: 1639201949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3028968\n",
      "  training_iteration: 379\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         43709.8</td><td style=\"text-align: right;\">3028968</td><td style=\"text-align: right;\"> 0.19712</td><td style=\"text-align: right;\">             1.84616</td><td style=\"text-align: right;\">           -0.889271</td><td style=\"text-align: right;\">           55.3862</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12147840\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7900079259331912\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05674469384191456\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7742066760974415\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 57.166666666666664\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4446879363375231\n",
      "  episode_reward_mean: 0.16706058244491725\n",
      "  episode_reward_min: -1.0462628787498929\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 44614\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5962168241739273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01559118876606226\n",
      "          policy_loss: -0.09750429537147283\n",
      "          total_loss: 0.21678929924964904\n",
      "          vf_explained_var: 0.47858870029449463\n",
      "          vf_loss: 0.2906144775152206\n",
      "    num_agent_steps_sampled: 12147840\n",
      "    num_agent_steps_trained: 12147840\n",
      "    num_steps_sampled: 3036960\n",
      "    num_steps_trained: 3036960\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.25496688741722\n",
      "    gpu_util_percent0: 0.18033112582781455\n",
      "    ram_util_percent: 94.89006622516558\n",
      "    vram_util_percent0: 0.510209440627254\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9069684816700523\n",
      "  policy_reward_mean:\n",
      "    main: 0.041765145611229314\n",
      "  policy_reward_min:\n",
      "    main: -1.7373045778003582\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30643859132975304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.107001767759975\n",
      "    mean_inference_ms: 4.464098477920456\n",
      "    mean_raw_obs_processing_ms: 1.327361648952968\n",
      "  time_since_restore: 43827.255046367645\n",
      "  time_this_iter_s: 117.47143983840942\n",
      "  time_total_s: 43827.255046367645\n",
      "  timers:\n",
      "    learn_throughput: 89.085\n",
      "    learn_time_ms: 89711.899\n",
      "    sample_throughput: 299.69\n",
      "    sample_time_ms: 26667.558\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639202067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3036960\n",
      "  training_iteration: 380\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         43827.3</td><td style=\"text-align: right;\">3036960</td><td style=\"text-align: right;\">0.167061</td><td style=\"text-align: right;\">             1.44469</td><td style=\"text-align: right;\">            -1.04626</td><td style=\"text-align: right;\">           57.1667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12179808\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9670147853229435\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.045782958533458845\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7506667417118438\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 56.61805555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5665963037408108\n",
      "  episode_reward_mean: 0.19750710819097492\n",
      "  episode_reward_min: -1.2112729060944605\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 44758\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5924476245641709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01523424332216382\n",
      "          policy_loss: -0.09562826935574412\n",
      "          total_loss: 0.1953996567428112\n",
      "          vf_explained_var: 0.5354093909263611\n",
      "          vf_loss: 0.26789091849327085\n",
      "    num_agent_steps_sampled: 12179808\n",
      "    num_agent_steps_trained: 12179808\n",
      "    num_steps_sampled: 3044952\n",
      "    num_steps_trained: 3044952\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.305298013245036\n",
      "    gpu_util_percent0: 0.17807947019867548\n",
      "    ram_util_percent: 94.89337748344371\n",
      "    vram_util_percent0: 0.5102083510391402\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9557319235501107\n",
      "  policy_reward_mean:\n",
      "    main: 0.04937677704774372\n",
      "  policy_reward_min:\n",
      "    main: -1.7506667417118438\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.306832128891748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.117866820715967\n",
      "    mean_inference_ms: 4.468369496413004\n",
      "    mean_raw_obs_processing_ms: 1.327793976839906\n",
      "  time_since_restore: 43943.69482302666\n",
      "  time_this_iter_s: 116.43977665901184\n",
      "  time_total_s: 43943.69482302666\n",
      "  timers:\n",
      "    learn_throughput: 89.046\n",
      "    learn_time_ms: 89751.492\n",
      "    sample_throughput: 299.65\n",
      "    sample_time_ms: 26671.092\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1639202183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3044952\n",
      "  training_iteration: 381\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         43943.7</td><td style=\"text-align: right;\">3044952</td><td style=\"text-align: right;\">0.197507</td><td style=\"text-align: right;\">              1.5666</td><td style=\"text-align: right;\">            -1.21127</td><td style=\"text-align: right;\">           56.6181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12211776\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9919870260253831\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04645669057097543\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.633097650939866\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_02-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 61.23076923076923\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2880994663568366\n",
      "  episode_reward_mean: 0.10333042499256748\n",
      "  episode_reward_min: -0.8912824284408591\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 44888\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5933144127130509\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015058741319924593\n",
      "          policy_loss: -0.09386954368650913\n",
      "          total_loss: 0.21303075113892556\n",
      "          vf_explained_var: 0.48305103182792664\n",
      "          vf_loss: 0.28402983140945437\n",
      "    num_agent_steps_sampled: 12211776\n",
      "    num_agent_steps_trained: 12211776\n",
      "    num_steps_sampled: 3052944\n",
      "    num_steps_trained: 3052944\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.351655629139074\n",
      "    gpu_util_percent0: 0.17847682119205296\n",
      "    ram_util_percent: 94.90397350993378\n",
      "    vram_util_percent0: 0.510200723922343\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0511917205602375\n",
      "  policy_reward_mean:\n",
      "    main: 0.025832606248141855\n",
      "  policy_reward_min:\n",
      "    main: -1.6695140759545544\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30663033739952367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.111857205236607\n",
      "    mean_inference_ms: 4.466106593055317\n",
      "    mean_raw_obs_processing_ms: 1.3277633062007987\n",
      "  time_since_restore: 44061.28839468956\n",
      "  time_this_iter_s: 117.59357166290283\n",
      "  time_total_s: 44061.28839468956\n",
      "  timers:\n",
      "    learn_throughput: 88.837\n",
      "    learn_time_ms: 89962.883\n",
      "    sample_throughput: 299.778\n",
      "    sample_time_ms: 26659.689\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1639202301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3052944\n",
      "  training_iteration: 382\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         44061.3</td><td style=\"text-align: right;\">3052944</td><td style=\"text-align: right;\"> 0.10333</td><td style=\"text-align: right;\">              1.2881</td><td style=\"text-align: right;\">           -0.891282</td><td style=\"text-align: right;\">           61.2308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12243744\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1374548952213834\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03192372977070132\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8942737918709281\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 62.512\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4283655477359385\n",
      "  episode_reward_mean: 0.12404478098758619\n",
      "  episode_reward_min: -1.0264812217052863\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 45013\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5933795372247695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015423576556146144\n",
      "          policy_loss: -0.09750659048184752\n",
      "          total_loss: 0.20189013837650419\n",
      "          vf_explained_var: 0.46772217750549316\n",
      "          vf_loss: 0.27597217375040056\n",
      "    num_agent_steps_sampled: 12243744\n",
      "    num_agent_steps_trained: 12243744\n",
      "    num_steps_sampled: 3060936\n",
      "    num_steps_trained: 3060936\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.19602649006623\n",
      "    gpu_util_percent0: 0.1811258278145695\n",
      "    ram_util_percent: 94.97019867549672\n",
      "    vram_util_percent0: 0.510200723922343\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0149587935765054\n",
      "  policy_reward_mean:\n",
      "    main: 0.03101119524689654\n",
      "  policy_reward_min:\n",
      "    main: -1.894273791870928\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30611871074629404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.09919187103342\n",
      "    mean_inference_ms: 4.461381729190163\n",
      "    mean_raw_obs_processing_ms: 1.3275744471397577\n",
      "  time_since_restore: 44177.501765728\n",
      "  time_this_iter_s: 116.21337103843689\n",
      "  time_total_s: 44177.501765728\n",
      "  timers:\n",
      "    learn_throughput: 88.945\n",
      "    learn_time_ms: 89852.908\n",
      "    sample_throughput: 299.981\n",
      "    sample_time_ms: 26641.719\n",
      "    update_time_ms: 2.829\n",
      "  timestamp: 1639202417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3060936\n",
      "  training_iteration: 383\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         44177.5</td><td style=\"text-align: right;\">3060936</td><td style=\"text-align: right;\">0.124045</td><td style=\"text-align: right;\">             1.42837</td><td style=\"text-align: right;\">            -1.02648</td><td style=\"text-align: right;\">            62.512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12275712\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8290467124395863\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07989886142768647\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6780875983727485\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 61.29545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8593795963445228\n",
      "  episode_reward_mean: 0.20722195597757456\n",
      "  episode_reward_min: -1.3743627292922578\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 45145\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5872220234870911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015274775907397271\n",
      "          policy_loss: -0.0956901588216424\n",
      "          total_loss: 0.221019620873034\n",
      "          vf_explained_var: 0.46630969643592834\n",
      "          vf_loss: 0.29351121294498445\n",
      "    num_agent_steps_sampled: 12275712\n",
      "    num_agent_steps_trained: 12275712\n",
      "    num_steps_sampled: 3068928\n",
      "    num_steps_trained: 3068928\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.26225165562914\n",
      "    gpu_util_percent0: 0.18165562913907285\n",
      "    ram_util_percent: 94.9728476821192\n",
      "    vram_util_percent0: 0.510209440627254\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9860023269390554\n",
      "  policy_reward_mean:\n",
      "    main: 0.051805488994393634\n",
      "  policy_reward_min:\n",
      "    main: -1.8226161478012861\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3066260188630415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11179251673372\n",
      "    mean_inference_ms: 4.465955350434947\n",
      "    mean_raw_obs_processing_ms: 1.3280339175592277\n",
      "  time_since_restore: 44295.0392935276\n",
      "  time_this_iter_s: 117.53752779960632\n",
      "  time_total_s: 44295.0392935276\n",
      "  timers:\n",
      "    learn_throughput: 88.896\n",
      "    learn_time_ms: 89902.9\n",
      "    sample_throughput: 298.992\n",
      "    sample_time_ms: 26729.836\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1639202535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3068928\n",
      "  training_iteration: 384\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">           44295</td><td style=\"text-align: right;\">3068928</td><td style=\"text-align: right;\">0.207222</td><td style=\"text-align: right;\">             1.85938</td><td style=\"text-align: right;\">            -1.37436</td><td style=\"text-align: right;\">           61.2955</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12307680\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8974225812808144\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.12136532308618772\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4762251580035002\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 64.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2376998830197365\n",
      "  episode_reward_mean: 0.27127650281565435\n",
      "  episode_reward_min: -0.8767977053917013\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 45271\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5959729776382446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015194819901138544\n",
      "          policy_loss: -0.09564079064130783\n",
      "          total_loss: 0.20031949012726546\n",
      "          vf_explained_var: 0.47948578000068665\n",
      "          vf_loss: 0.27288314765691757\n",
      "    num_agent_steps_sampled: 12307680\n",
      "    num_agent_steps_trained: 12307680\n",
      "    num_steps_sampled: 3076920\n",
      "    num_steps_trained: 3076920\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.482894736842105\n",
      "    gpu_util_percent0: 0.17868421052631578\n",
      "    ram_util_percent: 95.0875\n",
      "    vram_util_percent0: 0.5102180426386795\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8274757741893093\n",
      "  policy_reward_mean:\n",
      "    main: 0.0678191257039136\n",
      "  policy_reward_min:\n",
      "    main: -1.6066972353756217\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3065296836667797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.108577872018675\n",
      "    mean_inference_ms: 4.464770442791034\n",
      "    mean_raw_obs_processing_ms: 1.3280319834454344\n",
      "  time_since_restore: 44412.56351494789\n",
      "  time_this_iter_s: 117.52422142028809\n",
      "  time_total_s: 44412.56351494789\n",
      "  timers:\n",
      "    learn_throughput: 88.791\n",
      "    learn_time_ms: 90009.378\n",
      "    sample_throughput: 299.011\n",
      "    sample_time_ms: 26728.118\n",
      "    update_time_ms: 2.879\n",
      "  timestamp: 1639202652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3076920\n",
      "  training_iteration: 385\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         44412.6</td><td style=\"text-align: right;\">3076920</td><td style=\"text-align: right;\">0.271277</td><td style=\"text-align: right;\">              1.2377</td><td style=\"text-align: right;\">           -0.876798</td><td style=\"text-align: right;\">                64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12339648\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8240828786850102\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0823153768216263\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5046142590083122\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-06-09\n",
      "  done: false\n",
      "  episode_len_mean: 59.45652173913044\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6703043031177156\n",
      "  episode_reward_mean: 0.19087885112473316\n",
      "  episode_reward_min: -0.9149347454547552\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 45409\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5964002842903138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015426864154636859\n",
      "          policy_loss: -0.09776643044874073\n",
      "          total_loss: 0.22479183865338564\n",
      "          vf_explained_var: 0.4699540436267853\n",
      "          vf_loss: 0.2991287199258804\n",
      "    num_agent_steps_sampled: 12339648\n",
      "    num_agent_steps_trained: 12339648\n",
      "    num_steps_sampled: 3084912\n",
      "    num_steps_trained: 3084912\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.592000000000006\n",
      "    gpu_util_percent0: 0.17973333333333333\n",
      "    ram_util_percent: 95.14866666666666\n",
      "    vram_util_percent0: 0.5102160798508283\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0143439002547936\n",
      "  policy_reward_mean:\n",
      "    main: 0.047719712781183296\n",
      "  policy_reward_min:\n",
      "    main: -1.7212471721969447\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.306579911733475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11325617697974\n",
      "    mean_inference_ms: 4.467428905007806\n",
      "    mean_raw_obs_processing_ms: 1.328229926432906\n",
      "  time_since_restore: 44529.26204276085\n",
      "  time_this_iter_s: 116.69852781295776\n",
      "  time_total_s: 44529.26204276085\n",
      "  timers:\n",
      "    learn_throughput: 88.82\n",
      "    learn_time_ms: 89979.559\n",
      "    sample_throughput: 300.046\n",
      "    sample_time_ms: 26635.899\n",
      "    update_time_ms: 2.89\n",
      "  timestamp: 1639202769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3084912\n",
      "  training_iteration: 386\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         44529.3</td><td style=\"text-align: right;\">3084912</td><td style=\"text-align: right;\">0.190879</td><td style=\"text-align: right;\">              1.6703</td><td style=\"text-align: right;\">           -0.914935</td><td style=\"text-align: right;\">           59.4565</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12371616\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7973016749400442\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0750093706014376\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7977815952584102\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-08-06\n",
      "  done: false\n",
      "  episode_len_mean: 62.3671875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.326051925664157\n",
      "  episode_reward_mean: 0.20121316389149918\n",
      "  episode_reward_min: -0.9926390361489092\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 45537\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.597462886095047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015385883718729019\n",
      "          policy_loss: -0.09678201991692185\n",
      "          total_loss: 0.2041645254008472\n",
      "          vf_explained_var: 0.48936817049980164\n",
      "          vf_loss: 0.2775792361497879\n",
      "    num_agent_steps_sampled: 12371616\n",
      "    num_agent_steps_trained: 12371616\n",
      "    num_steps_sampled: 3092904\n",
      "    num_steps_trained: 3092904\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.60728476821192\n",
      "    gpu_util_percent0: 0.18052980132450333\n",
      "    ram_util_percent: 86.0662251655629\n",
      "    vram_util_percent0: 0.5102159781559376\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8429562960192953\n",
      "  policy_reward_mean:\n",
      "    main: 0.05030329097287481\n",
      "  policy_reward_min:\n",
      "    main: -1.8692101666869816\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30641747550351744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.106108405708675\n",
      "    mean_inference_ms: 4.463842803039624\n",
      "    mean_raw_obs_processing_ms: 1.3281576860410433\n",
      "  time_since_restore: 44646.16585493088\n",
      "  time_this_iter_s: 116.90381217002869\n",
      "  time_total_s: 44646.16585493088\n",
      "  timers:\n",
      "    learn_throughput: 88.85\n",
      "    learn_time_ms: 89949.752\n",
      "    sample_throughput: 299.118\n",
      "    sample_time_ms: 26718.508\n",
      "    update_time_ms: 2.916\n",
      "  timestamp: 1639202886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3092904\n",
      "  training_iteration: 387\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         44646.2</td><td style=\"text-align: right;\">3092904</td><td style=\"text-align: right;\">0.201213</td><td style=\"text-align: right;\">             1.32605</td><td style=\"text-align: right;\">           -0.992639</td><td style=\"text-align: right;\">           62.3672</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12403584\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.776231670385728\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0466982757711529\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7928900239906035\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-10-02\n",
      "  done: false\n",
      "  episode_len_mean: 63.203125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3134870501322915\n",
      "  episode_reward_mean: 0.1174027166008314\n",
      "  episode_reward_min: -0.8560671339732406\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 45665\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5861807870864868\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015540107674896718\n",
      "          policy_loss: -0.09653317350521684\n",
      "          total_loss: 0.21694845930486917\n",
      "          vf_explained_var: 0.48075562715530396\n",
      "          vf_loss: 0.289880095243454\n",
      "    num_agent_steps_sampled: 12403584\n",
      "    num_agent_steps_trained: 12403584\n",
      "    num_steps_sampled: 3100896\n",
      "    num_steps_trained: 3100896\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.36466666666667\n",
      "    gpu_util_percent0: 0.18166666666666667\n",
      "    ram_util_percent: 86.17066666666663\n",
      "    vram_util_percent0: 0.5102358231874521\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0547555845186825\n",
      "  policy_reward_mean:\n",
      "    main: 0.029350679150207844\n",
      "  policy_reward_min:\n",
      "    main: -1.7928900239906036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30663325787015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.110327359734466\n",
      "    mean_inference_ms: 4.465232591167508\n",
      "    mean_raw_obs_processing_ms: 1.3283496281607357\n",
      "  time_since_restore: 44762.39739060402\n",
      "  time_this_iter_s: 116.23153567314148\n",
      "  time_total_s: 44762.39739060402\n",
      "  timers:\n",
      "    learn_throughput: 88.879\n",
      "    learn_time_ms: 89920.403\n",
      "    sample_throughput: 299.458\n",
      "    sample_time_ms: 26688.252\n",
      "    update_time_ms: 2.912\n",
      "  timestamp: 1639203002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3100896\n",
      "  training_iteration: 388\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         44762.4</td><td style=\"text-align: right;\">3100896</td><td style=\"text-align: right;\">0.117403</td><td style=\"text-align: right;\">             1.31349</td><td style=\"text-align: right;\">           -0.856067</td><td style=\"text-align: right;\">           63.2031</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12435552\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0392870260773999\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.051342370275441716\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5091073703247363\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-11-59\n",
      "  done: false\n",
      "  episode_len_mean: 60.653543307086615\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6944375158747804\n",
      "  episode_reward_mean: 0.17036889289148857\n",
      "  episode_reward_min: -0.9859296710475405\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 45792\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5965709503889084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01565520654991269\n",
      "          policy_loss: -0.09879491098225117\n",
      "          total_loss: 0.2146601229161024\n",
      "          vf_explained_var: 0.46813762187957764\n",
      "          vf_loss: 0.2896786885857582\n",
      "    num_agent_steps_sampled: 12435552\n",
      "    num_agent_steps_trained: 12435552\n",
      "    num_steps_sampled: 3108888\n",
      "    num_steps_trained: 3108888\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.454\n",
      "    gpu_util_percent0: 0.1814\n",
      "    ram_util_percent: 85.05400000000002\n",
      "    vram_util_percent0: 0.5102336294833827\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9909719778127406\n",
      "  policy_reward_mean:\n",
      "    main: 0.04259222322287212\n",
      "  policy_reward_min:\n",
      "    main: -1.796571096715407\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3067861657884214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11471104086518\n",
      "    mean_inference_ms: 4.467219254333862\n",
      "    mean_raw_obs_processing_ms: 1.3286400282530264\n",
      "  time_since_restore: 44878.83747506142\n",
      "  time_this_iter_s: 116.44008445739746\n",
      "  time_total_s: 44878.83747506142\n",
      "  timers:\n",
      "    learn_throughput: 88.937\n",
      "    learn_time_ms: 89861.622\n",
      "    sample_throughput: 299.494\n",
      "    sample_time_ms: 26684.97\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1639203119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3108888\n",
      "  training_iteration: 389\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         44878.8</td><td style=\"text-align: right;\">3108888</td><td style=\"text-align: right;\">0.170369</td><td style=\"text-align: right;\">             1.69444</td><td style=\"text-align: right;\">            -0.98593</td><td style=\"text-align: right;\">           60.6535</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12467520\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.800479975087514\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0953523022136024\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6970773867095333\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-14-00\n",
      "  done: false\n",
      "  episode_len_mean: 63.42741935483871\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3398380282702451\n",
      "  episode_reward_mean: 0.2433561141058095\n",
      "  episode_reward_min: -1.8364868970495776\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 45916\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5953160762786865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015412949498742818\n",
      "          policy_loss: -0.09784083589166402\n",
      "          total_loss: 0.21162684212625027\n",
      "          vf_explained_var: 0.45587483048439026\n",
      "          vf_loss: 0.2860592601299286\n",
      "    num_agent_steps_sampled: 12467520\n",
      "    num_agent_steps_trained: 12467520\n",
      "    num_steps_sampled: 3116880\n",
      "    num_steps_trained: 3116880\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.652229299363064\n",
      "    gpu_util_percent0: 0.1943312101910828\n",
      "    ram_util_percent: 85.1891719745223\n",
      "    vram_util_percent0: 0.5173665910048353\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.079127363142122\n",
      "  policy_reward_mean:\n",
      "    main: 0.060839028526452385\n",
      "  policy_reward_min:\n",
      "    main: -1.694404807910153\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3066446072123853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.11512254074561\n",
      "    mean_inference_ms: 4.468033057874042\n",
      "    mean_raw_obs_processing_ms: 1.328660499658013\n",
      "  time_since_restore: 45000.305668354034\n",
      "  time_this_iter_s: 121.4681932926178\n",
      "  time_total_s: 45000.305668354034\n",
      "  timers:\n",
      "    learn_throughput: 88.586\n",
      "    learn_time_ms: 90217.581\n",
      "    sample_throughput: 299.023\n",
      "    sample_time_ms: 26727.057\n",
      "    update_time_ms: 2.903\n",
      "  timestamp: 1639203240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3116880\n",
      "  training_iteration: 390\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         45000.3</td><td style=\"text-align: right;\">3116880</td><td style=\"text-align: right;\">0.243356</td><td style=\"text-align: right;\">             1.33984</td><td style=\"text-align: right;\">            -1.83649</td><td style=\"text-align: right;\">           63.4274</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12499488\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6587397698714255\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07962032389996287\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5052307389518985\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-16-01\n",
      "  done: false\n",
      "  episode_len_mean: 62.53787878787879\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.33539374392887\n",
      "  episode_reward_mean: 0.2306240070258404\n",
      "  episode_reward_min: -1.006653798850953\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 46048\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5968020609617233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01579765511676669\n",
      "          policy_loss: -0.09879171015322208\n",
      "          total_loss: 0.22739929512143134\n",
      "          vf_explained_var: 0.4623444080352783\n",
      "          vf_loss: 0.3021983162164688\n",
      "    num_agent_steps_sampled: 12499488\n",
      "    num_agent_steps_trained: 12499488\n",
      "    num_steps_sampled: 3124872\n",
      "    num_steps_trained: 3124872\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.370322580645166\n",
      "    gpu_util_percent0: 0.1889032258064516\n",
      "    ram_util_percent: 85.10709677419354\n",
      "    vram_util_percent0: 0.5253309131823923\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9613087972952312\n",
      "  policy_reward_mean:\n",
      "    main: 0.057656001756460094\n",
      "  policy_reward_min:\n",
      "    main: -1.592997394898394\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30642793315840117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.108380991563326\n",
      "    mean_inference_ms: 4.464243629246845\n",
      "    mean_raw_obs_processing_ms: 1.3287024445690088\n",
      "  time_since_restore: 45121.09981918335\n",
      "  time_this_iter_s: 120.79415082931519\n",
      "  time_total_s: 45121.09981918335\n",
      "  timers:\n",
      "    learn_throughput: 88.23\n",
      "    learn_time_ms: 90581.479\n",
      "    sample_throughput: 298.166\n",
      "    sample_time_ms: 26803.887\n",
      "    update_time_ms: 3.123\n",
      "  timestamp: 1639203361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3124872\n",
      "  training_iteration: 391\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         45121.1</td><td style=\"text-align: right;\">3124872</td><td style=\"text-align: right;\">0.230624</td><td style=\"text-align: right;\">             1.33539</td><td style=\"text-align: right;\">            -1.00665</td><td style=\"text-align: right;\">           62.5379</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12531456\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8298397148029496\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05196194440037873\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7114415940429929\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 56.391304347826086\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5029099090714242\n",
      "  episode_reward_mean: 0.27236959290184853\n",
      "  episode_reward_min: -0.8354018678589943\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 46186\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5892692424058914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01558761878311634\n",
      "          policy_loss: -0.09765601770952344\n",
      "          total_loss: 0.21550187224149703\n",
      "          vf_explained_var: 0.4918863773345947\n",
      "          vf_loss: 0.2894841933250427\n",
      "    num_agent_steps_sampled: 12531456\n",
      "    num_agent_steps_trained: 12531456\n",
      "    num_steps_sampled: 3132864\n",
      "    num_steps_trained: 3132864\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.739024390243905\n",
      "    gpu_util_percent0: 0.2069512195121951\n",
      "    ram_util_percent: 87.38048780487803\n",
      "    vram_util_percent0: 0.5258308654162555\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.999376900015104\n",
      "  policy_reward_mean:\n",
      "    main: 0.06809239822546213\n",
      "  policy_reward_min:\n",
      "    main: -1.7154076112645371\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3067886124166773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.12094736357765\n",
      "    mean_inference_ms: 4.468118098100737\n",
      "    mean_raw_obs_processing_ms: 1.329430449265549\n",
      "  time_since_restore: 45250.17743754387\n",
      "  time_this_iter_s: 129.0776183605194\n",
      "  time_total_s: 45250.17743754387\n",
      "  timers:\n",
      "    learn_throughput: 87.322\n",
      "    learn_time_ms: 91523.84\n",
      "    sample_throughput: 295.879\n",
      "    sample_time_ms: 27011.0\n",
      "    update_time_ms: 3.1\n",
      "  timestamp: 1639203490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3132864\n",
      "  training_iteration: 392\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         45250.2</td><td style=\"text-align: right;\">3132864</td><td style=\"text-align: right;\"> 0.27237</td><td style=\"text-align: right;\">             1.50291</td><td style=\"text-align: right;\">           -0.835402</td><td style=\"text-align: right;\">           56.3913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12563424\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7192950972515888\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03599467747932321\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6427842084372443\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 58.62595419847328\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3621834800965131\n",
      "  episode_reward_mean: 0.13484815141533396\n",
      "  episode_reward_min: -1.2158925185687388\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 46317\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.602042237997055\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01563641145080328\n",
      "          policy_loss: -0.09593002467602492\n",
      "          total_loss: 0.19147903683409095\n",
      "          vf_explained_var: 0.4736056923866272\n",
      "          vf_loss: 0.2636612609028816\n",
      "    num_agent_steps_sampled: 12563424\n",
      "    num_agent_steps_trained: 12563424\n",
      "    num_steps_sampled: 3140856\n",
      "    num_steps_trained: 3140856\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.14647058823529\n",
      "    gpu_util_percent0: 0.24052941176470588\n",
      "    ram_util_percent: 86.78823529411764\n",
      "    vram_util_percent0: 0.5272951628825272\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.773656524869669\n",
      "  policy_reward_mean:\n",
      "    main: 0.03371203785383349\n",
      "  policy_reward_min:\n",
      "    main: -1.7379052793748708\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30655388490968666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.128480310543026\n",
      "    mean_inference_ms: 4.46783971213742\n",
      "    mean_raw_obs_processing_ms: 1.330092689639015\n",
      "  time_since_restore: 45382.68487739563\n",
      "  time_this_iter_s: 132.50743985176086\n",
      "  time_total_s: 45382.68487739563\n",
      "  timers:\n",
      "    learn_throughput: 86.321\n",
      "    learn_time_ms: 92585.134\n",
      "    sample_throughput: 289.802\n",
      "    sample_time_ms: 27577.464\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1639203623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3140856\n",
      "  training_iteration: 393\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         45382.7</td><td style=\"text-align: right;\">3140856</td><td style=\"text-align: right;\">0.134848</td><td style=\"text-align: right;\">             1.36218</td><td style=\"text-align: right;\">            -1.21589</td><td style=\"text-align: right;\">            58.626</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12595392\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9597240949372857\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.055510356953681596\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.0123134932114133\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-22-25\n",
      "  done: false\n",
      "  episode_len_mean: 67.99186991869918\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2714782504856958\n",
      "  episode_reward_mean: 0.10660500533214862\n",
      "  episode_reward_min: -1.418377539284193\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 46440\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5864409621953964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015210150979459285\n",
      "          policy_loss: -0.09634574245288968\n",
      "          total_loss: 0.1891715629696846\n",
      "          vf_explained_var: 0.490445613861084\n",
      "          vf_loss: 0.2624168909192085\n",
      "    num_agent_steps_sampled: 12595392\n",
      "    num_agent_steps_trained: 12595392\n",
      "    num_steps_sampled: 3148848\n",
      "    num_steps_trained: 3148848\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.991772151898736\n",
      "    gpu_util_percent0: 0.1807594936708861\n",
      "    ram_util_percent: 86.29430379746836\n",
      "    vram_util_percent0: 0.5333408308029374\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9271585596013137\n",
      "  policy_reward_mean:\n",
      "    main: 0.02665125133303715\n",
      "  policy_reward_min:\n",
      "    main: -2.0123134932114133\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3071224598663416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.14713817641699\n",
      "    mean_inference_ms: 4.474265561429574\n",
      "    mean_raw_obs_processing_ms: 1.3306639155091544\n",
      "  time_since_restore: 45504.80621981621\n",
      "  time_this_iter_s: 122.121342420578\n",
      "  time_total_s: 45504.80621981621\n",
      "  timers:\n",
      "    learn_throughput: 85.995\n",
      "    learn_time_ms: 92936.009\n",
      "    sample_throughput: 288.721\n",
      "    sample_time_ms: 27680.707\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1639203745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3148848\n",
      "  training_iteration: 394\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         45504.8</td><td style=\"text-align: right;\">3148848</td><td style=\"text-align: right;\">0.106605</td><td style=\"text-align: right;\">             1.27148</td><td style=\"text-align: right;\">            -1.41838</td><td style=\"text-align: right;\">           67.9919</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12627360\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.889752025082431\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07366983585135475\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.45070457091199023\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-24-30\n",
      "  done: false\n",
      "  episode_len_mean: 55.94405594405595\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4224289561980852\n",
      "  episode_reward_mean: 0.19047186534882557\n",
      "  episode_reward_min: -0.799440738896379\n",
      "  episodes_this_iter: 143\n",
      "  episodes_total: 46583\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5916836103200912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015472408514469863\n",
      "          policy_loss: -0.09648318024724722\n",
      "          total_loss: 0.2307996230274439\n",
      "          vf_explained_var: 0.48521289229393005\n",
      "          vf_loss: 0.3037840836644173\n",
      "    num_agent_steps_sampled: 12627360\n",
      "    num_agent_steps_trained: 12627360\n",
      "    num_steps_sampled: 3156840\n",
      "    num_steps_trained: 3156840\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.9937106918239\n",
      "    gpu_util_percent0: 0.1970440251572327\n",
      "    ram_util_percent: 86.67232704402517\n",
      "    vram_util_percent0: 0.5364237656792928\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9836886841607353\n",
      "  policy_reward_mean:\n",
      "    main: 0.04761796633720639\n",
      "  policy_reward_min:\n",
      "    main: -1.5436166860886618\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3070071980856035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.145865138413335\n",
      "    mean_inference_ms: 4.472150603991087\n",
      "    mean_raw_obs_processing_ms: 1.331014981409244\n",
      "  time_since_restore: 45629.03509378433\n",
      "  time_this_iter_s: 124.22887396812439\n",
      "  time_total_s: 45629.03509378433\n",
      "  timers:\n",
      "    learn_throughput: 85.552\n",
      "    learn_time_ms: 93417.226\n",
      "    sample_throughput: 286.756\n",
      "    sample_time_ms: 27870.339\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1639203870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3156840\n",
      "  training_iteration: 395\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">           45629</td><td style=\"text-align: right;\">3156840</td><td style=\"text-align: right;\">0.190472</td><td style=\"text-align: right;\">             1.42243</td><td style=\"text-align: right;\">           -0.799441</td><td style=\"text-align: right;\">           55.9441</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12659328\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9455707835074896\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1090740321655948\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5848301585000865\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 60.42962962962963\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4715424852817325\n",
      "  episode_reward_mean: 0.22223776741058204\n",
      "  episode_reward_min: -1.1508136901053518\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 46718\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5944642218351364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015340833269059658\n",
      "          policy_loss: -0.0956802067309618\n",
      "          total_loss: 0.22773408716917037\n",
      "          vf_explained_var: 0.46879804134368896\n",
      "          vf_loss: 0.3001154065132141\n",
      "    num_agent_steps_sampled: 12659328\n",
      "    num_agent_steps_trained: 12659328\n",
      "    num_steps_sampled: 3164832\n",
      "    num_steps_trained: 3164832\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.91428571428572\n",
      "    gpu_util_percent0: 0.1648051948051948\n",
      "    ram_util_percent: 84.49090909090908\n",
      "    vram_util_percent0: 0.533869223898839\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9701065072482042\n",
      "  policy_reward_mean:\n",
      "    main: 0.055559441852645504\n",
      "  policy_reward_min:\n",
      "    main: -1.7025287172788044\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30704458505719484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.15015120989357\n",
      "    mean_inference_ms: 4.473643711741001\n",
      "    mean_raw_obs_processing_ms: 1.3312730025592792\n",
      "  time_since_restore: 45748.979563474655\n",
      "  time_this_iter_s: 119.94446969032288\n",
      "  time_total_s: 45748.979563474655\n",
      "  timers:\n",
      "    learn_throughput: 85.341\n",
      "    learn_time_ms: 93647.309\n",
      "    sample_throughput: 285.804\n",
      "    sample_time_ms: 27963.247\n",
      "    update_time_ms: 3.261\n",
      "  timestamp: 1639203990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3164832\n",
      "  training_iteration: 396\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">           45749</td><td style=\"text-align: right;\">3164832</td><td style=\"text-align: right;\">0.222238</td><td style=\"text-align: right;\">             1.47154</td><td style=\"text-align: right;\">            -1.15081</td><td style=\"text-align: right;\">           60.4296</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12691296\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0183537422094844\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10397238101058892\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6258855901729844\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 69.37837837837837\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5243606768759876\n",
      "  episode_reward_mean: 0.22088763356152805\n",
      "  episode_reward_min: -0.7327301376529234\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 46829\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5922257339954377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015310055937618017\n",
      "          policy_loss: -0.09596685508266091\n",
      "          total_loss: 0.1842945264726877\n",
      "          vf_explained_var: 0.48564499616622925\n",
      "          vf_loss: 0.2570092351436615\n",
      "    num_agent_steps_sampled: 12691296\n",
      "    num_agent_steps_trained: 12691296\n",
      "    num_steps_sampled: 3172824\n",
      "    num_steps_trained: 3172824\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.818831168831174\n",
      "    gpu_util_percent0: 0.1875974025974026\n",
      "    ram_util_percent: 84.52597402597402\n",
      "    vram_util_percent0: 0.5319600603411068\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9359272175914277\n",
      "  policy_reward_mean:\n",
      "    main: 0.05522190839038203\n",
      "  policy_reward_min:\n",
      "    main: -1.6317850097054243\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3070182273342902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.152146882535195\n",
      "    mean_inference_ms: 4.472886281273338\n",
      "    mean_raw_obs_processing_ms: 1.331452846180268\n",
      "  time_since_restore: 45868.68587613106\n",
      "  time_this_iter_s: 119.70631265640259\n",
      "  time_total_s: 45868.68587613106\n",
      "  timers:\n",
      "    learn_throughput: 85.23\n",
      "    learn_time_ms: 93770.281\n",
      "    sample_throughput: 284.127\n",
      "    sample_time_ms: 28128.273\n",
      "    update_time_ms: 3.222\n",
      "  timestamp: 1639204109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3172824\n",
      "  training_iteration: 397\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         45868.7</td><td style=\"text-align: right;\">3172824</td><td style=\"text-align: right;\">0.220888</td><td style=\"text-align: right;\">             1.52436</td><td style=\"text-align: right;\">            -0.73273</td><td style=\"text-align: right;\">           69.3784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12723264\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.135558881077347\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07252497764276221\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6325551448313617\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 65.04878048780488\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7627277042958633\n",
      "  episode_reward_mean: 0.20236702105078722\n",
      "  episode_reward_min: -0.8770606642662484\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 46952\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5905669202804565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015352111976593733\n",
      "          policy_loss: -0.09728530717268587\n",
      "          total_loss: 0.22849675328284502\n",
      "          vf_explained_var: 0.43915408849716187\n",
      "          vf_loss: 0.3024660418629646\n",
      "    num_agent_steps_sampled: 12723264\n",
      "    num_agent_steps_trained: 12723264\n",
      "    num_steps_sampled: 3180816\n",
      "    num_steps_trained: 3180816\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.462585034013607\n",
      "    gpu_util_percent0: 0.18170068027210884\n",
      "    ram_util_percent: 84.29319727891158\n",
      "    vram_util_percent0: 0.5234771104888155\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.128030910736153\n",
      "  policy_reward_mean:\n",
      "    main: 0.05059175526269681\n",
      "  policy_reward_min:\n",
      "    main: -1.6653792589206031\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30732990811745475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.158696883582277\n",
      "    mean_inference_ms: 4.4763703465182365\n",
      "    mean_raw_obs_processing_ms: 1.331628710496856\n",
      "  time_since_restore: 45982.845888614655\n",
      "  time_this_iter_s: 114.1600124835968\n",
      "  time_total_s: 45982.845888614655\n",
      "  timers:\n",
      "    learn_throughput: 85.336\n",
      "    learn_time_ms: 93653.42\n",
      "    sample_throughput: 284.991\n",
      "    sample_time_ms: 28043.007\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1639204224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3180816\n",
      "  training_iteration: 398\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         45982.8</td><td style=\"text-align: right;\">3180816</td><td style=\"text-align: right;\">0.202367</td><td style=\"text-align: right;\">             1.76273</td><td style=\"text-align: right;\">           -0.877061</td><td style=\"text-align: right;\">           65.0488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12755232\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9162057495634616\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06289730988553463\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.48104057291577784\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 62.208\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5622274395266784\n",
      "  episode_reward_mean: 0.15993629002331386\n",
      "  episode_reward_min: -1.0796322005977745\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 47077\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5901471191644668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015575091484934092\n",
      "          policy_loss: -0.09699413765221834\n",
      "          total_loss: 0.2064299608245492\n",
      "          vf_explained_var: 0.4573400020599365\n",
      "          vf_loss: 0.27976943099498747\n",
      "    num_agent_steps_sampled: 12755232\n",
      "    num_agent_steps_trained: 12755232\n",
      "    num_steps_sampled: 3188808\n",
      "    num_steps_trained: 3188808\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.27161290322581\n",
      "    gpu_util_percent0: 0.19129032258064518\n",
      "    ram_util_percent: 84.55548387096776\n",
      "    vram_util_percent0: 0.5202825632370581\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8854062208412508\n",
      "  policy_reward_mean:\n",
      "    main: 0.039984072505828465\n",
      "  policy_reward_min:\n",
      "    main: -1.618717974826551\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3072152670067647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.158522697493375\n",
      "    mean_inference_ms: 4.475904046839815\n",
      "    mean_raw_obs_processing_ms: 1.3316936376570467\n",
      "  time_since_restore: 46102.38149142265\n",
      "  time_this_iter_s: 119.53560280799866\n",
      "  time_total_s: 46102.38149142265\n",
      "  timers:\n",
      "    learn_throughput: 85.14\n",
      "    learn_time_ms: 93869.205\n",
      "    sample_throughput: 284.099\n",
      "    sample_time_ms: 28131.074\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1639204343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3188808\n",
      "  training_iteration: 399\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         46102.4</td><td style=\"text-align: right;\">3188808</td><td style=\"text-align: right;\">0.159936</td><td style=\"text-align: right;\">             1.56223</td><td style=\"text-align: right;\">            -1.07963</td><td style=\"text-align: right;\">            62.208</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12787200\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9222671155776289\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09135639169996844\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6232481201464198\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-34-23\n",
      "  done: false\n",
      "  episode_len_mean: 64.04580152671755\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4944832371385228\n",
      "  episode_reward_mean: 0.24758203897910663\n",
      "  episode_reward_min: -0.9033512705579445\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 47208\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5824349417686462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01573684084042907\n",
      "          policy_loss: -0.0970749572571367\n",
      "          total_loss: 0.23977571995556354\n",
      "          vf_explained_var: 0.4378044307231903\n",
      "          vf_loss: 0.3129503521323204\n",
      "    num_agent_steps_sampled: 12787200\n",
      "    num_agent_steps_trained: 12787200\n",
      "    num_steps_sampled: 3196800\n",
      "    num_steps_trained: 3196800\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.5141935483871\n",
      "    gpu_util_percent0: 0.18722580645161294\n",
      "    ram_util_percent: 84.46903225806452\n",
      "    vram_util_percent0: 0.5194949527115243\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.872468196617656\n",
      "  policy_reward_mean:\n",
      "    main: 0.06189550974477666\n",
      "  policy_reward_min:\n",
      "    main: -1.710449354381916\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30700931679693977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.155308709133802\n",
      "    mean_inference_ms: 4.474342573215673\n",
      "    mean_raw_obs_processing_ms: 1.331684526102045\n",
      "  time_since_restore: 46222.36661076546\n",
      "  time_this_iter_s: 119.98511934280396\n",
      "  time_total_s: 46222.36661076546\n",
      "  timers:\n",
      "    learn_throughput: 85.322\n",
      "    learn_time_ms: 93668.944\n",
      "    sample_throughput: 283.576\n",
      "    sample_time_ms: 28182.894\n",
      "    update_time_ms: 3.293\n",
      "  timestamp: 1639204463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3196800\n",
      "  training_iteration: 400\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         46222.4</td><td style=\"text-align: right;\">3196800</td><td style=\"text-align: right;\">0.247582</td><td style=\"text-align: right;\">             1.49448</td><td style=\"text-align: right;\">           -0.903351</td><td style=\"text-align: right;\">           64.0458</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12819168\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1280289035524083\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.12140165286295487\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.44915752582088264\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 59.92481203007519\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4759902266712794\n",
      "  episode_reward_mean: 0.220305016430832\n",
      "  episode_reward_min: -1.2292305727368475\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 47341\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5911720932722092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01588277243822813\n",
      "          policy_loss: -0.09812572756782174\n",
      "          total_loss: 0.23869408550858498\n",
      "          vf_explained_var: 0.4375588893890381\n",
      "          vf_loss: 0.3126978530883789\n",
      "    num_agent_steps_sampled: 12819168\n",
      "    num_agent_steps_trained: 12819168\n",
      "    num_steps_sampled: 3204792\n",
      "    num_steps_trained: 3204792\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.99144736842105\n",
      "    gpu_util_percent0: 0.18881578947368421\n",
      "    ram_util_percent: 84.66118421052632\n",
      "    vram_util_percent0: 0.517686739058901\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0030869322085727\n",
      "  policy_reward_mean:\n",
      "    main: 0.055076254107708014\n",
      "  policy_reward_min:\n",
      "    main: -1.815978978096184\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3069144077063223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.149663147665187\n",
      "    mean_inference_ms: 4.471112333769645\n",
      "    mean_raw_obs_processing_ms: 1.3318578714004439\n",
      "  time_since_restore: 46340.18657040596\n",
      "  time_this_iter_s: 117.81995964050293\n",
      "  time_total_s: 46340.18657040596\n",
      "  timers:\n",
      "    learn_throughput: 85.552\n",
      "    learn_time_ms: 93416.706\n",
      "    sample_throughput: 284.016\n",
      "    sample_time_ms: 28139.219\n",
      "    update_time_ms: 3.045\n",
      "  timestamp: 1639204581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3204792\n",
      "  training_iteration: 401\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         46340.2</td><td style=\"text-align: right;\">3204792</td><td style=\"text-align: right;\">0.220305</td><td style=\"text-align: right;\">             1.47599</td><td style=\"text-align: right;\">            -1.22923</td><td style=\"text-align: right;\">           59.9248</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12851136\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9629432537921425\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08675054925264498\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6155805919814572\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 59.51127819548872\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3969573889199196\n",
      "  episode_reward_mean: 0.24345009974258927\n",
      "  episode_reward_min: -1.0063725018852612\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 47474\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5877383284568787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015666005060076715\n",
      "          policy_loss: -0.09894265013560653\n",
      "          total_loss: 0.22084819522500038\n",
      "          vf_explained_var: 0.47056546807289124\n",
      "          vf_loss: 0.29599809950590134\n",
      "    num_agent_steps_sampled: 12851136\n",
      "    num_agent_steps_trained: 12851136\n",
      "    num_steps_sampled: 3212784\n",
      "    num_steps_trained: 3212784\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.56887417218543\n",
      "    gpu_util_percent0: 0.1760927152317881\n",
      "    ram_util_percent: 84.52119205298014\n",
      "    vram_util_percent0: 0.5165889790341456\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.918826166461187\n",
      "  policy_reward_mean:\n",
      "    main: 0.06086252493564732\n",
      "  policy_reward_min:\n",
      "    main: -1.6155805919814572\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3074078256900442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.165648052460856\n",
      "    mean_inference_ms: 4.477971304679079\n",
      "    mean_raw_obs_processing_ms: 1.3323596208255888\n",
      "  time_since_restore: 46456.231973171234\n",
      "  time_this_iter_s: 116.04540276527405\n",
      "  time_total_s: 46456.231973171234\n",
      "  timers:\n",
      "    learn_throughput: 86.598\n",
      "    learn_time_ms: 92288.261\n",
      "    sample_throughput: 285.837\n",
      "    sample_time_ms: 27959.988\n",
      "    update_time_ms: 3.056\n",
      "  timestamp: 1639204698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3212784\n",
      "  training_iteration: 402\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         46456.2</td><td style=\"text-align: right;\">3212784</td><td style=\"text-align: right;\"> 0.24345</td><td style=\"text-align: right;\">             1.39696</td><td style=\"text-align: right;\">            -1.00637</td><td style=\"text-align: right;\">           59.5113</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12883104\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8820560075440562\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06661893618056038\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49492746839380636\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 61.3046875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9079974980444314\n",
      "  episode_reward_mean: 0.19493211599931792\n",
      "  episode_reward_min: -1.1179021316824516\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 47602\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5908430144786835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01543634081631899\n",
      "          policy_loss: -0.09778914993256331\n",
      "          total_loss: 0.22492252722010017\n",
      "          vf_explained_var: 0.44734692573547363\n",
      "          vf_loss: 0.2992677365541458\n",
      "    num_agent_steps_sampled: 12883104\n",
      "    num_agent_steps_trained: 12883104\n",
      "    num_steps_sampled: 3220776\n",
      "    num_steps_trained: 3220776\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.905960264900656\n",
      "    gpu_util_percent0: 0.18582781456953643\n",
      "    ram_util_percent: 84.64238410596026\n",
      "    vram_util_percent0: 0.5146952748921851\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.826561264941652\n",
      "  policy_reward_mean:\n",
      "    main: 0.04873302899982949\n",
      "  policy_reward_min:\n",
      "    main: -1.582133413668434\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3067552572566978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.148859334546128\n",
      "    mean_inference_ms: 4.471232228507991\n",
      "    mean_raw_obs_processing_ms: 1.3318900907854077\n",
      "  time_since_restore: 46573.37487888336\n",
      "  time_this_iter_s: 117.14290571212769\n",
      "  time_total_s: 46573.37487888336\n",
      "  timers:\n",
      "    learn_throughput: 87.496\n",
      "    learn_time_ms: 91341.79\n",
      "    sample_throughput: 291.917\n",
      "    sample_time_ms: 27377.62\n",
      "    update_time_ms: 2.815\n",
      "  timestamp: 1639204815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3220776\n",
      "  training_iteration: 403\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         46573.4</td><td style=\"text-align: right;\">3220776</td><td style=\"text-align: right;\">0.194932</td><td style=\"text-align: right;\">               1.908</td><td style=\"text-align: right;\">             -1.1179</td><td style=\"text-align: right;\">           61.3047</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12915072\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8441510221488657\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10557935644409593\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5551559158662147\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 61.40909090909091\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.861172708102507\n",
      "  episode_reward_mean: 0.17371778355706258\n",
      "  episode_reward_min: -0.9830538320639284\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 47734\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5829516659975051\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015281497593969106\n",
      "          policy_loss: -0.09578226854838431\n",
      "          total_loss: 0.21109287425503134\n",
      "          vf_explained_var: 0.47261735796928406\n",
      "          vf_loss: 0.2836663678884506\n",
      "    num_agent_steps_sampled: 12915072\n",
      "    num_agent_steps_trained: 12915072\n",
      "    num_steps_sampled: 3228768\n",
      "    num_steps_trained: 3228768\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.434868421052634\n",
      "    gpu_util_percent0: 0.1794078947368421\n",
      "    ram_util_percent: 84.74802631578946\n",
      "    vram_util_percent0: 0.5152837671671776\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.910874773318349\n",
      "  policy_reward_mean:\n",
      "    main: 0.043429445889265644\n",
      "  policy_reward_min:\n",
      "    main: -1.6972273347670703\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30711074950854966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.159478943340154\n",
      "    mean_inference_ms: 4.475291590837601\n",
      "    mean_raw_obs_processing_ms: 1.3323964468198655\n",
      "  time_since_restore: 46690.73066806793\n",
      "  time_this_iter_s: 117.35578918457031\n",
      "  time_total_s: 46690.73066806793\n",
      "  timers:\n",
      "    learn_throughput: 87.859\n",
      "    learn_time_ms: 90963.49\n",
      "    sample_throughput: 292.966\n",
      "    sample_time_ms: 27279.602\n",
      "    update_time_ms: 2.789\n",
      "  timestamp: 1639204932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3228768\n",
      "  training_iteration: 404\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         46690.7</td><td style=\"text-align: right;\">3228768</td><td style=\"text-align: right;\">0.173718</td><td style=\"text-align: right;\">             1.86117</td><td style=\"text-align: right;\">           -0.983054</td><td style=\"text-align: right;\">           61.4091</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12947040\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8831599705337544\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08868642813077274\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6047177279423235\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 70.30357142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8250858508464036\n",
      "  episode_reward_mean: 0.1366361292220735\n",
      "  episode_reward_min: -0.8041066949753057\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 47846\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5907198388576508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01563255287706852\n",
      "          policy_loss: -0.09734506783634424\n",
      "          total_loss: 0.19403523882478477\n",
      "          vf_explained_var: 0.4488604664802551\n",
      "          vf_loss: 0.26763836777210237\n",
      "    num_agent_steps_sampled: 12947040\n",
      "    num_agent_steps_trained: 12947040\n",
      "    num_steps_sampled: 3236760\n",
      "    num_steps_trained: 3236760\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.43\n",
      "    gpu_util_percent0: 0.17986666666666667\n",
      "    ram_util_percent: 84.80733333333332\n",
      "    vram_util_percent0: 0.5153745749698365\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.14160389870573\n",
      "  policy_reward_mean:\n",
      "    main: 0.03415903230551837\n",
      "  policy_reward_min:\n",
      "    main: -1.6117125532436611\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3073034992072005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.16337939695351\n",
      "    mean_inference_ms: 4.476811102514348\n",
      "    mean_raw_obs_processing_ms: 1.3323742970612453\n",
      "  time_since_restore: 46806.94558429718\n",
      "  time_this_iter_s: 116.21491622924805\n",
      "  time_total_s: 46806.94558429718\n",
      "  timers:\n",
      "    learn_throughput: 88.434\n",
      "    learn_time_ms: 90372.296\n",
      "    sample_throughput: 295.233\n",
      "    sample_time_ms: 27070.19\n",
      "    update_time_ms: 2.783\n",
      "  timestamp: 1639205048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3236760\n",
      "  training_iteration: 405\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         46806.9</td><td style=\"text-align: right;\">3236760</td><td style=\"text-align: right;\">0.136636</td><td style=\"text-align: right;\">             1.82509</td><td style=\"text-align: right;\">           -0.804107</td><td style=\"text-align: right;\">           70.3036</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 12979008\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0163452407522124\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07004122500856765\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.595286950794921\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 63.96875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.892617546284545\n",
      "  episode_reward_mean: 0.1999697354672455\n",
      "  episode_reward_min: -1.0351315656471325\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 47974\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5879677263498306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015789056941866875\n",
      "          policy_loss: -0.09849828618206084\n",
      "          total_loss: 0.2072736243456602\n",
      "          vf_explained_var: 0.47047239542007446\n",
      "          vf_loss: 0.28179228204488754\n",
      "    num_agent_steps_sampled: 12979008\n",
      "    num_agent_steps_trained: 12979008\n",
      "    num_steps_sampled: 3244752\n",
      "    num_steps_trained: 3244752\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.2317880794702\n",
      "    gpu_util_percent0: 0.1790066225165563\n",
      "    ram_util_percent: 84.83973509933777\n",
      "    vram_util_percent0: 0.5153326839388176\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.976530290960811\n",
      "  policy_reward_mean:\n",
      "    main: 0.049992433866811375\n",
      "  policy_reward_min:\n",
      "    main: -1.6542474458521905\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3072867471825913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.16031441166139\n",
      "    mean_inference_ms: 4.4749199704772495\n",
      "    mean_raw_obs_processing_ms: 1.332383783144904\n",
      "  time_since_restore: 46923.45270085335\n",
      "  time_this_iter_s: 116.5071165561676\n",
      "  time_total_s: 46923.45270085335\n",
      "  timers:\n",
      "    learn_throughput: 88.715\n",
      "    learn_time_ms: 90085.983\n",
      "    sample_throughput: 295.874\n",
      "    sample_time_ms: 27011.519\n",
      "    update_time_ms: 2.792\n",
      "  timestamp: 1639205165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3244752\n",
      "  training_iteration: 406\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         46923.5</td><td style=\"text-align: right;\">3244752</td><td style=\"text-align: right;\"> 0.19997</td><td style=\"text-align: right;\">             1.89262</td><td style=\"text-align: right;\">            -1.03513</td><td style=\"text-align: right;\">           63.9688</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13010976\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9088893818803551\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09205549353927449\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5279908834027212\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-48-05\n",
      "  done: false\n",
      "  episode_len_mean: 64.984\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.538388122462731\n",
      "  episode_reward_mean: 0.2922009231332278\n",
      "  episode_reward_min: -1.0739888316919974\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 48099\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5860138474702835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015882108833640815\n",
      "          policy_loss: -0.09906416576355696\n",
      "          total_loss: 0.2065511596724391\n",
      "          vf_explained_var: 0.48231035470962524\n",
      "          vf_loss: 0.2814943741559982\n",
      "    num_agent_steps_sampled: 13010976\n",
      "    num_agent_steps_trained: 13010976\n",
      "    num_steps_sampled: 3252744\n",
      "    num_steps_trained: 3252744\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.12207792207792\n",
      "    gpu_util_percent0: 0.18370129870129867\n",
      "    ram_util_percent: 85.01363636363635\n",
      "    vram_util_percent0: 0.5152989491587715\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.161597286371899\n",
      "  policy_reward_mean:\n",
      "    main: 0.07305023078330694\n",
      "  policy_reward_min:\n",
      "    main: -1.5686214058985017\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3068425703486204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.15486341172539\n",
      "    mean_inference_ms: 4.472838184903304\n",
      "    mean_raw_obs_processing_ms: 1.3323700235321878\n",
      "  time_since_restore: 47043.06719207764\n",
      "  time_this_iter_s: 119.61449122428894\n",
      "  time_total_s: 47043.06719207764\n",
      "  timers:\n",
      "    learn_throughput: 88.683\n",
      "    learn_time_ms: 90118.99\n",
      "    sample_throughput: 296.467\n",
      "    sample_time_ms: 26957.438\n",
      "    update_time_ms: 2.833\n",
      "  timestamp: 1639205285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3252744\n",
      "  training_iteration: 407\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         47043.1</td><td style=\"text-align: right;\">3252744</td><td style=\"text-align: right;\">0.292201</td><td style=\"text-align: right;\">             1.53839</td><td style=\"text-align: right;\">            -1.07399</td><td style=\"text-align: right;\">            64.984</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13042944\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9486237682092771\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03894676158474162\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7432082921446161\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 66.23275862068965\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.132533759207297\n",
      "  episode_reward_mean: 0.11941345904294348\n",
      "  episode_reward_min: -1.1019631738721065\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 48215\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5881211103200913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015424532681703568\n",
      "          policy_loss: -0.09671249566599727\n",
      "          total_loss: 0.19456101404875517\n",
      "          vf_explained_var: 0.48243916034698486\n",
      "          vf_loss: 0.2678475015163422\n",
      "    num_agent_steps_sampled: 13042944\n",
      "    num_agent_steps_trained: 13042944\n",
      "    num_steps_sampled: 3260736\n",
      "    num_steps_trained: 3260736\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.26753246753247\n",
      "    gpu_util_percent0: 0.18993506493506493\n",
      "    ram_util_percent: 85.16558441558443\n",
      "    vram_util_percent0: 0.5196845766934612\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8389643096732406\n",
      "  policy_reward_mean:\n",
      "    main: 0.029853364760735877\n",
      "  policy_reward_min:\n",
      "    main: -1.7432082921446161\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30714061801813863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.165384428762763\n",
      "    mean_inference_ms: 4.4756982655114195\n",
      "    mean_raw_obs_processing_ms: 1.3327425528974606\n",
      "  time_since_restore: 47162.55196475983\n",
      "  time_this_iter_s: 119.48477268218994\n",
      "  time_total_s: 47162.55196475983\n",
      "  timers:\n",
      "    learn_throughput: 88.453\n",
      "    learn_time_ms: 90352.938\n",
      "    sample_throughput: 293.202\n",
      "    sample_time_ms: 27257.703\n",
      "    update_time_ms: 2.811\n",
      "  timestamp: 1639205404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3260736\n",
      "  training_iteration: 408\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         47162.6</td><td style=\"text-align: right;\">3260736</td><td style=\"text-align: right;\">0.119413</td><td style=\"text-align: right;\">             1.13253</td><td style=\"text-align: right;\">            -1.10196</td><td style=\"text-align: right;\">           66.2328</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13074912\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9643387742157218\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09228450738182623\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.0444660946330306\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 75.1651376146789\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6007576867734192\n",
      "  episode_reward_mean: 0.223663825063673\n",
      "  episode_reward_min: -0.9997309515223296\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 48324\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5853959959745407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015594691392034293\n",
      "          policy_loss: -0.09750115681812167\n",
      "          total_loss: 0.18290653536096216\n",
      "          vf_explained_var: 0.4711180329322815\n",
      "          vf_loss: 0.25672325521707534\n",
      "    num_agent_steps_sampled: 13074912\n",
      "    num_agent_steps_trained: 13074912\n",
      "    num_steps_sampled: 3268728\n",
      "    num_steps_trained: 3268728\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.721428571428575\n",
      "    gpu_util_percent0: 0.18779220779220776\n",
      "    ram_util_percent: 85.4077922077922\n",
      "    vram_util_percent0: 0.5192615051943779\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0294364692032802\n",
      "  policy_reward_mean:\n",
      "    main: 0.05591595626591827\n",
      "  policy_reward_min:\n",
      "    main: -1.6764353441404745\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3078274122153165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.179744832161465\n",
      "    mean_inference_ms: 4.480375320165286\n",
      "    mean_raw_obs_processing_ms: 1.3331734316638661\n",
      "  time_since_restore: 47281.55275321007\n",
      "  time_this_iter_s: 119.00078845024109\n",
      "  time_total_s: 47281.55275321007\n",
      "  timers:\n",
      "    learn_throughput: 88.439\n",
      "    learn_time_ms: 90367.66\n",
      "    sample_throughput: 293.918\n",
      "    sample_time_ms: 27191.235\n",
      "    update_time_ms: 2.809\n",
      "  timestamp: 1639205523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3268728\n",
      "  training_iteration: 409\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         47281.6</td><td style=\"text-align: right;\">3268728</td><td style=\"text-align: right;\">0.223664</td><td style=\"text-align: right;\">             1.60076</td><td style=\"text-align: right;\">           -0.999731</td><td style=\"text-align: right;\">           75.1651</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13106880\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9838463394312538\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09233277627879073\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7818387410372314\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 66.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.808061586123527\n",
      "  episode_reward_mean: 0.2532964531397638\n",
      "  episode_reward_min: -1.1029965130159933\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 48440\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5867583388090134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015574715800583363\n",
      "          policy_loss: -0.09803664379566908\n",
      "          total_loss: 0.212024797976017\n",
      "          vf_explained_var: 0.44859564304351807\n",
      "          vf_loss: 0.2864073431491852\n",
      "    num_agent_steps_sampled: 13106880\n",
      "    num_agent_steps_trained: 13106880\n",
      "    num_steps_sampled: 3276720\n",
      "    num_steps_trained: 3276720\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.671523178807945\n",
      "    gpu_util_percent0: 0.1832450331125828\n",
      "    ram_util_percent: 85.25629139072846\n",
      "    vram_util_percent0: 0.5187038695632279\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9869203087633902\n",
      "  policy_reward_mean:\n",
      "    main: 0.06332411328494095\n",
      "  policy_reward_min:\n",
      "    main: -1.7818387410372314\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3066568513875024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.15279541267576\n",
      "    mean_inference_ms: 4.47041075108167\n",
      "    mean_raw_obs_processing_ms: 1.332190856341809\n",
      "  time_since_restore: 47398.957612514496\n",
      "  time_this_iter_s: 117.4048593044281\n",
      "  time_total_s: 47398.957612514496\n",
      "  timers:\n",
      "    learn_throughput: 88.647\n",
      "    learn_time_ms: 90155.198\n",
      "    sample_throughput: 294.359\n",
      "    sample_time_ms: 27150.558\n",
      "    update_time_ms: 2.773\n",
      "  timestamp: 1639205641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3276720\n",
      "  training_iteration: 410\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">           47399</td><td style=\"text-align: right;\">3276720</td><td style=\"text-align: right;\">0.253296</td><td style=\"text-align: right;\">             1.80806</td><td style=\"text-align: right;\">              -1.103</td><td style=\"text-align: right;\">             66.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13138848\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9011131826651981\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0651099082572486\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5152090109015274\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 71.75652173913043\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4747628166767894\n",
      "  episode_reward_mean: 0.17185279824622085\n",
      "  episode_reward_min: -1.0438393968878623\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 48555\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5903607275485993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015784916784614324\n",
      "          policy_loss: -0.09748235359042882\n",
      "          total_loss: 0.19184179489687087\n",
      "          vf_explained_var: 0.4565020799636841\n",
      "          vf_loss: 0.2653508053421974\n",
      "    num_agent_steps_sampled: 13138848\n",
      "    num_agent_steps_trained: 13138848\n",
      "    num_steps_sampled: 3284712\n",
      "    num_steps_trained: 3284712\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.314935064935064\n",
      "    gpu_util_percent0: 0.18246753246753247\n",
      "    ram_util_percent: 85.47402597402599\n",
      "    vram_util_percent0: 0.5192294543232353\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9504475150778409\n",
      "  policy_reward_mean:\n",
      "    main: 0.042963199561555206\n",
      "  policy_reward_min:\n",
      "    main: -1.7892702631162685\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3078037087628468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18439678002897\n",
      "    mean_inference_ms: 4.481699376476023\n",
      "    mean_raw_obs_processing_ms: 1.3332675003357704\n",
      "  time_since_restore: 47517.79159903526\n",
      "  time_this_iter_s: 118.83398652076721\n",
      "  time_total_s: 47517.79159903526\n",
      "  timers:\n",
      "    learn_throughput: 88.656\n",
      "    learn_time_ms: 90145.923\n",
      "    sample_throughput: 293.208\n",
      "    sample_time_ms: 27257.057\n",
      "    update_time_ms: 2.761\n",
      "  timestamp: 1639205760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3284712\n",
      "  training_iteration: 411\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         47517.8</td><td style=\"text-align: right;\">3284712</td><td style=\"text-align: right;\">0.171853</td><td style=\"text-align: right;\">             1.47476</td><td style=\"text-align: right;\">            -1.04384</td><td style=\"text-align: right;\">           71.7565</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13170816\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9599824469111728\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.11353740046365197\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5830377510003164\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 59.8796992481203\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4550297268196344\n",
      "  episode_reward_mean: 0.21630260930152703\n",
      "  episode_reward_min: -0.8736789792731012\n",
      "  episodes_this_iter: 133\n",
      "  episodes_total: 48688\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5891875569820404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015850417520850896\n",
      "          policy_loss: -0.09980588082969188\n",
      "          total_loss: 0.2306062894165516\n",
      "          vf_explained_var: 0.468292236328125\n",
      "          vf_loss: 0.30633934831619264\n",
      "    num_agent_steps_sampled: 13170816\n",
      "    num_agent_steps_trained: 13170816\n",
      "    num_steps_sampled: 3292704\n",
      "    num_steps_trained: 3292704\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.77862068965517\n",
      "    gpu_util_percent0: 0.13696551724137931\n",
      "    ram_util_percent: 85.11931034482758\n",
      "    vram_util_percent0: 0.5188877920368543\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9691604725340144\n",
      "  policy_reward_mean:\n",
      "    main: 0.05407565232538176\n",
      "  policy_reward_min:\n",
      "    main: -1.6328977681682586\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30744480899184967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.172126903872613\n",
      "    mean_inference_ms: 4.477535250822723\n",
      "    mean_raw_obs_processing_ms: 1.333028693213275\n",
      "  time_since_restore: 47630.29207777977\n",
      "  time_this_iter_s: 112.50047874450684\n",
      "  time_total_s: 47630.29207777977\n",
      "  timers:\n",
      "    learn_throughput: 88.875\n",
      "    learn_time_ms: 89923.813\n",
      "    sample_throughput: 294.587\n",
      "    sample_time_ms: 27129.519\n",
      "    update_time_ms: 2.763\n",
      "  timestamp: 1639205872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3292704\n",
      "  training_iteration: 412\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         47630.3</td><td style=\"text-align: right;\">3292704</td><td style=\"text-align: right;\">0.216303</td><td style=\"text-align: right;\">             1.45503</td><td style=\"text-align: right;\">           -0.873679</td><td style=\"text-align: right;\">           59.8797</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13202784\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7099758678878698\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06987424244144243\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4441897109688018\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_03-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 62.92063492063492\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.305756022576829\n",
      "  episode_reward_mean: 0.1619705744651903\n",
      "  episode_reward_min: -1.125446630937585\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 48814\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5938545684814454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015735494125634433\n",
      "          policy_loss: -0.09710147077776492\n",
      "          total_loss: 0.1827806824594736\n",
      "          vf_explained_var: 0.5063896179199219\n",
      "          vf_loss: 0.25598387336730954\n",
      "    num_agent_steps_sampled: 13202784\n",
      "    num_agent_steps_trained: 13202784\n",
      "    num_steps_sampled: 3300696\n",
      "    num_steps_trained: 3300696\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.95960264900663\n",
      "    gpu_util_percent0: 0.15483443708609274\n",
      "    ram_util_percent: 85.4907284768212\n",
      "    vram_util_percent0: 0.5181547171538214\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9460574871766152\n",
      "  policy_reward_mean:\n",
      "    main: 0.04049264361629757\n",
      "  policy_reward_min:\n",
      "    main: -1.6972052621934766\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3078076393811631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.183017007284104\n",
      "    mean_inference_ms: 4.481859891673611\n",
      "    mean_raw_obs_processing_ms: 1.3334874150550984\n",
      "  time_since_restore: 47747.27110147476\n",
      "  time_this_iter_s: 116.97902369499207\n",
      "  time_total_s: 47747.27110147476\n",
      "  timers:\n",
      "    learn_throughput: 88.897\n",
      "    learn_time_ms: 89902.152\n",
      "    sample_throughput: 294.604\n",
      "    sample_time_ms: 27127.929\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1639205989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3300696\n",
      "  training_iteration: 413\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         47747.3</td><td style=\"text-align: right;\">3300696</td><td style=\"text-align: right;\">0.161971</td><td style=\"text-align: right;\">             1.30576</td><td style=\"text-align: right;\">            -1.12545</td><td style=\"text-align: right;\">           62.9206</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13234752\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9581527174376353\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07849005335254404\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5613514549370314\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 58.52142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6848756104704739\n",
      "  episode_reward_mean: 0.18615699456866347\n",
      "  episode_reward_min: -0.9016339528916086\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 48954\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5890415818691254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01583873948827386\n",
      "          policy_loss: -0.0976708867996931\n",
      "          total_loss: 0.21567765725404023\n",
      "          vf_explained_var: 0.4919731318950653\n",
      "          vf_loss: 0.28929345923662186\n",
      "    num_agent_steps_sampled: 13234752\n",
      "    num_agent_steps_trained: 13234752\n",
      "    num_steps_sampled: 3308688\n",
      "    num_steps_trained: 3308688\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.098684210526315\n",
      "    gpu_util_percent0: 0.16467105263157897\n",
      "    ram_util_percent: 85.48026315789474\n",
      "    vram_util_percent0: 0.5216873625326891\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9124413542389571\n",
      "  policy_reward_mean:\n",
      "    main: 0.04653924864216587\n",
      "  policy_reward_min:\n",
      "    main: -1.7000592551239828\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3077541864046619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18241961637932\n",
      "    mean_inference_ms: 4.481308740971828\n",
      "    mean_raw_obs_processing_ms: 1.3336588589327942\n",
      "  time_since_restore: 47864.660969018936\n",
      "  time_this_iter_s: 117.3898675441742\n",
      "  time_total_s: 47864.660969018936\n",
      "  timers:\n",
      "    learn_throughput: 88.888\n",
      "    learn_time_ms: 89911.191\n",
      "    sample_throughput: 294.656\n",
      "    sample_time_ms: 27123.15\n",
      "    update_time_ms: 2.866\n",
      "  timestamp: 1639206107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3308688\n",
      "  training_iteration: 414\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         47864.7</td><td style=\"text-align: right;\">3308688</td><td style=\"text-align: right;\">0.186157</td><td style=\"text-align: right;\">             1.68488</td><td style=\"text-align: right;\">           -0.901634</td><td style=\"text-align: right;\">           58.5214</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13266720\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9635000735195391\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10411041452983051\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.451513315413688\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 65.35593220338983\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.810560170817919\n",
      "  episode_reward_mean: 0.2634227879997556\n",
      "  episode_reward_min: -1.017781781836817\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 49072\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5891856858730317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015706218417733907\n",
      "          policy_loss: -0.09861589246988296\n",
      "          total_loss: 0.2232510218024254\n",
      "          vf_explained_var: 0.4323170483112335\n",
      "          vf_loss: 0.298013095498085\n",
      "    num_agent_steps_sampled: 13266720\n",
      "    num_agent_steps_trained: 13266720\n",
      "    num_steps_sampled: 3316680\n",
      "    num_steps_trained: 3316680\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.10397350993378\n",
      "    gpu_util_percent0: 0.1796026490066225\n",
      "    ram_util_percent: 85.54768211920533\n",
      "    vram_util_percent0: 0.5209429731372948\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.951273196105664\n",
      "  policy_reward_mean:\n",
      "    main: 0.06585569699993889\n",
      "  policy_reward_min:\n",
      "    main: -1.6464870068642048\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3071070617610098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.168941828525377\n",
      "    mean_inference_ms: 4.475957030379358\n",
      "    mean_raw_obs_processing_ms: 1.3332012496830852\n",
      "  time_since_restore: 47982.161554813385\n",
      "  time_this_iter_s: 117.50058579444885\n",
      "  time_total_s: 47982.161554813385\n",
      "  timers:\n",
      "    learn_throughput: 88.853\n",
      "    learn_time_ms: 89946.241\n",
      "    sample_throughput: 293.641\n",
      "    sample_time_ms: 27216.874\n",
      "    update_time_ms: 2.89\n",
      "  timestamp: 1639206224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3316680\n",
      "  training_iteration: 415\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         47982.2</td><td style=\"text-align: right;\">3316680</td><td style=\"text-align: right;\">0.263423</td><td style=\"text-align: right;\">             1.81056</td><td style=\"text-align: right;\">            -1.01778</td><td style=\"text-align: right;\">           65.3559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13298688\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8996457906603073\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07552807916001476\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.42628590565103575\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 65.368\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5208241900146555\n",
      "  episode_reward_mean: 0.2569750617057212\n",
      "  episode_reward_min: -0.7142408208049407\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 49197\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5889000535011292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015929200310260058\n",
      "          policy_loss: -0.09915055994689465\n",
      "          total_loss: 0.21162563144415616\n",
      "          vf_explained_var: 0.4681990444660187\n",
      "          vf_loss: 0.2865837194919586\n",
      "    num_agent_steps_sampled: 13298688\n",
      "    num_agent_steps_trained: 13298688\n",
      "    num_steps_sampled: 3324672\n",
      "    num_steps_trained: 3324672\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.44276315789474\n",
      "    gpu_util_percent0: 0.1888157894736842\n",
      "    ram_util_percent: 85.52697368421055\n",
      "    vram_util_percent0: 0.5191545002684401\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.167670116497555\n",
      "  policy_reward_mean:\n",
      "    main: 0.06424376542643032\n",
      "  policy_reward_min:\n",
      "    main: -1.908102788961691\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30807098973536057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18897440469153\n",
      "    mean_inference_ms: 4.482667191741974\n",
      "    mean_raw_obs_processing_ms: 1.3340708175185152\n",
      "  time_since_restore: 48099.59357690811\n",
      "  time_this_iter_s: 117.43202209472656\n",
      "  time_total_s: 48099.59357690811\n",
      "  timers:\n",
      "    learn_throughput: 88.775\n",
      "    learn_time_ms: 90025.119\n",
      "    sample_throughput: 293.49\n",
      "    sample_time_ms: 27230.886\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1639206342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3324672\n",
      "  training_iteration: 416\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         48099.6</td><td style=\"text-align: right;\">3324672</td><td style=\"text-align: right;\">0.256975</td><td style=\"text-align: right;\">             1.52082</td><td style=\"text-align: right;\">           -0.714241</td><td style=\"text-align: right;\">            65.368</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13330656\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.056815009042498\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08751870305718286\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6177832092791604\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 72.23636363636363\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3694068093002125\n",
      "  episode_reward_mean: 0.2291152956537758\n",
      "  episode_reward_min: -1.1223684000443788\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 49307\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.587929795384407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01552423782646656\n",
      "          policy_loss: -0.09642153182253242\n",
      "          total_loss: 0.1970168402902782\n",
      "          vf_explained_var: 0.4389544129371643\n",
      "          vf_loss: 0.26986093616485596\n",
      "    num_agent_steps_sampled: 13330656\n",
      "    num_agent_steps_trained: 13330656\n",
      "    num_steps_sampled: 3332664\n",
      "    num_steps_trained: 3332664\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.71933333333333\n",
      "    gpu_util_percent0: 0.17700000000000002\n",
      "    ram_util_percent: 85.74733333333333\n",
      "    vram_util_percent0: 0.5194142810134912\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.915490331293412\n",
      "  policy_reward_mean:\n",
      "    main: 0.057278823913443924\n",
      "  policy_reward_min:\n",
      "    main: -1.635813057631378\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3072850247802988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.173176888233026\n",
      "    mean_inference_ms: 4.47678661557839\n",
      "    mean_raw_obs_processing_ms: 1.3333781912434983\n",
      "  time_since_restore: 48216.070376873016\n",
      "  time_this_iter_s: 116.47679996490479\n",
      "  time_total_s: 48216.070376873016\n",
      "  timers:\n",
      "    learn_throughput: 88.972\n",
      "    learn_time_ms: 89825.636\n",
      "    sample_throughput: 294.685\n",
      "    sample_time_ms: 27120.458\n",
      "    update_time_ms: 3.117\n",
      "  timestamp: 1639206458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3332664\n",
      "  training_iteration: 417\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         48216.1</td><td style=\"text-align: right;\">3332664</td><td style=\"text-align: right;\">0.229115</td><td style=\"text-align: right;\">             1.36941</td><td style=\"text-align: right;\">            -1.12237</td><td style=\"text-align: right;\">           72.2364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13362624\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9215309119710863\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09115943221448339\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8493464216502125\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-09-35\n",
      "  done: false\n",
      "  episode_len_mean: 68.66666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5715662452471935\n",
      "  episode_reward_mean: 0.23425572993009808\n",
      "  episode_reward_min: -1.698328074014614\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 49424\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5847705199718475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015836875069886448\n",
      "          policy_loss: -0.09905279462784529\n",
      "          total_loss: 0.20855001360177994\n",
      "          vf_explained_var: 0.4532777667045593\n",
      "          vf_loss: 0.28355055397748946\n",
      "    num_agent_steps_sampled: 13362624\n",
      "    num_agent_steps_trained: 13362624\n",
      "    num_steps_sampled: 3340656\n",
      "    num_steps_trained: 3340656\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.92666666666667\n",
      "    gpu_util_percent0: 0.17926666666666666\n",
      "    ram_util_percent: 85.84400000000001\n",
      "    vram_util_percent0: 0.5152901173631678\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9592539936383542\n",
      "  policy_reward_mean:\n",
      "    main: 0.05856393248252454\n",
      "  policy_reward_min:\n",
      "    main: -1.8493464216502125\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30752287360027014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.180299465476097\n",
      "    mean_inference_ms: 4.479681387595934\n",
      "    mean_raw_obs_processing_ms: 1.3335280666034852\n",
      "  time_since_restore: 48332.297761917114\n",
      "  time_this_iter_s: 116.2273850440979\n",
      "  time_total_s: 48332.297761917114\n",
      "  timers:\n",
      "    learn_throughput: 89.138\n",
      "    learn_time_ms: 89658.663\n",
      "    sample_throughput: 296.465\n",
      "    sample_time_ms: 26957.623\n",
      "    update_time_ms: 6.349\n",
      "  timestamp: 1639206575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3340656\n",
      "  training_iteration: 418\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         48332.3</td><td style=\"text-align: right;\">3340656</td><td style=\"text-align: right;\">0.234256</td><td style=\"text-align: right;\">             1.57157</td><td style=\"text-align: right;\">            -1.69833</td><td style=\"text-align: right;\">           68.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13394592\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7354240937161609\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09172622759987507\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5320813015522634\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 62.43307086614173\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5637649062201069\n",
      "  episode_reward_mean: 0.2106993840468314\n",
      "  episode_reward_min: -1.2246623995145904\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 49551\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5872124437093734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01557675826177001\n",
      "          policy_loss: -0.09803147605806589\n",
      "          total_loss: 0.21567590729892253\n",
      "          vf_explained_var: 0.4629088342189789\n",
      "          vf_loss: 0.2900501829981804\n",
      "    num_agent_steps_sampled: 13394592\n",
      "    num_agent_steps_trained: 13394592\n",
      "    num_steps_sampled: 3348648\n",
      "    num_steps_trained: 3348648\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.94039735099338\n",
      "    gpu_util_percent0: 0.18033112582781458\n",
      "    ram_util_percent: 85.92980132450332\n",
      "    vram_util_percent0: 0.5109198520775176\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.751714628848776\n",
      "  policy_reward_mean:\n",
      "    main: 0.05267484601170786\n",
      "  policy_reward_min:\n",
      "    main: -1.6361245414626988\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3077311184603252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.186512424331937\n",
      "    mean_inference_ms: 4.48184772939417\n",
      "    mean_raw_obs_processing_ms: 1.3337796205602306\n",
      "  time_since_restore: 48449.0282702446\n",
      "  time_this_iter_s: 116.73050832748413\n",
      "  time_total_s: 48449.0282702446\n",
      "  timers:\n",
      "    learn_throughput: 89.332\n",
      "    learn_time_ms: 89464.391\n",
      "    sample_throughput: 296.858\n",
      "    sample_time_ms: 26921.991\n",
      "    update_time_ms: 6.417\n",
      "  timestamp: 1639206691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3348648\n",
      "  training_iteration: 419\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">           48449</td><td style=\"text-align: right;\">3348648</td><td style=\"text-align: right;\">0.210699</td><td style=\"text-align: right;\">             1.56376</td><td style=\"text-align: right;\">            -1.22466</td><td style=\"text-align: right;\">           62.4331</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13426560\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6008474554864076\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05906561237555072\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5112636847342827\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 61.323076923076925\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7245816933628633\n",
      "  episode_reward_mean: 0.13546699318565225\n",
      "  episode_reward_min: -1.2397878357319132\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 49681\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5934468491077423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01589187092706561\n",
      "          policy_loss: -0.09939965103194118\n",
      "          total_loss: 0.21347565195709467\n",
      "          vf_explained_var: 0.4594857394695282\n",
      "          vf_loss: 0.2887395248413086\n",
      "    num_agent_steps_sampled: 13426560\n",
      "    num_agent_steps_trained: 13426560\n",
      "    num_steps_sampled: 3356640\n",
      "    num_steps_trained: 3356640\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.34585987261146\n",
      "    gpu_util_percent0: 0.20101910828025477\n",
      "    ram_util_percent: 86.18853503184715\n",
      "    vram_util_percent0: 0.5110411780610031\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0593729632781694\n",
      "  policy_reward_mean:\n",
      "    main: 0.03386674829641306\n",
      "  policy_reward_min:\n",
      "    main: -1.7251641346551394\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3076605865194264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18457635814009\n",
      "    mean_inference_ms: 4.479885814520667\n",
      "    mean_raw_obs_processing_ms: 1.333940616845465\n",
      "  time_since_restore: 48571.30541086197\n",
      "  time_this_iter_s: 122.2771406173706\n",
      "  time_total_s: 48571.30541086197\n",
      "  timers:\n",
      "    learn_throughput: 88.959\n",
      "    learn_time_ms: 89838.909\n",
      "    sample_throughput: 295.622\n",
      "    sample_time_ms: 27034.489\n",
      "    update_time_ms: 6.405\n",
      "  timestamp: 1639206814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3356640\n",
      "  training_iteration: 420\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         48571.3</td><td style=\"text-align: right;\">3356640</td><td style=\"text-align: right;\">0.135467</td><td style=\"text-align: right;\">             1.72458</td><td style=\"text-align: right;\">            -1.23979</td><td style=\"text-align: right;\">           61.3231</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13458528\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8333667329056972\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08113810652754325\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6468854238628621\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 68.81666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.114836408082792\n",
      "  episode_reward_mean: 0.16652588891903236\n",
      "  episode_reward_min: -1.6579873474392413\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 49801\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5882452795505524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015803576212376356\n",
      "          policy_loss: -0.098062350589782\n",
      "          total_loss: 0.19791942693293094\n",
      "          vf_explained_var: 0.47237932682037354\n",
      "          vf_loss: 0.2719800949692726\n",
      "    num_agent_steps_sampled: 13458528\n",
      "    num_agent_steps_trained: 13458528\n",
      "    num_steps_sampled: 3364632\n",
      "    num_steps_trained: 3364632\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.334640522875816\n",
      "    gpu_util_percent0: 0.18215686274509799\n",
      "    ram_util_percent: 86.12026143790851\n",
      "    vram_util_percent0: 0.51773136588188\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0690877089198634\n",
      "  policy_reward_mean:\n",
      "    main: 0.04163147222975812\n",
      "  policy_reward_min:\n",
      "    main: -2.0023984535100667\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30768197536322817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.189498778879784\n",
      "    mean_inference_ms: 4.481074046755638\n",
      "    mean_raw_obs_processing_ms: 1.334101327138399\n",
      "  time_since_restore: 48689.26054954529\n",
      "  time_this_iter_s: 117.95513868331909\n",
      "  time_total_s: 48689.26054954529\n",
      "  timers:\n",
      "    learn_throughput: 89.075\n",
      "    learn_time_ms: 89721.82\n",
      "    sample_throughput: 295.304\n",
      "    sample_time_ms: 27063.613\n",
      "    update_time_ms: 6.389\n",
      "  timestamp: 1639206932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3364632\n",
      "  training_iteration: 421\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         48689.3</td><td style=\"text-align: right;\">3364632</td><td style=\"text-align: right;\">0.166526</td><td style=\"text-align: right;\">             2.11484</td><td style=\"text-align: right;\">            -1.65799</td><td style=\"text-align: right;\">           68.8167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13490496\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0672293821520824\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09603432299946457\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8443533332292249\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 58.04477611940298\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2685854564377443\n",
      "  episode_reward_mean: 0.2296680960280834\n",
      "  episode_reward_min: -0.6868463785741412\n",
      "  episodes_this_iter: 134\n",
      "  episodes_total: 49935\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5821974093914032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01590339719131589\n",
      "          policy_loss: -0.10006827471777796\n",
      "          total_loss: 0.22551582495868205\n",
      "          vf_explained_var: 0.4659351408481598\n",
      "          vf_loss: 0.30143081617355344\n",
      "    num_agent_steps_sampled: 13490496\n",
      "    num_agent_steps_trained: 13490496\n",
      "    num_steps_sampled: 3372624\n",
      "    num_steps_trained: 3372624\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.04635761589404\n",
      "    gpu_util_percent0: 0.18231788079470196\n",
      "    ram_util_percent: 86.02847682119204\n",
      "    vram_util_percent0: 0.5179324411785858\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9702664668624807\n",
      "  policy_reward_mean:\n",
      "    main: 0.057417024007020846\n",
      "  policy_reward_min:\n",
      "    main: -1.8485117365601091\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30770469893836466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19249563957384\n",
      "    mean_inference_ms: 4.482598702725238\n",
      "    mean_raw_obs_processing_ms: 1.3342754893558788\n",
      "  time_since_restore: 48806.06972146034\n",
      "  time_this_iter_s: 116.80917191505432\n",
      "  time_total_s: 48806.06972146034\n",
      "  timers:\n",
      "    learn_throughput: 88.807\n",
      "    learn_time_ms: 89992.462\n",
      "    sample_throughput: 293.631\n",
      "    sample_time_ms: 27217.85\n",
      "    update_time_ms: 6.438\n",
      "  timestamp: 1639207049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3372624\n",
      "  training_iteration: 422\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         48806.1</td><td style=\"text-align: right;\">3372624</td><td style=\"text-align: right;\">0.229668</td><td style=\"text-align: right;\">             1.26859</td><td style=\"text-align: right;\">           -0.686846</td><td style=\"text-align: right;\">           58.0448</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13522464\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.2006444614428935\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07432175454720867\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.49577272591708355\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-19-29\n",
      "  done: false\n",
      "  episode_len_mean: 59.97709923664122\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4543757438826161\n",
      "  episode_reward_mean: 0.14180635610045034\n",
      "  episode_reward_min: -0.6585398528565929\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 50066\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5853381289243698\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01578591128066182\n",
      "          policy_loss: -0.09916282151266932\n",
      "          total_loss: 0.22102154748514294\n",
      "          vf_explained_var: 0.45742079615592957\n",
      "          vf_loss: 0.29620951730012895\n",
      "    num_agent_steps_sampled: 13522464\n",
      "    num_agent_steps_trained: 13522464\n",
      "    num_steps_sampled: 3380616\n",
      "    num_steps_trained: 3380616\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.45548387096775\n",
      "    gpu_util_percent0: 0.19851612903225804\n",
      "    ram_util_percent: 86.27548387096773\n",
      "    vram_util_percent0: 0.5194270186500228\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0139571805169054\n",
      "  policy_reward_mean:\n",
      "    main: 0.035451589025112584\n",
      "  policy_reward_min:\n",
      "    main: -1.7048760718375495\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3075745473106473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18905682925261\n",
      "    mean_inference_ms: 4.4805279546041445\n",
      "    mean_raw_obs_processing_ms: 1.3342916948473038\n",
      "  time_since_restore: 48926.73343157768\n",
      "  time_this_iter_s: 120.66371011734009\n",
      "  time_total_s: 48926.73343157768\n",
      "  timers:\n",
      "    learn_throughput: 88.533\n",
      "    learn_time_ms: 90271.772\n",
      "    sample_throughput: 292.597\n",
      "    sample_time_ms: 27314.022\n",
      "    update_time_ms: 6.322\n",
      "  timestamp: 1639207169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3380616\n",
      "  training_iteration: 423\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         48926.7</td><td style=\"text-align: right;\">3380616</td><td style=\"text-align: right;\">0.141806</td><td style=\"text-align: right;\">             1.45438</td><td style=\"text-align: right;\">            -0.65854</td><td style=\"text-align: right;\">           59.9771</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13554432\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8869273330961824\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07270662641326896\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7644652485767282\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-21-24\n",
      "  done: false\n",
      "  episode_len_mean: 68.60330578512396\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.297333201408842\n",
      "  episode_reward_mean: 0.17772658896242408\n",
      "  episode_reward_min: -0.9011718166339838\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 50187\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5813515447378158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01545682442933321\n",
      "          policy_loss: -0.09652850864082575\n",
      "          total_loss: 0.19476512780040503\n",
      "          vf_explained_var: 0.47186094522476196\n",
      "          vf_loss: 0.2678185834884644\n",
      "    num_agent_steps_sampled: 13554432\n",
      "    num_agent_steps_trained: 13554432\n",
      "    num_steps_sampled: 3388608\n",
      "    num_steps_trained: 3388608\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.47027027027027\n",
      "    gpu_util_percent0: 0.13635135135135135\n",
      "    ram_util_percent: 86.26891891891891\n",
      "    vram_util_percent0: 0.5199578897752639\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9387601272027686\n",
      "  policy_reward_mean:\n",
      "    main: 0.04443164724060602\n",
      "  policy_reward_min:\n",
      "    main: -1.8319920110868986\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3075093509343373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18745523397356\n",
      "    mean_inference_ms: 4.479910575734331\n",
      "    mean_raw_obs_processing_ms: 1.3341746726149455\n",
      "  time_since_restore: 49041.15521836281\n",
      "  time_this_iter_s: 114.42178678512573\n",
      "  time_total_s: 49041.15521836281\n",
      "  timers:\n",
      "    learn_throughput: 88.756\n",
      "    learn_time_ms: 90044.24\n",
      "    sample_throughput: 293.288\n",
      "    sample_time_ms: 27249.631\n",
      "    update_time_ms: 6.337\n",
      "  timestamp: 1639207284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3388608\n",
      "  training_iteration: 424\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         49041.2</td><td style=\"text-align: right;\">3388608</td><td style=\"text-align: right;\">0.177727</td><td style=\"text-align: right;\">             1.29733</td><td style=\"text-align: right;\">           -0.901172</td><td style=\"text-align: right;\">           68.6033</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13586400\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.2046495480350143\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.009374097840517813\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6728150777379319\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-23-18\n",
      "  done: false\n",
      "  episode_len_mean: 60.07936507936508\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.140543309706386\n",
      "  episode_reward_mean: 0.11184288518954837\n",
      "  episode_reward_min: -1.2334457558516583\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 50313\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.584447923541069\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015933091811835765\n",
      "          policy_loss: -0.0963703635931015\n",
      "          total_loss: 0.2227524189427495\n",
      "          vf_explained_var: 0.4395373463630676\n",
      "          vf_loss: 0.29492440074682236\n",
      "    num_agent_steps_sampled: 13586400\n",
      "    num_agent_steps_trained: 13586400\n",
      "    num_steps_sampled: 3396600\n",
      "    num_steps_trained: 3396600\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.31756756756757\n",
      "    gpu_util_percent0: 0.13614864864864867\n",
      "    ram_util_percent: 86.13851351351352\n",
      "    vram_util_percent0: 0.5192686516724031\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0898495716861376\n",
      "  policy_reward_mean:\n",
      "    main: 0.0279607212973871\n",
      "  policy_reward_min:\n",
      "    main: -1.8066289545261285\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30779632866941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.197353642539984\n",
      "    mean_inference_ms: 4.483741438181582\n",
      "    mean_raw_obs_processing_ms: 1.3345511407019872\n",
      "  time_since_restore: 49155.278753995895\n",
      "  time_this_iter_s: 114.12353563308716\n",
      "  time_total_s: 49155.278753995895\n",
      "  timers:\n",
      "    learn_throughput: 89.014\n",
      "    learn_time_ms: 89783.701\n",
      "    sample_throughput: 294.071\n",
      "    sample_time_ms: 27177.071\n",
      "    update_time_ms: 6.325\n",
      "  timestamp: 1639207398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3396600\n",
      "  training_iteration: 425\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         49155.3</td><td style=\"text-align: right;\">3396600</td><td style=\"text-align: right;\">0.111843</td><td style=\"text-align: right;\">             2.14054</td><td style=\"text-align: right;\">            -1.23345</td><td style=\"text-align: right;\">           60.0794</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13618368\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7807058028557583\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.006061915585290296\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7326701329242186\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 64.95275590551181\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4140680374654848\n",
      "  episode_reward_mean: 0.09543897496366315\n",
      "  episode_reward_min: -1.1164045436131302\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 50440\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5829163395166397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01588761907070875\n",
      "          policy_loss: -0.09830239818617702\n",
      "          total_loss: 0.21023637399822473\n",
      "          vf_explained_var: 0.46591588854789734\n",
      "          vf_loss: 0.28440945070981977\n",
      "    num_agent_steps_sampled: 13618368\n",
      "    num_agent_steps_trained: 13618368\n",
      "    num_steps_sampled: 3404592\n",
      "    num_steps_trained: 3404592\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.087755102040816\n",
      "    gpu_util_percent0: 0.1363945578231293\n",
      "    ram_util_percent: 86.19319727891157\n",
      "    vram_util_percent0: 0.5181517819368617\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9086264053445747\n",
      "  policy_reward_mean:\n",
      "    main: 0.023859743740915783\n",
      "  policy_reward_min:\n",
      "    main: -1.7326701329242185\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3076361565600615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.191063032445562\n",
      "    mean_inference_ms: 4.481127812363183\n",
      "    mean_raw_obs_processing_ms: 1.334434281628628\n",
      "  time_since_restore: 49268.76985955238\n",
      "  time_this_iter_s: 113.49110555648804\n",
      "  time_total_s: 49268.76985955238\n",
      "  timers:\n",
      "    learn_throughput: 89.35\n",
      "    learn_time_ms: 89446.455\n",
      "    sample_throughput: 294.642\n",
      "    sample_time_ms: 27124.481\n",
      "    update_time_ms: 6.341\n",
      "  timestamp: 1639207512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3404592\n",
      "  training_iteration: 426\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         49268.8</td><td style=\"text-align: right;\">3404592</td><td style=\"text-align: right;\">0.095439</td><td style=\"text-align: right;\">             1.41407</td><td style=\"text-align: right;\">             -1.1164</td><td style=\"text-align: right;\">           64.9528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13650336\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7285321318424883\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0868174485656152\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5533474936086513\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 66.66666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.671643829529697\n",
      "  episode_reward_mean: 0.22242432023388445\n",
      "  episode_reward_min: -1.27056780436636\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 50560\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5837785609960556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015702226020395754\n",
      "          policy_loss: -0.09760162908956409\n",
      "          total_loss: 0.2124644087329507\n",
      "          vf_explained_var: 0.4507245719432831\n",
      "          vf_loss: 0.28621828347444533\n",
      "    num_agent_steps_sampled: 13650336\n",
      "    num_agent_steps_trained: 13650336\n",
      "    num_steps_sampled: 3412584\n",
      "    num_steps_trained: 3412584\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.423648648648644\n",
      "    gpu_util_percent0: 0.13466216216216215\n",
      "    ram_util_percent: 86.4581081081081\n",
      "    vram_util_percent0: 0.5172943180100141\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9024419085041888\n",
      "  policy_reward_mean:\n",
      "    main: 0.05560608005847111\n",
      "  policy_reward_min:\n",
      "    main: -1.6831883973496764\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.307827639123615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19463901472693\n",
      "    mean_inference_ms: 4.482478529315148\n",
      "    mean_raw_obs_processing_ms: 1.3345590490438761\n",
      "  time_since_restore: 49382.82269644737\n",
      "  time_this_iter_s: 114.05283689498901\n",
      "  time_total_s: 49382.82269644737\n",
      "  timers:\n",
      "    learn_throughput: 89.538\n",
      "    learn_time_ms: 89258.356\n",
      "    sample_throughput: 295.196\n",
      "    sample_time_ms: 27073.513\n",
      "    update_time_ms: 6.072\n",
      "  timestamp: 1639207626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3412584\n",
      "  training_iteration: 427\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         49382.8</td><td style=\"text-align: right;\">3412584</td><td style=\"text-align: right;\">0.222424</td><td style=\"text-align: right;\">             1.67164</td><td style=\"text-align: right;\">            -1.27057</td><td style=\"text-align: right;\">           66.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13682304\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.778186993143789\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09834093532631266\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.36099728551961835\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-28-59\n",
      "  done: false\n",
      "  episode_len_mean: 64.968\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.362759973757866\n",
      "  episode_reward_mean: 0.18367177533639797\n",
      "  episode_reward_min: -1.447048961839369\n",
      "  episodes_this_iter: 125\n",
      "  episodes_total: 50685\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5842796434164047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015799807012081148\n",
      "          policy_loss: -0.09861601401865483\n",
      "          total_loss: 0.23595552796125413\n",
      "          vf_explained_var: 0.439319372177124\n",
      "          vf_loss: 0.3105755857825279\n",
      "    num_agent_steps_sampled: 13682304\n",
      "    num_agent_steps_trained: 13682304\n",
      "    num_steps_sampled: 3420576\n",
      "    num_steps_trained: 3420576\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.015068493150686\n",
      "    gpu_util_percent0: 0.1378767123287671\n",
      "    ram_util_percent: 86.34041095890409\n",
      "    vram_util_percent0: 0.5158138266462923\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.857899912297275\n",
      "  policy_reward_mean:\n",
      "    main: 0.04591794383409949\n",
      "  policy_reward_min:\n",
      "    main: -2.160159634129826\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30801016734142556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19970771312033\n",
      "    mean_inference_ms: 4.484350910694439\n",
      "    mean_raw_obs_processing_ms: 1.3346621637279559\n",
      "  time_since_restore: 49496.251979112625\n",
      "  time_this_iter_s: 113.42928266525269\n",
      "  time_total_s: 49496.251979112625\n",
      "  timers:\n",
      "    learn_throughput: 89.769\n",
      "    learn_time_ms: 89028.088\n",
      "    sample_throughput: 295.692\n",
      "    sample_time_ms: 27028.083\n",
      "    update_time_ms: 2.825\n",
      "  timestamp: 1639207739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3420576\n",
      "  training_iteration: 428\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         49496.3</td><td style=\"text-align: right;\">3420576</td><td style=\"text-align: right;\">0.183672</td><td style=\"text-align: right;\">             1.36276</td><td style=\"text-align: right;\">            -1.44705</td><td style=\"text-align: right;\">            64.968</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13714272\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9425093160329095\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07235610892462253\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5558174697887308\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-30-54\n",
      "  done: false\n",
      "  episode_len_mean: 68.62280701754386\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3907666873023605\n",
      "  episode_reward_mean: 0.158728423402932\n",
      "  episode_reward_min: -0.8573527935715699\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 50799\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5837393226623535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015668936379253866\n",
      "          policy_loss: -0.09820819499716163\n",
      "          total_loss: 0.20327209558337928\n",
      "          vf_explained_var: 0.44908562302589417\n",
      "          vf_loss: 0.2776830941438675\n",
      "    num_agent_steps_sampled: 13714272\n",
      "    num_agent_steps_trained: 13714272\n",
      "    num_steps_sampled: 3428568\n",
      "    num_steps_trained: 3428568\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.53557046979866\n",
      "    gpu_util_percent0: 0.13281879194630872\n",
      "    ram_util_percent: 86.46174496644295\n",
      "    vram_util_percent0: 0.5150592631362754\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9585157845202432\n",
      "  policy_reward_mean:\n",
      "    main: 0.03968210585073299\n",
      "  policy_reward_min:\n",
      "    main: -1.6843447268285012\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.307780144531068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19331222612895\n",
      "    mean_inference_ms: 4.481510422309509\n",
      "    mean_raw_obs_processing_ms: 1.3344641623919753\n",
      "  time_since_restore: 49610.73233175278\n",
      "  time_this_iter_s: 114.48035264015198\n",
      "  time_total_s: 49610.73233175278\n",
      "  timers:\n",
      "    learn_throughput: 89.995\n",
      "    learn_time_ms: 88805.399\n",
      "    sample_throughput: 295.681\n",
      "    sample_time_ms: 27029.097\n",
      "    update_time_ms: 2.728\n",
      "  timestamp: 1639207854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3428568\n",
      "  training_iteration: 429\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         49610.7</td><td style=\"text-align: right;\">3428568</td><td style=\"text-align: right;\">0.158728</td><td style=\"text-align: right;\">             1.39077</td><td style=\"text-align: right;\">           -0.857353</td><td style=\"text-align: right;\">           68.6228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13746240\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0232301103787182\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08366110140740401\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.3716562792914916\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-32-48\n",
      "  done: false\n",
      "  episode_len_mean: 62.86153846153846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6898790991521737\n",
      "  episode_reward_mean: 0.22411050195236454\n",
      "  episode_reward_min: -1.7085008547318445\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 50929\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.581789813041687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01568581950291991\n",
      "          policy_loss: -0.09830229604430497\n",
      "          total_loss: 0.24133513128757478\n",
      "          vf_explained_var: 0.44266197085380554\n",
      "          vf_loss: 0.3158145887255669\n",
      "    num_agent_steps_sampled: 13746240\n",
      "    num_agent_steps_trained: 13746240\n",
      "    num_steps_sampled: 3436560\n",
      "    num_steps_trained: 3436560\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.13333333333333\n",
      "    gpu_util_percent0: 0.13489795918367348\n",
      "    ram_util_percent: 86.5938775510204\n",
      "    vram_util_percent0: 0.5150235151645389\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9574889731952148\n",
      "  policy_reward_mean:\n",
      "    main: 0.056027625488091135\n",
      "  policy_reward_min:\n",
      "    main: -1.9027986394076712\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3074140178000509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.186139346361173\n",
      "    mean_inference_ms: 4.479159377797424\n",
      "    mean_raw_obs_processing_ms: 1.3342837145404403\n",
      "  time_since_restore: 49724.54972290993\n",
      "  time_this_iter_s: 113.81739115715027\n",
      "  time_total_s: 49724.54972290993\n",
      "  timers:\n",
      "    learn_throughput: 90.687\n",
      "    learn_time_ms: 88126.814\n",
      "    sample_throughput: 297.572\n",
      "    sample_time_ms: 26857.35\n",
      "    update_time_ms: 2.722\n",
      "  timestamp: 1639207968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3436560\n",
      "  training_iteration: 430\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         49724.5</td><td style=\"text-align: right;\">3436560</td><td style=\"text-align: right;\">0.224111</td><td style=\"text-align: right;\">             1.68988</td><td style=\"text-align: right;\">             -1.7085</td><td style=\"text-align: right;\">           62.8615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13778208\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6917122612756504\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0727566776015737\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -1.054219462604777\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 66.27586206896552\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2771396595691824\n",
      "  episode_reward_mean: 0.17587403033437896\n",
      "  episode_reward_min: -1.2258431282448417\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 51045\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5817527544498443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01564939369633794\n",
      "          policy_loss: -0.09674451710283756\n",
      "          total_loss: 0.19052302089333534\n",
      "          vf_explained_var: 0.492424875497818\n",
      "          vf_loss: 0.26350002247095106\n",
      "    num_agent_steps_sampled: 13778208\n",
      "    num_agent_steps_trained: 13778208\n",
      "    num_steps_sampled: 3444552\n",
      "    num_steps_trained: 3444552\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.10816326530612\n",
      "    gpu_util_percent0: 0.13598639455782313\n",
      "    ram_util_percent: 86.50204081632654\n",
      "    vram_util_percent0: 0.515025753638079\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9642882569883926\n",
      "  policy_reward_mean:\n",
      "    main: 0.04396850758359477\n",
      "  policy_reward_min:\n",
      "    main: -2.055622889220376\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.308721788985534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.212494885964496\n",
      "    mean_inference_ms: 4.488717784406538\n",
      "    mean_raw_obs_processing_ms: 1.3351167994923527\n",
      "  time_since_restore: 49838.51141452789\n",
      "  time_this_iter_s: 113.9616916179657\n",
      "  time_total_s: 49838.51141452789\n",
      "  timers:\n",
      "    learn_throughput: 90.901\n",
      "    learn_time_ms: 87919.857\n",
      "    sample_throughput: 299.719\n",
      "    sample_time_ms: 26664.951\n",
      "    update_time_ms: 2.744\n",
      "  timestamp: 1639208082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3444552\n",
      "  training_iteration: 431\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         49838.5</td><td style=\"text-align: right;\">3444552</td><td style=\"text-align: right;\">0.175874</td><td style=\"text-align: right;\">             1.27714</td><td style=\"text-align: right;\">            -1.22584</td><td style=\"text-align: right;\">           66.2759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13810176\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9933481108354825\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04751082576154472\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5884186280118189\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-36-37\n",
      "  done: false\n",
      "  episode_len_mean: 56.147887323943664\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5351600489225734\n",
      "  episode_reward_mean: 0.1592993730055849\n",
      "  episode_reward_min: -1.0531667286337019\n",
      "  episodes_this_iter: 142\n",
      "  episodes_total: 51187\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5795521492958069\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015656770549714566\n",
      "          policy_loss: -0.09875507133826614\n",
      "          total_loss: 0.25374045231193304\n",
      "          vf_explained_var: 0.4310239553451538\n",
      "          vf_loss: 0.32871680468320846\n",
      "    num_agent_steps_sampled: 13810176\n",
      "    num_agent_steps_trained: 13810176\n",
      "    num_steps_sampled: 3452544\n",
      "    num_steps_trained: 3452544\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.073154362416105\n",
      "    gpu_util_percent0: 0.15946308724832214\n",
      "    ram_util_percent: 86.58657718120806\n",
      "    vram_util_percent0: 0.5158476715450817\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8431224374020312\n",
      "  policy_reward_mean:\n",
      "    main: 0.039824843251396236\n",
      "  policy_reward_min:\n",
      "    main: -1.7860716278009794\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3078050185828799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.190433767916257\n",
      "    mean_inference_ms: 4.4802043334993185\n",
      "    mean_raw_obs_processing_ms: 1.3345357343751207\n",
      "  time_since_restore: 49954.17366814613\n",
      "  time_this_iter_s: 115.66225361824036\n",
      "  time_total_s: 49954.17366814613\n",
      "  timers:\n",
      "    learn_throughput: 90.973\n",
      "    learn_time_ms: 87850.225\n",
      "    sample_throughput: 300.212\n",
      "    sample_time_ms: 26621.143\n",
      "    update_time_ms: 2.692\n",
      "  timestamp: 1639208197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3452544\n",
      "  training_iteration: 432\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         49954.2</td><td style=\"text-align: right;\">3452544</td><td style=\"text-align: right;\">0.159299</td><td style=\"text-align: right;\">             1.53516</td><td style=\"text-align: right;\">            -1.05317</td><td style=\"text-align: right;\">           56.1479</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13842144\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.842625293443657\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05736201419450754\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8266031982323889\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 60.0863309352518\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.283985533106864\n",
      "  episode_reward_mean: 0.05991506211962911\n",
      "  episode_reward_min: -1.0451527700442287\n",
      "  episodes_this_iter: 139\n",
      "  episodes_total: 51326\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5879793571233749\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015733106210827828\n",
      "          policy_loss: -0.09897991197928786\n",
      "          total_loss: 0.20474290877580642\n",
      "          vf_explained_var: 0.5050251483917236\n",
      "          vf_loss: 0.2798281673789024\n",
      "    num_agent_steps_sampled: 13842144\n",
      "    num_agent_steps_trained: 13842144\n",
      "    num_steps_sampled: 3460536\n",
      "    num_steps_trained: 3460536\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.60933333333333\n",
      "    gpu_util_percent0: 0.1794\n",
      "    ram_util_percent: 86.57866666666669\n",
      "    vram_util_percent0: 0.5161324997257871\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8460677597984285\n",
      "  policy_reward_mean:\n",
      "    main: 0.014978765529907281\n",
      "  policy_reward_min:\n",
      "    main: -1.6121701038398608\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3080472207621783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19751388143978\n",
      "    mean_inference_ms: 4.48290111484396\n",
      "    mean_raw_obs_processing_ms: 1.3349117218572768\n",
      "  time_since_restore: 50070.52415275574\n",
      "  time_this_iter_s: 116.35048460960388\n",
      "  time_total_s: 50070.52415275574\n",
      "  timers:\n",
      "    learn_throughput: 91.377\n",
      "    learn_time_ms: 87461.948\n",
      "    sample_throughput: 300.702\n",
      "    sample_time_ms: 26577.773\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1639208314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3460536\n",
      "  training_iteration: 433\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         50070.5</td><td style=\"text-align: right;\">3460536</td><td style=\"text-align: right;\">0.0599151</td><td style=\"text-align: right;\">             1.28399</td><td style=\"text-align: right;\">            -1.04515</td><td style=\"text-align: right;\">           60.0863</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13874112\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8663022405475768\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09043733970211203\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5112636194817056\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 72.79439252336448\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7193860153837655\n",
      "  episode_reward_mean: 0.26038129062314896\n",
      "  episode_reward_min: -1.4216460223859546\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 51433\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5802374676465988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015386359434574842\n",
      "          policy_loss: -0.09660970675945282\n",
      "          total_loss: 0.2071155394613743\n",
      "          vf_explained_var: 0.44706347584724426\n",
      "          vf_loss: 0.2803572129011154\n",
      "    num_agent_steps_sampled: 13874112\n",
      "    num_agent_steps_trained: 13874112\n",
      "    num_steps_sampled: 3468528\n",
      "    num_steps_trained: 3468528\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.103973509933766\n",
      "    gpu_util_percent0: 0.17874172185430465\n",
      "    ram_util_percent: 86.58543046357617\n",
      "    vram_util_percent0: 0.5161237249095099\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1711627231259216\n",
      "  policy_reward_mean:\n",
      "    main: 0.06509532265578725\n",
      "  policy_reward_min:\n",
      "    main: -1.7440502696944862\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30781759340116943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.18792614940365\n",
      "    mean_inference_ms: 4.479104925551208\n",
      "    mean_raw_obs_processing_ms: 1.3345127999257542\n",
      "  time_since_restore: 50187.31828832626\n",
      "  time_this_iter_s: 116.79413557052612\n",
      "  time_total_s: 50187.31828832626\n",
      "  timers:\n",
      "    learn_throughput: 91.171\n",
      "    learn_time_ms: 87658.991\n",
      "    sample_throughput: 300.77\n",
      "    sample_time_ms: 26571.777\n",
      "    update_time_ms: 2.673\n",
      "  timestamp: 1639208431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3468528\n",
      "  training_iteration: 434\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         50187.3</td><td style=\"text-align: right;\">3468528</td><td style=\"text-align: right;\">0.260381</td><td style=\"text-align: right;\">             1.71939</td><td style=\"text-align: right;\">            -1.42165</td><td style=\"text-align: right;\">           72.7944</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13906080\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6991493531101869\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07514536659024666\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4857504398754306\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 59.11594202898551\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.651812173844308\n",
      "  episode_reward_mean: 0.2026317121681567\n",
      "  episode_reward_min: -0.8017845027469366\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 51571\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5849000757932663\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01584746816009283\n",
      "          policy_loss: -0.09963119046017528\n",
      "          total_loss: 0.21410786352306604\n",
      "          vf_explained_var: 0.49595409631729126\n",
      "          vf_loss: 0.2896707140207291\n",
      "    num_agent_steps_sampled: 13906080\n",
      "    num_agent_steps_trained: 13906080\n",
      "    num_steps_sampled: 3476520\n",
      "    num_steps_trained: 3476520\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.85460526315789\n",
      "    gpu_util_percent0: 0.18585526315789472\n",
      "    ram_util_percent: 86.75197368421054\n",
      "    vram_util_percent0: 0.5163921650127293\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.052564070737625\n",
      "  policy_reward_mean:\n",
      "    main: 0.05065792804203919\n",
      "  policy_reward_min:\n",
      "    main: -1.6471606090926096\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30842504789286207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.204489395559634\n",
      "    mean_inference_ms: 4.485516690908338\n",
      "    mean_raw_obs_processing_ms: 1.3352021755203036\n",
      "  time_since_restore: 50305.28556728363\n",
      "  time_this_iter_s: 117.96727895736694\n",
      "  time_total_s: 50305.28556728363\n",
      "  timers:\n",
      "    learn_throughput: 90.803\n",
      "    learn_time_ms: 88014.951\n",
      "    sample_throughput: 300.445\n",
      "    sample_time_ms: 26600.559\n",
      "    update_time_ms: 2.63\n",
      "  timestamp: 1639208549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3476520\n",
      "  training_iteration: 435\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         50305.3</td><td style=\"text-align: right;\">3476520</td><td style=\"text-align: right;\">0.202632</td><td style=\"text-align: right;\">             1.65181</td><td style=\"text-align: right;\">           -0.801785</td><td style=\"text-align: right;\">           59.1159</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13938048\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.66939357886879\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05725707976847759\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.41969601896723774\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-44-25\n",
      "  done: false\n",
      "  episode_len_mean: 64.71544715447155\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.919311974107331\n",
      "  episode_reward_mean: 0.20182859049278395\n",
      "  episode_reward_min: -1.1037252428300692\n",
      "  episodes_this_iter: 123\n",
      "  episodes_total: 51694\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5877677446603775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015359074287116527\n",
      "          policy_loss: -0.09508164111897349\n",
      "          total_loss: 0.18981743049621583\n",
      "          vf_explained_var: 0.5049220323562622\n",
      "          vf_loss: 0.2615724794268608\n",
      "    num_agent_steps_sampled: 13938048\n",
      "    num_agent_steps_trained: 13938048\n",
      "    num_steps_sampled: 3484512\n",
      "    num_steps_trained: 3484512\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.98476821192053\n",
      "    gpu_util_percent0: 0.17754966887417215\n",
      "    ram_util_percent: 86.84172185430462\n",
      "    vram_util_percent0: 0.5126828056458098\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9263898375835407\n",
      "  policy_reward_mean:\n",
      "    main: 0.05045714762319599\n",
      "  policy_reward_min:\n",
      "    main: -1.5849604829456447\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3080324142215473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.194213122693558\n",
      "    mean_inference_ms: 4.4810207100841515\n",
      "    mean_raw_obs_processing_ms: 1.3348973110399036\n",
      "  time_since_restore: 50421.745453834534\n",
      "  time_this_iter_s: 116.45988655090332\n",
      "  time_total_s: 50421.745453834534\n",
      "  timers:\n",
      "    learn_throughput: 90.568\n",
      "    learn_time_ms: 88243.376\n",
      "    sample_throughput: 299.659\n",
      "    sample_time_ms: 26670.313\n",
      "    update_time_ms: 2.597\n",
      "  timestamp: 1639208665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3484512\n",
      "  training_iteration: 436\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         50421.7</td><td style=\"text-align: right;\">3484512</td><td style=\"text-align: right;\">0.201829</td><td style=\"text-align: right;\">             1.91931</td><td style=\"text-align: right;\">            -1.10373</td><td style=\"text-align: right;\">           64.7154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 13970016\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8261964319513646\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07219941880414509\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5028953964245775\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 59.64393939393939\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2757409274597076\n",
      "  episode_reward_mean: 0.18245861210406958\n",
      "  episode_reward_min: -1.0773500487853054\n",
      "  episodes_this_iter: 132\n",
      "  episodes_total: 51826\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5842102113962173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015747994273900986\n",
      "          policy_loss: -0.09739879728108644\n",
      "          total_loss: 0.22555305355787278\n",
      "          vf_explained_var: 0.4612162113189697\n",
      "          vf_loss: 0.29903458362817764\n",
      "    num_agent_steps_sampled: 13970016\n",
      "    num_agent_steps_trained: 13970016\n",
      "    num_steps_sampled: 3492504\n",
      "    num_steps_trained: 3492504\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.07533333333333\n",
      "    gpu_util_percent0: 0.1794\n",
      "    ram_util_percent: 86.86666666666666\n",
      "    vram_util_percent0: 0.509880443128222\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8194859670865795\n",
      "  policy_reward_mean:\n",
      "    main: 0.045614653026017395\n",
      "  policy_reward_min:\n",
      "    main: -1.7229675950431724\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30818934861932484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.196994814123993\n",
      "    mean_inference_ms: 4.48184317682674\n",
      "    mean_raw_obs_processing_ms: 1.3351442003270837\n",
      "  time_since_restore: 50537.90694689751\n",
      "  time_this_iter_s: 116.16149306297302\n",
      "  time_total_s: 50537.90694689751\n",
      "  timers:\n",
      "    learn_throughput: 90.384\n",
      "    learn_time_ms: 88422.427\n",
      "    sample_throughput: 299.259\n",
      "    sample_time_ms: 26705.931\n",
      "    update_time_ms: 2.639\n",
      "  timestamp: 1639208782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3492504\n",
      "  training_iteration: 437\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         50537.9</td><td style=\"text-align: right;\">3492504</td><td style=\"text-align: right;\">0.182459</td><td style=\"text-align: right;\">             1.27574</td><td style=\"text-align: right;\">            -1.07735</td><td style=\"text-align: right;\">           59.6439</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14001984\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8706868359654502\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07239745109532247\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5762043424062347\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 65.20967741935483\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4586709407186573\n",
      "  episode_reward_mean: 0.14328988182926905\n",
      "  episode_reward_min: -1.1893167926984773\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 51950\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.581071645617485\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01559009065106511\n",
      "          policy_loss: -0.09631331010907888\n",
      "          total_loss: 0.21318089086562395\n",
      "          vf_explained_var: 0.4618823230266571\n",
      "          vf_loss: 0.28581675231456755\n",
      "    num_agent_steps_sampled: 14001984\n",
      "    num_agent_steps_trained: 14001984\n",
      "    num_steps_sampled: 3500496\n",
      "    num_steps_trained: 3500496\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.07866666666666\n",
      "    gpu_util_percent0: 0.17746666666666666\n",
      "    ram_util_percent: 86.77133333333332\n",
      "    vram_util_percent0: 0.5098957990567075\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9331194265629534\n",
      "  policy_reward_mean:\n",
      "    main: 0.03582247045731727\n",
      "  policy_reward_min:\n",
      "    main: -1.6223302547817304\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3079243642017598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.198024775847138\n",
      "    mean_inference_ms: 4.482928910491922\n",
      "    mean_raw_obs_processing_ms: 1.3350828519851503\n",
      "  time_since_restore: 50654.37141561508\n",
      "  time_this_iter_s: 116.46446871757507\n",
      "  time_total_s: 50654.37141561508\n",
      "  timers:\n",
      "    learn_throughput: 90.145\n",
      "    learn_time_ms: 88657.001\n",
      "    sample_throughput: 298.549\n",
      "    sample_time_ms: 26769.488\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1639208898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3500496\n",
      "  training_iteration: 438\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         50654.4</td><td style=\"text-align: right;\">3500496</td><td style=\"text-align: right;\"> 0.14329</td><td style=\"text-align: right;\">             1.45867</td><td style=\"text-align: right;\">            -1.18932</td><td style=\"text-align: right;\">           65.2097</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14033952\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.7961882832753708\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07404261456764052\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8191883078608337\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-50-14\n",
      "  done: false\n",
      "  episode_len_mean: 64.05555555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.3611757752978928\n",
      "  episode_reward_mean: 0.20458500491566456\n",
      "  episode_reward_min: -1.0571020733478176\n",
      "  episodes_this_iter: 126\n",
      "  episodes_total: 52076\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5839290236234664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015374670337885619\n",
      "          policy_loss: -0.09731400657445193\n",
      "          total_loss: 0.21748654134571552\n",
      "          vf_explained_var: 0.4695097804069519\n",
      "          vf_loss: 0.29145026934146884\n",
      "    num_agent_steps_sampled: 14033952\n",
      "    num_agent_steps_trained: 14033952\n",
      "    num_steps_sampled: 3508488\n",
      "    num_steps_trained: 3508488\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.584\n",
      "    gpu_util_percent0: 0.17953333333333332\n",
      "    ram_util_percent: 86.88533333333332\n",
      "    vram_util_percent0: 0.5094921575079523\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.096363693963599\n",
      "  policy_reward_mean:\n",
      "    main: 0.05114625122891614\n",
      "  policy_reward_min:\n",
      "    main: -1.8088580081617733\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3079160902532949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.196043228751492\n",
      "    mean_inference_ms: 4.481717728493831\n",
      "    mean_raw_obs_processing_ms: 1.3351022861332922\n",
      "  time_since_restore: 50770.017260074615\n",
      "  time_this_iter_s: 115.64584445953369\n",
      "  time_total_s: 50770.017260074615\n",
      "  timers:\n",
      "    learn_throughput: 90.02\n",
      "    learn_time_ms: 88780.279\n",
      "    sample_throughput: 298.636\n",
      "    sample_time_ms: 26761.632\n",
      "    update_time_ms: 2.639\n",
      "  timestamp: 1639209014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3508488\n",
      "  training_iteration: 439\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">           50770</td><td style=\"text-align: right;\">3508488</td><td style=\"text-align: right;\">0.204585</td><td style=\"text-align: right;\">             1.36118</td><td style=\"text-align: right;\">             -1.0571</td><td style=\"text-align: right;\">           64.0556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14065920\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.234326889924167\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0806185883719284\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7008128255335118\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-52-11\n",
      "  done: false\n",
      "  episode_len_mean: 57.94074074074074\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.457233103595799\n",
      "  episode_reward_mean: 0.18522320643419077\n",
      "  episode_reward_min: -0.8095689465472553\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 52211\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5781569967269897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01563145413249731\n",
      "          policy_loss: -0.09812016317620874\n",
      "          total_loss: 0.22668019659817218\n",
      "          vf_explained_var: 0.47405511140823364\n",
      "          vf_loss: 0.30106009149551394\n",
      "    num_agent_steps_sampled: 14065920\n",
      "    num_agent_steps_trained: 14065920\n",
      "    num_steps_sampled: 3516480\n",
      "    num_steps_trained: 3516480\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.32866666666667\n",
      "    gpu_util_percent0: 0.18206666666666668\n",
      "    ram_util_percent: 87.05266666666668\n",
      "    vram_util_percent0: 0.5088910825929582\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.1548280911574067\n",
      "  policy_reward_mean:\n",
      "    main: 0.04630580160854768\n",
      "  policy_reward_min:\n",
      "    main: -1.7008128255335118\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30836552106921244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.205971656633686\n",
      "    mean_inference_ms: 4.485284511746356\n",
      "    mean_raw_obs_processing_ms: 1.3356735320366768\n",
      "  time_since_restore: 50886.931743621826\n",
      "  time_this_iter_s: 116.9144835472107\n",
      "  time_total_s: 50886.931743621826\n",
      "  timers:\n",
      "    learn_throughput: 89.8\n",
      "    learn_time_ms: 88997.463\n",
      "    sample_throughput: 297.555\n",
      "    sample_time_ms: 26858.9\n",
      "    update_time_ms: 2.663\n",
      "  timestamp: 1639209131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3516480\n",
      "  training_iteration: 440\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         50886.9</td><td style=\"text-align: right;\">3516480</td><td style=\"text-align: right;\">0.185223</td><td style=\"text-align: right;\">             1.45723</td><td style=\"text-align: right;\">           -0.809569</td><td style=\"text-align: right;\">           57.9407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14097888\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.879469251425583\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06788717910818201\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5598904126797671\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 59.953125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.538609750679301\n",
      "  episode_reward_mean: 0.127961035992399\n",
      "  episode_reward_min: -1.1398130170936778\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 52339\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5808554091453553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015743688348680736\n",
      "          policy_loss: -0.0965289917588234\n",
      "          total_loss: 0.22737386932969092\n",
      "          vf_explained_var: 0.46088290214538574\n",
      "          vf_loss: 0.2999921346306801\n",
      "    num_agent_steps_sampled: 14097888\n",
      "    num_agent_steps_trained: 14097888\n",
      "    num_steps_sampled: 3524472\n",
      "    num_steps_trained: 3524472\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.95733333333333\n",
      "    gpu_util_percent0: 0.17906666666666665\n",
      "    ram_util_percent: 87.03\n",
      "    vram_util_percent0: 0.5089020511133049\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.8384284823818124\n",
      "  policy_reward_mean:\n",
      "    main: 0.03199025899809975\n",
      "  policy_reward_min:\n",
      "    main: -1.6696770807104286\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3083043799445115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.204570682204626\n",
      "    mean_inference_ms: 4.484605801190957\n",
      "    mean_raw_obs_processing_ms: 1.3357737112183166\n",
      "  time_since_restore: 51003.3456993103\n",
      "  time_this_iter_s: 116.41395568847656\n",
      "  time_total_s: 51003.3456993103\n",
      "  timers:\n",
      "    learn_throughput: 89.594\n",
      "    learn_time_ms: 89201.991\n",
      "    sample_throughput: 297.058\n",
      "    sample_time_ms: 26903.847\n",
      "    update_time_ms: 2.684\n",
      "  timestamp: 1639209247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3524472\n",
      "  training_iteration: 441\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         51003.3</td><td style=\"text-align: right;\">3524472</td><td style=\"text-align: right;\">0.127961</td><td style=\"text-align: right;\">             1.53861</td><td style=\"text-align: right;\">            -1.13981</td><td style=\"text-align: right;\">           59.9531</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14129856\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8237576824813458\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.008879772356791469\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5352178259873563\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 63.71653543307087\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1910554080904037\n",
      "  episode_reward_mean: 0.06646192124485335\n",
      "  episode_reward_min: -1.2497332400933603\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 52466\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5840917810201645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015796825855970382\n",
      "          policy_loss: -0.09747552252933384\n",
      "          total_loss: 0.2231915697157383\n",
      "          vf_explained_var: 0.42650800943374634\n",
      "          vf_loss: 0.2966756621003151\n",
      "    num_agent_steps_sampled: 14129856\n",
      "    num_agent_steps_trained: 14129856\n",
      "    num_steps_sampled: 3532464\n",
      "    num_steps_trained: 3532464\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.69006622516556\n",
      "    gpu_util_percent0: 0.1770860927152318\n",
      "    ram_util_percent: 87.14834437086094\n",
      "    vram_util_percent0: 0.5089150099479396\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0280703009242163\n",
      "  policy_reward_mean:\n",
      "    main: 0.016615480311213356\n",
      "  policy_reward_min:\n",
      "    main: -2.0299487650064254\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3082499244298013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.206931295546852\n",
      "    mean_inference_ms: 4.486194730698903\n",
      "    mean_raw_obs_processing_ms: 1.3358408544043536\n",
      "  time_since_restore: 51119.714862823486\n",
      "  time_this_iter_s: 116.3691635131836\n",
      "  time_total_s: 51119.714862823486\n",
      "  timers:\n",
      "    learn_throughput: 89.512\n",
      "    learn_time_ms: 89284.285\n",
      "    sample_throughput: 297.194\n",
      "    sample_time_ms: 26891.551\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1639209364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3532464\n",
      "  training_iteration: 442\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         51119.7</td><td style=\"text-align: right;\">3532464</td><td style=\"text-align: right;\">0.0664619</td><td style=\"text-align: right;\">             1.19106</td><td style=\"text-align: right;\">            -1.24973</td><td style=\"text-align: right;\">           63.7165</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14161824\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9849535167913345\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.07857635253943016\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6746352763558898\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 63.48461538461538\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.2630936286507475\n",
      "  episode_reward_mean: 0.1280945994422293\n",
      "  episode_reward_min: -1.1533937095905515\n",
      "  episodes_this_iter: 130\n",
      "  episodes_total: 52596\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5785082548856735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01581781033053994\n",
      "          policy_loss: -0.09836964645981788\n",
      "          total_loss: 0.2103835674226284\n",
      "          vf_explained_var: 0.45851898193359375\n",
      "          vf_loss: 0.28472991490364075\n",
      "    num_agent_steps_sampled: 14161824\n",
      "    num_agent_steps_trained: 14161824\n",
      "    num_steps_sampled: 3540456\n",
      "    num_steps_trained: 3540456\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.014\n",
      "    gpu_util_percent0: 0.17866666666666667\n",
      "    ram_util_percent: 87.16466666666668\n",
      "    vram_util_percent0: 0.5090292859493255\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0086936781472446\n",
      "  policy_reward_mean:\n",
      "    main: 0.03202364986055734\n",
      "  policy_reward_min:\n",
      "    main: -1.678053637104651\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3080179867449482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.200583513786164\n",
      "    mean_inference_ms: 4.483523047891247\n",
      "    mean_raw_obs_processing_ms: 1.335804635974292\n",
      "  time_since_restore: 51235.90639948845\n",
      "  time_this_iter_s: 116.19153666496277\n",
      "  time_total_s: 51235.90639948845\n",
      "  timers:\n",
      "    learn_throughput: 89.527\n",
      "    learn_time_ms: 89269.106\n",
      "    sample_throughput: 297.257\n",
      "    sample_time_ms: 26885.828\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1639209480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3540456\n",
      "  training_iteration: 443\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         51235.9</td><td style=\"text-align: right;\">3540456</td><td style=\"text-align: right;\">0.128095</td><td style=\"text-align: right;\">             1.26309</td><td style=\"text-align: right;\">            -1.15339</td><td style=\"text-align: right;\">           63.4846</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14193792\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.9770517441540005\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.08669769610416818\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5919131031580547\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_04-59-56\n",
      "  done: false\n",
      "  episode_len_mean: 65.77310924369748\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4009680630341905\n",
      "  episode_reward_mean: 0.15487112062377933\n",
      "  episode_reward_min: -1.153806903087569\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 52715\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5857247617244721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015911118533462287\n",
      "          policy_loss: -0.0996393880136311\n",
      "          total_loss: 0.2203196504712105\n",
      "          vf_explained_var: 0.43789294362068176\n",
      "          vf_loss: 0.2957940278053284\n",
      "    num_agent_steps_sampled: 14193792\n",
      "    num_agent_steps_trained: 14193792\n",
      "    num_steps_sampled: 3548448\n",
      "    num_steps_trained: 3548448\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.07986577181208\n",
      "    gpu_util_percent0: 0.17771812080536917\n",
      "    ram_util_percent: 87.23758389261744\n",
      "    vram_util_percent0: 0.5099279831982881\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9126519973444316\n",
      "  policy_reward_mean:\n",
      "    main: 0.03871778015594486\n",
      "  policy_reward_min:\n",
      "    main: -1.8022985141654662\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30798948230969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.200427620122152\n",
      "    mean_inference_ms: 4.48364492982854\n",
      "    mean_raw_obs_processing_ms: 1.3357785283835124\n",
      "  time_since_restore: 51351.51858711243\n",
      "  time_this_iter_s: 115.61218762397766\n",
      "  time_total_s: 51351.51858711243\n",
      "  timers:\n",
      "    learn_throughput: 89.611\n",
      "    learn_time_ms: 89185.042\n",
      "    sample_throughput: 297.169\n",
      "    sample_time_ms: 26893.783\n",
      "    update_time_ms: 2.658\n",
      "  timestamp: 1639209596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3548448\n",
      "  training_iteration: 444\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         51351.5</td><td style=\"text-align: right;\">3548448</td><td style=\"text-align: right;\">0.154871</td><td style=\"text-align: right;\">             1.40097</td><td style=\"text-align: right;\">            -1.15381</td><td style=\"text-align: right;\">           65.7731</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14225760\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.012610861861842\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.13397976284011767\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4806460759747835\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 58.87323943661972\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.539632728768624\n",
      "  episode_reward_mean: 0.18494898038697746\n",
      "  episode_reward_min: -1.1201557927587245\n",
      "  episodes_this_iter: 142\n",
      "  episodes_total: 52857\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5762273287773132\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016128010980784892\n",
      "          policy_loss: -0.09927862341701985\n",
      "          total_loss: 0.2507628087177873\n",
      "          vf_explained_var: 0.4521743059158325\n",
      "          vf_loss: 0.3255470187664032\n",
      "    num_agent_steps_sampled: 14225760\n",
      "    num_agent_steps_trained: 14225760\n",
      "    num_steps_sampled: 3556440\n",
      "    num_steps_trained: 3556440\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.420529801324506\n",
      "    gpu_util_percent0: 0.17927152317880796\n",
      "    ram_util_percent: 87.3569536423841\n",
      "    vram_util_percent0: 0.5099250581295259\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0799838183092536\n",
      "  policy_reward_mean:\n",
      "    main: 0.04623724509674435\n",
      "  policy_reward_min:\n",
      "    main: -1.7071894490478465\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30854099038734584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.21153463028867\n",
      "    mean_inference_ms: 4.487738162311414\n",
      "    mean_raw_obs_processing_ms: 1.3363708419857057\n",
      "  time_since_restore: 51468.60172700882\n",
      "  time_this_iter_s: 117.08313989639282\n",
      "  time_total_s: 51468.60172700882\n",
      "  timers:\n",
      "    learn_throughput: 89.696\n",
      "    learn_time_ms: 89101.301\n",
      "    sample_throughput: 297.224\n",
      "    sample_time_ms: 26888.84\n",
      "    update_time_ms: 2.669\n",
      "  timestamp: 1639209713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3556440\n",
      "  training_iteration: 445\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         51468.6</td><td style=\"text-align: right;\">3556440</td><td style=\"text-align: right;\">0.184949</td><td style=\"text-align: right;\">             1.53963</td><td style=\"text-align: right;\">            -1.12016</td><td style=\"text-align: right;\">           58.8732</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14257728\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.6821937900244369\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04251557576884394\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5137871947829924\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 59.91338582677165\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.320385343046893\n",
      "  episode_reward_mean: 0.13078573161708518\n",
      "  episode_reward_min: -0.7056899561378622\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 52984\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5807379972934723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015747322481125592\n",
      "          policy_loss: -0.09825994482636452\n",
      "          total_loss: 0.22618740166723728\n",
      "          vf_explained_var: 0.4339844286441803\n",
      "          vf_loss: 0.30053110271692274\n",
      "    num_agent_steps_sampled: 14257728\n",
      "    num_agent_steps_trained: 14257728\n",
      "    num_steps_sampled: 3564432\n",
      "    num_steps_trained: 3564432\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.79139072847682\n",
      "    gpu_util_percent0: 0.17940397350993378\n",
      "    ram_util_percent: 87.39072847682118\n",
      "    vram_util_percent0: 0.509911983072159\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7904548318590763\n",
      "  policy_reward_mean:\n",
      "    main: 0.032696432904271266\n",
      "  policy_reward_min:\n",
      "    main: -1.585885542864471\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30815952365901944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.207145676239808\n",
      "    mean_inference_ms: 4.486752548383437\n",
      "    mean_raw_obs_processing_ms: 1.33633124490736\n",
      "  time_since_restore: 51584.99018955231\n",
      "  time_this_iter_s: 116.38846254348755\n",
      "  time_total_s: 51584.99018955231\n",
      "  timers:\n",
      "    learn_throughput: 89.688\n",
      "    learn_time_ms: 89109.102\n",
      "    sample_throughput: 297.448\n",
      "    sample_time_ms: 26868.6\n",
      "    update_time_ms: 2.652\n",
      "  timestamp: 1639209829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3564432\n",
      "  training_iteration: 446\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">           51585</td><td style=\"text-align: right;\">3564432</td><td style=\"text-align: right;\">0.130786</td><td style=\"text-align: right;\">             1.32039</td><td style=\"text-align: right;\">            -0.70569</td><td style=\"text-align: right;\">           59.9134</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14289696\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.950159334996598\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10685365556978109\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5320025824477144\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-05-45\n",
      "  done: false\n",
      "  episode_len_mean: 66.8099173553719\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.737610721494314\n",
      "  episode_reward_mean: 0.20074492268423044\n",
      "  episode_reward_min: -0.8811104896220399\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 53105\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5825502835512161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016221390929073095\n",
      "          policy_loss: -0.10060976986587047\n",
      "          total_loss: 0.22629169280081987\n",
      "          vf_explained_var: 0.4387385845184326\n",
      "          vf_loss: 0.30226522773504255\n",
      "    num_agent_steps_sampled: 14289696\n",
      "    num_agent_steps_trained: 14289696\n",
      "    num_steps_sampled: 3572424\n",
      "    num_steps_trained: 3572424\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.3744966442953\n",
      "    gpu_util_percent0: 0.17885906040268457\n",
      "    ram_util_percent: 87.41610738255034\n",
      "    vram_util_percent0: 0.5092632466967456\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.069893572950071\n",
      "  policy_reward_mean:\n",
      "    main: 0.050186230671057604\n",
      "  policy_reward_min:\n",
      "    main: -1.7870558413857238\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30808970420685045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19947172644719\n",
      "    mean_inference_ms: 4.483164578766749\n",
      "    mean_raw_obs_processing_ms: 1.3360851440794672\n",
      "  time_since_restore: 51700.415417432785\n",
      "  time_this_iter_s: 115.4252278804779\n",
      "  time_total_s: 51700.415417432785\n",
      "  timers:\n",
      "    learn_throughput: 89.693\n",
      "    learn_time_ms: 89103.486\n",
      "    sample_throughput: 298.253\n",
      "    sample_time_ms: 26796.064\n",
      "    update_time_ms: 2.599\n",
      "  timestamp: 1639209945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3572424\n",
      "  training_iteration: 447\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         51700.4</td><td style=\"text-align: right;\">3572424</td><td style=\"text-align: right;\">0.200745</td><td style=\"text-align: right;\">             1.73761</td><td style=\"text-align: right;\">            -0.88111</td><td style=\"text-align: right;\">           66.8099</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14321664\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.2208838156397885\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10498212841286371\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5714286726319286\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 65.68333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.6582699508414929\n",
      "  episode_reward_mean: 0.20310042397678005\n",
      "  episode_reward_min: -1.157404621870032\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 53225\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5792482249736786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01578104056045413\n",
      "          policy_loss: -0.09875513764098287\n",
      "          total_loss: 0.20262322636693716\n",
      "          vf_explained_var: 0.44389230012893677\n",
      "          vf_loss: 0.27741090720891953\n",
      "    num_agent_steps_sampled: 14321664\n",
      "    num_agent_steps_trained: 14321664\n",
      "    num_steps_sampled: 3580416\n",
      "    num_steps_trained: 3580416\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.598013245033115\n",
      "    gpu_util_percent0: 0.17496688741721855\n",
      "    ram_util_percent: 87.46490066225167\n",
      "    vram_util_percent0: 0.5089455184151288\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.108161052633619\n",
      "  policy_reward_mean:\n",
      "    main: 0.05077510599419499\n",
      "  policy_reward_min:\n",
      "    main: -1.637556664485733\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30775067297785824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.197699212022673\n",
      "    mean_inference_ms: 4.483323288809392\n",
      "    mean_raw_obs_processing_ms: 1.3360484253552378\n",
      "  time_since_restore: 51817.299150943756\n",
      "  time_this_iter_s: 116.88373351097107\n",
      "  time_total_s: 51817.299150943756\n",
      "  timers:\n",
      "    learn_throughput: 89.641\n",
      "    learn_time_ms: 89155.288\n",
      "    sample_throughput: 298.354\n",
      "    sample_time_ms: 26786.951\n",
      "    update_time_ms: 2.636\n",
      "  timestamp: 1639210062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3580416\n",
      "  training_iteration: 448\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         51817.3</td><td style=\"text-align: right;\">3580416</td><td style=\"text-align: right;\">  0.2031</td><td style=\"text-align: right;\">             1.65827</td><td style=\"text-align: right;\">             -1.1574</td><td style=\"text-align: right;\">           65.6833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14353632\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8208337307672462\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.04655434024498716\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.8630243202201007\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-09-38\n",
      "  done: false\n",
      "  episode_len_mean: 65.74166666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.875381450877865\n",
      "  episode_reward_mean: 0.19403262424875095\n",
      "  episode_reward_min: -0.9910122110870321\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 53345\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5861293475627899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015884008925408123\n",
      "          policy_loss: -0.09816401398554445\n",
      "          total_loss: 0.20605822607129812\n",
      "          vf_explained_var: 0.43701672554016113\n",
      "          vf_loss: 0.28009840309619904\n",
      "    num_agent_steps_sampled: 14353632\n",
      "    num_agent_steps_trained: 14353632\n",
      "    num_steps_sampled: 3588408\n",
      "    num_steps_trained: 3588408\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.785333333333334\n",
      "    gpu_util_percent0: 0.17953333333333332\n",
      "    ram_util_percent: 87.55533333333332\n",
      "    vram_util_percent0: 0.508927278710102\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0970693183201137\n",
      "  policy_reward_mean:\n",
      "    main: 0.04850815606218774\n",
      "  policy_reward_min:\n",
      "    main: -1.868638221723167\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30813719015319035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.20313781868379\n",
      "    mean_inference_ms: 4.484696298163783\n",
      "    mean_raw_obs_processing_ms: 1.3362134447221146\n",
      "  time_since_restore: 51933.18635034561\n",
      "  time_this_iter_s: 115.88719940185547\n",
      "  time_total_s: 51933.18635034561\n",
      "  timers:\n",
      "    learn_throughput: 89.595\n",
      "    learn_time_ms: 89201.746\n",
      "    sample_throughput: 298.591\n",
      "    sample_time_ms: 26765.746\n",
      "    update_time_ms: 2.66\n",
      "  timestamp: 1639210178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3588408\n",
      "  training_iteration: 449\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         51933.2</td><td style=\"text-align: right;\">3588408</td><td style=\"text-align: right;\">0.194033</td><td style=\"text-align: right;\">             1.87538</td><td style=\"text-align: right;\">           -0.991012</td><td style=\"text-align: right;\">           65.7417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14385600\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8697856954745454\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.09206304789036969\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5104416741960461\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-11-33\n",
      "  done: false\n",
      "  episode_len_mean: 69.67768595041322\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.153467818852701\n",
      "  episode_reward_mean: 0.19621440979477986\n",
      "  episode_reward_min: -0.8944010912305731\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 53466\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5764200712442398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01599261610209942\n",
      "          policy_loss: -0.09913542558997869\n",
      "          total_loss: 0.2203241738602519\n",
      "          vf_explained_var: 0.421261191368103\n",
      "          vf_loss: 0.2951708128452301\n",
      "    num_agent_steps_sampled: 14385600\n",
      "    num_agent_steps_trained: 14385600\n",
      "    num_steps_sampled: 3596400\n",
      "    num_steps_trained: 3596400\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.151677852348996\n",
      "    gpu_util_percent0: 0.18087248322147648\n",
      "    ram_util_percent: 87.57248322147653\n",
      "    vram_util_percent0: 0.5089595879958747\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.0147077895309513\n",
      "  policy_reward_mean:\n",
      "    main: 0.04905360244869497\n",
      "  policy_reward_min:\n",
      "    main: -1.7255040921032045\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30810447766537274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.203659747661135\n",
      "    mean_inference_ms: 4.4852101379816585\n",
      "    mean_raw_obs_processing_ms: 1.3362463324701748\n",
      "  time_since_restore: 52048.784529447556\n",
      "  time_this_iter_s: 115.59817910194397\n",
      "  time_total_s: 52048.784529447556\n",
      "  timers:\n",
      "    learn_throughput: 89.653\n",
      "    learn_time_ms: 89143.924\n",
      "    sample_throughput: 299.466\n",
      "    sample_time_ms: 26687.477\n",
      "    update_time_ms: 2.655\n",
      "  timestamp: 1639210293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3596400\n",
      "  training_iteration: 450\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         52048.8</td><td style=\"text-align: right;\">3596400</td><td style=\"text-align: right;\">0.196214</td><td style=\"text-align: right;\">             2.15347</td><td style=\"text-align: right;\">           -0.894401</td><td style=\"text-align: right;\">           69.6777</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14417568\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8329738103169475\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05942408352356331\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5091737150758814\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-13-31\n",
      "  done: false\n",
      "  episode_len_mean: 60.55118110236221\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0971870869571543\n",
      "  episode_reward_mean: 0.16181048477641513\n",
      "  episode_reward_min: -1.2117670325421663\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 53593\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5832677899599076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0160088555701077\n",
      "          policy_loss: -0.09858647572994232\n",
      "          total_loss: 0.2030853571668267\n",
      "          vf_explained_var: 0.48181775212287903\n",
      "          vf_loss: 0.27735838550329206\n",
      "    num_agent_steps_sampled: 14417568\n",
      "    num_agent_steps_trained: 14417568\n",
      "    num_steps_sampled: 3604392\n",
      "    num_steps_trained: 3604392\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.113245033112584\n",
      "    gpu_util_percent0: 0.18033112582781455\n",
      "    ram_util_percent: 87.58145695364239\n",
      "    vram_util_percent0: 0.509262588556274\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 2.3089292619509614\n",
      "  policy_reward_mean:\n",
      "    main: 0.04045262119410379\n",
      "  policy_reward_min:\n",
      "    main: -1.7134593416401893\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30777662805645173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.193702398082834\n",
      "    mean_inference_ms: 4.480843468179499\n",
      "    mean_raw_obs_processing_ms: 1.3359862572601053\n",
      "  time_since_restore: 52166.0413582325\n",
      "  time_this_iter_s: 117.25682878494263\n",
      "  time_total_s: 52166.0413582325\n",
      "  timers:\n",
      "    learn_throughput: 89.588\n",
      "    learn_time_ms: 89208.751\n",
      "    sample_throughput: 299.253\n",
      "    sample_time_ms: 26706.531\n",
      "    update_time_ms: 2.699\n",
      "  timestamp: 1639210411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3604392\n",
      "  training_iteration: 451\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">           52166</td><td style=\"text-align: right;\">3604392</td><td style=\"text-align: right;\"> 0.16181</td><td style=\"text-align: right;\">             2.09719</td><td style=\"text-align: right;\">            -1.21177</td><td style=\"text-align: right;\">           60.5512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14449536\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.71416835402874\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.0637802874356276\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.7007929467297574\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 55.205479452054796\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4807289498515817\n",
      "  episode_reward_mean: 0.09776655286234114\n",
      "  episode_reward_min: -1.7704307044258831\n",
      "  episodes_this_iter: 146\n",
      "  episodes_total: 53739\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5794927879571915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01588501140102744\n",
      "          policy_loss: -0.09814506816118955\n",
      "          total_loss: 0.25058252234756945\n",
      "          vf_explained_var: 0.4425744414329529\n",
      "          vf_loss: 0.3246022291183472\n",
      "    num_agent_steps_sampled: 14449536\n",
      "    num_agent_steps_trained: 14449536\n",
      "    num_steps_sampled: 3612384\n",
      "    num_steps_trained: 3612384\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.77905405405406\n",
      "    gpu_util_percent0: 0.13445945945945945\n",
      "    ram_util_percent: 87.62229729729731\n",
      "    vram_util_percent0: 0.5226470300507813\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.927363930868422\n",
      "  policy_reward_mean:\n",
      "    main: 0.024441638215585273\n",
      "  policy_reward_min:\n",
      "    main: -1.7093569473633226\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30834175474262765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.212243369118937\n",
      "    mean_inference_ms: 4.487132490880858\n",
      "    mean_raw_obs_processing_ms: 1.3368904757857327\n",
      "  time_since_restore: 52280.96515703201\n",
      "  time_this_iter_s: 114.92379879951477\n",
      "  time_total_s: 52280.96515703201\n",
      "  timers:\n",
      "    learn_throughput: 89.951\n",
      "    learn_time_ms: 88848.015\n",
      "    sample_throughput: 296.797\n",
      "    sample_time_ms: 26927.499\n",
      "    update_time_ms: 2.742\n",
      "  timestamp: 1639210526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3612384\n",
      "  training_iteration: 452\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">           52281</td><td style=\"text-align: right;\">3612384</td><td style=\"text-align: right;\">0.0977666</td><td style=\"text-align: right;\">             1.48073</td><td style=\"text-align: right;\">            -1.77043</td><td style=\"text-align: right;\">           55.2055</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14481504\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0241859037279524\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.056016255819146904\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5446875598507767\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-17-18\n",
      "  done: false\n",
      "  episode_len_mean: 70.62931034482759\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.5600240365476017\n",
      "  episode_reward_mean: 0.21777275641408228\n",
      "  episode_reward_min: -1.0935767271308308\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 53855\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5791649090051652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015645640064030887\n",
      "          policy_loss: -0.09777091166377068\n",
      "          total_loss: 0.22910566356033088\n",
      "          vf_explained_var: 0.41608211398124695\n",
      "          vf_loss: 0.3031147621870041\n",
      "    num_agent_steps_sampled: 14481504\n",
      "    num_agent_steps_trained: 14481504\n",
      "    num_steps_sampled: 3620376\n",
      "    num_steps_trained: 3620376\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.53469387755102\n",
      "    gpu_util_percent0: 0.1253061224489796\n",
      "    ram_util_percent: 87.69863945578231\n",
      "    vram_util_percent0: 0.522436220292658\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.92878592795128\n",
      "  policy_reward_mean:\n",
      "    main: 0.05444318910352057\n",
      "  policy_reward_min:\n",
      "    main: -1.9855033136659255\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3074765126044612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.193857876929034\n",
      "    mean_inference_ms: 4.480676139905584\n",
      "    mean_raw_obs_processing_ms: 1.3362905336261328\n",
      "  time_since_restore: 52393.70111274719\n",
      "  time_this_iter_s: 112.73595571517944\n",
      "  time_total_s: 52393.70111274719\n",
      "  timers:\n",
      "    learn_throughput: 90.263\n",
      "    learn_time_ms: 88541.465\n",
      "    sample_throughput: 297.218\n",
      "    sample_time_ms: 26889.377\n",
      "    update_time_ms: 2.736\n",
      "  timestamp: 1639210638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3620376\n",
      "  training_iteration: 453\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         52393.7</td><td style=\"text-align: right;\">3620376</td><td style=\"text-align: right;\">0.217773</td><td style=\"text-align: right;\">             1.56002</td><td style=\"text-align: right;\">            -1.09358</td><td style=\"text-align: right;\">           70.6293</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14513472\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.1395588280978413\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.10797454992075042\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6070691133447004\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 59.0763358778626\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.1765598723260227\n",
      "  episode_reward_mean: 0.25360639919697614\n",
      "  episode_reward_min: -0.867334020760703\n",
      "  episodes_this_iter: 131\n",
      "  episodes_total: 53986\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5788749415874481\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01629503584653139\n",
      "          policy_loss: -0.10168204578384757\n",
      "          total_loss: 0.245287228256464\n",
      "          vf_explained_var: 0.4198412299156189\n",
      "          vf_loss: 0.3222211893200874\n",
      "    num_agent_steps_sampled: 14513472\n",
      "    num_agent_steps_trained: 14513472\n",
      "    num_steps_sampled: 3628368\n",
      "    num_steps_trained: 3628368\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.61862068965518\n",
      "    gpu_util_percent0: 0.12558620689655173\n",
      "    ram_util_percent: 87.58068965517243\n",
      "    vram_util_percent0: 0.5221488466033518\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9807446782105167\n",
      "  policy_reward_mean:\n",
      "    main: 0.06340159979924402\n",
      "  policy_reward_min:\n",
      "    main: -1.6221033685501587\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3080159320388549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.201293333333393\n",
      "    mean_inference_ms: 4.482862879312003\n",
      "    mean_raw_obs_processing_ms: 1.3366080575024024\n",
      "  time_since_restore: 52505.87972021103\n",
      "  time_this_iter_s: 112.17860746383667\n",
      "  time_total_s: 52505.87972021103\n",
      "  timers:\n",
      "    learn_throughput: 90.572\n",
      "    learn_time_ms: 88238.922\n",
      "    sample_throughput: 297.631\n",
      "    sample_time_ms: 26852.024\n",
      "    update_time_ms: 2.793\n",
      "  timestamp: 1639210751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3628368\n",
      "  training_iteration: 454\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         52505.9</td><td style=\"text-align: right;\">3628368</td><td style=\"text-align: right;\">0.253606</td><td style=\"text-align: right;\">             2.17656</td><td style=\"text-align: right;\">           -0.867334</td><td style=\"text-align: right;\">           59.0763</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14545440\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.885450596043589\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.05611894697749828\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.5422725173014733\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-21-03\n",
      "  done: false\n",
      "  episode_len_mean: 62.5859375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.1374439076316056\n",
      "  episode_reward_mean: 0.14110135939562785\n",
      "  episode_reward_min: -0.9033138691651248\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 54114\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5799011744260788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01612272911146283\n",
      "          policy_loss: -0.09940006900206208\n",
      "          total_loss: 0.2156123733073473\n",
      "          vf_explained_var: 0.46250152587890625\n",
      "          vf_loss: 0.2905260490179062\n",
      "    num_agent_steps_sampled: 14545440\n",
      "    num_agent_steps_trained: 14545440\n",
      "    num_steps_sampled: 3636360\n",
      "    num_steps_trained: 3636360\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.47586206896552\n",
      "    gpu_util_percent0: 0.12682758620689655\n",
      "    ram_util_percent: 87.85310344827587\n",
      "    vram_util_percent0: 0.518711917486469\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.7966314475376732\n",
      "  policy_reward_mean:\n",
      "    main: 0.035275339848906954\n",
      "  policy_reward_min:\n",
      "    main: -1.596770512531847\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30825433991697837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.205515270293837\n",
      "    mean_inference_ms: 4.484381263801315\n",
      "    mean_raw_obs_processing_ms: 1.336805005976442\n",
      "  time_since_restore: 52618.43211197853\n",
      "  time_this_iter_s: 112.55239176750183\n",
      "  time_total_s: 52618.43211197853\n",
      "  timers:\n",
      "    learn_throughput: 90.962\n",
      "    learn_time_ms: 87860.641\n",
      "    sample_throughput: 298.459\n",
      "    sample_time_ms: 26777.573\n",
      "    update_time_ms: 2.799\n",
      "  timestamp: 1639210863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3636360\n",
      "  training_iteration: 455\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         52618.4</td><td style=\"text-align: right;\">3636360</td><td style=\"text-align: right;\">0.141101</td><td style=\"text-align: right;\">             1.13744</td><td style=\"text-align: right;\">           -0.903314</td><td style=\"text-align: right;\">           62.5859</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14577408\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.8924077901397355\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06969960823939574\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.588918401274288\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 62.30708661417323\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.8188064401993222\n",
      "  episode_reward_mean: 0.17477461405606762\n",
      "  episode_reward_min: -0.7748396505296755\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 54241\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5823779431581497\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016017034374177456\n",
      "          policy_loss: -0.0988307853974402\n",
      "          total_loss: 0.23811599639058112\n",
      "          vf_explained_var: 0.43624237179756165\n",
      "          vf_loss: 0.31262091094255445\n",
      "    num_agent_steps_sampled: 14577408\n",
      "    num_agent_steps_trained: 14577408\n",
      "    num_steps_sampled: 3644352\n",
      "    num_steps_trained: 3644352\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.724657534246575\n",
      "    gpu_util_percent0: 0.12363013698630138\n",
      "    ram_util_percent: 87.77534246575341\n",
      "    vram_util_percent0: 0.5173227494624674\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.916069058605339\n",
      "  policy_reward_mean:\n",
      "    main: 0.0436936535140169\n",
      "  policy_reward_min:\n",
      "    main: -1.6603469727028592\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30811275289330964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.201283981827572\n",
      "    mean_inference_ms: 4.482923677007833\n",
      "    mean_raw_obs_processing_ms: 1.336713154708597\n",
      "  time_since_restore: 52731.320240974426\n",
      "  time_this_iter_s: 112.88812899589539\n",
      "  time_total_s: 52731.320240974426\n",
      "  timers:\n",
      "    learn_throughput: 91.283\n",
      "    learn_time_ms: 87551.884\n",
      "    sample_throughput: 298.86\n",
      "    sample_time_ms: 26741.614\n",
      "    update_time_ms: 2.811\n",
      "  timestamp: 1639210976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3644352\n",
      "  training_iteration: 456\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         52731.3</td><td style=\"text-align: right;\">3644352</td><td style=\"text-align: right;\">0.174775</td><td style=\"text-align: right;\">             1.81881</td><td style=\"text-align: right;\">            -0.77484</td><td style=\"text-align: right;\">           62.3071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14609376\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0966103653405108\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.016415087553672016\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.9035109045051111\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 77.47115384615384\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.4222043048395707\n",
      "  episode_reward_mean: 0.09731738492504106\n",
      "  episode_reward_min: -1.3679633064458714\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 54345\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5833555316925049\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0156556484811008\n",
      "          policy_loss: -0.09726802826672792\n",
      "          total_loss: 0.20747688349708915\n",
      "          vf_explained_var: 0.4070204496383667\n",
      "          vf_loss: 0.2809678992629051\n",
      "    num_agent_steps_sampled: 14609376\n",
      "    num_agent_steps_trained: 14609376\n",
      "    num_steps_sampled: 3652344\n",
      "    num_steps_trained: 3652344\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.19793103448276\n",
      "    gpu_util_percent0: 0.12510344827586206\n",
      "    ram_util_percent: 87.89793103448274\n",
      "    vram_util_percent0: 0.5146077997526408\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.89961033863763\n",
      "  policy_reward_mean:\n",
      "    main: 0.024329346231260266\n",
      "  policy_reward_min:\n",
      "    main: -1.9090049778964064\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3076527496226241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.19186070845441\n",
      "    mean_inference_ms: 4.4798107972403125\n",
      "    mean_raw_obs_processing_ms: 1.336308528879445\n",
      "  time_since_restore: 52843.233025312424\n",
      "  time_this_iter_s: 111.91278433799744\n",
      "  time_total_s: 52843.233025312424\n",
      "  timers:\n",
      "    learn_throughput: 91.617\n",
      "    learn_time_ms: 87233.056\n",
      "    sample_throughput: 299.218\n",
      "    sample_time_ms: 26709.58\n",
      "    update_time_ms: 2.803\n",
      "  timestamp: 1639211088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3652344\n",
      "  training_iteration: 457\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         52843.2</td><td style=\"text-align: right;\">3652344</td><td style=\"text-align: right;\">0.0973174</td><td style=\"text-align: right;\">              1.4222</td><td style=\"text-align: right;\">            -1.36796</td><td style=\"text-align: right;\">           77.4712</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14641344\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.0211751196334595\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.1023593638766771\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.42541221929249745\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 61.140625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.379246228706546\n",
      "  episode_reward_mean: 0.18431538002741837\n",
      "  episode_reward_min: -1.0128734401362673\n",
      "  episodes_this_iter: 128\n",
      "  episodes_total: 54473\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5798888367414474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01612937442958355\n",
      "          policy_loss: -0.10039468651264906\n",
      "          total_loss: 0.21569412464648485\n",
      "          vf_explained_var: 0.46339017152786255\n",
      "          vf_loss: 0.2915923236012459\n",
      "    num_agent_steps_sampled: 14641344\n",
      "    num_agent_steps_trained: 14641344\n",
      "    num_steps_sampled: 3660336\n",
      "    num_steps_trained: 3660336\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.843835616438355\n",
      "    gpu_util_percent0: 0.1226027397260274\n",
      "    ram_util_percent: 87.93287671232876\n",
      "    vram_util_percent0: 0.5144818275658447\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9188670315659047\n",
      "  policy_reward_mean:\n",
      "    main: 0.0460788450068546\n",
      "  policy_reward_min:\n",
      "    main: -1.691626536222135\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30798354674178924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.20014483527627\n",
      "    mean_inference_ms: 4.483248799115458\n",
      "    mean_raw_obs_processing_ms: 1.33666286080792\n",
      "  time_since_restore: 52956.160648584366\n",
      "  time_this_iter_s: 112.92762327194214\n",
      "  time_total_s: 52956.160648584366\n",
      "  timers:\n",
      "    learn_throughput: 91.957\n",
      "    learn_time_ms: 86909.978\n",
      "    sample_throughput: 300.035\n",
      "    sample_time_ms: 26636.904\n",
      "    update_time_ms: 2.774\n",
      "  timestamp: 1639211201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3660336\n",
      "  training_iteration: 458\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         52956.2</td><td style=\"text-align: right;\">3660336</td><td style=\"text-align: right;\">0.184315</td><td style=\"text-align: right;\">             1.37925</td><td style=\"text-align: right;\">            -1.01287</td><td style=\"text-align: right;\">           61.1406</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14673312\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 0.5736015505833182\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.03947655572987133\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.4105609758120331\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 64.65322580645162\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0588646567848579\n",
      "  episode_reward_mean: 0.19378583247260742\n",
      "  episode_reward_min: -0.988709192286275\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 54597\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5816890300512314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015910386856645347\n",
      "          policy_loss: -0.09859643172472715\n",
      "          total_loss: 0.19704287166893483\n",
      "          vf_explained_var: 0.48119357228279114\n",
      "          vf_loss: 0.2714754056334496\n",
      "    num_agent_steps_sampled: 14673312\n",
      "    num_agent_steps_trained: 14673312\n",
      "    num_steps_sampled: 3668328\n",
      "    num_steps_trained: 3668328\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.21232876712329\n",
      "    gpu_util_percent0: 0.13116438356164384\n",
      "    ram_util_percent: 88.0623287671233\n",
      "    vram_util_percent0: 0.5152165681753641\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.813362226897336\n",
      "  policy_reward_mean:\n",
      "    main: 0.04844645811815184\n",
      "  policy_reward_min:\n",
      "    main: -1.7500564050302547\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.30807301295112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.202945396488925\n",
      "    mean_inference_ms: 4.484421838544902\n",
      "    mean_raw_obs_processing_ms: 1.3366846845731197\n",
      "  time_since_restore: 53069.15649104118\n",
      "  time_this_iter_s: 112.99584245681763\n",
      "  time_total_s: 53069.15649104118\n",
      "  timers:\n",
      "    learn_throughput: 92.21\n",
      "    learn_time_ms: 86672.088\n",
      "    sample_throughput: 300.578\n",
      "    sample_time_ms: 26588.793\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1639211314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3668328\n",
      "  training_iteration: 459\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         53069.2</td><td style=\"text-align: right;\">3668328</td><td style=\"text-align: right;\">0.193786</td><td style=\"text-align: right;\">             1.05886</td><td style=\"text-align: right;\">           -0.988709</td><td style=\"text-align: right;\">           64.6532</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 05:30:05,798\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Soccer_99818_00000:\n",
      "  agent_timesteps_total: 14705280\n",
      "  custom_metrics:\n",
      "    agent_0_total_ball_to_goal_speed_reward_max: 1.111516821065538\n",
      "    agent_0_total_ball_to_goal_speed_reward_mean: 0.06396839169438663\n",
      "    agent_0_total_ball_to_goal_speed_reward_min: -0.6172069569580765\n",
      "    agent_0_total_env_reward_max: 0.0\n",
      "    agent_0_total_env_reward_mean: 0.0\n",
      "    agent_0_total_env_reward_min: 0.0\n",
      "  date: 2021-12-11_05-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 65.70967741935483\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.7167837254269966\n",
      "  episode_reward_mean: 0.15595280564260766\n",
      "  episode_reward_min: -1.921030598347008\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 54721\n",
      "  experiment_id: 9678828e9628402594233c225cebca19\n",
      "  hostname: bruno-odyssey-mint\n",
      "  info:\n",
      "    learner:\n",
      "      main:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5187500000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5752486907243729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015772890005260706\n",
      "          policy_loss: -0.09875402462854982\n",
      "          total_loss: 0.22580122251808643\n",
      "          vf_explained_var: 0.4330516755580902\n",
      "          vf_loss: 0.30060016924142835\n",
      "    num_agent_steps_sampled: 14705280\n",
      "    num_agent_steps_trained: 14705280\n",
      "    num_steps_sampled: 3676320\n",
      "    num_steps_trained: 3676320\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.0.104\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.89473684210526\n",
      "    gpu_util_percent0: 0.15717105263157896\n",
      "    ram_util_percent: 88.36776315789473\n",
      "    vram_util_percent0: 0.5201384198403215\n",
      "  pid: 78457\n",
      "  policy_reward_max:\n",
      "    main: 1.9713391646957152\n",
      "  policy_reward_mean:\n",
      "    main: 0.0389882014106519\n",
      "  policy_reward_min:\n",
      "    main: -1.6534633936515934\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3087980927379322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.21563462297709\n",
      "    mean_inference_ms: 4.488684308591635\n",
      "    mean_raw_obs_processing_ms: 1.3371067754725832\n",
      "  time_since_restore: 53187.01505422592\n",
      "  time_this_iter_s: 117.85856318473816\n",
      "  time_total_s: 53187.01505422592\n",
      "  timers:\n",
      "    learn_throughput: 91.958\n",
      "    learn_time_ms: 86908.885\n",
      "    sample_throughput: 300.706\n",
      "    sample_time_ms: 26577.423\n",
      "    update_time_ms: 3.006\n",
      "  timestamp: 1639211432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3676320\n",
      "  training_iteration: 460\n",
      "  trial_id: '99818_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">           53187</td><td style=\"text-align: right;\">3676320</td><td style=\"text-align: right;\">0.155953</td><td style=\"text-align: right;\">             1.71678</td><td style=\"text-align: right;\">            -1.92103</td><td style=\"text-align: right;\">           65.7097</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 1.0/1 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Soccer_99818_00000</td><td>RUNNING </td><td>192.168.0.104:78457</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">           53187</td><td style=\"text-align: right;\">3676320</td><td style=\"text-align: right;\">0.155953</td><td style=\"text-align: right;\">             1.71678</td><td style=\"text-align: right;\">            -1.92103</td><td style=\"text-align: right;\">           65.7097</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 05:30:33,858\tERROR tune.py:545 -- Trials did not complete: [PPO_Soccer_99818_00000]\n",
      "2021-12-11 05:30:33,889\tINFO tune.py:549 -- Total run time: 53261.68 seconds (53260.09 seconds for the tuning loop).\n",
      "2021-12-11 05:30:33,890\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO_Soccer_99818_00000\n",
      "/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_99818_00000_0_2021-12-10_14-42-52/checkpoint_000350/checkpoint-350\n",
      "Done training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f30e078de20>,\n",
       " PPO_Soccer_99818_00000,\n",
       " '/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_99818_00000_0_2021-12-10_14-42-52/checkpoint_000350/checkpoint-350')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    ray.init(num_cpus=8, include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "    tune.registry.register_env(ENVIRONMENT_ID, create_custom_env)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        \"PPO\",\n",
    "        num_samples=1,\n",
    "        name=\"PPO_multiagent_rewards_1.4\",\n",
    "        config=config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=50,\n",
    "        checkpoint_at_end=True,\n",
    "        local_dir=\"../../ray_results\",\n",
    "        # restore=\"../../ray_results/PPO_selfplay_1/PPO_Soccer_ID/checkpoint_00X/checkpoint-X\",\n",
    "        # resume=True\n",
    "    )\n",
    "\n",
    "    # Gets best trial based on max accuracy across all training iterations.\n",
    "    best_trial = analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\")\n",
    "    print(best_trial)\n",
    "    # Gets best checkpoint for trial based on accuracy.\n",
    "    best_checkpoint = analysis.get_best_checkpoint(\n",
    "        trial=best_trial, metric=\"episode_reward_mean\", mode=\"max\"\n",
    "    )\n",
    "    print(best_checkpoint)\n",
    "    print(\"Done training\")\n",
    "    return analysis, best_trial, best_checkpoint\n",
    "\n",
    "run_experiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_99818_00000_0_2021-12-10_14-42-52 /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_99818_00000_0_2021-12-10_14-42-52/checkpoint_000450/checkpoint-450\n",
      "TRIALLL /home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_99818_00000_0_2021-12-10_14-42-52\n"
     ]
    }
   ],
   "source": [
    "this_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "# print('this_path', this_path)\n",
    "\n",
    "\n",
    "def export_agent(agent_file: str, TRIAL, agent_name=\"bajai_belzonte\", makeZip=False):\n",
    "    agent_path = os.path.join(f'{this_path}/agents', agent_name)\n",
    "    os.makedirs(agent_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    shutil.rmtree(agent_path)\n",
    "    os.makedirs(agent_path)\n",
    "\n",
    "    # salva a classe do agente\n",
    "    with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
    "        f.write(agent_file)\n",
    "\n",
    "    # salva um __init__ para criar o módulo Python\n",
    "    with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
    "        f.write(\"from .agent import MyRaySoccerAgent\")\n",
    "\n",
    "    # copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
    "    print(f\"TRIALLL {TRIAL}\")\n",
    "    shutil.copytree(TRIAL, os.path.join(\n",
    "        agent_path, TRIAL.split(\"ray_results/\")[1]), )\n",
    "\n",
    "    # empacota tudo num arquivo .zip\n",
    "    if makeZip:\n",
    "        shutil.make_archive(os.path.join(agent_path, agent_name),\n",
    "                            \"zip\", os.path.join(agent_path, agent_name))\n",
    "\n",
    "\n",
    "def get_agent_file_str(ALGORITHM, CHECKPOINT, POLICY_NAME=\"main\"):\n",
    "    return f\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from soccer_twos import AgentInterface\n",
    "\n",
    "ALGORITHM = \"{ALGORITHM}\"\n",
    "CHECKPOINT_PATH = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)), \n",
    "    \"{CHECKPOINT.split(\"ray_results/\")[1]}\"\n",
    ")\n",
    "POLICY_NAME = \"{POLICY_NAME}\"\n",
    "\n",
    "\n",
    "class MyRaySoccerAgent(AgentInterface):\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__()\n",
    "        ray.init(ignore_reinit_error=True)\n",
    "\n",
    "        # Load configuration from checkpoint file.\n",
    "        config_path = \"\"\n",
    "        if CHECKPOINT_PATH:\n",
    "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
    "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
    "            # Try parent directory.\n",
    "            if not os.path.exists(config_path):\n",
    "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
    "\n",
    "        # Load the config from pickled.\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                config = pickle.load(f)\n",
    "        else:\n",
    "            # If no config in given checkpoint -> Error.\n",
    "            raise ValueError(\n",
    "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
    "                \"its parent directory!\"\n",
    "            )\n",
    "\n",
    "        # no need for parallelism on evaluation\n",
    "        config[\"num_workers\"] = 0\n",
    "        config[\"num_gpus\"] = 0\n",
    "\n",
    "        # create a dummy env since it's required but we only care about the policy\n",
    "        tune.registry.register_env(\"DummyEnv\", lambda *_: BaseEnv())\n",
    "        config[\"env\"] = \"DummyEnv\"\n",
    "\n",
    "        # create the Trainer from config\n",
    "        cls = get_trainable_cls(ALGORITHM)\n",
    "        agent = cls(env=config[\"env\"], config=config)\n",
    "        # load state from checkpoint\n",
    "        agent.restore(CHECKPOINT_PATH)\n",
    "        # get policy for evaluation\n",
    "        self.policy = agent.get_policy(POLICY_NAME)\n",
    "\n",
    "    def act(self, observation: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:\n",
    "        actions = {{}}\n",
    "        for player_id in observation:\n",
    "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
    "            # as we only need the action, we discard the other elements\n",
    "            actions[player_id], *_ = self.policy.compute_single_action(\n",
    "                observation[player_id]\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def getAnalysis(experiment: str):\n",
    "    return Analysis(experiment)\n",
    "\n",
    "\n",
    "def export():\n",
    "    # PPO_Soccer_18d23_00000\n",
    "    # /home/bruno/Workspace/soccer-tows-player/src/ray_results/Testing_env/PPO_Soccer_18d23_00000_0_2021-11-24_20-34-41/checkpoint_000500/checkpoint-500\n",
    "    analysis = getAnalysis(\n",
    "        \"/home/bruno/Workspace/soccer-tows-player/src/ray_results/PPO_multiagent_rewards_1.4/PPO_Soccer_24976_00000_0_2021-12-08_15-46-24\")\n",
    "\n",
    "    ALGORITHM = \"PPO\"\n",
    "    TRIAL = analysis.get_best_logdir(\"training_iteration\", \"max\")\n",
    "    CHECKPOINT = analysis.get_best_checkpoint(\n",
    "        TRIAL,\n",
    "        \"training_iteration\",\n",
    "        \"max\",\n",
    "    )\n",
    "\n",
    "    print(TRIAL, CHECKPOINT)\n",
    "    agent_file = get_agent_file_str(ALGORITHM, CHECKPOINT)\n",
    "    export_agent(agent_file, TRIAL)\n",
    "\n",
    "\n",
    "export()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc78d2fc8ff5a3f2899ca866557bf34365716d778f0b2d6535952587b702a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('soccer-twos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
